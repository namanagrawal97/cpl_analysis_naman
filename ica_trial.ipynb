{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp39-cp39-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from scikit-learn) (1.13.0)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp39-cp39-win_amd64.whl (11.2 MB)\n",
      "   ---------------------------------------- 0.0/11.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/11.2 MB 3.6 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.5/11.2 MB 5.8 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.1/11.2 MB 9.0 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.0/11.2 MB 11.6 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.2/11.2 MB 14.7 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.9/11.2 MB 18.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.2/11.2 MB 21.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.2/11.2 MB 27.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.2/11.2 MB 38.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.2/11.2 MB 34.5 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "   ---------------------------------------- 0.0/301.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 301.8/301.8 kB 18.2 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'plotting_styles' from 'c:\\\\Users\\\\sinha\\\\Documents\\\\GitHub\\\\cpl_analysis_naman\\\\plotting_styles.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import getpass\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import functions\n",
    "import spectrogram_plotting_functions\n",
    "import plotting_styles\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "\n",
    "import scipy.stats\n",
    "import mne_connectivity\n",
    "importlib.reload(functions) #loads our custom made functions.py file\n",
    "importlib.reload(spectrogram_plotting_functions)\n",
    "importlib.reload(plotting_styles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello sinha\n",
      "['C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230529_dk1_nocontext.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230529_dk3_nocontext.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230529_dk5_nocontext.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230529_dk6_nocontext.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230531_dk1_nocontext_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230531_dk3_nocontext_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230531_dk5_nocontext_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230531_dk6_nocontext_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230609_dk1_BW_nocontext_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230609_dk3_BW_nocontext_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230610_dk1_BW_nocontext_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230610_dk3_BW_nocontext_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230615_dk5_BW_context_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230615_dk6_BW_context_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230616_dk5_BW_context_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230616_dk6_BW_context_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230623_dk1_BW_context_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230626_dk1_BW_context_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230626_dk5_BW_nocontext_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230626_dk6_BW_nocontext_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230627_dk1_BW_context_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230627_dk5_BW_nocontext_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230628_dk6_BW_nocontext_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230718_dk1_nocontext_os2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230718_dk5_nocontext_os2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230718_dk6_nocontext_os2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230719_dk1_nocontext_os2_day2_part1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230719_dk1_nocontext_os2_day2_part2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230719_dk5_nocontext_os2_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230719_dk6_nocontext_os2_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230807_dk3_BW_context_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230808_dk3_BW_context_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230808_dk5_BW_nocontext_day1_os2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230810_dk5_BW_nocontext_day2_os2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230817_dk1_BW_context_os2_day1_pt1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230817_dk1_BW_context_os2_day1_pt2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230818_dk1_BW_context_os2_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230818_dk3_BW_context_os2_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230821_dk3_BW_context_os2_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230821_dk5_BW_context_day1_os2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230822_dk1_BW_nocontext_os2_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230823_dk1_BW_nocontext_os2_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230823_dk5_BW_context_day2_os2.mat']\n"
     ]
    }
   ],
   "source": [
    "#Fetch the current user\n",
    "user= (getpass.getuser())\n",
    "print(\"Hello\", user)\n",
    "\n",
    "#Set the basepath, savepath and load the data files\n",
    "base='C:\\\\Users\\\\{}\\\\Dropbox\\\\CPLab'.format(user)\n",
    "files = glob.glob(base+'\\\\all_data_mat\\\\*.mat')\n",
    "savepath = base+'\\\\results\\\\'\n",
    "print(files)\n",
    "\n",
    "all_bands_dict = {'total':[1,100], 'beta':[12,20], 'gamma':[30,80], 'theta':[4,12]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20230615 dk6 BWcontext\n",
      "['Keyboard', 'LFP1_vHp', 'LFP2_vHp', 'LFP3_AON', 'LFP4_AON', 'Ref', 'Respirat', 'file']\n",
      "LFP3_AON\n",
      "(2695355,) (2695355,) 2000\n",
      "notch filter applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:598: UserWarning: n_components is too large: it will be set to 1\n",
      "  warnings.warn(\n",
      "c:\\Users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:598: UserWarning: n_components is too large: it will be set to 1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LFP3_AON\n",
      "(2695355,) (2695355,) 2000\n",
      "notch filter applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:598: UserWarning: n_components is too large: it will be set to 1\n",
      "  warnings.warn(\n",
      "c:\\Users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:598: UserWarning: n_components is too large: it will be set to 1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LFP3_AON\n",
      "(2695356,) (2695356,) 2000\n",
      "notch filter applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:598: UserWarning: n_components is too large: it will be set to 1\n",
      "  warnings.warn(\n",
      "c:\\Users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:598: UserWarning: n_components is too large: it will be set to 1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LFP3_AON\n",
      "(2695356,) (2695356,) 2000\n",
      "notch filter applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:598: UserWarning: n_components is too large: it will be set to 1\n",
      "  warnings.warn(\n",
      "c:\\Users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:598: UserWarning: n_components is too large: it will be set to 1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20230626 dk6 BWnocontext\n",
      "['Keyboard', 'LFP1_vHp', 'LFP2_vHp', 'LFP3_AON', 'Ref', 'Respirat', 'file']\n",
      "LFP3_AON\n",
      "(2070661,) (2070661,) 2000\n",
      "notch filter applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:598: UserWarning: n_components is too large: it will be set to 1\n",
      "  warnings.warn(\n",
      "c:\\Users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:598: UserWarning: n_components is too large: it will be set to 1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LFP3_AON\n",
      "(2070661,) (2070661,) 2000\n",
      "notch filter applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:598: UserWarning: n_components is too large: it will be set to 1\n",
      "  warnings.warn(\n",
      "c:\\Users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:598: UserWarning: n_components is too large: it will be set to 1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LFP3_AON\n",
      "(2070661,) (2070661,) 2000\n",
      "notch filter applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:598: UserWarning: n_components is too large: it will be set to 1\n",
      "  warnings.warn(\n",
      "c:\\Users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:598: UserWarning: n_components is too large: it will be set to 1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "keyboard_dict={'98':'b','119':'w','120':'nc','49':'1','48':'0'} #specifying the map of keyboard annotations to their meanings.\n",
    "all_bands={'total':[1,100],'beta':[12,30], 'gamma':[30,80], 'theta':[4,12]}\n",
    "\n",
    "files=[f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230615_dk6_BW_context_day1.mat', f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230626_dk6_BW_nocontext_day1.mat'] #This is just for testing purposes\n",
    "\n",
    "#Initializing a few empty things to store data\n",
    "events_codes_all = {}\n",
    "compiled_data_all_epochs = []\n",
    "compiled_data_list=[]\n",
    "baseline_lfp_all = []\n",
    "normalization_comparison_all = []\n",
    "for file in files: #Looping through data files\n",
    "    \n",
    "    ## Get the date, mouse_id and task from the file name\n",
    "    base_name = os.path.basename(file)\n",
    "    base_name, _ = os.path.splitext(base_name)\n",
    "    date, mouse_id, task=functions.exp_params(base_name) #Using a custom made function [see functions.py]\n",
    "    print(date, mouse_id, task)\n",
    "    if task == 'nocontextday2' or task == 'nocontextos2':\n",
    "        task = 'nocontext'\n",
    "    if task =='nocontext':\n",
    "        continue\n",
    "    f=h5py.File(file, 'r')  ## Open the data file\n",
    "    channels = list(f.keys()) ## Extract channels list from the data file\n",
    "    print(channels)\n",
    "    if not any(\"AON\" in channel or \"vHp\" in channel for channel in channels):\n",
    "        continue    \n",
    "    events,reference_electrode=functions.get_keyboard_and_ref_channels(f,channels)\n",
    "\n",
    "    events_codes=np.array(events['codes'][0]) #saving the keyboard annotations of the events (door open, door close etc.)\n",
    "    events_times=np.array(events['times'][0]) #saving when the events happened\n",
    "    events_codes_all[base_name] = events_codes #saving the codes in a dictionary to be analyzed later for events other than the ones in our keyboard_dict map\n",
    "    \n",
    "    #Generating epochs from events (epochs are basically start of a trial and end of a trial)\n",
    "    epochs=functions.generate_epochs_with_first_event(events_codes, events_times)\n",
    "\n",
    "    # task Start time\n",
    "    first_event=events_times[0]\n",
    "    #finding global start and end time of all channels, since they start and end recordings at different times\n",
    "    global_start_time, global_end_time=functions.find_global_start_end_times(f,channels)\n",
    "    \n",
    "    ## Reference electrode finding and padding\n",
    "    reference_time = np.array(reference_electrode['times']).flatten()\n",
    "    reference_value = np.array(reference_electrode['values']).flatten()\n",
    "    padd_ref_data,padded_ref_time=functions.pad_raw_data_raw_time(reference_value,reference_time,global_start_time,global_end_time,sampling_rate=2000)\n",
    "\n",
    "\n",
    "    for channeli in channels:\n",
    "        if \"AON\" in channeli or  \"vHp\" in channeli :\n",
    "\n",
    "            # Extracting raw data and time\n",
    "            data_all=f[channeli]\n",
    "            raw_data=np.array(data_all['values']).flatten()\n",
    "            raw_time = np.array(data_all['times']).flatten()\n",
    "            sampling_rate = 2000\n",
    "            print(channel_id)\n",
    "            print(raw_data.shape, raw_time.shape, sampling_rate)\n",
    "            \n",
    "            padded_data,padded_time=functions.pad_raw_data_raw_time(raw_data,raw_time,global_start_time,global_end_time,sampling_rate)\n",
    "            subtracted_data = padded_data - padd_ref_data\n",
    "            raw_data=subtracted_data\n",
    "            notch_filtered_data = functions.iir_notch(raw_data, sampling_rate, 60)\n",
    "            ica=FastICA(n_components=6, random_state=0)\n",
    "            ica.fit(notch_filtered_data.reshape(-1,1))\n",
    "            ica_data=ica.fit_transform(notch_filtered_data.reshape(-1,1))\n",
    "            #notch_filtered_data=ica_data.flatten()\n",
    "            \n",
    "            \n",
    "#             data_before, time, baseline_mean, baseline_std=functions.baseline_data_normalization(notch_filtered_data, raw_time, first_event, sampling_rate)\n",
    "#             first_event_index=np.where(raw_time>first_event)[0][0]\n",
    "\n",
    "#             baseline_row=[mouse_id,task,channel_id,np.array(data_before)]\n",
    "#             baseline_lfp_all.append(baseline_row)\n",
    "#             normalized_data=notch_filtered_data\n",
    "\n",
    "#             #Saving non-normalized data and normalized data for plotting\n",
    "#             normalization_row=[mouse_id,task,channel_id,[notch_filtered_data[first_event_index:first_event_index+30*sampling_rate]],np.mean(data_before),np.std(data_before),[normalized_data[first_event_index:first_event_index+30*sampling_rate]]]\n",
    "#             normalization_comparison_all.append(normalization_row)\n",
    "\n",
    "\n",
    "#             for i,epochi in enumerate(epochs):\n",
    "                \n",
    "#                 compiled_data = pd.DataFrame() # Initializing a dataframe to store the data of a single epoch\n",
    "\n",
    "#                 door_timestamp = epochi[0][0]\n",
    "#                 trial_type = epochi[0][1]\n",
    "#                 dig_type = epochi[1, 1]\n",
    "#                 dig_timestamp = epochi[1, 0]\n",
    "#                 print(door_timestamp,trial_type,dig_timestamp,dig_type)\n",
    "                \n",
    "                \n",
    "#                 data_complete_trial=functions.extract_complete_trial_data(notch_filtered_data,time,door_timestamp,dig_timestamp,sampling_rate)\n",
    "#                 data_trial_before, data_trial_after=functions.extract_door_data(notch_filtered_data,time,door_timestamp,sampling_rate)\n",
    "#                 data_dig_before, data_dig_after=functions.extract_dig_data(notch_filtered_data,time,dig_timestamp,sampling_rate)\n",
    "#                 epoch_data = [data_complete_trial, data_trial_before, data_trial_after, data_dig_before, data_dig_after]\n",
    "#                 epoch_data = [functions.zscore_event_data(x, baseline_mean, baseline_std) for x in epoch_data]\n",
    "\n",
    "#                 compiled_data = {\n",
    "#                     'rat': mouse_id,\n",
    "#                     'date': date,\n",
    "#                     'task': task,\n",
    "#                     'channel': channel_id,\n",
    "#                     'trial': i,\n",
    "#                     'timestamps': [door_timestamp, dig_timestamp],\n",
    "#                     'side': keyboard_dict[str(int(trial_type))],\n",
    "#                     'correct?': keyboard_dict[str(int(dig_type))],\n",
    "#                     'first 30 seconds power': functions.calculate_power_1D(data_before),\n",
    "#                     'time': time,\n",
    "#                     'complete_trial': epoch_data[0],\n",
    "#                     'pre_door': epoch_data[1],\n",
    "#                     'post_door': epoch_data[2],\n",
    "#                     'pre_dig': epoch_data[3],\n",
    "#                     'post_dig': epoch_data[4]\n",
    "#                 }\n",
    "\n",
    "#                 compiled_data_list.append(compiled_data)\n",
    "\n",
    "# compiled_data_all_epochs.extend(compiled_data_list)\n",
    "# compiled_data_all_epochs = pd.DataFrame(compiled_data_all_epochs)\n",
    "# compiled_data_all_epochs= compiled_data_all_epochs[compiled_data_all_epochs['task']!='nocontext']\n",
    "# baseline_lfp_all = pd.DataFrame(baseline_lfp_all, columns=['rat', 'task', 'channel', 'data'])\n",
    "# normalization_comparison_all = pd.DataFrame(normalization_comparison_all, columns=['rat', 'task', 'channel', 'non_normalized_data', 'baseline_mean', 'baseline_std', 'normalized_data'])   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lfp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
