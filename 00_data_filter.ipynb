{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import getpass\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Thomas\n",
      "base =  C:\\Users\\Thomas\\Dropbox\\CPLab\n",
      "filtered files will be stored at C:\\Users\\Thomas\\Dropbox\\CPLab\\all_data_filtered_mat\\\n",
      "plots and stuff will be at C:\\Users\\Thomas\\Dropbox\\CPLab\\results\\\n",
      "['C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230529_dk1_nocontext.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230529_dk3_nocontext.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230529_dk5_nocontext.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230529_dk6_nocontext.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230531_dk1_nocontext_day2.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230531_dk3_nocontext_day2.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230531_dk5_nocontext_day2.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230531_dk6_nocontext_day2.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230609_dk1_BW_nocontext_day1.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230609_dk3_BW_nocontext_day1.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230610_dk1_BW_nocontext_day2.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230610_dk3_BW_nocontext_day2.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230615_dk5_BW_context_day1.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230615_dk6_BW_context_day1.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230616_dk5_BW_context_day2.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230616_dk6_BW_context_day2.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230623_dk1_BW_context_day1.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230626_dk1_BW_context_day1.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230626_dk5_BW_nocontext_day1.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230626_dk6_BW_nocontext_day1.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230627_dk1_BW_context_day2.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230627_dk5_BW_nocontext_day2.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230628_dk6_BW_nocontext_day2.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230718_dk1_nocontext_os2.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230718_dk5_nocontext_os2.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230718_dk6_nocontext_os2.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230719_dk1_nocontext_os2_day2_part1.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230719_dk1_nocontext_os2_day2_part2.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230719_dk5_nocontext_os2_day2.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230719_dk6_nocontext_os2_day2.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230807_dk3_BW_context_day1.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230808_dk3_BW_context_day2.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230808_dk5_BW_nocontext_day1_os2.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230810_dk5_BW_nocontext_day2_os2.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230817_dk1_BW_context_os2_day1_pt1.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230817_dk1_BW_context_os2_day1_pt2.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230818_dk1_BW_context_os2_day2.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230818_dk3_BW_context_os2_day1.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230821_dk3_BW_context_os2_day2.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230821_dk5_BW_context_day1_os2.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230822_dk1_BW_nocontext_os2_day1.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230823_dk1_BW_nocontext_os2_day2.mat', 'C:\\\\\\\\Users\\\\\\\\Thomas\\\\\\\\Dropbox\\\\\\\\CPLab\\\\\\\\all_data_mat\\\\20230823_dk5_BW_context_day2_os2.mat']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the current user\n",
    "user = getpass.getuser()\n",
    "print(\"Hello\", user)\n",
    "\n",
    "# Define base directory paths\n",
    "base = 'C:\\\\Users\\\\{}\\\\Dropbox\\\\CPLab'.format(user)\n",
    "print('base = ', base)\n",
    "\n",
    "# Define paths for saving filtered files and results\n",
    "files_savepath = base + '\\\\all_data_filtered_mat\\\\'\n",
    "print('filtered files will be stored at', files_savepath)\n",
    "\n",
    "savepath = base + '\\\\results\\\\'\n",
    "print('plots and stuff will be at', savepath)\n",
    "\n",
    "files = glob.glob(r'C:\\\\Users\\\\{}\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\*.mat'.format(user))\n",
    "print(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Load and clean filter data from an Excel file\"\"\"\n",
    "\n",
    "filter_data = pd.read_excel(\"C:\\\\Users\\\\{}\\\\Dropbox\\\\CPLab\\\\filecheck.xlsx\".format(user)) #Loading up the data that contains the manual annotation from Christiane\n",
    "filter_data = filter_data.dropna(axis=1, how='all') #Removes the columns where all the values are NA\n",
    "filter_data = filter_data.dropna(axis=0, how='all') # Removes the rows where all the values are NA\n",
    "filter_data = filter_data.drop(filter_data.columns[-1], axis=1) #Drop the last column that just contains comments\n",
    "filter_data.reset_index(drop=True, inplace=True) # Reset the index becuase of the dropped rows\n",
    "filter_data.columns = ['file', 'LFP1_AON', 'LFP2_AON', 'LFP3_AON', 'LFP4_AON', 'LFP1_vHp', 'LFP2_vHp', 'date'] #Manually set the column names\n",
    "\n",
    "\"Extract file names without extensions from filter data\"\n",
    "\n",
    "filter_data_files = list(filter_data['file']) #take the 'file' column from the dataframe\n",
    "filter_base_names_without_ext = [os.path.splitext(os.path.basename(file))[0] for file in filter_data_files] #extract the file names, removes the paths from the filenames and stores it as a list\n",
    "print(filter_base_names_without_ext)\n",
    "\n",
    "# Update filter data with base names without extensions\n",
    "filter_data['file'] = filter_base_names_without_ext\n",
    "\n",
    "# Extract annotations from filter data\n",
    "filter_data_annotations = filter_data.iloc[:, 1:6] #Extracting annotations \"no\" and \"ok\"\n",
    "filter_data_annotations = np.array(filter_data_annotations) #saving it as an array\n",
    "print(np.unique(filter_data_annotations)) #printing out the uniques annotations to make sure that annotations are \"no\" and \"ok\" only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find all .mat files in the specified directory\n",
    "files = glob.glob(r'C:\\\\Users\\\\{}\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\*.mat'.format(user))\n",
    "print(files)\n",
    "\n",
    "# Print the number of files and the shape of the filter data\n",
    "print(len(files))\n",
    "print(filter_data.shape)\n",
    "\n",
    "# Extract base names without extensions from the list of files\n",
    "files_list = []\n",
    "for file in files:\n",
    "    base_name = os.path.basename(file)\n",
    "    base_name_without_ext = os.path.splitext(base_name)[0]\n",
    "    files_list.append(base_name_without_ext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find uncommon elements between the list of files and filter data\n",
    "set1 = set(files_list)\n",
    "set2 = set(filter_data['file'])\n",
    "uncommon_elements = set1.symmetric_difference(set2)\n",
    "uncommon_elements_list = list(uncommon_elements)\n",
    "print(uncommon_elements_list)\n",
    "\n",
    "# These two files don't exist and are a typo in the filecheck.xlsx file\n",
    "# ['20230609_dk1_BW_nocontext_day2', '20230822_dk1_BW_nocontext_os2_day2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Here we will drop the data file columns that are \"NO\" in the manual annotation file(filecheck.xlsx)\"\"\"\n",
    "\n",
    "# Define channel names\n",
    "channel_names = ['LFP1_AON', 'LFP2_AON', 'LFP3_AON', 'LFP4_AON', 'LFP1_vHp', 'LFP2_vHp']\n",
    "\n",
    "# Initialize a list to store files that are not found\n",
    "not_here_list = []\n",
    "\n",
    "# Iterate over each file and delete specified columns if necessary\n",
    "for file in files: #Loading the data files\n",
    "    f = h5py.File(file, 'r+') #reading the file with the option of writing to it by using \"r+\" \n",
    "    try:\n",
    "        base_name = os.path.basename(file)\n",
    "        base_name_without_ext = os.path.splitext(base_name)[0]\n",
    "        if base_name_without_ext in filter_base_names_without_ext: #checking if the data file exists in filecheck.xlsx \n",
    "            print(base_name_without_ext, 'yes')\n",
    "            row = filter_data.loc[filter_data['file'] == base_name_without_ext] #locates the row of the file in filecheck.xlsx\n",
    "            for column in row.columns: #Iterating through the manual annotations\n",
    "                value = row[column].values[0]\n",
    "                if value == 'no' or value == 'NO': #If the annotation of a channel is \"NO\"\n",
    "                    print(f\"Column '{column}' has value 'no'\")\n",
    "                    if column in f.keys():\n",
    "                        del f[column] # Deletes that channel in the data file\n",
    "                        print(f\"Deleted key '{column}' from file '{file}'\")\n",
    "                elif value == 'ok' or value == 'OK': # If the annotation of a channel is  \"OK\"\n",
    "                    print(f\"Column '{column}' has value 'ok'\") #Do nothing\n",
    "        else:\n",
    "            print(base_name_without_ext, 'this file not here') #if the data file does not exist in filecheck.xlsx\n",
    "            not_here_list.append(base_name_without_ext) \n",
    "    finally:\n",
    "        f.close() #Close the data file. This saves the manipulations we have made in the above code.\n",
    "\n",
    "print(not_here_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Check the channels in each file\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfiles\u001b[49m:\n\u001b[0;32m      3\u001b[0m     f \u001b[38;5;241m=\u001b[39m h5py\u001b[38;5;241m.\u001b[39mFile(file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m     channels \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mkeys()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'files' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check the channels in each file\n",
    "for file in files:\n",
    "    f = h5py.File(file, 'r')\n",
    "    channels = f.keys()\n",
    "    print(file, channels)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================== End of Main Code =============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The below code is to check for Keyboard annotations in data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C', '€']\n",
      "Hello world this is a test\n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "N o   c o m m e n t\n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 77 101 109 111 114 121]\n",
      "Memory\n",
      "N o   c o m m e n t\n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 77 101 109 111 114 121]\n",
      "Memory\n",
      "N o   c o m m e n t\n",
      "[ 77 101 109 111 114 121]\n",
      "Memory\n",
      "N o   c o m m e n t\n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "N o   c o m m e n t\n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 77 101 109 111 114 121]\n",
      "Memory\n",
      "N o   c o m m e n t\n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "N o   c o m m e n t\n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "N o   c o m m e n t\n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "N o   c o m m e n t\n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "N o   c o m m e n t\n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "N o   c o m m e n t\n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "N o   c o m m e n t\n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "N o   c o m m e n t\n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[ 75 101 121  98 111  97 114 100]\n",
      "Keyboard\n",
      "t o   m a r k   d o o r   o p e n i n g  \n",
      "[  0   1  13  32  43  48  49  50  52  92  96  98 103 119 120]\n",
      "[ 32  78  97  99 100 101 103 105 107 109 110 111 112 114 116]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define a function to convert decimal numbers to Unicode characters\n",
    "def decimal_to_unicode(decimal_list):\n",
    "    \"\"\"\n",
    "    Convert a list of decimal numbers to their corresponding Unicode characters.\n",
    "    \n",
    "    Parameters:\n",
    "    decimal_list (list): List of decimal numbers.\n",
    "    \n",
    "    Returns:\n",
    "    list: List of Unicode characters.\n",
    "    \"\"\"\n",
    "    return [chr(decimal) for decimal in decimal_list]\n",
    "\n",
    "# Example usage of decimal_to_unicode function\n",
    "decimal_list = [65, 66, 67, 8364]  # A, B, C, €\n",
    "unicode_characters = decimal_to_unicode(decimal_list)\n",
    "print(unicode_characters)\n",
    "\n",
    "# Define a function to combine strings with an optional separator\n",
    "def combine_strings(string_list, separator=''):\n",
    "    \"\"\"\n",
    "    Combine strings in a list into a single string with an optional separator.\n",
    "    \n",
    "    Parameters:\n",
    "    string_list (list): List of strings to combine.\n",
    "    separator (str): Separator to use between strings. Default is an empty string.\n",
    "    \n",
    "    Returns:\n",
    "    str: Combined string.\n",
    "    \"\"\"\n",
    "    return separator.join(string_list)\n",
    "\n",
    "# Example usage of combine_strings function\n",
    "string_list = ['Hello', 'world', 'this', 'is', 'a', 'test']\n",
    "combined_string = combine_strings(string_list, ' ')\n",
    "print(combined_string)\n",
    "\n",
    "# Initialize lists to store codes and comments from all files\n",
    "codes_all = []\n",
    "comments_all = []\n",
    "\n",
    "# Iterate over each file and extract codes and comments\n",
    "for file in files:\n",
    "    f = h5py.File(file, 'r')\n",
    "    channels = list(f.keys())\n",
    "\n",
    "    # Check for the presence of specific channels and extract events\n",
    "    if 'Keyboard' in channels:\n",
    "        events = f['Keyboard']\n",
    "    elif 'keyboard' in channels:\n",
    "        events = f['keyboard']\n",
    "    elif 'memory' in channels:\n",
    "        events = f['memory']\n",
    "    elif 'Memory' in channels:\n",
    "        events = f['Memory']\n",
    "\n",
    "    # Print the title of the events\n",
    "    print(events['title'][:, 0])\n",
    "    print(combine_strings(decimal_to_unicode(events['title'][:, 0]), ''))\n",
    "\n",
    "    # Append codes and comments to the respective lists\n",
    "    codes_all.append(np.array(events['codes'][0]))\n",
    "    comments_all.append(np.array(events['comment']))\n",
    "\n",
    "    # Convert comments to Unicode and combine them into a single string\n",
    "    please = decimal_to_unicode(events['comment'][:, 0])\n",
    "    combined_string = combine_strings(please, ' ')\n",
    "    print(combined_string)\n",
    "\n",
    "# Concatenate all codes and comments\n",
    "codes_all = np.concatenate(codes_all)\n",
    "comments_all = np.concatenate(comments_all)\n",
    "\n",
    "# Print unique codes and comments\n",
    "codes_all_unique = np.unique(codes_all)\n",
    "comments_all_unique = np.unique(comments_all)\n",
    "print(codes_all_unique)\n",
    "print(comments_all_unique)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lfp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
