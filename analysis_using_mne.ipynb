{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62b9d38b",
   "metadata": {},
   "source": [
    "## Loading packages and savepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6026b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import getpass\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import functions\n",
    "import lfp_pre_processing_functions\n",
    "import power_functions\n",
    "import coherence_functions\n",
    "import spectrogram_plotting_functions\n",
    "import plotting_styles\n",
    "import scipy.stats\n",
    "import mne_connectivity\n",
    "importlib.reload(functions) #loads our custom made functions.py file\n",
    "importlib.reload(spectrogram_plotting_functions)\n",
    "importlib.reload(plotting_styles)\n",
    "\n",
    "linestyle = plotting_styles.linestyles\n",
    "colors = plotting_styles.colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d1c359a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mne-qt-browser in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (0.6.3)\n",
      "Requirement already satisfied: darkdetect in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from mne-qt-browser) (0.8.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from mne-qt-browser) (3.8.4)\n",
      "Requirement already satisfied: mne>=1.0 in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from mne-qt-browser) (1.8.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from mne-qt-browser) (1.26.4)\n",
      "Requirement already satisfied: pyqtgraph>=0.12.3 in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from mne-qt-browser) (0.13.7)\n",
      "Requirement already satisfied: qdarkstyle in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from mne-qt-browser) (3.2.3)\n",
      "Requirement already satisfied: qtpy in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from mne-qt-browser) (2.4.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from mne-qt-browser) (1.13.0)\n",
      "Requirement already satisfied: scooby in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from mne-qt-browser) (0.10.1)\n",
      "Requirement already satisfied: decorator in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from mne>=1.0->mne-qt-browser) (5.1.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from mne>=1.0->mne-qt-browser) (3.1.4)\n",
      "Requirement already satisfied: lazy-loader>=0.3 in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from mne>=1.0->mne-qt-browser) (0.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from mne>=1.0->mne-qt-browser) (23.2)\n",
      "Requirement already satisfied: pooch>=1.5 in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from mne>=1.0->mne-qt-browser) (1.8.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from mne>=1.0->mne-qt-browser) (4.66.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from matplotlib->mne-qt-browser) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from matplotlib->mne-qt-browser) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from matplotlib->mne-qt-browser) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from matplotlib->mne-qt-browser) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from matplotlib->mne-qt-browser) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from matplotlib->mne-qt-browser) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from matplotlib->mne-qt-browser) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from matplotlib->mne-qt-browser) (6.1.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->mne-qt-browser) (3.17.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from pooch>=1.5->mne>=1.0->mne-qt-browser) (3.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from pooch>=1.5->mne>=1.0->mne-qt-browser) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mne-qt-browser) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from jinja2->mne>=1.0->mne-qt-browser) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from tqdm->mne>=1.0->mne-qt-browser) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.0->mne-qt-browser) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.0->mne-qt-browser) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.0->mne-qt-browser) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.0->mne-qt-browser) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "{'MNE_BROWSER_BACKEND': 'qt', 'MNE_BROWSE_RAW_SIZE': '20.0,12.354166666666666'}\n",
      "Using qt as 2D backend.\n"
     ]
    }
   ],
   "source": [
    "%pip install mne-qt-browser\n",
    "%matplotlib qt\n",
    "import mne\n",
    "print(mne.get_config())  # same as mne.get_config(key=None)\n",
    "mne.set_config('MNE_BROWSER_BACKEND', 'qt')  # set the backend to matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "776d7ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello sinha\n",
      "Base path: C:\\Users\\sinha\\Dropbox\\CPLab\n",
      "Save path: C:\\Users\\sinha\\Dropbox\\CPLab\\results\\\n",
      "['C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230529_dk1_nocontext.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230529_dk3_nocontext.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230529_dk5_nocontext.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230529_dk6_nocontext.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230531_dk1_nocontext_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230531_dk3_nocontext_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230531_dk5_nocontext_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230531_dk6_nocontext_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230609_dk1_BW_nocontext_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230609_dk3_BW_nocontext_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230610_dk1_BW_nocontext_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230610_dk3_BW_nocontext_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230615_dk5_BW_context_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230615_dk6_BW_context_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230616_dk5_BW_context_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230616_dk6_BW_context_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230623_dk1_BW_context_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230626_dk1_BW_context_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230626_dk5_BW_nocontext_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230626_dk6_BW_nocontext_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230627_dk1_BW_context_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230627_dk5_BW_nocontext_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230628_dk6_BW_nocontext_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230718_dk1_nocontext_os2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230718_dk5_nocontext_os2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230718_dk6_nocontext_os2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230719_dk1_nocontext_os2_day2_part1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230719_dk1_nocontext_os2_day2_part2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230719_dk5_nocontext_os2_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230719_dk6_nocontext_os2_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230807_dk3_BW_context_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230808_dk3_BW_context_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230808_dk5_BW_nocontext_day1_os2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230810_dk5_BW_nocontext_day2_os2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230817_dk1_BW_context_os2_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230818_dk1_BW_context_os2_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230818_dk3_BW_context_os2_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230821_dk3_BW_context_os2_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230821_dk5_BW_context_day1_os2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230822_dk1_BW_nocontext_os2_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230823_dk1_BW_nocontext_os2_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230823_dk5_BW_context_day2_os2.mat']\n"
     ]
    }
   ],
   "source": [
    "#Fetch the current user\n",
    "user= (getpass.getuser())\n",
    "print(\"Hello\", user)\n",
    "\n",
    "if user == 'CPLab':\n",
    "    base='D:\\\\Dropbox\\\\CPLab'\n",
    "else:\n",
    "    base='C:\\\\Users\\\\{}\\\\Dropbox\\\\CPLab'.format(user)\n",
    "#Set the basepath, savepath and load the data files\n",
    "files = glob.glob(base+'\\\\all_data_mat_250825\\\\*.mat')\n",
    "savepath = base+'\\\\results\\\\'\n",
    "print(\"Base path:\", base)\n",
    "print(\"Save path:\", savepath)\n",
    "print(files)\n",
    "\n",
    "\n",
    "all_bands_dict = {'total':[1,100], 'beta':[12,30], 'gamma':[30,80], 'theta':[4,12]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24f2f3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 1\n",
    "fs = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f222371",
   "metadata": {},
   "source": [
    "## Doing annotations of bad data segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c01163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import mne_connectivity\n",
    "import sys\n",
    "importlib.reload(lfp_pre_processing_functions)\n",
    "#files=[f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230616_dk6_BW_context_day2.mat']\n",
    "event_data_df=[]\n",
    "con_data_df=[]\n",
    "\n",
    "con_data_df_shuffled=[]\n",
    "shuffled_event_data_df=[]\n",
    "events_codes_all = {}\n",
    "random_baseline_data=[]\n",
    "baseline_lfp_all=[]\n",
    "\n",
    "files_short=[files[10]] ### TEST CHANGE THIS \n",
    "\n",
    "\n",
    "for file_num,file in enumerate(files):\n",
    "    #if 'dk1' in file:\n",
    "        \n",
    "        #print(file)\n",
    "        base_name = os.path.basename(file)\n",
    "        base_name, _ = os.path.splitext(base_name)\n",
    "\n",
    "        date, rat_id, task = lfp_pre_processing_functions.exp_params(base_name)\n",
    "        print(date, rat_id, task)\n",
    "        if task == 'nocontextday2' or task == 'nocontextos2':\n",
    "            task = 'nocontext'\n",
    "        if task =='nocontext':\n",
    "            continue\n",
    "        # if rat_id=='dk1': #REMOVING DK1 TEMPORARLILY . PLEASE CHANGE LATER\n",
    "        #     continue\n",
    "        f = h5py.File(file, 'r')\n",
    "        channels = list(f.keys())\n",
    "        #print(channels)\n",
    "         \n",
    "        if not any(\"AON\" in channel or \"vHp\" in channel for channel in channels):\n",
    "            print(\"No AON or vHp channels in this file\")\n",
    "            continue\n",
    "\n",
    "        events,reference_electrode=lfp_pre_processing_functions.get_keyboard_and_ref_channels(f,channels)\n",
    "\n",
    "    #finding global start and end time of all channels, since they start and end recordings at different times\n",
    "        global_start_time, global_end_time=lfp_pre_processing_functions.find_global_start_end_times(f,channels)\n",
    "        \n",
    "        ## Reference electrode finding and padding\n",
    "        reference_time = np.array(reference_electrode['times']).flatten()\n",
    "        reference_value = np.array(reference_electrode['values']).flatten()\n",
    "        padd_ref_data,padded_ref_time=lfp_pre_processing_functions.pad_raw_data_raw_time(reference_value,reference_time,global_start_time,global_end_time,sampling_rate=2000)\n",
    "\n",
    "        events_codes = np.array(events['codes'][0])\n",
    "        events_times = np.array(events['times'][0])\n",
    "        events_codes_all[base_name] = events_codes\n",
    "        epochs = lfp_pre_processing_functions.generate_epochs_with_first_event(events_codes, events_times)\n",
    "        #epochs = functions.generate_specific_num_of_epochs_with_first_event(events_codes, events_times,5)\n",
    "        aon_lfp_channels=[x for x in channels if 'AON' in x ]\n",
    "        vHp_lfp_channels=[x for x in channels if 'vHp' in x ]\n",
    "        ref_lfp_channels=[x for x in channels if \"Ref\" in x or 'REF' in x or 'ref' in x]\n",
    "        all_channels=np.concatenate((aon_lfp_channels,vHp_lfp_channels, ref_lfp_channels))\n",
    "        #print(all_channels)\n",
    "        \n",
    "        mne_baseline_data=np.zeros((1,len(all_channels),4000))\n",
    "        mne_epoch_door_before=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_door_after=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_dig_before=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_dig_after=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_around_door=np.zeros((len(epochs),len(all_channels),int(time_window*fs)*2))\n",
    "        mne_epoch_around_dig=np.zeros((len(epochs),len(all_channels),int(time_window*fs)*2))\n",
    "        \n",
    "        mne_baseline_data_shuffled=np.zeros((1,len(all_channels),4000))\n",
    "        mne_epoch_door_before_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_door_after_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_dig_before_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_dig_after_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_around_door_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs*2)))\n",
    "        mne_epoch_around_dig_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs*2)))\n",
    "\n",
    "        print(f'File {rat_id} {task} {date} has {len(epochs)} epochs and {len(all_channels)} channels')\n",
    "\n",
    "\n",
    "        first_event = events_times[0]\n",
    "        \n",
    "        for channel_num,channeli in enumerate(all_channels):\n",
    "            if \"AON\" in channeli or \"vHp\" in channeli or \"Ref\" in channeli:\n",
    "                channel_id = channeli\n",
    "                data_all = f[channeli]\n",
    "                raw_data = np.array(data_all['values']).flatten()\n",
    "                raw_time = np.array(data_all['times']).flatten()\n",
    "                sampling_rate = int(1 / data_all['interval'][0][0])\n",
    "                #print(raw_data.shape, raw_time.shape, sampling_rate)\n",
    "                \n",
    "                padded_data,padded_time=lfp_pre_processing_functions.pad_raw_data_raw_time(raw_data,raw_time,global_start_time,global_end_time,sampling_rate)\n",
    "\n",
    "                # padded_data = padded_data/8000 #Accounting for the 8000Amplifier\n",
    "                # padd_ref_data = padd_ref_data/8000\n",
    "                if \"Ref\" in channeli:\n",
    "                    subtracted_data = padded_data\n",
    "                else:                \n",
    "                    subtracted_data = padded_data - padd_ref_data # Subtracting reference channel\n",
    "\n",
    "                notch_data = lfp_pre_processing_functions.iir_notch(subtracted_data, sampling_rate, 60) ###CHANGE notch_data to notch_filtered_data\n",
    "\n",
    "                #print(notch_data.nbytes)\n",
    "                #notch_data_detrended = scipy.signal.detrend(notch_data)\n",
    "                #notch_filtered_data=lfp_pre_processing_functions.sosbandpass(notch_data_detrended, fs=2000, start_freq=1,end_freq=100, order=8) ###CHANGE THIS FOR NOT BANDBASS FILTERTING\n",
    "                #print(notch_filtered_data.nbytes)\n",
    "                \n",
    "                data_before, time, baseline_mean, baseline_std=lfp_pre_processing_functions.baseline_data_normalization(notch_data, raw_time, first_event, sampling_rate)\n",
    "                first_event_index=np.where(raw_time>first_event)[0][0]\n",
    "\n",
    "                mne_baseline_data[0,channel_num,:]=list(data_before)\n",
    "                mne_baseline_data_shuffled[0,channel_num,:]=list(np.random.permutation(data_before))\n",
    "                total = notch_data\n",
    "\n",
    "                \n",
    "                for i, epochi in enumerate(epochs):\n",
    "                    door_timestamp = epochi[0][0]\n",
    "                    trial_type = epochi[0][1]\n",
    "                    dig_type = epochi[1, 1]\n",
    "                    #print(dig_type)\n",
    "                    dig_timestamp = epochi[1, 0]\n",
    "                    #print(door_timestamp, trial_type, dig_timestamp, dig_type)\n",
    "                    data_trial_before, data_trial_after=lfp_pre_processing_functions.extract_event_data(notch_data,time,door_timestamp,sampling_rate,truncation_time=time_window)\n",
    "                    data_dig_before, data_dig_after=lfp_pre_processing_functions.extract_event_data(notch_data,time,dig_timestamp,sampling_rate,truncation_time=time_window)\n",
    "                    data_around_door=np.concatenate((data_trial_before, data_trial_after))\n",
    "                    data_around_dig=np.concatenate((data_dig_before, data_dig_after))\n",
    "\n",
    "                    epoch_data = [data_trial_before, data_trial_after, data_dig_before, data_dig_after, data_around_door, data_around_dig]\n",
    "                    event_data_list = [lfp_pre_processing_functions.zscore_event_data(x, baseline_std) for x in epoch_data]\n",
    "\n",
    "                    #event_data_list = [x for x in epoch_data]\n",
    "\n",
    "                    mne_epoch_door_before[i,channel_num,:]=list(event_data_list[0][-int(time_window*fs):])\n",
    "                    mne_epoch_door_after[i,channel_num,:]=list(event_data_list[1][:int(time_window*fs)])\n",
    "                    mne_epoch_dig_before[i,channel_num,:]=list(event_data_list[2][-int(time_window*fs):])\n",
    "                    mne_epoch_dig_after[i,channel_num,:]=list(event_data_list[3][:int(time_window*fs)])\n",
    "                    mid_point = int(len(event_data_list[4]) / 2)\n",
    "                    mne_epoch_around_door[i,channel_num,:]=list(event_data_list[4][mid_point-int(time_window*fs):mid_point+int(time_window*fs)])\n",
    "                    mne_epoch_around_dig[i,channel_num,:]=list(event_data_list[5][mid_point-int(time_window*fs):mid_point+int(time_window*fs)])\n",
    "\n",
    "                    mne_epoch_door_before_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[0][-int(time_window*fs):]))\n",
    "                    mne_epoch_door_after_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[1][:int(time_window*fs)]))\n",
    "                    mne_epoch_dig_before_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[2][-int(time_window*fs):]))\n",
    "                    mne_epoch_dig_after_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[3][:int(time_window*fs)]))\n",
    "                    mne_epoch_around_door_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[4][mid_point-int(time_window*fs):mid_point+int(time_window*fs)]))\n",
    "                    mne_epoch_around_dig_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[5][mid_point-int(time_window*fs):mid_point+int(time_window*fs)]))\n",
    "\n",
    "        if len(all_channels)>0:\n",
    "            fs=2000\n",
    "            freqs = np.arange(1,100)\n",
    "            n_cycles = freqs/3\n",
    "            info = mne.create_info(ch_names=list(all_channels), sfreq=fs, ch_types='eeg')\n",
    "            mne_baseline = mne.EpochsArray(mne_baseline_data, info)\n",
    "            mne_epoch_door_before = mne.EpochsArray(mne_epoch_door_before, info)\n",
    "            mne_epoch_door_after= mne.EpochsArray(mne_epoch_door_after, info)\n",
    "            mne_epoch_dig_before = mne.EpochsArray(mne_epoch_dig_before, info)\n",
    "            mne_epoch_dig_after = mne.EpochsArray(mne_epoch_dig_after, info)\n",
    "            mne_epoch_around_door = mne.EpochsArray(mne_epoch_around_door, info)\n",
    "            mne_epoch_around_dig = mne.EpochsArray(mne_epoch_around_dig, info)\n",
    "            \n",
    "            row_list=[file_num,date,rat_id,task,mne_baseline,mne_epoch_door_before,mne_epoch_door_after,mne_epoch_dig_before,mne_epoch_dig_after,mne_epoch_around_door,mne_epoch_around_dig]\n",
    "            \n",
    "            mne_baseline_shuffled = mne.EpochsArray(mne_baseline_data_shuffled, info)\n",
    "            mne_epoch_door_before_shuffled = mne.EpochsArray(mne_epoch_door_before_shuffled, info)\n",
    "            mne_epoch_door_after_shuffled = mne.EpochsArray(mne_epoch_door_after_shuffled, info)\n",
    "            mne_epoch_dig_before_shuffled = mne.EpochsArray(mne_epoch_dig_before_shuffled, info)\n",
    "            mne_epoch_dig_after_shuffled = mne.EpochsArray(mne_epoch_dig_after_shuffled, info)\n",
    "            mne_epoch_around_door_shuffled = mne.EpochsArray(mne_epoch_around_door_shuffled, info)\n",
    "            mne_epoch_around_dig_shuffled = mne.EpochsArray(mne_epoch_around_dig_shuffled, info)\n",
    "            row_list_shuffled=[file_num,date,rat_id,task,mne_baseline_shuffled,mne_epoch_door_before_shuffled,mne_epoch_door_after_shuffled,mne_epoch_dig_before_shuffled,mne_epoch_dig_after_shuffled,mne_epoch_around_door_shuffled,mne_epoch_around_dig_shuffled]\n",
    "            shuffled_event_data_df.append(row_list_shuffled)\n",
    "\n",
    "            con_data_df.append(row_list)\n",
    "            con_data_df_shuffled.append(row_list_shuffled)\n",
    "\n",
    "\n",
    "con_data_df=pd.DataFrame(con_data_df, columns=['experiment','date','rat_id','task','mne_baseline','mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after','mne_epoch_around_door','mne_epoch_around_dig'])\n",
    "con_data_df.to_pickle(savepath+f'raw_mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "con_data_df_shuffled=pd.DataFrame(con_data_df_shuffled, columns=['experiment','date','rat_id','task','mne_baseline','mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after','mne_epoch_around_door','mne_epoch_around_dig'])\n",
    "con_data_df_shuffled.to_pickle(savepath+f'raw_mne_epochs_array_df_shuffled_truncated_{int(time_window*fs)}.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c530c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df = pd.read_pickle(savepath+f'raw_mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f4958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_epoch = con_data_df.iloc[0]['mne_epoch_dig_before']\n",
    "aon_indices = [i for i, ch in enumerate(test_epoch.ch_names) if 'AON' in ch]\n",
    "vHp_indices = [i for i, ch in enumerate(test_epoch.ch_names) if 'vHp' in ch]\n",
    "test_epoch.plot(events=True, scalings='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90d76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(con_data_df.shape[0])\n",
    "for row in range(con_data_df.shape[0]):\n",
    "    epoch1 = con_data_df.iloc[row]['mne_epoch_dig_before']\n",
    "    epoch1.plot(events=True, scalings='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0613973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(con_data_df.shape[0]):\n",
    "    epoch = con_data_df.iloc[row]['mne_epoch_dig_after']\n",
    "    epoch.plot(events=True, scalings='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5730e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(con_data_df.shape[0]):\n",
    "    epoch = con_data_df.iloc[row]['mne_epoch_door_before']\n",
    "    epoch.plot(events=True, scalings='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c6b31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df.to_pickle(savepath + f'marked_mne_epochs_array_df_truncated_{int(time_window*fs)}_181125.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b76bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "con_data_df_marked = pd.read_pickle(savepath + f'marked_mne_epochs_array_df_truncated_{int(time_window*fs)}_181125.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ae7484",
   "metadata": {},
   "outputs": [],
   "source": [
    "door_before_dropped_total = 0\n",
    "dig_before_dropped_total = 0\n",
    "dig_after_dropped_total = 0\n",
    "\n",
    "for row in range(con_data_df_marked.shape[0]):\n",
    "    door_before = con_data_df_marked.iloc[row]['mne_epoch_door_before']\n",
    "    dig_before = con_data_df_marked.iloc[row]['mne_epoch_dig_before']    \n",
    "    dig_after = con_data_df_marked.iloc[row]['mne_epoch_dig_after']\n",
    "    \n",
    "    door_before_epochs = len(door_before)\n",
    "    dig_before_epochs = len(dig_before)\n",
    "    dig_after_epochs = len(dig_after)\n",
    "    \n",
    "    door_before_dropped = int(20) - int(door_before_epochs)\n",
    "    dig_before_dropped = int(20) - int(dig_before_epochs)\n",
    "    dig_after_dropped = int(20) - int(dig_after_epochs)\n",
    "    \n",
    "    door_before_dropped_total = door_before_dropped_total + door_before_dropped\n",
    "    dig_before_dropped_total = dig_before_dropped_total + dig_before_dropped\n",
    "    dig_after_dropped_total = dig_after_dropped_total + dig_after_dropped\n",
    "\n",
    "print(\"Total Door Before Epochs Dropped - \", door_before_dropped_total, \"Out of\", 20*27)\n",
    "print(\"Total Dig Before Epochs Dropped - \", dig_before_dropped_total, \"Out of\", 20*27)\n",
    "print(\"Total Dig After Epochs Dropped - \", dig_after_dropped_total, \"Out of\", 20*27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ce3900",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "\n",
    "time_window = 1\n",
    "fs = 2000  # Sampling frequency\n",
    "tanh_norm = True\n",
    "###############\n",
    "def coherogram_pkl(time_window, fs, tanh_norm):\n",
    "    if tanh_norm:\n",
    "        suffix ='_normalized'\n",
    "    else:\n",
    "        suffix ='_non-normalized'\n",
    "\n",
    "\n",
    "    con_data_df_clean=con_data_df_marked\n",
    "\n",
    "    event_list=['mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after']\n",
    "\n",
    "    print(event_list)\n",
    "    BWcontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWcontext')]\n",
    "    BWnocontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWnocontext')]\n",
    "    task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "\n",
    "    all_con_data=[]\n",
    "    all_con_data_mean=[]\n",
    "    for task_num,task_name in enumerate(task_data_dict.keys()):\n",
    "            task_data=task_data_dict[task_name]\n",
    "            row=[task_name]\n",
    "            #print(row)\n",
    "            row_2=[task_name]\n",
    "            for event in event_list:\n",
    "                #print(event)\n",
    "                event_epoch_list=task_data[event]\n",
    "                aon_vHp_con=[]\n",
    "                for event_epoch in event_epoch_list:\n",
    "                        #print(row,event, event_epoch) \n",
    "                        print(event_epoch.events.shape[0])\n",
    "                        if event_epoch.events.shape[0] <5:\n",
    "                            \n",
    "                            print(f\"Skipping {event} for {task_name} due to insufficient events\")\n",
    "                            continue\n",
    "                        fmin=1\n",
    "                        fmax=100\n",
    "                        fs=2000\n",
    "                        freqs = np.arange(fmin,fmax)\n",
    "                        n_cycles = freqs/3\n",
    "                        \n",
    "                        con = mne_connectivity.spectral_connectivity_epochs(event_epoch, method='coh', sfreq=int(fs),\n",
    "                                                            mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                                            cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False)\n",
    "                        \n",
    "                        ########TRYING HIGHPASS FILTERING FOR ARTIFACT REMOVAL################\n",
    "                        # epoch_highpass = event_epoch.copy().filter(l_freq = 1, h_freq=None, filter_length = \"0.7s\" )\n",
    "                        # con = mne_connectivity.spectral_connectivity_epochs(epoch_highpass, method='coh', sfreq=int(fs),\n",
    "                        #                                     mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                        #                                     cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        coh = con.get_data(output='dense')\n",
    "                        indices = con.names\n",
    "                        \n",
    "\n",
    "                        for i in range(coh.shape[0]):\n",
    "                            for j in range(coh.shape[1]):\n",
    "                                if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                                    coherence= coh[i,j,:,:]\n",
    "                                    if tanh_norm:\n",
    "                                        coherence=np.arctanh(coherence)\n",
    "                                    aon_vHp_con.append(coherence)\n",
    "                row.append(np.mean(aon_vHp_con, axis=0))\n",
    "                row_2.append(np.mean(aon_vHp_con))\n",
    "            all_con_data.append(row)                    \n",
    "            all_con_data_mean.append(row_2)\n",
    "    # Convert all_con_data to a DataFrame for easier manipulation\n",
    "    all_con_data_df = pd.DataFrame(all_con_data, columns=['task'] + event_list)\n",
    "    all_con_data_df.to_pickle(savepath+'marked_coherence_spectrogram_before_after_door_dig_truncated_{}{}_181125.pkl'.format(int(time_window*fs), suffix))\n",
    "\n",
    "coherogram_pkl(time_window=time_window, fs=fs, tanh_norm=tanh_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b638786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "\n",
    "time_window = 1\n",
    "fs = 2000  # Sampling frequency\n",
    "tanh_norm = True\n",
    "\n",
    "###############\n",
    "\n",
    "if tanh_norm:\n",
    "    suffix ='_normalized'\n",
    "else:\n",
    "    suffix ='_non-normalized'\n",
    "\n",
    "all_con_data_df=pd.read_pickle(savepath+'marked_coherence_spectrogram_before_after_door_dig_truncated_{}{}_181125.pkl'.format(int(time_window*fs), suffix))\n",
    "event_list=['mne_epoch_door_before','mne_epoch_dig_before','mne_epoch_dig_after']\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "times=np.arange(0, time_window, 1/fs)\n",
    "fig, axs=plt.subplots(2,3, figsize=(15,10), sharey=True)\n",
    "vmin = all_con_data_df[event_list].applymap(np.min).min().min()\n",
    "vmax = all_con_data_df[event_list].applymap(np.max).max().max()\n",
    "event_names=['Before Door','Before Dig','After Dig']\n",
    "for i, event in enumerate(event_list):\n",
    "    axs[0,i].imshow(all_con_data_df[event][0], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[0,i].set_xlabel('Time (s)')\n",
    "    axs[0,i].set_ylabel('Frequency (Hz)')\n",
    "    axs[0,i].set_title(event_names[i])\n",
    "\n",
    "    axs[1,i].imshow(all_con_data_df[event][1], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[1,i].set_xlabel('Time (s)')\n",
    "    axs[1,i].set_ylabel('Frequency (Hz)')\n",
    "    axs[1,i].set_title(event_names[i])\n",
    "    axs[0,0].text(-0.3, 0.5, 'Context', transform=axs[0,0].transAxes, fontsize=14, verticalalignment='center', rotation=90)\n",
    "    axs[1,0].text(-0.3, 0.5, 'No Context', transform=axs[1,0].transAxes, fontsize=14, verticalalignment='center', rotation=90)\n",
    "    # Add a colorbar\n",
    "cbar = fig.colorbar(axs[0,0].images[0], ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "cbar.set_label('Coherence (A.U.)', fontsize=12)\n",
    "fig.savefig(savepath+f'marked_coherence_spectrogram_before_after_door_dig_{int(time_window*fs/2)}ms{suffix}_181125.png', dpi=1200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8cf7b4",
   "metadata": {},
   "source": [
    "### Coherence for Individual Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca458ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208e7766",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "time_window = 1\n",
    "fs = 2000  # Sampling frequency\n",
    "tanh_norm = True\n",
    "###############\n",
    "def coherogram_perexperiment_pkl(time_window, fs, tanh_norm):\n",
    "\n",
    "    if tanh_norm:\n",
    "        suffix ='normalized'\n",
    "    else:\n",
    "        suffix ='nonnormalized'\n",
    "\n",
    "    con_data_df_clean=pd.read_pickle(savepath+f'marked_mne_epochs_array_df_truncated_{int(time_window*fs)}_181125.pkl')\n",
    "\n",
    "    event_list=['mne_epoch_door_before','mne_epoch_dig_before', 'mne_epoch_dig_after']\n",
    "\n",
    "    print(event_list)\n",
    "\n",
    "    test_list = [con_data_df_clean.iloc[0]]\n",
    "    power_data=pd.DataFrame()\n",
    "    def epoch_coherogram(epoch, fmin=1, fmax=100, fs=2000):\n",
    "            print(epoch.events.shape)\n",
    "        # if epoch.events.shape[0] < 5:\n",
    "        #     print(\"Not enough events in the epoch\")\n",
    "        #     return None\n",
    "        # else:\n",
    "            freqs = np.arange(fmin, fmax)\n",
    "            n_cycles = freqs / 3\n",
    "            con = mne_connectivity.spectral_connectivity_epochs(epoch, method='coh', sfreq=int(fs),\n",
    "                                                                mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                                                cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False)\n",
    "            coh = con.get_data(output='dense')\n",
    "            indices = con.names\n",
    "            aon_vHp_con = []\n",
    "            for i in range(coh.shape[0]):\n",
    "                for j in range(coh.shape[1]):\n",
    "                    if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                        coherence= coh[i,j,:,:]\n",
    "                        if tanh_norm:\n",
    "                            coherence=np.arctanh(coherence)\n",
    "                        aon_vHp_con.append(coherence)\n",
    "            \n",
    "            mean_con = np.mean(aon_vHp_con, axis=0)\n",
    "            return mean_con\n",
    "    power_data['mne_epoch_door_before'] = con_data_df_clean['mne_epoch_door_before'].apply(epoch_coherogram)\n",
    "    power_data['mne_epoch_dig_before'] = con_data_df_clean['mne_epoch_dig_before'].apply(epoch_coherogram)\n",
    "    power_data['mne_epoch_dig_after'] = con_data_df_clean['mne_epoch_dig_after'].apply(epoch_coherogram)\n",
    "\n",
    "    power_data['experiment'] = con_data_df_clean['experiment']\n",
    "    power_data['date'] = con_data_df_clean['date']\n",
    "    power_data['task'] = con_data_df_clean['task']\n",
    "    power_data['rat_id'] = con_data_df_clean['rat_id']\n",
    "    power_data.dropna(inplace=True)\n",
    "    power_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    power_data.to_pickle(savepath + f'marked_coherence_around_events_mean_{int(time_window*fs)}_{suffix}.pkl')\n",
    "\n",
    "coherogram_perexperiment_pkl(time_window=time_window, fs=fs, tanh_norm=tanh_norm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0494cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dk1' 'dk3' 'dk5' 'dk6'] [5 4 4 2]\n",
      "5\n",
      "Plotting group: BWcontext\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "Plotting group: BWnocontext\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "time_window = 1\n",
    "fs = 2000  # Sampling frequency\n",
    "tanh_norm = True\n",
    "#################\n",
    "\n",
    "event_of_interest = 'mne_epoch_door_before'\n",
    "\n",
    "def plot_coherogram_perexperiment(time_window, fs, tanh_norm):\n",
    "    if tanh_norm:\n",
    "        suffix ='normalized'\n",
    "    else:\n",
    "        suffix ='nonnormalized'\n",
    "\n",
    "    power_data=pd.read_pickle(savepath + f'marked_coherence_around_events_mean_{int(time_window*fs)}_{suffix}.pkl')\n",
    "    vmin = power_data[event_of_interest].apply(np.min).min()\n",
    "    vmax = power_data[event_of_interest].apply(np.max).max()\n",
    "\n",
    "    BWcontext_data=power_data[(power_data['task']=='BWcontext')]\n",
    "    BWnocontext_data=power_data[(power_data['task']=='BWnocontext')]\n",
    "    task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "    rat_ids, rat_nums = np.unique(BWcontext_data['rat_id'], return_counts=True)\n",
    "    print(rat_ids, rat_nums)\n",
    "    rat_nums_max = rat_nums.max()\n",
    "    print(rat_nums_max)\n",
    "    import matplotlib.pyplot as plt\n",
    "    writer = pd.ExcelWriter(savepath + 'coherogram_perexperiment_{}_{}.xlsx'.format(int(time_window*fs),suffix))\n",
    "\n",
    "    for group_name, group_df in task_data_dict.items():\n",
    "        print(f\"Plotting group: {group_name}\")\n",
    "        group_dict = {'BWcontext': 'Context', 'BWnocontext': 'No Context'}\n",
    "        rat_ids, rat_nums = np.unique(group_df['rat_id'], return_counts=True)\n",
    "        rat_nums_max = rat_nums.max()\n",
    "\n",
    "        num_of_rows = 4 # Each row should be a rats\n",
    "        num_of_cols = rat_nums_max # Each column should be the max number of experiments for a rat\n",
    "\n",
    "        fig, axs = plt.subplots(num_of_rows, num_of_cols, figsize=(25, 10), sharex=True, sharey=True)\n",
    "        dk1_count = 0\n",
    "        dk3_count = 0\n",
    "        dk5_count = 0\n",
    "        dk6_count = 0\n",
    "        for i, (idx, row) in enumerate(group_df.iterrows()):\n",
    "            rat_id = row['rat_id']\n",
    "            data = np.array(row[event_of_interest])\n",
    "            if rat_id == 'dk1':\n",
    "                ax=axs[0, dk1_count]\n",
    "                dk1_count += 1\n",
    "            elif rat_id == 'dk3':\n",
    "                ax=axs[1, dk3_count]\n",
    "                dk3_count += 1\n",
    "            elif rat_id == 'dk5':\n",
    "                ax=axs[2, dk5_count]\n",
    "                dk5_count += 1\n",
    "            elif rat_id == 'dk6':\n",
    "                ax=axs[3, dk6_count]\n",
    "                dk6_count += 1\n",
    "            im = ax.imshow(data, extent=[0, time_window, 1, 100], aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "            ax.set_title(f\"{row['rat_id']} {row['date']}\")\n",
    "\n",
    "            ##### Writing to excel\n",
    "\n",
    "            freqs = [f'{int(freq)}Hz' for freq in np.linspace(1, 100, data.shape[0])]\n",
    "            freqs.insert(0, 'Frequency (Hz) / Time (s)')\n",
    "            print(len(freqs))\n",
    "            time_points = [f'{np.round(t, 3)}s' for t in np.linspace(0, time_window, data.shape[1])]\n",
    "\n",
    "            df_towrite = pd.DataFrame(data)\n",
    "            df_towrite.loc[-1] = time_points  # Add time points as the first row\n",
    "            df_towrite.index = df_towrite.index + 1  # Shift index\n",
    "            df_towrite = df_towrite.sort_index()\n",
    "            df_towrite.insert(0, 'Frequency (Hz)/ Time (s)', freqs)\n",
    "            df_towrite.to_excel(writer, sheet_name=f'{group_dict[group_name]}_{rat_id}_{row[\"date\"]}', index=False)\n",
    "\n",
    "        for j in range(i + 1, len(axs)):\n",
    "            fig.delaxes(axs[j])\n",
    "        fig.suptitle(f\"{group_dict[group_name]} Coherence {suffix} {event_of_interest}\", fontsize=16)\n",
    "        fig.colorbar(im, ax=axs, orientation='vertical', fraction=0.02, label=f'Coherence {suffix}')\n",
    "        fig.savefig(savepath + f'marked_{group_name}_coherogram_per_experiment_{event_of_interest}_{int(time_window*fs)}_{suffix}.png', dpi=300, bbox_inches='tight')\n",
    "        #plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.show()\n",
    "    writer.close()\n",
    "plot_coherogram_perexperiment(time_window=time_window, fs=fs, tanh_norm=tanh_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3383e89c",
   "metadata": {},
   "source": [
    "## Calculating Power and Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49135539",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix ='_normalized'\n",
    "\n",
    "mne_epochs = pd.read_pickle(savepath + f'marked_mne_epochs_array_df_truncated_{int(time_window*fs)}_181125.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b94bc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_power_tfr(epoch):\n",
    "    fmin=2.5\n",
    "    fmax=100\n",
    "    fs=2000\n",
    "    freqs = np.arange(fmin,fmax)\n",
    "    n_cycles = freqs/3\n",
    "\n",
    "    power = epoch.compute_tfr(\n",
    "        method=\"morlet\", freqs=freqs, n_cycles=n_cycles, return_itc=False, average=False,\n",
    "    )\n",
    "\n",
    "    return power\n",
    "results = []\n",
    "for row in mne_epochs.itertuples(index=False):\n",
    "    experiment, rat_id, task = row.experiment, row.rat_id, row.task\n",
    "    door_before,door_after = row.mne_epoch_door_before, row.mne_epoch_door_after\n",
    "    dig_before,dig_after = row.mne_epoch_dig_before, row.mne_epoch_dig_after\n",
    "    around_door, around_dig = row.mne_epoch_around_door, row.mne_epoch_around_dig\n",
    "\n",
    "    power_door_before = get_power_tfr(door_before)\n",
    "    power_door_after = get_power_tfr(door_after)\n",
    "    power_dig_before = get_power_tfr(dig_before)\n",
    "    power_dig_after = get_power_tfr(dig_after)\n",
    "    power_around_door = get_power_tfr(around_door)\n",
    "    power_around_dig = get_power_tfr(around_dig)\n",
    "\n",
    "    #net_power = power_dig_before - power_door_before\n",
    "    channel_names = door_before.ch_names\n",
    "    new_row = [experiment, rat_id, task,power_door_before,power_door_after,power_dig_before,power_dig_after, power_around_door, power_around_dig, channel_names]\n",
    "    results.append(new_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0ea58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_df = pd.DataFrame(results, columns=['experiment', 'rat_id', 'task', 'power_pre_door', 'power_post_door','power_pre_dig','power_post_dig','power_around_door','power_around_dig', 'channel_names'])\n",
    "results_df.to_pickle(savepath + 'marked_power_tfr_epochs_mrlt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e960dd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task: BWcontext, Area: AON, Rows in task_data: 15\n",
      "0 power_pre_door BWcontext AON\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (1, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (3, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (3, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Valid arrays found: 15\n",
      "Averaged array shape: (98, 2000)\n",
      "1 power_pre_dig BWcontext AON\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (1, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (3, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (3, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Valid arrays found: 15\n",
      "Averaged array shape: (98, 2000)\n",
      "2 power_post_dig BWcontext AON\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (1, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (3, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (3, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Valid arrays found: 15\n",
      "Averaged array shape: (98, 2000)\n",
      "\n",
      "Task: BWnocontext, Area: AON, Rows in task_data: 12\n",
      "0 power_pre_door BWnocontext AON\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (1, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (1, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (1, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Valid arrays found: 12\n",
      "Averaged array shape: (98, 2000)\n",
      "1 power_pre_dig BWnocontext AON\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (1, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (1, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (1, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Valid arrays found: 12\n",
      "Averaged array shape: (98, 2000)\n",
      "2 power_post_dig BWnocontext AON\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (1, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (1, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (1, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (4, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Valid arrays found: 12\n",
      "Averaged array shape: (98, 2000)\n",
      "\n",
      "Task: BWcontext, Area: vHp, Rows in task_data: 15\n",
      "0 power_pre_door BWcontext vHp\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (1, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Valid arrays found: 15\n",
      "Averaged array shape: (98, 2000)\n",
      "1 power_pre_dig BWcontext vHp\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (1, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Valid arrays found: 15\n",
      "Averaged array shape: (98, 2000)\n",
      "2 power_post_dig BWcontext vHp\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (1, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Valid arrays found: 15\n",
      "Averaged array shape: (98, 2000)\n",
      "\n",
      "Task: BWnocontext, Area: vHp, Rows in task_data: 12\n",
      "0 power_pre_door BWnocontext vHp\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Valid arrays found: 12\n",
      "Averaged array shape: (98, 2000)\n",
      "1 power_pre_dig BWnocontext vHp\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Valid arrays found: 12\n",
      "Averaged array shape: (98, 2000)\n",
      "2 power_post_dig BWnocontext vHp\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Data shape before mean: (2, 98, 2000)\n",
      "Data shape after mean: (98, 2000)\n",
      "Valid arrays found: 12\n",
      "Averaged array shape: (98, 2000)\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.read_pickle(savepath+'marked_power_tfr_epochs_mrlt.pkl')\n",
    "\n",
    "def make_averaged_power(epoch, area):\n",
    "    #print(epoch.ch_names)\n",
    "    area_channels = [channel for channel in epoch.ch_names if area in channel]\n",
    "    #print(area_channels)\n",
    "\n",
    "    if len(area_channels)==0:\n",
    "        print(\"Error\")\n",
    "        return None\n",
    "    else:\n",
    "        area_epoch = epoch.copy()\n",
    "        area_epoch.pick(area_channels)\n",
    "        averaged_epoch_power = area_epoch.average(dim='epochs')\n",
    "        print(f\"Data shape before mean: {averaged_epoch_power.shape}\")  # DEBUG\n",
    "        mean_ch_power = np.mean(averaged_epoch_power.get_data(), axis = 0)\n",
    "        print(f\"Data shape after mean: {mean_ch_power.shape}\")  # DEBUG\n",
    "        return mean_ch_power\n",
    "\n",
    "# test_averaged_epoch_power = make_averaged_power(test_power_epoch, \"vHp\")\n",
    "# print(test_averaged_epoch_power.shape)\n",
    "\n",
    "for area in [\"AON\", \"vHp\"]:\n",
    "    area_df = pd.DataFrame()\n",
    "    fig, axs = plt.subplots(2,3, figsize= (15,10))\n",
    "    fig.suptitle(f'Average {area} Power')\n",
    "    for rowi, task in enumerate([\"BWcontext\", \"BWnocontext\"]):\n",
    "        task_data=results_df[results_df['task']==task]\n",
    "        print(f\"\\nTask: {task}, Area: {area}, Rows in task_data: {len(task_data)}\")\n",
    "        for coli, event in enumerate(['power_pre_door', 'power_pre_dig','power_post_dig']):\n",
    "            print(coli,event, task, area)\n",
    "            event_arrays = task_data[event].apply(lambda x: make_averaged_power(x, area))\n",
    "            \n",
    "            valid_arrays = [arr for arr in event_arrays.values if arr is not None]\n",
    "            \n",
    "            print(f\"Valid arrays found: {len(valid_arrays)}\")\n",
    "            \n",
    "            if len(valid_arrays) > 0:\n",
    "                averaged_array = np.mean(np.stack(valid_arrays), axis=0)\n",
    "                print(f\"Averaged array shape: {averaged_array.shape}\")\n",
    "                \n",
    "                ax = axs[rowi, coli]\n",
    "                im = ax.imshow(X= averaged_array, cmap = 'viridis', aspect='auto', origin='lower')\n",
    "                                # Add titles and labels\n",
    "                ax.set_title(f'{event.replace(\"_\", \" \").title()}')\n",
    "                ax.set_xlabel('Time (samples)')\n",
    "                ax.set_ylabel('Frequency (Hz)')\n",
    "                ax.set_xticks(list(np.arange(0,1600,200)))\n",
    "                ax.set_xticklabels(list(np.round(np.arange(0,0.8,0.1), decimals = 1)))\n",
    "                # Add colorbar\n",
    "                plt.colorbar(im, ax=ax, label='Power (mV^2/Hz)')\n",
    "                \n",
    "                # Add row labels\n",
    "                if coli == 0:\n",
    "                    ax.set_ylabel(f'{task}\\nFrequency (Hz)', fontweight='bold')\n",
    "                # Add your plotting code here\n",
    "            else:\n",
    "                print(f\"WARNING: No valid data for {area}, {task}, {event}\")\n",
    "                ax = axs[rowi, coli]\n",
    "                ax.text(0.5, 0.5, 'No data', ha='center', va='center')\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(savepath+f'marked_power_spectrogram_{area}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9077926a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "time_window = 1\n",
    "fs = 2000  # Sampling frequency\n",
    "tanh_norm = True\n",
    "#################\n",
    "\n",
    "event_of_interest = 'mne_epoch_door_before'\n",
    "\n",
    "def plot_coherogram_perexperiment(time_window, fs, tanh_norm):\n",
    "    if tanh_norm:\n",
    "        suffix ='normalized'\n",
    "    else:\n",
    "        suffix ='nonnormalized'\n",
    "\n",
    "    power_data=pd.read_pickle(savepath + f'marked_power_tfr_epochs_mrlt.pkl')\n",
    "    vmin = power_data[event_of_interest].apply(np.min).min()\n",
    "    vmax = power_data[event_of_interest].apply(np.max).max()\n",
    "\n",
    "    BWcontext_data=power_data[(power_data['task']=='BWcontext')]\n",
    "    BWnocontext_data=power_data[(power_data['task']=='BWnocontext')]\n",
    "    task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "    rat_ids, rat_nums = np.unique(BWcontext_data['rat_id'], return_counts=True)\n",
    "    print(rat_ids, rat_nums)\n",
    "    rat_nums_max = rat_nums.max()\n",
    "    print(rat_nums_max)\n",
    "    import matplotlib.pyplot as plt\n",
    "    writer = pd.ExcelWriter(savepath + 'coherogram_perexperiment_{}_{}.xlsx'.format(int(time_window*fs),suffix))\n",
    "\n",
    "    for group_name, group_df in task_data_dict.items():\n",
    "        print(f\"Plotting group: {group_name}\")\n",
    "        group_dict = {'BWcontext': 'Context', 'BWnocontext': 'No Context'}\n",
    "        rat_ids, rat_nums = np.unique(group_df['rat_id'], return_counts=True)\n",
    "        rat_nums_max = rat_nums.max()\n",
    "\n",
    "        num_of_rows = 4 # Each row should be a rats\n",
    "        num_of_cols = rat_nums_max # Each column should be the max number of experiments for a rat\n",
    "\n",
    "        fig, axs = plt.subplots(num_of_rows, num_of_cols, figsize=(25, 10), sharex=True, sharey=True)\n",
    "        dk1_count = 0\n",
    "        dk3_count = 0\n",
    "        dk5_count = 0\n",
    "        dk6_count = 0\n",
    "        for i, (idx, row) in enumerate(group_df.iterrows()):\n",
    "            rat_id = row['rat_id']\n",
    "            data = np.array(row[event_of_interest])\n",
    "            if rat_id == 'dk1':\n",
    "                ax=axs[0, dk1_count]\n",
    "                dk1_count += 1\n",
    "            elif rat_id == 'dk3':\n",
    "                ax=axs[1, dk3_count]\n",
    "                dk3_count += 1\n",
    "            elif rat_id == 'dk5':\n",
    "                ax=axs[2, dk5_count]\n",
    "                dk5_count += 1\n",
    "            elif rat_id == 'dk6':\n",
    "                ax=axs[3, dk6_count]\n",
    "                dk6_count += 1\n",
    "            im = ax.imshow(data, extent=[0, time_window, 1, 100], aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "            ax.set_title(f\"{row['rat_id']} {row['date']}\")\n",
    "\n",
    "            ##### Writing to excel\n",
    "\n",
    "            freqs = [f'{int(freq)}Hz' for freq in np.linspace(1, 100, data.shape[0])]\n",
    "            freqs.insert(0, 'Frequency (Hz) / Time (s)')\n",
    "            print(len(freqs))\n",
    "            time_points = [f'{np.round(t, 3)}s' for t in np.linspace(0, time_window, data.shape[1])]\n",
    "\n",
    "            df_towrite = pd.DataFrame(data)\n",
    "            df_towrite.loc[-1] = time_points  # Add time points as the first row\n",
    "            df_towrite.index = df_towrite.index + 1  # Shift index\n",
    "            df_towrite = df_towrite.sort_index()\n",
    "            df_towrite.insert(0, 'Frequency (Hz)/ Time (s)', freqs)\n",
    "            df_towrite.to_excel(writer, sheet_name=f'{group_dict[group_name]}_{rat_id}_{row[\"date\"]}', index=False)\n",
    "\n",
    "        for j in range(i + 1, len(axs)):\n",
    "            fig.delaxes(axs[j])\n",
    "        fig.suptitle(f\"{group_dict[group_name]} Coherence {suffix} {event_of_interest}\", fontsize=16)\n",
    "        fig.colorbar(im, ax=axs, orientation='vertical', fraction=0.02, label=f'Coherence {suffix}')\n",
    "        fig.savefig(savepath + f'marked_{group_name}_coherogram_per_experiment_{event_of_interest}_{int(time_window*fs)}_{suffix}.png', dpi=300, bbox_inches='tight')\n",
    "        #plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.show()\n",
    "    writer.close()\n",
    "plot_coherogram_perexperiment(time_window=time_window, fs=fs, tanh_norm=tanh_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405c0869",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#fig = test_epoch.compute_psd(fmin=1, fmax=100, method='multitaper').plot(average=True, amplitude=False, picks=\"data\")\n",
    "test_epoch.plot_image(combine=\"mean\", picks=aon_indices)\n",
    "test_epoch.plot_image(combine=\"mean\", picks=vHp_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7176730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903d6902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "import mne\n",
    "import mne_connectivity\n",
    "#files=[f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230616_dk6_BW_context_day2.mat']\n",
    "event_data_df=[]\n",
    "con_data_df=[]\n",
    "\n",
    "con_data_df_shuffled=[]\n",
    "shuffled_event_data_df=[]\n",
    "events_codes_all = {}\n",
    "random_baseline_data=[]\n",
    "baseline_lfp_all=[]\n",
    "\n",
    "for file_num,file in enumerate(files):\n",
    "    #if 'dk1' in file:\n",
    "        \n",
    "        print(file)\n",
    "        base_name = os.path.basename(file)\n",
    "        base_name, _ = os.path.splitext(base_name)\n",
    "\n",
    "        date, rat_id, task = lfp_pre_processing_functions.exp_params(base_name)\n",
    "        print(date, rat_id, task)\n",
    "        if task == 'nocontextday2' or task == 'nocontextos2':\n",
    "            task = 'nocontext'\n",
    "        if task =='nocontext':\n",
    "            continue\n",
    "\n",
    "        if rat_id=='dk3':\n",
    "            continue\n",
    "        f = h5py.File(file, 'r')\n",
    "        channels = list(f.keys())\n",
    "        print(channels)\n",
    "         \n",
    "        if not any(\"AON\" in channel or \"vHp\" in channel for channel in channels):\n",
    "            print(\"No AON or vHp channels in this file\")\n",
    "            continue\n",
    "\n",
    "        events,reference_electrode=lfp_pre_processing_functions.get_keyboard_and_ref_channels(f,channels)\n",
    "\n",
    "    #finding global start and end time of all channels, since they start and end recordings at different times\n",
    "        global_start_time, global_end_time=lfp_pre_processing_functions.find_global_start_end_times(f,channels)\n",
    "        \n",
    "        ## Reference electrode finding and padding\n",
    "        reference_time = np.array(reference_electrode['times']).flatten()\n",
    "        reference_value = np.array(reference_electrode['values']).flatten()\n",
    "        padd_ref_data,padded_ref_time=lfp_pre_processing_functions.pad_raw_data_raw_time(reference_value,reference_time,global_start_time,global_end_time,sampling_rate=2000)\n",
    "        print(padd_ref_data, 'Reference electrode data padded to global start and end time')\n",
    "        events_codes = np.array(events['codes'][0])\n",
    "        events_times = np.array(events['times'][0])\n",
    "        print(events_codes.shape, events_times.shape, 'Keyboard')\n",
    "        print(events_codes)\n",
    "        events_codes_all[base_name] = events_codes\n",
    "        print(events_codes_all)\n",
    "        epochs = lfp_pre_processing_functions.generate_epochs_with_first_event(events_codes, events_times)\n",
    "        #epochs = functions.generate_specific_num_of_epochs_with_first_event(events_codes, events_times,5)\n",
    "        aon_lfp_channels=[x for x in channels if 'AON' in x ]\n",
    "        vHp_lfp_channels=[x for x in channels if 'vHp' in x ]\n",
    "        data_channels=np.concatenate((aon_lfp_channels,vHp_lfp_channels))\n",
    "        #print(all_channels)\n",
    "        # Convert events_codes to strings for annotation descriptions\n",
    "        annotation_descriptions = [str(code) for code in events_codes]\n",
    "        annotations = mne.Annotations(onset=events_times, duration=np.zeros_like(events_times), description=annotation_descriptions)\n",
    "        # Remove any keys from keyboard_dict that are not present in annotation_descriptions\n",
    "        \n",
    "        present_keys = set(annotation_descriptions) & set(keyboard_dict.keys())\n",
    "        filtered_keyboard_dict = {k: v for k, v in keyboard_dict.items() if k in present_keys}\n",
    "        if len(filtered_keyboard_dict) < len(keyboard_dict):\n",
    "            print(f\"Filtered keyboard_dict to only present annotation descriptions: {filtered_keyboard_dict}\")\n",
    "        if filtered_keyboard_dict:\n",
    "            annotations.rename(filtered_keyboard_dict)\n",
    "        else:\n",
    "            print(\"No matching annotation descriptions found for renaming.\")\n",
    "        \n",
    "        all_channels = list(data_channels) + ['reference']\n",
    "        info = mne.create_info(ch_names=list(all_channels), sfreq=2000, ch_types='eeg')\n",
    "\n",
    "        all_channels_data= []\n",
    "\n",
    "        first_event = events_times[0]\n",
    "        raw_data = []\n",
    "        for channel_num,channeli in enumerate(data_channels):\n",
    "            if \"AON\" in channeli or \"vHp\" in channeli:\n",
    "                channel_id = channeli\n",
    "                data_all = f[channeli]\n",
    "                raw_data = np.array(data_all['values']).flatten()\n",
    "                raw_time = np.array(data_all['times']).flatten()\n",
    "\n",
    "                padded_data,padded_time=lfp_pre_processing_functions.pad_raw_data_raw_time(raw_data,raw_time,global_start_time,global_end_time,2000)\n",
    "\n",
    "                print(len(padded_data), len(padded_data), channel_id)\n",
    "                all_channels_data.append(padded_data)\n",
    "            \n",
    "        all_channels_data = np.array(all_channels_data)\n",
    "        all_channels_data = np.vstack((all_channels_data, padd_ref_data))\n",
    "        print(all_channels_data.shape)\n",
    "        mne_all_channels_data = mne.io.RawArray(all_channels_data, info)\n",
    "        print(mne_all_channels_data.info)\n",
    "        #mne_all_channels_data.set_eeg_reference(ref_channels='average', projection=True, ch_type='eeg')\n",
    "        mne_all_channels_data.set_annotations(annotations)\n",
    "#mne.viz.plot_raw(mne_all_channels_data, duration=60, scalings='auto', show=True, block=True)\n",
    "        all_data_epochs= mne.Epochs(mne_all_channels_data, events=None, event_id=None, tmin=-0.7, tmax=0.7, baseline=None, preload=True, verbose=True, event_repeated='drop')\n",
    "        row= [file_num, rat_id, task, all_data_epochs]\n",
    "        event_data_df.append(row)\n",
    "event_data_df=pd.DataFrame(event_data_df, columns=['experiment','rat_id','task','epochs'])\n",
    "print(event_data_df)\n",
    "\n",
    "#                 sampling_rate = int(1 / data_all['interval'][0][0])\n",
    "#                 #print(raw_data.shape, raw_time.shape, sampling_rate)\n",
    "#                 subtracted_data = padded_data - padd_ref_data\n",
    "#                 raw_data=subtracted_data\n",
    "#                 notch_filtered_data = lfp_pre_processing_functions.iir_notch(raw_data, sampling_rate, 60)\n",
    "\n",
    "#                 data_before, time, baseline_mean, baseline_std=lfp_pre_processing_functions.baseline_data_normalization(notch_filtered_data, raw_time, first_event, sampling_rate)\n",
    "#                 first_event_index=np.where(raw_time>first_event)[0][0]\n",
    "\n",
    "#                 mne_baseline_data[0,channel_num,:]=list(data_before)\n",
    "#                 mne_baseline_data_shuffled[0,channel_num,:]=list(np.random.permutation(data_before))\n",
    "#                 total = notch_filtered_data\n",
    "\n",
    "                \n",
    "#                 for i, epochi in enumerate(epochs):\n",
    "#                     door_timestamp = epochi[0][0]\n",
    "#                     trial_type = epochi[0][1]\n",
    "#                     dig_type = epochi[1, 1]\n",
    "#                     #print(dig_type)\n",
    "#                     dig_timestamp = epochi[1, 0]\n",
    "#                     #print(door_timestamp, trial_type, dig_timestamp, dig_type)\n",
    "#                     data_trial_before, data_trial_after=lfp_pre_processing_functions.extract_event_data(notch_filtered_data,time,door_timestamp,sampling_rate,0.7)\n",
    "#                     data_dig_before, data_dig_after=lfp_pre_processing_functions.extract_event_data(notch_filtered_data,time,dig_timestamp,sampling_rate,0.7)\n",
    "#                     data_around_door=np.concatenate((data_trial_before, data_trial_after))\n",
    "#                     data_around_dig=np.concatenate((data_dig_before, data_dig_after))\n",
    "\n",
    "#                     epoch_data = [data_trial_before, data_trial_after, data_dig_before, data_dig_after, data_around_door, data_around_dig]\n",
    "#                     event_data_list = [lfp_pre_processing_functions.zscore_event_data(x, baseline_std) for x in epoch_data]\n",
    "\n",
    "#                     mne_epoch_door_before[i,channel_num,:]=list(event_data_list[0])\n",
    "#                     mne_epoch_door_after[i,channel_num,:]=list(event_data_list[1])\n",
    "#                     mne_epoch_dig_before[i,channel_num,:]=list(event_data_list[2])\n",
    "#                     mne_epoch_dig_after[i,channel_num,:]=list(event_data_list[3])\n",
    "#                     mne_epoch_around_door[i,channel_num,:]=list(event_data_list[4])\n",
    "#                     mne_epoch_around_dig[i,channel_num,:]=list(event_data_list[5])\n",
    "\n",
    "#                     mne_epoch_door_before_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[0]))\n",
    "#                     mne_epoch_door_after_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[1]))\n",
    "#                     mne_epoch_dig_before_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[2]))\n",
    "#                     mne_epoch_dig_after_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[3]))\n",
    "#                     mne_epoch_around_door_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[4]))\n",
    "#                     mne_epoch_around_dig_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[5]))\n",
    "\n",
    "#         if len(all_channels)>0:\n",
    "#             fs=2000\n",
    "#             freqs = np.arange(1,100)\n",
    "#             n_cycles = freqs/3\n",
    "#             info = mne.create_info(ch_names=list(all_channels), sfreq=fs, ch_types='eeg')\n",
    "#             mne_baseline = mne.EpochsArray(mne_baseline_data, info)\n",
    "#             mne_epoch_door_before = mne.EpochsArray(mne_epoch_door_before, info)\n",
    "#             mne_epoch_door_after= mne.EpochsArray(mne_epoch_door_after, info)\n",
    "#             mne_epoch_dig_before = mne.EpochsArray(mne_epoch_dig_before, info)\n",
    "#             mne_epoch_dig_after = mne.EpochsArray(mne_epoch_dig_after, info)\n",
    "#             mne_epoch_around_door = mne.EpochsArray(mne_epoch_around_door, info)\n",
    "#             mne_epoch_around_dig = mne.EpochsArray(mne_epoch_around_dig, info)\n",
    "            \n",
    "#             row_list=[file_num,rat_id,task,mne_baseline,mne_epoch_door_before,mne_epoch_door_after,mne_epoch_dig_before,mne_epoch_dig_after,mne_epoch_around_door,mne_epoch_around_dig]\n",
    "            \n",
    "#             mne_baseline_shuffled = mne.EpochsArray(mne_baseline_data_shuffled, info)\n",
    "#             mne_epoch_door_before_shuffled = mne.EpochsArray(mne_epoch_door_before_shuffled, info)\n",
    "#             mne_epoch_door_after_shuffled = mne.EpochsArray(mne_epoch_door_after_shuffled, info)\n",
    "#             mne_epoch_dig_before_shuffled = mne.EpochsArray(mne_epoch_dig_before_shuffled, info)\n",
    "#             mne_epoch_dig_after_shuffled = mne.EpochsArray(mne_epoch_dig_after_shuffled, info)\n",
    "#             mne_epoch_around_door_shuffled = mne.EpochsArray(mne_epoch_around_door_shuffled, info)\n",
    "#             mne_epoch_around_dig_shuffled = mne.EpochsArray(mne_epoch_around_dig_shuffled, info)\n",
    "#             row_list_shuffled=[file_num,rat_id,task,mne_baseline_shuffled,mne_epoch_door_before_shuffled,mne_epoch_door_after_shuffled,mne_epoch_dig_before_shuffled,mne_epoch_dig_after_shuffled,mne_epoch_around_door_shuffled,mne_epoch_around_dig_shuffled]\n",
    "#             shuffled_event_data_df.append(row_list_shuffled)\n",
    "\n",
    "#             con_data_df.append(row_list)\n",
    "#             con_data_df_shuffled.append(row_list_shuffled)\n",
    "\n",
    "\n",
    "# con_data_df=pd.DataFrame(con_data_df, columns=['experiment','rat_id','task','mne_baseline','mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after','mne_epoch_around_door','mne_epoch_around_dig'])\n",
    "# con_data_df.to_pickle(savepath+'mne_epochs_array_df_truncated.pkl')\n",
    "\n",
    "# con_data_df_shuffled=pd.DataFrame(con_data_df_shuffled, columns=['experiment','rat_id','task','mne_baseline','mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after','mne_epoch_around_door','mne_epoch_around_dig'])\n",
    "# con_data_df_shuffled.to_pickle(savepath+'mne_epochs_array_df_shuffled_truncated.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02171d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mne_all_channels_data = mne.io.RawArray(all_channels_data, info)\n",
    "mne_all_channels_data.plot(scalings='auto', show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f183ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_epochs.plot(n_epochs=10, events=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be07ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_epochs[\"1\"].plot_image(combine=\"mean\", colorbar=True, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffcf7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_epochs[\"1\"].plot_tfr(fmin=4, fmax=100, method='multitaper', show=True, picks=data_channels, spatial_colors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79794ad",
   "metadata": {},
   "source": [
    "## Plotting TFR of raw data (complete experiment) to check for noise in each trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ef5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfr= mne_all_channels_data.compute_tfr(method='morlet', freqs=np.arange(1, 100), n_cycles=np.arange(1, 100)/3,picks=data_channels, decim=1, n_jobs=1, verbose=True)\n",
    "print(tfr.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf313826",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfr.plot([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16f3b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the TFR for each channel\n",
    "n_channels = all_ch_data_tfr.shape[1]\n",
    "n_freqs = all_ch_data_tfr.shape[2]\n",
    "n_times = all_ch_data_tfr.shape[3]\n",
    "\n",
    "fig, axes = plt.subplots(n_channels, 1, figsize=(12, 3 * n_channels), sharex=True)\n",
    "if n_channels == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ch in range(n_channels):\n",
    "    ax = axes[ch]\n",
    "    im = ax.imshow(\n",
    "        all_ch_data_tfr[0, ch], \n",
    "        aspect='auto', \n",
    "        origin='lower',\n",
    "        extent=[times[0], times[-1], freqs[0], freqs[-1]],\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    ax.set_title(f\"TFR - {all_channels[ch]}\")\n",
    "    ax.set_ylabel(\"Frequency (Hz)\")\n",
    "    fig.colorbar(im, ax=ax, orientation='vertical', label='Power')\n",
    "\n",
    "axes[-1].set_xlabel(\"Time (s)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lfp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
