{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6026b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import getpass\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import functions\n",
    "import lfp_pre_processing_functions\n",
    "import power_functions\n",
    "import coherence_functions\n",
    "import spectrogram_plotting_functions\n",
    "import plotting_styles\n",
    "import scipy.stats\n",
    "import mne_connectivity\n",
    "importlib.reload(functions) #loads our custom made functions.py file\n",
    "importlib.reload(spectrogram_plotting_functions)\n",
    "importlib.reload(plotting_styles)\n",
    "\n",
    "linestyle = plotting_styles.linestyles\n",
    "colors = plotting_styles.colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1c359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mne-qt-browser\n",
    "%matplotlib qt\n",
    "import mne\n",
    "print(mne.get_config())  # same as mne.get_config(key=None)\n",
    "mne.set_config('MNE_BROWSER_BACKEND', 'qt')  # set the backend to matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776d7ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch the current user\n",
    "user= (getpass.getuser())\n",
    "print(\"Hello\", user)\n",
    "\n",
    "if user == 'CPLab':\n",
    "    base='D:\\\\Dropbox\\\\CPLab'\n",
    "else:\n",
    "    base='C:\\\\Users\\\\{}\\\\Dropbox\\\\CPLab'.format(user)\n",
    "#Set the basepath, savepath and load the data files\n",
    "files = glob.glob(base+'\\\\all_data_mat_250825\\\\*.mat')\n",
    "savepath = base+'\\\\results\\\\'\n",
    "print(\"Base path:\", base)\n",
    "print(\"Save path:\", savepath)\n",
    "print(files)\n",
    "\n",
    "\n",
    "all_bands_dict = {'total':[1,100], 'beta':[12,30], 'gamma':[30,80], 'theta':[4,12]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f2f3d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f222371",
   "metadata": {},
   "source": [
    "## Doing annotations of bad data segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c01163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import mne_connectivity\n",
    "import sys\n",
    "importlib.reload(lfp_pre_processing_functions)\n",
    "#files=[f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230616_dk6_BW_context_day2.mat']\n",
    "event_data_df=[]\n",
    "con_data_df=[]\n",
    "\n",
    "con_data_df_shuffled=[]\n",
    "shuffled_event_data_df=[]\n",
    "events_codes_all = {}\n",
    "random_baseline_data=[]\n",
    "baseline_lfp_all=[]\n",
    "\n",
    "time_window = 0.7\n",
    "fs= 2000\n",
    "\n",
    "files_short=[files[10]] ### TEST CHANGE THIS \n",
    "\n",
    "\n",
    "for file_num,file in enumerate(files):\n",
    "    #if 'dk1' in file:\n",
    "        \n",
    "        #print(file)\n",
    "        base_name = os.path.basename(file)\n",
    "        base_name, _ = os.path.splitext(base_name)\n",
    "\n",
    "        date, rat_id, task = lfp_pre_processing_functions.exp_params(base_name)\n",
    "        print(date, rat_id, task)\n",
    "        if task == 'nocontextday2' or task == 'nocontextos2':\n",
    "            task = 'nocontext'\n",
    "        if task =='nocontext':\n",
    "            continue\n",
    "        # if rat_id=='dk1': #REMOVING DK1 TEMPORARLILY . PLEASE CHANGE LATER\n",
    "        #     continue\n",
    "        f = h5py.File(file, 'r')\n",
    "        channels = list(f.keys())\n",
    "        #print(channels)\n",
    "         \n",
    "        if not any(\"AON\" in channel or \"vHp\" in channel for channel in channels):\n",
    "            print(\"No AON or vHp channels in this file\")\n",
    "            continue\n",
    "\n",
    "        events,reference_electrode=lfp_pre_processing_functions.get_keyboard_and_ref_channels(f,channels)\n",
    "\n",
    "    #finding global start and end time of all channels, since they start and end recordings at different times\n",
    "        global_start_time, global_end_time=lfp_pre_processing_functions.find_global_start_end_times(f,channels)\n",
    "        \n",
    "        ## Reference electrode finding and padding\n",
    "        reference_time = np.array(reference_electrode['times']).flatten()\n",
    "        reference_value = np.array(reference_electrode['values']).flatten()\n",
    "        padd_ref_data,padded_ref_time=lfp_pre_processing_functions.pad_raw_data_raw_time(reference_value,reference_time,global_start_time,global_end_time,sampling_rate=2000)\n",
    "\n",
    "        events_codes = np.array(events['codes'][0])\n",
    "        events_times = np.array(events['times'][0])\n",
    "        events_codes_all[base_name] = events_codes\n",
    "        epochs = lfp_pre_processing_functions.generate_epochs_with_first_event(events_codes, events_times)\n",
    "        #epochs = functions.generate_specific_num_of_epochs_with_first_event(events_codes, events_times,5)\n",
    "        aon_lfp_channels=[x for x in channels if 'AON' in x ]\n",
    "        vHp_lfp_channels=[x for x in channels if 'vHp' in x ]\n",
    "        ref_lfp_channels=[x for x in channels if \"Ref\" in x or 'REF' in x or 'ref' in x]\n",
    "        all_channels=np.concatenate((aon_lfp_channels,vHp_lfp_channels, ref_lfp_channels))\n",
    "        #print(all_channels)\n",
    "        \n",
    "        mne_baseline_data=np.zeros((1,len(all_channels),4000))\n",
    "        mne_epoch_door_before=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_door_after=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_dig_before=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_dig_after=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_around_door=np.zeros((len(epochs),len(all_channels),int(time_window*fs)*2))\n",
    "        mne_epoch_around_dig=np.zeros((len(epochs),len(all_channels),int(time_window*fs)*2))\n",
    "        \n",
    "        mne_baseline_data_shuffled=np.zeros((1,len(all_channels),4000))\n",
    "        mne_epoch_door_before_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_door_after_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_dig_before_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_dig_after_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_around_door_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs*2)))\n",
    "        mne_epoch_around_dig_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs*2)))\n",
    "\n",
    "        print(f'File {rat_id} {task} {date} has {len(epochs)} epochs and {len(all_channels)} channels')\n",
    "\n",
    "\n",
    "        first_event = events_times[0]\n",
    "        \n",
    "        for channel_num,channeli in enumerate(all_channels):\n",
    "            if \"AON\" in channeli or \"vHp\" in channeli or \"Ref\" in channeli:\n",
    "                channel_id = channeli\n",
    "                data_all = f[channeli]\n",
    "                raw_data = np.array(data_all['values']).flatten()\n",
    "                raw_time = np.array(data_all['times']).flatten()\n",
    "                sampling_rate = int(1 / data_all['interval'][0][0])\n",
    "                #print(raw_data.shape, raw_time.shape, sampling_rate)\n",
    "                \n",
    "                padded_data,padded_time=lfp_pre_processing_functions.pad_raw_data_raw_time(raw_data,raw_time,global_start_time,global_end_time,sampling_rate)\n",
    "\n",
    "\n",
    "                #subtracted_data = padded_data - padd_ref_data # Subtracting reference channel\n",
    "\n",
    "                raw_data=padded_data\n",
    "\n",
    "                #notch_data = lfp_pre_processing_functions.iir_notch(raw_data, sampling_rate, 60) ###CHANGE notch_data to notch_filtered_data\n",
    "\n",
    "                #print(notch_data.nbytes)\n",
    "                #notch_data_detrended = scipy.signal.detrend(notch_data)\n",
    "                #notch_filtered_data=lfp_pre_processing_functions.sosbandpass(notch_data_detrended, fs=2000, start_freq=1,end_freq=100, order=8) ###CHANGE THIS FOR NOT BANDBASS FILTERTING\n",
    "                #print(notch_filtered_data.nbytes)\n",
    "                \n",
    "                data_before, time, baseline_mean, baseline_std=lfp_pre_processing_functions.baseline_data_normalization(raw_data, raw_time, first_event, sampling_rate)\n",
    "                first_event_index=np.where(raw_time>first_event)[0][0]\n",
    "\n",
    "                mne_baseline_data[0,channel_num,:]=list(data_before)\n",
    "                mne_baseline_data_shuffled[0,channel_num,:]=list(np.random.permutation(data_before))\n",
    "                total = raw_data\n",
    "\n",
    "                \n",
    "                for i, epochi in enumerate(epochs):\n",
    "                    door_timestamp = epochi[0][0]\n",
    "                    trial_type = epochi[0][1]\n",
    "                    dig_type = epochi[1, 1]\n",
    "                    #print(dig_type)\n",
    "                    dig_timestamp = epochi[1, 0]\n",
    "                    #print(door_timestamp, trial_type, dig_timestamp, dig_type)\n",
    "                    data_trial_before, data_trial_after=lfp_pre_processing_functions.extract_event_data(raw_data,time,door_timestamp,sampling_rate,truncation_time=time_window)\n",
    "                    data_dig_before, data_dig_after=lfp_pre_processing_functions.extract_event_data(raw_data,time,dig_timestamp,sampling_rate,truncation_time=time_window)\n",
    "                    data_around_door=np.concatenate((data_trial_before, data_trial_after))\n",
    "                    data_around_dig=np.concatenate((data_dig_before, data_dig_after))\n",
    "\n",
    "                    epoch_data = [data_trial_before, data_trial_after, data_dig_before, data_dig_after, data_around_door, data_around_dig]\n",
    "                    event_data_list = [x for x in epoch_data]\n",
    "\n",
    "                    mne_epoch_door_before[i,channel_num,:]=list(event_data_list[0][-int(time_window*fs):])\n",
    "                    mne_epoch_door_after[i,channel_num,:]=list(event_data_list[1][:int(time_window*fs)])\n",
    "                    mne_epoch_dig_before[i,channel_num,:]=list(event_data_list[2][-int(time_window*fs):])\n",
    "                    mne_epoch_dig_after[i,channel_num,:]=list(event_data_list[3][:int(time_window*fs)])\n",
    "                    mid_point = int(len(event_data_list[4]) / 2)\n",
    "                    mne_epoch_around_door[i,channel_num,:]=list(event_data_list[4][mid_point-int(time_window*fs):mid_point+int(time_window*fs)])\n",
    "                    mne_epoch_around_dig[i,channel_num,:]=list(event_data_list[5][mid_point-int(time_window*fs):mid_point+int(time_window*fs)])\n",
    "\n",
    "                    mne_epoch_door_before_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[0][-int(time_window*fs):]))\n",
    "                    mne_epoch_door_after_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[1][:int(time_window*fs)]))\n",
    "                    mne_epoch_dig_before_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[2][-int(time_window*fs):]))\n",
    "                    mne_epoch_dig_after_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[3][:int(time_window*fs)]))\n",
    "                    mne_epoch_around_door_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[4][mid_point-int(time_window*fs):mid_point+int(time_window*fs)]))\n",
    "                    mne_epoch_around_dig_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[5][mid_point-int(time_window*fs):mid_point+int(time_window*fs)]))\n",
    "\n",
    "        if len(all_channels)>0:\n",
    "            fs=2000\n",
    "            freqs = np.arange(1,100)\n",
    "            n_cycles = freqs/3\n",
    "            info = mne.create_info(ch_names=list(all_channels), sfreq=fs, ch_types='eeg')\n",
    "            mne_baseline = mne.EpochsArray(mne_baseline_data, info)\n",
    "            mne_epoch_door_before = mne.EpochsArray(mne_epoch_door_before, info)\n",
    "            mne_epoch_door_after= mne.EpochsArray(mne_epoch_door_after, info)\n",
    "            mne_epoch_dig_before = mne.EpochsArray(mne_epoch_dig_before, info)\n",
    "            mne_epoch_dig_after = mne.EpochsArray(mne_epoch_dig_after, info)\n",
    "            mne_epoch_around_door = mne.EpochsArray(mne_epoch_around_door, info)\n",
    "            mne_epoch_around_dig = mne.EpochsArray(mne_epoch_around_dig, info)\n",
    "            \n",
    "            row_list=[file_num,date,rat_id,task,mne_baseline,mne_epoch_door_before,mne_epoch_door_after,mne_epoch_dig_before,mne_epoch_dig_after,mne_epoch_around_door,mne_epoch_around_dig]\n",
    "            \n",
    "            mne_baseline_shuffled = mne.EpochsArray(mne_baseline_data_shuffled, info)\n",
    "            mne_epoch_door_before_shuffled = mne.EpochsArray(mne_epoch_door_before_shuffled, info)\n",
    "            mne_epoch_door_after_shuffled = mne.EpochsArray(mne_epoch_door_after_shuffled, info)\n",
    "            mne_epoch_dig_before_shuffled = mne.EpochsArray(mne_epoch_dig_before_shuffled, info)\n",
    "            mne_epoch_dig_after_shuffled = mne.EpochsArray(mne_epoch_dig_after_shuffled, info)\n",
    "            mne_epoch_around_door_shuffled = mne.EpochsArray(mne_epoch_around_door_shuffled, info)\n",
    "            mne_epoch_around_dig_shuffled = mne.EpochsArray(mne_epoch_around_dig_shuffled, info)\n",
    "            row_list_shuffled=[file_num,date,rat_id,task,mne_baseline_shuffled,mne_epoch_door_before_shuffled,mne_epoch_door_after_shuffled,mne_epoch_dig_before_shuffled,mne_epoch_dig_after_shuffled,mne_epoch_around_door_shuffled,mne_epoch_around_dig_shuffled]\n",
    "            shuffled_event_data_df.append(row_list_shuffled)\n",
    "\n",
    "            con_data_df.append(row_list)\n",
    "            con_data_df_shuffled.append(row_list_shuffled)\n",
    "\n",
    "\n",
    "con_data_df=pd.DataFrame(con_data_df, columns=['experiment','date','rat_id','task','mne_baseline','mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after','mne_epoch_around_door','mne_epoch_around_dig'])\n",
    "con_data_df.to_pickle(savepath+f'raw_mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "con_data_df_shuffled=pd.DataFrame(con_data_df_shuffled, columns=['experiment','date','rat_id','task','mne_baseline','mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after','mne_epoch_around_door','mne_epoch_around_dig'])\n",
    "con_data_df_shuffled.to_pickle(savepath+f'raw_mne_epochs_array_df_shuffled_truncated_{int(time_window*fs)}.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c530c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df = pd.read_pickle(savepath+f'raw_mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f4958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_epoch = con_data_df.iloc[0]['mne_epoch_dig_before']\n",
    "aon_indices = [i for i, ch in enumerate(test_epoch.ch_names) if 'AON' in ch]\n",
    "vHp_indices = [i for i, ch in enumerate(test_epoch.ch_names) if 'vHp' in ch]\n",
    "test_epoch.plot(events=True, scalings='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90d76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(con_data_df.shape[0]\n",
    ")\n",
    "for row in range(con_data_df.shape[0]):\n",
    "    epoch = con_data_df.iloc[row]['mne_epoch_dig_before']\n",
    "    epoch.plot(events=True, scalings='auto')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c6b31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df.to_pickle(savepath + f'marked_mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0613973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(con_data_df.shape[0]):\n",
    "    epoch = con_data_df.iloc[row]['mne_epoch_door_before']\n",
    "    epoch.plot(events=True, scalings='auto')\n",
    "for row in range(con_data_df.shape[0]):\n",
    "    epoch = con_data_df.iloc[row]['mne_epoch_dig_after']\n",
    "    epoch.plot(events=True, scalings='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b76bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df_marked = pd.read_pickle(savepath + f'marked_mne_epochs_array_df_truncated_{int(time_window*fs)}')\n",
    "\n",
    "for row in range(con_data_df_marked.shape[0]):\n",
    "    epoch = con_data_df_marked.iloc[row]['mne_epoch_door_before']\n",
    "    epoch.plot(events=True, scalings='auto')\n",
    "for row in range(con_data_df_marked.shape[0]):\n",
    "    epoch = con_data_df_marked.iloc[row]['mne_epoch_dig_after']\n",
    "    epoch.plot(events=True, scalings='auto')\n",
    "    \n",
    "con_data_df_marked.to_pickle(savepath + f'marked_mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ae7484",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(con_data_df_marked.shape[0]):\n",
    "    epoch = con_data_df_marked.iloc[row]['mne_epoch_dig_after']\n",
    "    epoch2 = con_data_df_marked.iloc[row]['mne_epoch_door_before']\n",
    "    epoch3 = con_data_df_marked.iloc[row]['mne_epoch_dig_before']    \n",
    "    print(epoch, epoch2, epoch3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99ac60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df_marked.to_pickle(savepath + f'marked_mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d26c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "marked_data = pd.read_pickle(savepath + f'marked_mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "for row in range(marked_data.shape[0]):\n",
    "    epoch = marked_data.iloc[row]['mne_epoch_dig_after']\n",
    "    epoch2 = marked_data.iloc[row]['mne_epoch_door_before']\n",
    "    epoch3 = marked_data.iloc[row]['mne_epoch_dig_before']    \n",
    "    print(epoch, epoch2, epoch3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d4ce3900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mne_epoch_door_before', 'mne_epoch_door_after', 'mne_epoch_dig_before', 'mne_epoch_dig_after']\n",
      "15\n",
      "Setting up band-stop filter from 60 - 60 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 60.00\n",
      "- Lower transition bandwidth: 15.00 Hz (-6 dB cutoff frequency: 52.50 Hz)\n",
      "- Upper passband edge: 60.00 Hz\n",
      "- Upper transition bandwidth: 15.00 Hz (-6 dB cutoff frequency: 67.50 Hz)\n",
      "- Filter length: 441 samples (0.221 s)\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Stop bands are not sufficiently separated.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 80\u001b[0m\n\u001b[0;32m     77\u001b[0m     all_con_data_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_con_data, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m event_list)\n\u001b[0;32m     78\u001b[0m     all_con_data_df\u001b[38;5;241m.\u001b[39mto_pickle(savepath\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmarked_coherence_spectrogram_before_after_door_dig_truncated_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mint\u001b[39m(time_window\u001b[38;5;241m*\u001b[39mfs), suffix))\n\u001b[1;32m---> 80\u001b[0m \u001b[43mcoherogram_pkl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtanh_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtanh_norm\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[47], line 47\u001b[0m, in \u001b[0;36mcoherogram_pkl\u001b[1;34m(time_window, fs, tanh_norm)\u001b[0m\n\u001b[0;32m     44\u001b[0m freqs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(fmin,fmax)\n\u001b[0;32m     45\u001b[0m n_cycles \u001b[38;5;241m=\u001b[39m freqs\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[1;32m---> 47\u001b[0m \u001b[43mevent_epoch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mh_freq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m con \u001b[38;5;241m=\u001b[39m mne_connectivity\u001b[38;5;241m.\u001b[39mspectral_connectivity_epochs(event_epoch, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoh\u001b[39m\u001b[38;5;124m'\u001b[39m, sfreq\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(fs),\n\u001b[0;32m     50\u001b[0m                                     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcwt_morlet\u001b[39m\u001b[38;5;124m'\u001b[39m, cwt_freqs\u001b[38;5;241m=\u001b[39mfreqs,\n\u001b[0;32m     51\u001b[0m                                     cwt_n_cycles\u001b[38;5;241m=\u001b[39mn_cycles, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, fmin\u001b[38;5;241m=\u001b[39mfmin, fmax\u001b[38;5;241m=\u001b[39mfmax, faverage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m########TRYING HIGHPASS FILTERING FOR ARTIFACT REMOVAL################\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# epoch_highpass = event_epoch.copy().filter(l_freq = 1, h_freq=None, filter_length = \"0.7s\" )\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# con = mne_connectivity.spectral_connectivity_epochs(epoch_highpass, method='coh', sfreq=int(fs),\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m#                                     mode='cwt_morlet', cwt_freqs=freqs,\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m#                                     cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False)\u001b[39;00m\n",
      "File \u001b[1;32m<decorator-gen-101>:12\u001b[0m, in \u001b[0;36mfilter\u001b[1;34m(self, l_freq, h_freq, picks, filter_length, l_trans_bandwidth, h_trans_bandwidth, n_jobs, method, iir_params, phase, fir_window, fir_design, skip_by_annotation, pad, verbose)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages\\mne\\filter.py:2561\u001b[0m, in \u001b[0;36mFilterMixin.filter\u001b[1;34m(self, l_freq, h_freq, picks, filter_length, l_trans_bandwidth, h_trans_bandwidth, n_jobs, method, iir_params, phase, fir_window, fir_design, skip_by_annotation, pad, verbose)\u001b[0m\n\u001b[0;32m   2557\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m si, (start, stop) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(onsets, ends)):\n\u001b[0;32m   2558\u001b[0m     \u001b[38;5;66;03m# Only output filter params once (for info level), and only warn\u001b[39;00m\n\u001b[0;32m   2559\u001b[0m     \u001b[38;5;66;03m# once about the length criterion (longest segment is too short)\u001b[39;00m\n\u001b[0;32m   2560\u001b[0m     use_verbose \u001b[38;5;241m=\u001b[39m verbose \u001b[38;5;28;01mif\u001b[39;00m si \u001b[38;5;241m==\u001b[39m max_idx \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 2561\u001b[0m     \u001b[43mfilter_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2562\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2563\u001b[0m \u001b[43m        \u001b[49m\u001b[43ms_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2564\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mh_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpicks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilter_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2568\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml_trans_bandwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mh_trans_bandwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2572\u001b[0m \u001b[43m        \u001b[49m\u001b[43miir_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mphase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mphase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfir_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfir_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfir_design\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfir_design\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2580\u001b[0m \u001b[38;5;66;03m# update info if filter is applied to all data channels/vertices,\u001b[39;00m\n\u001b[0;32m   2581\u001b[0m \u001b[38;5;66;03m# and it's not a band-stop filter\u001b[39;00m\n\u001b[0;32m   2582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, _BaseSourceEstimate):\n",
      "File \u001b[1;32m<decorator-gen-96>:12\u001b[0m, in \u001b[0;36mfilter_data\u001b[1;34m(data, sfreq, l_freq, h_freq, picks, filter_length, l_trans_bandwidth, h_trans_bandwidth, n_jobs, method, iir_params, copy, phase, fir_window, fir_design, pad, verbose)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages\\mne\\filter.py:1012\u001b[0m, in \u001b[0;36mfilter_data\u001b[1;34m(data, sfreq, l_freq, h_freq, picks, filter_length, l_trans_bandwidth, h_trans_bandwidth, n_jobs, method, iir_params, copy, phase, fir_window, fir_design, pad, verbose)\u001b[0m\n\u001b[0;32m   1010\u001b[0m data \u001b[38;5;241m=\u001b[39m _check_filterable(data)\n\u001b[0;32m   1011\u001b[0m iir_params, method \u001b[38;5;241m=\u001b[39m _check_method(method, iir_params)\n\u001b[1;32m-> 1012\u001b[0m filt \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_filter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43msfreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43ml_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mh_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilter_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43ml_trans_bandwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mh_trans_bandwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43miir_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mphase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfir_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfir_design\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfir\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfft\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1027\u001b[0m     data \u001b[38;5;241m=\u001b[39m _overlap_add_filter(data, filt, \u001b[38;5;28;01mNone\u001b[39;00m, phase, picks, n_jobs, copy, pad)\n",
      "File \u001b[1;32m<decorator-gen-97>:12\u001b[0m, in \u001b[0;36mcreate_filter\u001b[1;34m(data, sfreq, l_freq, h_freq, filter_length, l_trans_bandwidth, h_trans_bandwidth, method, iir_params, phase, fir_window, fir_design, verbose)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\sinha\\anaconda3\\envs\\lfp\\lib\\site-packages\\mne\\filter.py:1408\u001b[0m, in \u001b[0;36mcreate_filter\u001b[1;34m(data, sfreq, l_freq, h_freq, filter_length, l_trans_bandwidth, h_trans_bandwidth, method, iir_params, phase, fir_window, fir_design, verbose)\u001b[0m\n\u001b[0;32m   1406\u001b[0m                 gain \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mr_[gain, [\u001b[38;5;241m1.0\u001b[39m]]\n\u001b[0;32m   1407\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(np\u001b[38;5;241m.\u001b[39mabs(np\u001b[38;5;241m.\u001b[39mdiff(gain, \u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m-> 1408\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStop bands are not sufficiently separated.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfir\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1410\u001b[0m     out \u001b[38;5;241m=\u001b[39m _construct_fir_filter(\n\u001b[0;32m   1411\u001b[0m         sfreq, freq, gain, filter_length, phase, fir_window, fir_design\n\u001b[0;32m   1412\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Stop bands are not sufficiently separated."
     ]
    }
   ],
   "source": [
    "##############\n",
    "\n",
    "time_window = 0.7\n",
    "fs = 2000  # Sampling frequency\n",
    "tanh_norm = True\n",
    "###############\n",
    "def coherogram_pkl(time_window, fs, tanh_norm):\n",
    "    if tanh_norm:\n",
    "        suffix ='_normalized'\n",
    "    else:\n",
    "        suffix ='_non-normalized'\n",
    "\n",
    "\n",
    "    con_data_df_clean=pd.read_pickle(savepath + f'marked_mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "    event_list=['mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after']\n",
    "\n",
    "    print(event_list)\n",
    "    BWcontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWcontext')]\n",
    "    BWnocontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWnocontext')]\n",
    "    task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "\n",
    "    all_con_data=[]\n",
    "    all_con_data_mean=[]\n",
    "    for task_num,task_name in enumerate(task_data_dict.keys()):\n",
    "            task_data=task_data_dict[task_name]\n",
    "            row=[task_name]\n",
    "            #print(row)\n",
    "            row_2=[task_name]\n",
    "            for event in event_list:\n",
    "                #print(event)\n",
    "                event_epoch_list=task_data[event]\n",
    "                aon_vHp_con=[]\n",
    "                for event_epoch in event_epoch_list:\n",
    "                        #print(row,event, event_epoch) \n",
    "                        print(event_epoch.events.shape[0])\n",
    "                        if event_epoch.events.shape[0] <5:\n",
    "                            \n",
    "                            print(f\"Skipping {event} for {task_name} due to insufficient events\")\n",
    "                            continue\n",
    "                        fmin=1\n",
    "                        fmax=100\n",
    "                        fs=2000\n",
    "                        freqs = np.arange(fmin,fmax)\n",
    "                        n_cycles = freqs/3\n",
    "\n",
    "                        event_epoch.filter(l_freq=60,h_freq = 60)\n",
    "                        \n",
    "                        con = mne_connectivity.spectral_connectivity_epochs(event_epoch, method='coh', sfreq=int(fs),\n",
    "                                                            mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                                            cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False)\n",
    "                        \n",
    "                        ########TRYING HIGHPASS FILTERING FOR ARTIFACT REMOVAL################\n",
    "                        # epoch_highpass = event_epoch.copy().filter(l_freq = 1, h_freq=None, filter_length = \"0.7s\" )\n",
    "                        # con = mne_connectivity.spectral_connectivity_epochs(epoch_highpass, method='coh', sfreq=int(fs),\n",
    "                        #                                     mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                        #                                     cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        coh = con.get_data(output='dense')\n",
    "                        indices = con.names\n",
    "                        \n",
    "\n",
    "                        for i in range(coh.shape[0]):\n",
    "                            for j in range(coh.shape[1]):\n",
    "                                if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                                    coherence= coh[i,j,:,:]\n",
    "                                    if tanh_norm:\n",
    "                                        coherence=np.arctanh(coherence)\n",
    "                                    aon_vHp_con.append(coherence)\n",
    "                row.append(np.mean(aon_vHp_con, axis=0))\n",
    "                row_2.append(np.mean(aon_vHp_con))\n",
    "            all_con_data.append(row)                    \n",
    "            all_con_data_mean.append(row_2)\n",
    "    # Convert all_con_data to a DataFrame for easier manipulation\n",
    "    all_con_data_df = pd.DataFrame(all_con_data, columns=['task'] + event_list)\n",
    "    all_con_data_df.to_pickle(savepath+'marked_coherence_spectrogram_before_after_door_dig_truncated_{}{}.pkl'.format(int(time_window*fs), suffix))\n",
    "\n",
    "coherogram_pkl(time_window=time_window, fs=fs, tanh_norm=tanh_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b638786c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sinha\\AppData\\Local\\Temp\\ipykernel_28412\\2613225508.py:18: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  vmin = all_con_data_df[event_list].applymap(np.min).min().min()\n",
      "C:\\Users\\sinha\\AppData\\Local\\Temp\\ipykernel_28412\\2613225508.py:19: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  vmax = all_con_data_df[event_list].applymap(np.max).max().max()\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "\n",
    "time_window = 0.7\n",
    "fs = 2000  # Sampling frequency\n",
    "tanh_norm = True\n",
    "###############\n",
    "\n",
    "if tanh_norm:\n",
    "    suffix ='_normalized'\n",
    "else:\n",
    "    suffix ='_non-normalized'\n",
    "\n",
    "all_con_data_df=pd.read_pickle(savepath+'marked_coherence_spectrogram_before_after_door_dig_truncated_{}{}.pkl'.format(int(time_window*fs), suffix))\n",
    "event_list=['mne_epoch_door_before','mne_epoch_dig_before','mne_epoch_dig_after']\n",
    "\n",
    "times=np.arange(0, time_window, 1/fs)\n",
    "fig, axs=plt.subplots(2,3, figsize=(15,10), sharey=True)\n",
    "vmin = all_con_data_df[event_list].applymap(np.min).min().min()\n",
    "vmax = all_con_data_df[event_list].applymap(np.max).max().max()\n",
    "event_names=['Before Door','Before Dig','After Dig']\n",
    "for i, event in enumerate(event_list):\n",
    "    axs[0,i].imshow(all_con_data_df[event][0], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[0,i].set_xlabel('Time (s)')\n",
    "    axs[0,i].set_ylabel('Frequency (Hz)')\n",
    "    axs[0,i].set_title(event_names[i])\n",
    "\n",
    "    axs[1,i].imshow(all_con_data_df[event][1], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[1,i].set_xlabel('Time (s)')\n",
    "    axs[1,i].set_ylabel('Frequency (Hz)')\n",
    "    axs[1,i].set_title(event_names[i])\n",
    "    axs[0,0].text(-0.3, 0.5, 'Context', transform=axs[0,0].transAxes, fontsize=14, verticalalignment='center', rotation=90)\n",
    "    axs[1,0].text(-0.3, 0.5, 'No Context', transform=axs[1,0].transAxes, fontsize=14, verticalalignment='center', rotation=90)\n",
    "    # Add a colorbar\n",
    "cbar = fig.colorbar(axs[0,0].images[0], ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "cbar.set_label('Coherence (A.U.)', fontsize=12)\n",
    "fig.savefig(savepath+f'marked_coherence_spectrogram_before_after_door_dig_{int(time_window*fs/2)}ms{suffix}.png', dpi=1200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405c0869",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#fig = test_epoch.compute_psd(fmin=1, fmax=100, method='multitaper').plot(average=True, amplitude=False, picks=\"data\")\n",
    "test_epoch.plot_image(combine=\"mean\", picks=aon_indices)\n",
    "test_epoch.plot_image(combine=\"mean\", picks=vHp_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7176730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903d6902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "import mne\n",
    "import mne_connectivity\n",
    "#files=[f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230616_dk6_BW_context_day2.mat']\n",
    "event_data_df=[]\n",
    "con_data_df=[]\n",
    "\n",
    "con_data_df_shuffled=[]\n",
    "shuffled_event_data_df=[]\n",
    "events_codes_all = {}\n",
    "random_baseline_data=[]\n",
    "baseline_lfp_all=[]\n",
    "\n",
    "for file_num,file in enumerate(files):\n",
    "    #if 'dk1' in file:\n",
    "        \n",
    "        print(file)\n",
    "        base_name = os.path.basename(file)\n",
    "        base_name, _ = os.path.splitext(base_name)\n",
    "\n",
    "        date, rat_id, task = lfp_pre_processing_functions.exp_params(base_name)\n",
    "        print(date, rat_id, task)\n",
    "        if task == 'nocontextday2' or task == 'nocontextos2':\n",
    "            task = 'nocontext'\n",
    "        if task =='nocontext':\n",
    "            continue\n",
    "\n",
    "        if rat_id=='dk3':\n",
    "            continue\n",
    "        f = h5py.File(file, 'r')\n",
    "        channels = list(f.keys())\n",
    "        print(channels)\n",
    "         \n",
    "        if not any(\"AON\" in channel or \"vHp\" in channel for channel in channels):\n",
    "            print(\"No AON or vHp channels in this file\")\n",
    "            continue\n",
    "\n",
    "        events,reference_electrode=lfp_pre_processing_functions.get_keyboard_and_ref_channels(f,channels)\n",
    "\n",
    "    #finding global start and end time of all channels, since they start and end recordings at different times\n",
    "        global_start_time, global_end_time=lfp_pre_processing_functions.find_global_start_end_times(f,channels)\n",
    "        \n",
    "        ## Reference electrode finding and padding\n",
    "        reference_time = np.array(reference_electrode['times']).flatten()\n",
    "        reference_value = np.array(reference_electrode['values']).flatten()\n",
    "        padd_ref_data,padded_ref_time=lfp_pre_processing_functions.pad_raw_data_raw_time(reference_value,reference_time,global_start_time,global_end_time,sampling_rate=2000)\n",
    "        print(padd_ref_data, 'Reference electrode data padded to global start and end time')\n",
    "        events_codes = np.array(events['codes'][0])\n",
    "        events_times = np.array(events['times'][0])\n",
    "        print(events_codes.shape, events_times.shape, 'Keyboard')\n",
    "        print(events_codes)\n",
    "        events_codes_all[base_name] = events_codes\n",
    "        print(events_codes_all)\n",
    "        epochs = lfp_pre_processing_functions.generate_epochs_with_first_event(events_codes, events_times)\n",
    "        #epochs = functions.generate_specific_num_of_epochs_with_first_event(events_codes, events_times,5)\n",
    "        aon_lfp_channels=[x for x in channels if 'AON' in x ]\n",
    "        vHp_lfp_channels=[x for x in channels if 'vHp' in x ]\n",
    "        data_channels=np.concatenate((aon_lfp_channels,vHp_lfp_channels))\n",
    "        #print(all_channels)\n",
    "        # Convert events_codes to strings for annotation descriptions\n",
    "        annotation_descriptions = [str(code) for code in events_codes]\n",
    "        annotations = mne.Annotations(onset=events_times, duration=np.zeros_like(events_times), description=annotation_descriptions)\n",
    "        # Remove any keys from keyboard_dict that are not present in annotation_descriptions\n",
    "        \n",
    "        present_keys = set(annotation_descriptions) & set(keyboard_dict.keys())\n",
    "        filtered_keyboard_dict = {k: v for k, v in keyboard_dict.items() if k in present_keys}\n",
    "        if len(filtered_keyboard_dict) < len(keyboard_dict):\n",
    "            print(f\"Filtered keyboard_dict to only present annotation descriptions: {filtered_keyboard_dict}\")\n",
    "        if filtered_keyboard_dict:\n",
    "            annotations.rename(filtered_keyboard_dict)\n",
    "        else:\n",
    "            print(\"No matching annotation descriptions found for renaming.\")\n",
    "        \n",
    "        all_channels = list(data_channels) + ['reference']\n",
    "        info = mne.create_info(ch_names=list(all_channels), sfreq=2000, ch_types='eeg')\n",
    "\n",
    "        all_channels_data= []\n",
    "\n",
    "        first_event = events_times[0]\n",
    "        raw_data = []\n",
    "        for channel_num,channeli in enumerate(data_channels):\n",
    "            if \"AON\" in channeli or \"vHp\" in channeli:\n",
    "                channel_id = channeli\n",
    "                data_all = f[channeli]\n",
    "                raw_data = np.array(data_all['values']).flatten()\n",
    "                raw_time = np.array(data_all['times']).flatten()\n",
    "\n",
    "                padded_data,padded_time=lfp_pre_processing_functions.pad_raw_data_raw_time(raw_data,raw_time,global_start_time,global_end_time,2000)\n",
    "\n",
    "                print(len(padded_data), len(padded_data), channel_id)\n",
    "                all_channels_data.append(padded_data)\n",
    "            \n",
    "        all_channels_data = np.array(all_channels_data)\n",
    "        all_channels_data = np.vstack((all_channels_data, padd_ref_data))\n",
    "        print(all_channels_data.shape)\n",
    "        mne_all_channels_data = mne.io.RawArray(all_channels_data, info)\n",
    "        print(mne_all_channels_data.info)\n",
    "        #mne_all_channels_data.set_eeg_reference(ref_channels='average', projection=True, ch_type='eeg')\n",
    "        mne_all_channels_data.set_annotations(annotations)\n",
    "#mne.viz.plot_raw(mne_all_channels_data, duration=60, scalings='auto', show=True, block=True)\n",
    "        all_data_epochs= mne.Epochs(mne_all_channels_data, events=None, event_id=None, tmin=-0.7, tmax=0.7, baseline=None, preload=True, verbose=True, event_repeated='drop')\n",
    "        row= [file_num, rat_id, task, all_data_epochs]\n",
    "        event_data_df.append(row)\n",
    "event_data_df=pd.DataFrame(event_data_df, columns=['experiment','rat_id','task','epochs'])\n",
    "print(event_data_df)\n",
    "\n",
    "#                 sampling_rate = int(1 / data_all['interval'][0][0])\n",
    "#                 #print(raw_data.shape, raw_time.shape, sampling_rate)\n",
    "#                 subtracted_data = padded_data - padd_ref_data\n",
    "#                 raw_data=subtracted_data\n",
    "#                 notch_filtered_data = lfp_pre_processing_functions.iir_notch(raw_data, sampling_rate, 60)\n",
    "\n",
    "#                 data_before, time, baseline_mean, baseline_std=lfp_pre_processing_functions.baseline_data_normalization(notch_filtered_data, raw_time, first_event, sampling_rate)\n",
    "#                 first_event_index=np.where(raw_time>first_event)[0][0]\n",
    "\n",
    "#                 mne_baseline_data[0,channel_num,:]=list(data_before)\n",
    "#                 mne_baseline_data_shuffled[0,channel_num,:]=list(np.random.permutation(data_before))\n",
    "#                 total = notch_filtered_data\n",
    "\n",
    "                \n",
    "#                 for i, epochi in enumerate(epochs):\n",
    "#                     door_timestamp = epochi[0][0]\n",
    "#                     trial_type = epochi[0][1]\n",
    "#                     dig_type = epochi[1, 1]\n",
    "#                     #print(dig_type)\n",
    "#                     dig_timestamp = epochi[1, 0]\n",
    "#                     #print(door_timestamp, trial_type, dig_timestamp, dig_type)\n",
    "#                     data_trial_before, data_trial_after=lfp_pre_processing_functions.extract_event_data(notch_filtered_data,time,door_timestamp,sampling_rate,0.7)\n",
    "#                     data_dig_before, data_dig_after=lfp_pre_processing_functions.extract_event_data(notch_filtered_data,time,dig_timestamp,sampling_rate,0.7)\n",
    "#                     data_around_door=np.concatenate((data_trial_before, data_trial_after))\n",
    "#                     data_around_dig=np.concatenate((data_dig_before, data_dig_after))\n",
    "\n",
    "#                     epoch_data = [data_trial_before, data_trial_after, data_dig_before, data_dig_after, data_around_door, data_around_dig]\n",
    "#                     event_data_list = [lfp_pre_processing_functions.zscore_event_data(x, baseline_std) for x in epoch_data]\n",
    "\n",
    "#                     mne_epoch_door_before[i,channel_num,:]=list(event_data_list[0])\n",
    "#                     mne_epoch_door_after[i,channel_num,:]=list(event_data_list[1])\n",
    "#                     mne_epoch_dig_before[i,channel_num,:]=list(event_data_list[2])\n",
    "#                     mne_epoch_dig_after[i,channel_num,:]=list(event_data_list[3])\n",
    "#                     mne_epoch_around_door[i,channel_num,:]=list(event_data_list[4])\n",
    "#                     mne_epoch_around_dig[i,channel_num,:]=list(event_data_list[5])\n",
    "\n",
    "#                     mne_epoch_door_before_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[0]))\n",
    "#                     mne_epoch_door_after_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[1]))\n",
    "#                     mne_epoch_dig_before_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[2]))\n",
    "#                     mne_epoch_dig_after_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[3]))\n",
    "#                     mne_epoch_around_door_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[4]))\n",
    "#                     mne_epoch_around_dig_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[5]))\n",
    "\n",
    "#         if len(all_channels)>0:\n",
    "#             fs=2000\n",
    "#             freqs = np.arange(1,100)\n",
    "#             n_cycles = freqs/3\n",
    "#             info = mne.create_info(ch_names=list(all_channels), sfreq=fs, ch_types='eeg')\n",
    "#             mne_baseline = mne.EpochsArray(mne_baseline_data, info)\n",
    "#             mne_epoch_door_before = mne.EpochsArray(mne_epoch_door_before, info)\n",
    "#             mne_epoch_door_after= mne.EpochsArray(mne_epoch_door_after, info)\n",
    "#             mne_epoch_dig_before = mne.EpochsArray(mne_epoch_dig_before, info)\n",
    "#             mne_epoch_dig_after = mne.EpochsArray(mne_epoch_dig_after, info)\n",
    "#             mne_epoch_around_door = mne.EpochsArray(mne_epoch_around_door, info)\n",
    "#             mne_epoch_around_dig = mne.EpochsArray(mne_epoch_around_dig, info)\n",
    "            \n",
    "#             row_list=[file_num,rat_id,task,mne_baseline,mne_epoch_door_before,mne_epoch_door_after,mne_epoch_dig_before,mne_epoch_dig_after,mne_epoch_around_door,mne_epoch_around_dig]\n",
    "            \n",
    "#             mne_baseline_shuffled = mne.EpochsArray(mne_baseline_data_shuffled, info)\n",
    "#             mne_epoch_door_before_shuffled = mne.EpochsArray(mne_epoch_door_before_shuffled, info)\n",
    "#             mne_epoch_door_after_shuffled = mne.EpochsArray(mne_epoch_door_after_shuffled, info)\n",
    "#             mne_epoch_dig_before_shuffled = mne.EpochsArray(mne_epoch_dig_before_shuffled, info)\n",
    "#             mne_epoch_dig_after_shuffled = mne.EpochsArray(mne_epoch_dig_after_shuffled, info)\n",
    "#             mne_epoch_around_door_shuffled = mne.EpochsArray(mne_epoch_around_door_shuffled, info)\n",
    "#             mne_epoch_around_dig_shuffled = mne.EpochsArray(mne_epoch_around_dig_shuffled, info)\n",
    "#             row_list_shuffled=[file_num,rat_id,task,mne_baseline_shuffled,mne_epoch_door_before_shuffled,mne_epoch_door_after_shuffled,mne_epoch_dig_before_shuffled,mne_epoch_dig_after_shuffled,mne_epoch_around_door_shuffled,mne_epoch_around_dig_shuffled]\n",
    "#             shuffled_event_data_df.append(row_list_shuffled)\n",
    "\n",
    "#             con_data_df.append(row_list)\n",
    "#             con_data_df_shuffled.append(row_list_shuffled)\n",
    "\n",
    "\n",
    "# con_data_df=pd.DataFrame(con_data_df, columns=['experiment','rat_id','task','mne_baseline','mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after','mne_epoch_around_door','mne_epoch_around_dig'])\n",
    "# con_data_df.to_pickle(savepath+'mne_epochs_array_df_truncated.pkl')\n",
    "\n",
    "# con_data_df_shuffled=pd.DataFrame(con_data_df_shuffled, columns=['experiment','rat_id','task','mne_baseline','mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after','mne_epoch_around_door','mne_epoch_around_dig'])\n",
    "# con_data_df_shuffled.to_pickle(savepath+'mne_epochs_array_df_shuffled_truncated.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02171d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mne_all_channels_data = mne.io.RawArray(all_channels_data, info)\n",
    "mne_all_channels_data.plot(scalings='auto', show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f183ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_epochs.plot(n_epochs=10, events=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be07ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_epochs[\"1\"].plot_image(combine=\"mean\", colorbar=True, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffcf7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_epochs[\"1\"].plot_tfr(fmin=4, fmax=100, method='multitaper', show=True, picks=data_channels, spatial_colors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79794ad",
   "metadata": {},
   "source": [
    "## Plotting TFR of raw data (complete experiment) to check for noise in each trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ef5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfr= mne_all_channels_data.compute_tfr(method='morlet', freqs=np.arange(1, 100), n_cycles=np.arange(1, 100)/3,picks=data_channels, decim=1, n_jobs=1, verbose=True)\n",
    "print(tfr.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf313826",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfr.plot([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16f3b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the TFR for each channel\n",
    "n_channels = all_ch_data_tfr.shape[1]\n",
    "n_freqs = all_ch_data_tfr.shape[2]\n",
    "n_times = all_ch_data_tfr.shape[3]\n",
    "\n",
    "fig, axes = plt.subplots(n_channels, 1, figsize=(12, 3 * n_channels), sharex=True)\n",
    "if n_channels == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ch in range(n_channels):\n",
    "    ax = axes[ch]\n",
    "    im = ax.imshow(\n",
    "        all_ch_data_tfr[0, ch], \n",
    "        aspect='auto', \n",
    "        origin='lower',\n",
    "        extent=[times[0], times[-1], freqs[0], freqs[-1]],\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    ax.set_title(f\"TFR - {all_channels[ch]}\")\n",
    "    ax.set_ylabel(\"Frequency (Hz)\")\n",
    "    fig.colorbar(im, ax=ax, orientation='vertical', label='Power')\n",
    "\n",
    "axes[-1].set_xlabel(\"Time (s)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lfp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
