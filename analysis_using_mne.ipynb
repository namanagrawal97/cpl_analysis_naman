{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62b9d38b",
   "metadata": {},
   "source": [
    "## Loading packages and savepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6026b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import getpass\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import functions\n",
    "import lfp_pre_processing_functions\n",
    "import power_functions\n",
    "import coherence_functions\n",
    "import spectrogram_plotting_functions\n",
    "import plotting_styles\n",
    "import scipy.stats\n",
    "import mne_connectivity\n",
    "importlib.reload(functions) #loads our custom made functions.py file\n",
    "importlib.reload(spectrogram_plotting_functions)\n",
    "importlib.reload(plotting_styles)\n",
    "\n",
    "linestyle = plotting_styles.linestyles\n",
    "colors = plotting_styles.colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1c359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mne-qt-browser\n",
    "%matplotlib qt\n",
    "import mne\n",
    "print(mne.get_config())  # same as mne.get_config(key=None)\n",
    "mne.set_config('MNE_BROWSER_BACKEND', 'qt')  # set the backend to matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776d7ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch the current user\n",
    "user= (getpass.getuser())\n",
    "print(\"Hello\", user)\n",
    "\n",
    "if user == 'CPLab':\n",
    "    base='D:\\\\Dropbox\\\\CPLab'\n",
    "else:\n",
    "    base='C:\\\\Users\\\\{}\\\\Dropbox\\\\CPLab'.format(user)\n",
    "#Set the basepath, savepath and load the data files\n",
    "files = glob.glob(base+'\\\\all_data_mat_250825\\\\*.mat')\n",
    "savepath = base+'\\\\results\\\\'\n",
    "print(\"Base path:\", base)\n",
    "print(\"Save path:\", savepath)\n",
    "print(files)\n",
    "\n",
    "\n",
    "all_bands_dict = {'total':[1,100], 'beta':[12,30], 'gamma':[30,80], 'theta':[4,12]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f2f3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 1\n",
    "fs = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f222371",
   "metadata": {},
   "source": [
    "## Making MNE Epochs Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c01163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import mne_connectivity\n",
    "import sys\n",
    "importlib.reload(lfp_pre_processing_functions)\n",
    "#files=[f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230616_dk6_BW_context_day2.mat']\n",
    "event_data_df=[]\n",
    "con_data_df=[]\n",
    "\n",
    "con_data_df_shuffled=[]\n",
    "shuffled_event_data_df=[]\n",
    "events_codes_all = {}\n",
    "random_baseline_data=[]\n",
    "baseline_lfp_all=[]\n",
    "\n",
    "files_short=[files[10]] ### TEST CHANGE THIS \n",
    "\n",
    "\n",
    "for file_num,file in enumerate(files):\n",
    "    #if 'dk1' in file:\n",
    "        \n",
    "        #print(file)\n",
    "        base_name = os.path.basename(file)\n",
    "        base_name, _ = os.path.splitext(base_name)\n",
    "\n",
    "        date, rat_id, task = lfp_pre_processing_functions.exp_params(base_name)\n",
    "        print(date, rat_id, task)\n",
    "        if task == 'nocontextday2' or task == 'nocontextos2':\n",
    "            task = 'nocontext'\n",
    "        if task =='nocontext':\n",
    "            continue\n",
    "        # if rat_id=='dk1': #REMOVING DK1 TEMPORARLILY . PLEASE CHANGE LATER\n",
    "        #     continue\n",
    "        f = h5py.File(file, 'r')\n",
    "        channels = list(f.keys())\n",
    "        #print(channels)\n",
    "         \n",
    "        if not any(\"AON\" in channel or \"vHp\" in channel for channel in channels):\n",
    "            print(\"No AON or vHp channels in this file\")\n",
    "            continue\n",
    "\n",
    "        events,reference_electrode=lfp_pre_processing_functions.get_keyboard_and_ref_channels(f,channels)\n",
    "\n",
    "    #finding global start and end time of all channels, since they start and end recordings at different times\n",
    "        global_start_time, global_end_time=lfp_pre_processing_functions.find_global_start_end_times(f,channels)\n",
    "        \n",
    "        ## Reference electrode finding and padding\n",
    "        reference_time = np.array(reference_electrode['times']).flatten()\n",
    "        reference_value = np.array(reference_electrode['values']).flatten()\n",
    "        padd_ref_data,padded_ref_time=lfp_pre_processing_functions.pad_raw_data_raw_time(reference_value,reference_time,global_start_time,global_end_time,sampling_rate=2000)\n",
    "\n",
    "        events_codes = np.array(events['codes'][0])\n",
    "        events_times = np.array(events['times'][0])\n",
    "        events_codes_all[base_name] = events_codes\n",
    "        epochs = lfp_pre_processing_functions.generate_epochs_with_first_event(events_codes, events_times)\n",
    "        #epochs = functions.generate_specific_num_of_epochs_with_first_event(events_codes, events_times,5)\n",
    "        aon_lfp_channels=[x for x in channels if 'AON' in x ]\n",
    "        vHp_lfp_channels=[x for x in channels if 'vHp' in x ]\n",
    "        ref_lfp_channels=[x for x in channels if \"Ref\" in x or 'REF' in x or 'ref' in x]\n",
    "        all_channels=np.concatenate((aon_lfp_channels,vHp_lfp_channels, ref_lfp_channels))\n",
    "        #print(all_channels)\n",
    "        \n",
    "        mne_baseline_data=np.zeros((1,len(all_channels),4000))\n",
    "        mne_epoch_door_before=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_door_after=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_dig_before=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_dig_after=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_around_door=np.zeros((len(epochs),len(all_channels),int(time_window*fs)*2))\n",
    "        mne_epoch_around_dig=np.zeros((len(epochs),len(all_channels),int(time_window*fs)*2))\n",
    "        \n",
    "        mne_baseline_data_shuffled=np.zeros((1,len(all_channels),4000))\n",
    "        mne_epoch_door_before_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_door_after_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_dig_before_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_dig_after_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_around_door_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs*2)))\n",
    "        mne_epoch_around_dig_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs*2)))\n",
    "\n",
    "        print(f'File {rat_id} {task} {date} has {len(epochs)} epochs and {len(all_channels)} channels')\n",
    "\n",
    "\n",
    "        first_event = events_times[0]\n",
    "        \n",
    "        for channel_num,channeli in enumerate(all_channels):\n",
    "            if \"AON\" in channeli or \"vHp\" in channeli or \"Ref\" in channeli:\n",
    "                channel_id = channeli\n",
    "                data_all = f[channeli]\n",
    "                raw_data = np.array(data_all['values']).flatten()\n",
    "                raw_time = np.array(data_all['times']).flatten()\n",
    "                sampling_rate = int(1 / data_all['interval'][0][0])\n",
    "                #print(raw_data.shape, raw_time.shape, sampling_rate)\n",
    "                \n",
    "                padded_data,padded_time=lfp_pre_processing_functions.pad_raw_data_raw_time(raw_data,raw_time,global_start_time,global_end_time,sampling_rate)\n",
    "\n",
    "                # padded_data = padded_data/8000 #Accounting for the 8000Amplifier\n",
    "                # padd_ref_data = padd_ref_data/8000\n",
    "                if \"Ref\" in channeli:\n",
    "                    subtracted_data = padded_data\n",
    "                else:                \n",
    "                    subtracted_data = padded_data - padd_ref_data # Subtracting reference channel\n",
    "\n",
    "                notch_data = lfp_pre_processing_functions.iir_notch(subtracted_data, sampling_rate, 60) ###CHANGE notch_data to notch_filtered_data\n",
    "\n",
    "                #print(notch_data.nbytes)\n",
    "                #notch_data_detrended = scipy.signal.detrend(notch_data)\n",
    "                #notch_filtered_data=lfp_pre_processing_functions.sosbandpass(notch_data_detrended, fs=2000, start_freq=1,end_freq=100, order=8) ###CHANGE THIS FOR NOT BANDBASS FILTERTING\n",
    "                #print(notch_filtered_data.nbytes)\n",
    "                \n",
    "                data_before, time, baseline_mean, baseline_std=lfp_pre_processing_functions.baseline_data_normalization(notch_data, raw_time, first_event, sampling_rate)\n",
    "                first_event_index=np.where(raw_time>first_event)[0][0]\n",
    "\n",
    "                mne_baseline_data[0,channel_num,:]=list(data_before)\n",
    "                mne_baseline_data_shuffled[0,channel_num,:]=list(np.random.permutation(data_before))\n",
    "                total = notch_data\n",
    "\n",
    "                epoch_metadata =[]\n",
    "\n",
    "                for i, epochi in enumerate(epochs):\n",
    "                    door_timestamp = epochi[0][0]\n",
    "                    trial_type = epochi[0][1]\n",
    "                    dig_type = epochi[1, 1]\n",
    "                    #print(dig_type)\n",
    "                    dig_timestamp = epochi[1, 0]\n",
    "                    #print(door_timestamp, trial_type, dig_timestamp, dig_type)\n",
    "\n",
    "                                        # Calculate the time difference\n",
    "                    time_diff = dig_timestamp - door_timestamp\n",
    "                    \n",
    "                    # Store metadata for this epoch\n",
    "                    epoch_metadata.append({\n",
    "                        'epoch_id': i,\n",
    "                        'door_timestamp': door_timestamp,\n",
    "                        'dig_timestamp': dig_timestamp,\n",
    "                        'time_diff': time_diff,\n",
    "                        'trial_type': trial_type,\n",
    "                        'dig_type': dig_type\n",
    "                    })\n",
    "\n",
    "\n",
    "                    data_trial_before, data_trial_after=lfp_pre_processing_functions.extract_event_data(notch_data,time,door_timestamp,sampling_rate,truncation_time=time_window)\n",
    "                    data_dig_before, data_dig_after=lfp_pre_processing_functions.extract_event_data(notch_data,time,dig_timestamp,sampling_rate,truncation_time=time_window)\n",
    "                    data_around_door=np.concatenate((data_trial_before, data_trial_after))\n",
    "                    data_around_dig=np.concatenate((data_dig_before, data_dig_after))\n",
    "\n",
    "                    epoch_data = [data_trial_before, data_trial_after, data_dig_before, data_dig_after, data_around_door, data_around_dig]\n",
    "                    event_data_list = [lfp_pre_processing_functions.zscore_event_data(x, baseline_std) for x in epoch_data]\n",
    "\n",
    "                    #event_data_list = [x for x in epoch_data]\n",
    "\n",
    "                    mne_epoch_door_before[i,channel_num,:]=list(event_data_list[0][-int(time_window*fs):])\n",
    "                    mne_epoch_door_after[i,channel_num,:]=list(event_data_list[1][:int(time_window*fs)])\n",
    "                    mne_epoch_dig_before[i,channel_num,:]=list(event_data_list[2][-int(time_window*fs):])\n",
    "                    mne_epoch_dig_after[i,channel_num,:]=list(event_data_list[3][:int(time_window*fs)])\n",
    "                    mid_point = int(len(event_data_list[4]) / 2)\n",
    "                    mne_epoch_around_door[i,channel_num,:]=list(event_data_list[4][mid_point-int(time_window*fs):mid_point+int(time_window*fs)])\n",
    "                    mne_epoch_around_dig[i,channel_num,:]=list(event_data_list[5][mid_point-int(time_window*fs):mid_point+int(time_window*fs)])\n",
    "\n",
    "                    mne_epoch_door_before_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[0][-int(time_window*fs):]))\n",
    "                    mne_epoch_door_after_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[1][:int(time_window*fs)]))\n",
    "                    mne_epoch_dig_before_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[2][-int(time_window*fs):]))\n",
    "                    mne_epoch_dig_after_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[3][:int(time_window*fs)]))\n",
    "                    mne_epoch_around_door_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[4][mid_point-int(time_window*fs):mid_point+int(time_window*fs)]))\n",
    "                    mne_epoch_around_dig_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[5][mid_point-int(time_window*fs):mid_point+int(time_window*fs)]))\n",
    "\n",
    "        if len(all_channels)>0:\n",
    "            fs=2000\n",
    "            freqs = np.arange(1,100)\n",
    "            n_cycles = freqs/3\n",
    "            \n",
    "            info = mne.create_info(ch_names=list(all_channels), sfreq=fs, ch_types='eeg')\n",
    "            \n",
    "            metadata_df = pd.DataFrame(epoch_metadata)\n",
    "\n",
    "            mne_baseline = mne.EpochsArray(mne_baseline_data, info)\n",
    "            mne_epoch_door_before = mne.EpochsArray(mne_epoch_door_before, info,metadata=metadata_df)\n",
    "            mne_epoch_door_after= mne.EpochsArray(mne_epoch_door_after, info,metadata=metadata_df)\n",
    "            mne_epoch_dig_before = mne.EpochsArray(mne_epoch_dig_before, info,metadata=metadata_df)\n",
    "            mne_epoch_dig_after = mne.EpochsArray(mne_epoch_dig_after, info,metadata=metadata_df)\n",
    "            mne_epoch_around_door = mne.EpochsArray(mne_epoch_around_door, info,metadata=metadata_df)\n",
    "            mne_epoch_around_dig = mne.EpochsArray(mne_epoch_around_dig, info,metadata=metadata_df)\n",
    "            \n",
    "            events_list = [mne_epoch_door_before,mne_epoch_door_after,mne_epoch_dig_before,mne_epoch_dig_after,mne_epoch_around_door,mne_epoch_around_dig]\n",
    "            \n",
    "            row_list=[file_num,date,rat_id,task,mne_baseline,mne_epoch_door_before,mne_epoch_door_after,mne_epoch_dig_before,mne_epoch_dig_after,mne_epoch_around_door,mne_epoch_around_dig]\n",
    "            \n",
    "            mne_baseline_shuffled = mne.EpochsArray(mne_baseline_data_shuffled, info)\n",
    "            mne_epoch_door_before_shuffled = mne.EpochsArray(mne_epoch_door_before_shuffled, info)\n",
    "            mne_epoch_door_after_shuffled = mne.EpochsArray(mne_epoch_door_after_shuffled, info)\n",
    "            mne_epoch_dig_before_shuffled = mne.EpochsArray(mne_epoch_dig_before_shuffled, info)\n",
    "            mne_epoch_dig_after_shuffled = mne.EpochsArray(mne_epoch_dig_after_shuffled, info)\n",
    "            mne_epoch_around_door_shuffled = mne.EpochsArray(mne_epoch_around_door_shuffled, info)\n",
    "            mne_epoch_around_dig_shuffled = mne.EpochsArray(mne_epoch_around_dig_shuffled, info)\n",
    "            row_list_shuffled=[file_num,date,rat_id,task,mne_baseline_shuffled,mne_epoch_door_before_shuffled,mne_epoch_door_after_shuffled,mne_epoch_dig_before_shuffled,mne_epoch_dig_after_shuffled,mne_epoch_around_door_shuffled,mne_epoch_around_dig_shuffled]\n",
    "            shuffled_event_data_df.append(row_list_shuffled)\n",
    "\n",
    "            con_data_df.append(row_list)\n",
    "            con_data_df_shuffled.append(row_list_shuffled)\n",
    "\n",
    "\n",
    "con_data_df=pd.DataFrame(con_data_df, columns=['experiment','date','rat_id','task','mne_baseline','mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after','mne_epoch_around_door','mne_epoch_around_dig'])\n",
    "con_data_df.to_pickle(savepath+f'raw_mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "con_data_df_shuffled=pd.DataFrame(con_data_df_shuffled, columns=['experiment','date','rat_id','task','mne_baseline','mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after','mne_epoch_around_door','mne_epoch_around_dig'])\n",
    "con_data_df_shuffled.to_pickle(savepath+f'raw_mne_epochs_array_df_shuffled_truncated_{int(time_window*fs)}.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ef2feb",
   "metadata": {},
   "source": [
    "## Bad Epoch Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c530c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df = pd.read_pickle(savepath+f'raw_mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f4958f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_epoch = con_data_df.iloc[0]['mne_epoch_dig_before']\n",
    "aon_indices = [i for i, ch in enumerate(test_epoch.ch_names) if 'AON' in ch]\n",
    "vHp_indices = [i for i, ch in enumerate(test_epoch.ch_names) if 'vHp' in ch]\n",
    "test_epoch.plot(scalings='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b8437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_epochs_with_metadata(epochs, metadata_cols=['time_diff']):\n",
    "    \"\"\"\n",
    "    Interactive epoch viewer that displays metadata for each epoch.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    epochs : mne.EpochsArray\n",
    "        The epochs object to visualize\n",
    "    metadata_cols : list\n",
    "        List of metadata column names to display\n",
    "    \n",
    "    Usage:\n",
    "    ------\n",
    "    bad_epochs = plot_epochs_with_metadata(mne_epoch_door_before, ['time_diff', 'trial_type'])\n",
    "    # Click through epochs, press 'b' to mark as bad, arrow keys to navigate\n",
    "    # Close window when done\n",
    "    \"\"\"\n",
    "    \n",
    "    n_epochs = len(epochs)\n",
    "    current_idx = [0]  # Use list to allow modification in nested function\n",
    "    bad_epochs = set()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10), \n",
    "                            gridspec_kw={'height_ratios': [4, 1]})\n",
    "    \n",
    "    def update_plot():\n",
    "        idx = current_idx[0]\n",
    "        \n",
    "        # Clear previous plots\n",
    "        axes[0].clear()\n",
    "        axes[1].clear()\n",
    "        \n",
    "        # Plot epoch data\n",
    "        epoch_data = epochs[idx].get_data()[0]  # shape: (n_channels, n_times)\n",
    "        times = epochs.times\n",
    "        \n",
    "        for ch_idx, ch_data in enumerate(epoch_data):\n",
    "            axes[0].plot(times, ch_data + ch_idx * 5, label=epochs.ch_names[ch_idx])\n",
    "        \n",
    "        axes[0].set_xlabel('Time (s)')\n",
    "        axes[0].set_ylabel('Channels (offset)')\n",
    "        axes[0].legend(loc='upper right', fontsize=8)\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Create title with metadata\n",
    "        title = f\"Epoch {idx}/{n_epochs-1}\"\n",
    "        if epochs.metadata is not None:\n",
    "            for col in metadata_cols:\n",
    "                if col in epochs.metadata.columns:\n",
    "                    val = epochs.metadata.iloc[idx][col]\n",
    "                    title += f\" | {col}: {val:.4f}\" if isinstance(val, (int, float)) else f\" | {col}: {val}\"\n",
    "        \n",
    "        # Add bad epoch indicator\n",
    "        if idx in bad_epochs:\n",
    "            title += \" | *** MARKED BAD ***\"\n",
    "            axes[0].set_facecolor('#ffcccc')\n",
    "        else:\n",
    "            axes[0].set_facecolor('white')\n",
    "            \n",
    "        axes[0].set_title(title, fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Show metadata table in bottom subplot\n",
    "        axes[1].axis('off')\n",
    "        if epochs.metadata is not None:\n",
    "            metadata_text = \"Metadata:\\n\"\n",
    "            for col in epochs.metadata.columns:\n",
    "                val = epochs.metadata.iloc[idx][col]\n",
    "                metadata_text += f\"{col}: {val}\\n\"\n",
    "            axes[1].text(0.1, 0.5, metadata_text, fontsize=10, verticalalignment='center',\n",
    "                        family='monospace')\n",
    "        \n",
    "        # Add instructions\n",
    "        instructions = \"Navigation: ← → arrows | Mark bad: 'b' | Quit: 'q' or close window\"\n",
    "        axes[1].text(0.1, 0.05, instructions, fontsize=9, style='italic')\n",
    "        \n",
    "        fig.canvas.draw()\n",
    "    \n",
    "    def on_key(event):\n",
    "        if event.key == 'right':\n",
    "            current_idx[0] = min(current_idx[0] + 1, n_epochs - 1)\n",
    "            update_plot()\n",
    "        elif event.key == 'left':\n",
    "            current_idx[0] = max(current_idx[0] - 1, 0)\n",
    "            update_plot()\n",
    "        elif event.key == 'b':\n",
    "            idx = current_idx[0]\n",
    "            if idx in bad_epochs:\n",
    "                bad_epochs.remove(idx)\n",
    "                print(f\"Epoch {idx} unmarked as bad (Total bad: {len(bad_epochs)})\")\n",
    "            else:\n",
    "                bad_epochs.add(idx)\n",
    "                print(f\"Epoch {idx} marked as bad (Total bad: {len(bad_epochs)})\")\n",
    "            update_plot()\n",
    "        elif event.key == 'q':\n",
    "            plt.close(fig)\n",
    "    \n",
    "    fig.canvas.mpl_connect('key_press_event', on_key)\n",
    "    \n",
    "    # Initial plot\n",
    "    update_plot()\n",
    "    plt.tight_layout()\n",
    "    plt.show(block=True)  # Ensure blocking behavior\n",
    "    \n",
    "    # Return list of bad epochs (after window closes)\n",
    "    bad_list = sorted(list(bad_epochs))\n",
    "    print(f\"\\n=== FINAL RESULTS ===\")\n",
    "    print(f\"Total bad epochs marked: {len(bad_list)}\")\n",
    "    print(f\"Bad epoch indices: {bad_list}\")\n",
    "    \n",
    "    return bad_list\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "# bad_epochs = plot_epochs_with_metadata(mne_epoch_door_before, ['time_diff', 'trial_type', 'dig_type'])\n",
    "# \n",
    "# # Then mark them in your epochs object:\n",
    "# if len(bad_epochs) > 0:\n",
    "#     mne_epoch_door_before.drop(bad_epochs, reason='manual_review')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e9f992",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_epochs = plot_epochs_with_metadata(test_epoch, ['time_diff', 'trial_type', 'dig_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3f52ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bad_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c46fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Then mark them in your epochs object:\n",
    "if len(bad_epochs) > 0:\n",
    "    test_epoch.drop(bad_epochs, reason='manual_review')\n",
    "print(test_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2fd540",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df_manual.loc['mne_epoch_dig_before_manual'] = bad_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a1694c",
   "metadata": {},
   "source": [
    "### Dig Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90d76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df_manual = con_data_df.copy()\n",
    "\n",
    "print(con_data_df_manual.shape[0])\n",
    "# Create the column FIRST with empty lists\n",
    "all_bad_epochs = []\n",
    "for row in range(con_data_df_manual.shape[0]):\n",
    "    epoch1 = con_data_df_manual.iloc[row]['mne_epoch_dig_before']\n",
    "    bad_epochs = plot_epochs_with_metadata(epoch1, ['time_diff', 'trial_type', 'dig_type'])\n",
    "    all_bad_epochs.append(bad_epochs)\n",
    "\n",
    "con_data_df_manual['bad_epochs_dig_before'] = all_bad_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa5c7bd",
   "metadata": {},
   "source": [
    "### Dig After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0613973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(con_data_df_manual.shape[0])\n",
    "# Create the column FIRST with empty lists\n",
    "all_bad_epochs = []\n",
    "for row in range(con_data_df_manual.shape[0]):\n",
    "    epoch1 = con_data_df_manual.iloc[row]['mne_epoch_dig_after']\n",
    "    bad_epochs = plot_epochs_with_metadata(epoch1, ['time_diff', 'trial_type', 'dig_type'])\n",
    "    all_bad_epochs.append(bad_epochs)\n",
    "\n",
    "con_data_df_manual['bad_epochs_dig_after'] = all_bad_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f04552a",
   "metadata": {},
   "source": [
    "### Door Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5730e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(con_data_df_manual.shape[0])\n",
    "# Create the column FIRST with empty lists\n",
    "all_bad_epochs = []\n",
    "for row in range(con_data_df_manual.shape[0]):\n",
    "    epoch1 = con_data_df_manual.iloc[row]['mne_epoch_door_before']\n",
    "    bad_epochs = plot_epochs_with_metadata(epoch1, ['time_diff', 'trial_type', 'dig_type'])\n",
    "    all_bad_epochs.append(bad_epochs)\n",
    "\n",
    "con_data_df_manual['bad_epochs_door_before'] = all_bad_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c6b31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df_manual.to_pickle(savepath + f'marked_mne_epochs_array_df_truncated_{int(time_window*fs)}_231125.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b76bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df_marked = pd.read_pickle(savepath + f'marked_mne_epochs_array_df_truncated_{int(time_window*fs)}_231125.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ae7484",
   "metadata": {},
   "outputs": [],
   "source": [
    "door_before_dropped_total = 0\n",
    "dig_before_dropped_total = 0\n",
    "dig_after_dropped_total = 0\n",
    "\n",
    "for row in range(con_data_df_marked.shape[0]):\n",
    "    door_before = con_data_df_marked.iloc[row]['mne_epoch_door_before']\n",
    "    dig_before = con_data_df_marked.iloc[row]['mne_epoch_dig_before']    \n",
    "    dig_after = con_data_df_marked.iloc[row]['mne_epoch_dig_after']\n",
    "    \n",
    "    door_before_bad_epochs = con_data_df_marked.iloc[row]['bad_epochs_door_before']\n",
    "    dig_before_bad_epochs = con_data_df_marked.iloc[row]['bad_epochs_dig_before']    \n",
    "    dig_after_bad_epochs = con_data_df_marked.iloc[row]['bad_epochs_dig_after']\n",
    "    \n",
    "    door_before.drop(door_before_bad_epochs)\n",
    "    dig_before.drop(dig_before_bad_epochs)\n",
    "    dig_after.drop(dig_after_bad_epochs)\n",
    "    \n",
    "    door_before_dropped_total = door_before_dropped_total + len(door_before_bad_epochs)\n",
    "    dig_before_dropped_total = dig_before_dropped_total + len(dig_before_bad_epochs)\n",
    "    dig_after_dropped_total = dig_after_dropped_total + len(dig_after_bad_epochs)\n",
    "\n",
    "print(\"Total Door Before Epochs Dropped - \", door_before_dropped_total, \"Out of\", 20*27)\n",
    "print(\"Total Dig Before Epochs Dropped - \", dig_before_dropped_total, \"Out of\", 20*27)\n",
    "print(\"Total Dig After Epochs Dropped - \", dig_after_dropped_total, \"Out of\", 20*27)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed7e09c",
   "metadata": {},
   "source": [
    "## Coherence average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ce3900",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "\n",
    "time_window = 1\n",
    "fs = 2000  # Sampling frequency\n",
    "tanh_norm = True\n",
    "###############\n",
    "def coherogram_pkl(time_window, fs, tanh_norm, filepath):\n",
    "    if tanh_norm:\n",
    "        suffix ='_normalized'\n",
    "    else:\n",
    "        suffix ='_non-normalized'\n",
    "\n",
    "\n",
    "    con_data_df_clean=pd.read_pickle(filepath)\n",
    "\n",
    "    event_list=['mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after']\n",
    "\n",
    "    print(event_list)\n",
    "    BWcontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWcontext')]\n",
    "    BWnocontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWnocontext')]\n",
    "    task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "\n",
    "    all_con_data=[]\n",
    "    all_con_data_mean=[]\n",
    "    for task_num,task_name in enumerate(task_data_dict.keys()):\n",
    "            task_data=task_data_dict[task_name]\n",
    "            row=[task_name]\n",
    "            #print(row)\n",
    "            row_2=[task_name]\n",
    "            for event in event_list:\n",
    "                #print(event)\n",
    "                event_epoch_list=task_data[event]\n",
    "                aon_vHp_con=[]\n",
    "                for event_epoch in event_epoch_list:\n",
    "                        #print(row,event, event_epoch) \n",
    "                        print(event_epoch.events.shape[0])\n",
    "                        if event_epoch.events.shape[0] <5:\n",
    "                            \n",
    "                            print(f\"Skipping {event} for {task_name} due to insufficient events\")\n",
    "                            continue\n",
    "                        fmin=1\n",
    "                        fmax=100\n",
    "                        fs=2000\n",
    "                        freqs = np.arange(fmin,fmax)\n",
    "                        n_cycles = freqs/3\n",
    "                        \n",
    "                        con = mne_connectivity.spectral_connectivity_epochs(event_epoch, method='coh', sfreq=int(fs),\n",
    "                                                            mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                                            cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False)\n",
    "                        \n",
    "                        ########TRYING HIGHPASS FILTERING FOR ARTIFACT REMOVAL################\n",
    "                        # epoch_highpass = event_epoch.copy().filter(l_freq = 1, h_freq=None, filter_length = \"0.7s\" )\n",
    "                        # con = mne_connectivity.spectral_connectivity_epochs(epoch_highpass, method='coh', sfreq=int(fs),\n",
    "                        #                                     mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                        #                                     cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        coh = con.get_data(output='dense')\n",
    "                        indices = con.names\n",
    "                        \n",
    "\n",
    "                        for i in range(coh.shape[0]):\n",
    "                            for j in range(coh.shape[1]):\n",
    "                                if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                                    coherence= coh[i,j,:,:]\n",
    "                                    if tanh_norm:\n",
    "                                        coherence=np.arctanh(coherence)\n",
    "                                    aon_vHp_con.append(coherence)\n",
    "                row.append(np.mean(aon_vHp_con, axis=0))\n",
    "                row_2.append(np.mean(aon_vHp_con))\n",
    "            all_con_data.append(row)                    \n",
    "            all_con_data_mean.append(row_2)\n",
    "    # Convert all_con_data to a DataFrame for easier manipulation\n",
    "    all_con_data_df = pd.DataFrame(all_con_data, columns=['task'] + event_list)\n",
    "    all_con_data_df.to_pickle(savepath+'marked_coherence_spectrogram_before_after_door_dig_truncated_{}{}_231125.pkl'.format(int(time_window*fs), suffix))\n",
    "\n",
    "coherogram_pkl(time_window=time_window, fs=fs, tanh_norm=tanh_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b638786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "\n",
    "time_window = 1\n",
    "fs = 2000  # Sampling frequency\n",
    "tanh_norm = True\n",
    "\n",
    "###############\n",
    "\n",
    "if tanh_norm:\n",
    "    suffix ='_normalized'\n",
    "else:\n",
    "    suffix ='_non-normalized'\n",
    "\n",
    "all_con_data_df=pd.read_pickle(savepath+'marked_coherence_spectrogram_before_after_door_dig_truncated_{}{}_231125.pkl'.format(int(time_window*fs), suffix))\n",
    "event_list=['mne_epoch_door_before','mne_epoch_dig_before','mne_epoch_dig_after']\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "times=np.arange(0, time_window, 1/fs)\n",
    "fig, axs=plt.subplots(2,3, figsize=(15,10), sharey=True)\n",
    "vmin = all_con_data_df[event_list].applymap(np.min).min().min()\n",
    "vmax = all_con_data_df[event_list].applymap(np.max).max().max()\n",
    "event_names=['Before Door','Before Dig','After Dig']\n",
    "for i, event in enumerate(event_list):\n",
    "    axs[0,i].imshow(all_con_data_df[event][0], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[0,i].set_xlabel('Time (s)')\n",
    "    axs[0,i].set_ylabel('Frequency (Hz)')\n",
    "    axs[0,i].set_title(event_names[i])\n",
    "\n",
    "    axs[1,i].imshow(all_con_data_df[event][1], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[1,i].set_xlabel('Time (s)')\n",
    "    axs[1,i].set_ylabel('Frequency (Hz)')\n",
    "    axs[1,i].set_title(event_names[i])\n",
    "    axs[0,0].text(-0.3, 0.5, 'Context', transform=axs[0,0].transAxes, fontsize=14, verticalalignment='center', rotation=90)\n",
    "    axs[1,0].text(-0.3, 0.5, 'No Context', transform=axs[1,0].transAxes, fontsize=14, verticalalignment='center', rotation=90)\n",
    "    # Add a colorbar\n",
    "cbar = fig.colorbar(axs[0,0].images[0], ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "cbar.set_label('Coherence (A.U.)', fontsize=12)\n",
    "fig.savefig(savepath+f'marked_coherence_spectrogram_before_after_door_dig_{int(time_window*fs/2)}ms{suffix}_231125.png', dpi=1200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05828db2",
   "metadata": {},
   "source": [
    "## Coherence Per Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecb224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "time_window = 1\n",
    "fs = 2000  # Sampling frequency\n",
    "tanh_norm = True\n",
    "###############\n",
    "def coherogram_perexperiment_pkl(time_window, fs, tanh_norm):\n",
    "\n",
    "    if tanh_norm:\n",
    "        suffix ='normalized'\n",
    "    else:\n",
    "        suffix ='nonnormalized'\n",
    "\n",
    "\n",
    "\n",
    "    con_data_df_clean=con_data_df_marked\n",
    "\n",
    "    event_list=['mne_epoch_door_before','mne_epoch_dig_before','mne_epoch_dig_after']\n",
    "\n",
    "    print(event_list)\n",
    "\n",
    "    test_list = [con_data_df_clean.iloc[0]]\n",
    "    mean_con_data=pd.DataFrame()\n",
    "    def epoch_coherogram(epoch, fmin=1, fmax=100, fs=2000):\n",
    "            print(epoch.events.shape)\n",
    "        # if epoch.events.shape[0] < 5:\n",
    "        #     print(\"Not enough events in the epoch\")\n",
    "        #     return None\n",
    "        # else:\n",
    "            freqs = np.arange(fmin, fmax)\n",
    "            n_cycles = freqs / 3\n",
    "            con = mne_connectivity.spectral_connectivity_epochs(epoch, method='coh', sfreq=int(fs),\n",
    "                                                                mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                                                cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False)\n",
    "            coh = con.get_data(output='dense')\n",
    "            indices = con.names\n",
    "            aon_vHp_con = []\n",
    "            for i in range(coh.shape[0]):\n",
    "                for j in range(coh.shape[1]):\n",
    "                    if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                        coherence= coh[i,j,:,:]\n",
    "                        if tanh_norm:\n",
    "                            coherence=np.arctanh(coherence)\n",
    "                        aon_vHp_con.append(coherence)\n",
    "            \n",
    "            mean_con = np.mean(aon_vHp_con, axis=0)\n",
    "            return mean_con\n",
    "    mean_con_data['mne_epoch_door_before'] = con_data_df_clean['mne_epoch_door_before'].apply(epoch_coherogram)\n",
    "    mean_con_data['mne_epoch_dig_before'] = con_data_df_clean['mne_epoch_dig_before'].apply(epoch_coherogram)\n",
    "    mean_con_data['mne_epoch_dig_after'] = con_data_df_clean['mne_epoch_dig_after'].apply(epoch_coherogram)\n",
    "\n",
    "    mean_con_data['experiment'] = con_data_df_clean['experiment']\n",
    "    mean_con_data['date'] = con_data_df_clean['date']\n",
    "    mean_con_data['task'] = con_data_df_clean['task']\n",
    "    mean_con_data['rat_id'] = con_data_df_clean['rat_id']\n",
    "    mean_con_data.dropna(inplace=True)\n",
    "    mean_con_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    mean_con_data.to_pickle(savepath + f'marked_coherence_around_events_mean_{int(time_window*fs)}_231125.pkl')\n",
    "\n",
    "coherogram_perexperiment_pkl(time_window=time_window, fs=fs, tanh_norm=tanh_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11045761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "time_window = 1\n",
    "fs = 2000  # Sampling frequency\n",
    "tanh_norm = True\n",
    "#################\n",
    "\n",
    "event_of_interest = 'mne_epoch_door_before'\n",
    "\n",
    "def plot_coherogram_perexperiment(time_window, fs, tanh_norm):\n",
    "    if tanh_norm:\n",
    "        suffix ='normalized'\n",
    "    else:\n",
    "        suffix ='nonnormalized'\n",
    "\n",
    "    mean_con_data=pd.read_pickle(savepath + f'marked_coherence_around_events_mean_{int(time_window*fs)}_231125.pkl')\n",
    "    vmin = mean_con_data[event_of_interest].apply(np.min).min()\n",
    "    vmax = mean_con_data[event_of_interest].apply(np.max).max()\n",
    "\n",
    "    BWcontext_data=mean_con_data[(mean_con_data['task']=='BWcontext')]\n",
    "    BWnocontext_data=mean_con_data[(mean_con_data['task']=='BWnocontext')]\n",
    "    task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "    rat_ids, rat_nums = np.unique(BWcontext_data['rat_id'], return_counts=True)\n",
    "    print(rat_ids, rat_nums)\n",
    "    rat_nums_max = rat_nums.max()\n",
    "    print(rat_nums_max)\n",
    "    import matplotlib.pyplot as plt\n",
    "    writer = pd.ExcelWriter(savepath + 'marked_coherogram_perexperiment_{}_{}_231125.xlsx'.format(int(time_window*fs),suffix))\n",
    "\n",
    "    for group_name, group_df in task_data_dict.items():\n",
    "        print(f\"Plotting group: {group_name}\")\n",
    "        group_dict = {'BWcontext': 'Context', 'BWnocontext': 'No Context'}\n",
    "        rat_ids, rat_nums = np.unique(group_df['rat_id'], return_counts=True)\n",
    "        rat_nums_max = rat_nums.max()\n",
    "\n",
    "        num_of_rows = 4 # Each row should be a rats\n",
    "        num_of_cols = rat_nums_max # Each column should be the max number of experiments for a rat\n",
    "\n",
    "        fig, axs = plt.subplots(num_of_rows, num_of_cols, figsize=(25, 10), sharex=True, sharey=True)\n",
    "        dk1_count = 0\n",
    "        dk3_count = 0\n",
    "        dk5_count = 0\n",
    "        dk6_count = 0\n",
    "        for i, (idx, row) in enumerate(group_df.iterrows()):\n",
    "            rat_id = row['rat_id']\n",
    "            data = np.array(row[event_of_interest])\n",
    "            if rat_id == 'dk1':\n",
    "                ax=axs[0, dk1_count]\n",
    "                dk1_count += 1\n",
    "            elif rat_id == 'dk3':\n",
    "                ax=axs[1, dk3_count]\n",
    "                dk3_count += 1\n",
    "            elif rat_id == 'dk5':\n",
    "                ax=axs[2, dk5_count]\n",
    "                dk5_count += 1\n",
    "            elif rat_id == 'dk6':\n",
    "                ax=axs[3, dk6_count]\n",
    "                dk6_count += 1\n",
    "            im = ax.imshow(data, extent=[0, time_window, 1, 100], aspect='auto', origin='lower', cmap='jet', vmin=0, vmax=1)\n",
    "            ax.set_title(f\"{row['rat_id']} {row['date']}\")\n",
    "\n",
    "            ##### Writing to excel\n",
    "\n",
    "            freqs = [f'{int(freq)}Hz' for freq in np.linspace(1, 100, data.shape[0])]\n",
    "            freqs.insert(0, 'Frequency (Hz) / Time (s)')\n",
    "            print(len(freqs))\n",
    "            time_points = [f'{np.round(t, 3)}s' for t in np.linspace(0, time_window, data.shape[1])]\n",
    "\n",
    "            df_towrite = pd.DataFrame(data)\n",
    "            df_towrite.loc[-1] = time_points  # Add time points as the first row\n",
    "            df_towrite.index = df_towrite.index + 1  # Shift index\n",
    "            df_towrite = df_towrite.sort_index()\n",
    "            df_towrite.insert(0, 'Frequency (Hz)/ Time (s)', freqs)\n",
    "            df_towrite.to_excel(writer, sheet_name=f'{group_dict[group_name]}_{rat_id}_{row[\"date\"]}', index=False)\n",
    "\n",
    "        for j in range(i + 1, len(axs)):\n",
    "            fig.delaxes(axs[j])\n",
    "        fig.suptitle(f\"{group_dict[group_name]} Coherence {suffix} {event_of_interest}\", fontsize=16)\n",
    "        fig.colorbar(im, ax=axs, orientation='vertical', fraction=0.02, label=f'Coherence {suffix}')\n",
    "        fig.savefig(savepath + f'{group_name}_coherogram_per_experiment_{event_of_interest}_{int(time_window*fs)}_231125.png', dpi=300, bbox_inches='tight')\n",
    "        #plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.show()\n",
    "    writer.close()\n",
    "plot_coherogram_perexperiment(time_window=time_window, fs=fs, tanh_norm=tanh_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3383e89c",
   "metadata": {},
   "source": [
    "## Calculating Power and Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49135539",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix ='_normalized'\n",
    "\n",
    "mne_epochs = pd.read_pickle(savepath + f'marked_mne_epochs_array_df_truncated_{int(time_window*fs)}_181125.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b94bc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_power_tfr(epoch):\n",
    "    fmin=1\n",
    "    fmax=100\n",
    "    fs=2000\n",
    "    freqs = np.arange(fmin,fmax)\n",
    "    n_cycles = freqs/3\n",
    "\n",
    "    power = epoch.compute_tfr(\n",
    "        method=\"morlet\", freqs=freqs, n_cycles=n_cycles, return_itc=False, average=False,\n",
    "    )\n",
    "\n",
    "    return power\n",
    "results = []\n",
    "for row in mne_epochs.itertuples(index=False):\n",
    "    experiment, rat_id, task, date = row.experiment, row.rat_id, row.task, row.date\n",
    "    door_before,door_after = row.mne_epoch_door_before, row.mne_epoch_door_after\n",
    "    dig_before,dig_after = row.mne_epoch_dig_before, row.mne_epoch_dig_after\n",
    "    around_door, around_dig = row.mne_epoch_around_door, row.mne_epoch_around_dig\n",
    "\n",
    "    power_door_before = get_power_tfr(door_before)\n",
    "    power_dig_before = get_power_tfr(dig_before)\n",
    "    power_dig_after = get_power_tfr(dig_after)\n",
    "\n",
    "    #net_power = power_dig_before - power_door_before\n",
    "    channel_names = door_before.ch_names\n",
    "    new_row = [experiment, rat_id, task, date,power_door_before,power_dig_before,power_dig_after, channel_names]\n",
    "    results.append(new_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0ea58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_df = pd.DataFrame(results, columns=['experiment', 'rat_id', 'task','date', 'power_pre_door','power_pre_dig','power_post_dig', 'channel_names'])\n",
    "results_df.to_pickle(savepath + 'marked_power_tfr_epochs_mrlt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e960dd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_pickle(savepath+'marked_power_tfr_epochs_mrlt.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ef0535",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_averaged_power(epoch, area):\n",
    "    print(epoch.ch_names)\n",
    "    area_channels = [channel for channel in epoch.ch_names if area in channel]\n",
    "    #print(area_channels)\n",
    "\n",
    "    if len(area_channels)==0:\n",
    "        print(\"Error\")\n",
    "        return None\n",
    "    else:\n",
    "        area_epoch = epoch.copy()\n",
    "        area_epoch.pick(area_channels)\n",
    "        averaged_epoch_power = area_epoch.average(dim='epochs')\n",
    "        print(f\"Data shape before mean: {averaged_epoch_power.shape}\")  # DEBUG\n",
    "        mean_ch_power = np.mean(averaged_epoch_power.get_data(), axis = 0)\n",
    "        print(f\"Data shape after mean: {mean_ch_power.shape}\")  # DEBUG\n",
    "        return mean_ch_power\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371030dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_averaged_epoch_power = make_averaged_power(test_power_epoch, \"vHp\")\n",
    "# print(test_averaged_epoch_power.shape)\n",
    "\n",
    "for area in [\"AON\", \"vHp\"]:\n",
    "    area_df = pd.DataFrame()\n",
    "    fig, axs = plt.subplots(2,3, figsize= (15,10))\n",
    "    fig.suptitle(f'Average {area} Power')\n",
    "    for rowi, task in enumerate([\"BWcontext\", \"BWnocontext\"]):\n",
    "        task_data=results_df[results_df['task']==task]\n",
    "        print(f\"\\nTask: {task}, Area: {area}, Rows in task_data: {len(task_data)}\")\n",
    "        for coli, event in enumerate(['power_pre_door', 'power_pre_dig','power_post_dig']):\n",
    "            print(coli,event, task, area)\n",
    "            event_arrays = task_data[event].apply(lambda x: make_averaged_power(x, area))\n",
    "            \n",
    "            valid_arrays = [arr for arr in event_arrays.values if arr is not None]\n",
    "            \n",
    "            print(f\"Valid arrays found: {len(valid_arrays)}\")\n",
    "            \n",
    "            if len(valid_arrays) > 0:\n",
    "                averaged_array = np.mean(np.stack(valid_arrays), axis=0)\n",
    "                print(f\"Averaged array shape: {averaged_array.shape}\")\n",
    "                \n",
    "                ax = axs[rowi, coli]\n",
    "                im = ax.imshow(X= averaged_array, cmap = 'viridis', aspect='auto', origin='lower')\n",
    "                                # Add titles and labels\n",
    "                ax.set_title(f'{event.replace(\"_\", \" \").title()}')\n",
    "                ax.set_xlabel('Time (samples)')\n",
    "                ax.set_ylabel('Frequency (Hz)')\n",
    "                ax.set_xticks(list(np.arange(0,int(time_window*fs)+200,200)))\n",
    "                ax.set_xticklabels(list(np.round(np.arange(0,time_window +0.1,0.1), decimals = 1)))\n",
    "                # Add colorbar\n",
    "                plt.colorbar(im, ax=ax, label='Power (mV^2/Hz)')\n",
    "                \n",
    "                # Add row labels\n",
    "                if coli == 0:\n",
    "                    ax.set_ylabel(f'{task}\\nFrequency (Hz)', fontweight='bold')\n",
    "                # Add your plotting code here\n",
    "            else:\n",
    "                print(f\"WARNING: No valid data for {area}, {task}, {event}\")\n",
    "                ax = axs[rowi, coli]\n",
    "                ax.text(0.5, 0.5, 'No data', ha='center', va='center')\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(savepath+f'marked_power_spectrogram_{area}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb84e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_con_data = pd.read_pickle(savepath+'marked_power_tfr_epochs_mrlt.pkl')\n",
    "BWcontext_data=mean_con_data[(mean_con_data['task']=='BWcontext')]\n",
    "BWnocontext_data=mean_con_data[(mean_con_data['task']=='BWnocontext')]\n",
    "task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "rat_ids, rat_nums = np.unique(BWcontext_data['rat_id'], return_counts=True)\n",
    "print(rat_ids, rat_nums)\n",
    "rat_nums_max = rat_nums.max()\n",
    "print(rat_nums_max)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "event_of_interest = 'power_pre_dig'\n",
    "area_of_interest = \"AON\"\n",
    "\n",
    "for event_of_interest in ['power_pre_door', 'power_pre_dig', 'power_post_dig']:\n",
    "    for area_of_interest in [\"AON\", \"vHp\"]:\n",
    "\n",
    "        for group_name, group_df in task_data_dict.items():\n",
    "            print(f\"Plotting group: {group_name}\")\n",
    "            group_dict = {'BWcontext': 'Context', 'BWnocontext': 'No Context'}\n",
    "            rat_ids, rat_nums = np.unique(group_df['rat_id'], return_counts=True)\n",
    "            rat_nums_max = rat_nums.max()\n",
    "\n",
    "            num_of_rows = 4 # Each row should be a rats\n",
    "            num_of_cols = rat_nums_max # Each column should be the max number of experiments for a rat\n",
    "\n",
    "            fig, axs = plt.subplots(num_of_rows, num_of_cols, figsize=(25, 10), sharex=True, sharey=True)\n",
    "            dk1_count = 0\n",
    "            dk3_count = 0\n",
    "            dk5_count = 0\n",
    "            dk6_count = 0\n",
    "            for i, (idx, row) in enumerate(group_df.iterrows()):\n",
    "                rat_id = row['rat_id']\n",
    "                data = row[event_of_interest]\n",
    "                \n",
    "                power_data = make_averaged_power(data, area_of_interest)\n",
    "                \n",
    "                if rat_id == 'dk1':\n",
    "                    ax=axs[0, dk1_count]\n",
    "                    dk1_count += 1\n",
    "                elif rat_id == 'dk3':\n",
    "                    ax=axs[1, dk3_count]\n",
    "                    dk3_count += 1\n",
    "                elif rat_id == 'dk5':\n",
    "                    ax=axs[2, dk5_count]\n",
    "                    dk5_count += 1\n",
    "                elif rat_id == 'dk6':\n",
    "                    ax=axs[3, dk6_count]\n",
    "                    dk6_count += 1\n",
    "                im = ax.imshow(power_data, extent=[0, time_window, 1, 100], aspect='auto', origin='lower', cmap='jet')\n",
    "                ax.set_title(f\"{row['rat_id']} {row['date']}\")\n",
    "            for j in range(i + 1, len(axs)):\n",
    "                fig.delaxes(axs[j])\n",
    "            fig.suptitle(f\"{group_dict[group_name]} {area_of_interest} {event_of_interest}\", fontsize=16)\n",
    "            fig.colorbar(im, ax=axs, orientation='vertical', fraction=0.02, label=f'Power (mV^2/Hz)')\n",
    "            fig.savefig(savepath + f'powerspec_per_experiment_{group_name}_{event_of_interest}_{area_of_interest}_{int(time_window*fs)}.png', dpi=300, bbox_inches='tight')\n",
    "            #plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79220fdc",
   "metadata": {},
   "source": [
    "## Individual Experiment Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f9a6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mne_epochs = pd.read_pickle(savepath + f'marked_mne_epochs_array_df_truncated_{int(time_window*fs)}_251125.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59961db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ind_exp_coherence(epoch):\n",
    "    \n",
    "    #print(row,event, event_epoch) \n",
    "    event_epoch=epoch\n",
    "    print(event_epoch.events.shape[0])\n",
    "    if event_epoch.events.shape[0] <5:\n",
    "        print(f\"Skipping {event} due to insufficient events\")\n",
    "        raise TypeError\n",
    "    fmin=1\n",
    "    fmax=100\n",
    "    fs=2000\n",
    "    freqs = np.arange(fmin,fmax)\n",
    "    n_cycles = freqs/3\n",
    "    \n",
    "    con = mne_connectivity.spectral_connectivity_epochs(event_epoch, method='coh', sfreq=int(fs),\n",
    "                                        mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                        cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False)\n",
    "    \n",
    "    ########TRYING HIGHPASS FILTERING FOR ARTIFACT REMOVAL################\n",
    "    # epoch_highpass = event_epoch.copy().filter(l_freq = 1, h_freq=None, filter_length = \"0.7s\" )\n",
    "    # con = mne_connectivity.spectral_connectivity_epochs(epoch_highpass, method='coh', sfreq=int(fs),\n",
    "    #                                     mode='cwt_morlet', cwt_freqs=freqs,\n",
    "    #                                     cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    coh = con.get_data(output='dense')\n",
    "    indices = con.names\n",
    "    \n",
    "    aon_vHp_con =[]\n",
    "    for i in range(coh.shape[0]):\n",
    "        for j in range(coh.shape[1]):\n",
    "            if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                coherence= coh[i,j,:,:]\n",
    "                coherence=np.arctanh(coherence)\n",
    "                aon_vHp_con.append(coherence)\n",
    "    return np.mean(aon_vHp_con, axis=0)\n",
    "\n",
    "def ind_exp_power(epoch, area):\n",
    "    fmin=1\n",
    "    fmax=100\n",
    "    fs=2000\n",
    "    freqs = np.arange(fmin,fmax)\n",
    "    n_cycles = freqs/3\n",
    "\n",
    "    power = epoch.compute_tfr(\n",
    "        method=\"morlet\", freqs=freqs, n_cycles=n_cycles, return_itc=False, average=False,\n",
    "    )\n",
    "    print(epoch.ch_names)\n",
    "    area_channels = [channel for channel in epoch.ch_names if area in channel]\n",
    "    area_epoch = power.copy()\n",
    "    area_epoch.pick(area_channels)\n",
    "    averaged_epoch_power = area_epoch.average(dim='epochs')\n",
    "    averaged_channel_power = np.mean(area_epoch.get_data(), axis =1)\n",
    "    num_of_epochs = area_epoch.get_data().shape[0] \n",
    "    print(f\"Data shape before mean: {averaged_epoch_power.shape}\")  # DEBUG\n",
    "    mean_ch_power = np.mean(averaged_epoch_power.get_data(), axis = 0)\n",
    "    print(f\"Data shape after mean: {mean_ch_power.shape}\")  # DEBUG\n",
    "    return num_of_epochs,averaged_channel_power, mean_ch_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabd185e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rat =\"dk1\"\n",
    "task = \"BWnocontext\"\n",
    "date = \"20230822\"\n",
    "\n",
    "experiment_data = mne_epochs[(mne_epochs.date==date) & (mne_epochs.rat_id==rat) & (mne_epochs.task==task)]\n",
    "event_of_interest = \"mne_epoch_dig_before\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed68a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "test_epoch = experiment_data.at[experiment_data.index[0],event_of_interest]\n",
    "\n",
    "coherence = ind_exp_coherence(test_epoch)\n",
    "\n",
    "num_of_epochs,aon_power, avg_aon_power = ind_exp_power(test_epoch, \"AON\")\n",
    "#print(aon_power)\n",
    "\n",
    "num_of_epochs,vhp_power, avg_vhp_power = ind_exp_power(test_epoch, \"vHp\")\n",
    "#print(vhp_power)\n",
    "\n",
    "fig, axs = plt.subplots(1,3, figsize=(15,5))\n",
    "fig.suptitle(f'{rat} {task} {date} {event_of_interest}')\n",
    "axs=axs.flatten()\n",
    "\n",
    "im0 = axs[0].imshow(coherence, extent=[0, time_window, 1, 100], aspect='auto', origin='lower', cmap='jet')\n",
    "axs[0].set_title(\"Coherence\")\n",
    "plt.colorbar(im0, ax=axs[0])\n",
    "\n",
    "im1 = axs[1].imshow(avg_aon_power, extent=[0, time_window, 1, 100], aspect='auto', origin='lower', cmap='jet')\n",
    "axs[1].set_title(\"AON Power\")\n",
    "plt.colorbar(im1, ax=axs[1])\n",
    "\n",
    "im2 = axs[2].imshow(avg_vhp_power, extent=[0, time_window, 1, 100], aspect='auto', origin='lower', cmap='jet')\n",
    "axs[2].set_title(\"vHp Power\")\n",
    "plt.colorbar(im2, ax=axs[2])\n",
    "\n",
    "fig2, axs2 = plt.subplots(nrows=2, ncols = num_of_epochs, figsize = (20,5))\n",
    "fig2.suptitle(f'{rat} {task} {date} {event_of_interest}')\n",
    "for rowi,power in enumerate([aon_power, vhp_power]):\n",
    "    # vmin = power.apply(np.min).min()\n",
    "    # vmax = power.apply(np.max).max()\n",
    "    for coli in range(power.shape[0]):\n",
    "        ax = axs2[rowi, coli]\n",
    "        im = ax.imshow(power[coli, :, :], extent=[0, time_window, 1, 100], aspect='auto', origin='lower', cmap='jet')#, vmin=vmin, vmax=vmax)\n",
    "        ax.set_title(f'{coli}')\n",
    "        if coli == 0 :\n",
    "            if rowi==0:\n",
    "                ax.text(-0.3, 0.5,\"AON Power\", transform=ax.transAxes, fontsize=10, verticalalignment='center', rotation=90)\n",
    "            else:\n",
    "                ax.text(-0.3, 0.5,\"vHp Power\", transform=ax.transAxes, fontsize=10, verticalalignment='center', rotation=90)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02835ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_epoch.plot(scalings=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f60acfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "coherence = ind_exp_coherence(test_epoch)\n",
    "\n",
    "num_of_epochs,aon_power, avg_aon_power = ind_exp_power(test_epoch, \"AON\")\n",
    "#print(aon_power)\n",
    "\n",
    "num_of_epochs,vhp_power, avg_vhp_power = ind_exp_power(test_epoch, \"vHp\")\n",
    "#print(vhp_power)\n",
    "\n",
    "fig, axs = plt.subplots(1,3, figsize=(15,5))\n",
    "fig.suptitle(f'{rat} {task} {date} {event_of_interest}')\n",
    "axs=axs.flatten()\n",
    "\n",
    "im0 = axs[0].imshow(coherence, extent=[0, time_window, 1, 100], aspect='auto', origin='lower', cmap='jet')\n",
    "axs[0].set_title(\"Coherence\")\n",
    "plt.colorbar(im0, ax=axs[0])\n",
    "\n",
    "im1 = axs[1].imshow(avg_aon_power, extent=[0, time_window, 1, 100], aspect='auto', origin='lower', cmap='jet')\n",
    "axs[1].set_title(\"AON Power\")\n",
    "plt.colorbar(im1, ax=axs[1])\n",
    "\n",
    "im2 = axs[2].imshow(avg_vhp_power, extent=[0, time_window, 1, 100], aspect='auto', origin='lower', cmap='jet')\n",
    "axs[2].set_title(\"vHp Power\")\n",
    "plt.colorbar(im2, ax=axs[2])\n",
    "\n",
    "fig2, axs2 = plt.subplots(nrows=2, ncols = num_of_epochs, figsize = (20,5))\n",
    "fig2.suptitle(f'{rat} {task} {date} {event_of_interest}')\n",
    "for rowi,power in enumerate([aon_power, vhp_power]):\n",
    "    # vmin = power.apply(np.min).min()\n",
    "    # vmax = power.apply(np.max).max()\n",
    "    for coli in range(power.shape[0]):\n",
    "        ax = axs2[rowi, coli]\n",
    "        im = ax.imshow(power[coli, :, :], extent=[0, time_window, 1, 100], aspect='auto', origin='lower', cmap='jet')#, vmin=vmin, vmax=vmax)\n",
    "        ax.set_title(f'{coli}')\n",
    "        if coli == 0 :\n",
    "            if rowi==0:\n",
    "                ax.text(-0.3, 0.5,\"AON Power\", transform=ax.transAxes, fontsize=10, verticalalignment='center', rotation=90)\n",
    "            else:\n",
    "                ax.text(-0.3, 0.5,\"vHp Power\", transform=ax.transAxes, fontsize=10, verticalalignment='center', rotation=90)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4134a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mne_epochs.to_pickle(savepath + f'marked_mne_epochs_array_df_truncated_{int(time_window*fs)}_251125.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b57505",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "\n",
    "time_window = 1\n",
    "fs = 2000  # Sampling frequency\n",
    "tanh_norm = True\n",
    "###############\n",
    "def coherogram_pkl(time_window, fs, tanh_norm):\n",
    "    if tanh_norm:\n",
    "        suffix ='_normalized'\n",
    "    else:\n",
    "        suffix ='_non-normalized'\n",
    "\n",
    "\n",
    "    con_data_df_clean=pd.read_pickle(savepath + f'marked_mne_epochs_array_df_truncated_{int(time_window*fs)}_251125.pkl')\n",
    "\n",
    "    event_list=['mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after']\n",
    "\n",
    "    print(event_list)\n",
    "    BWcontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWcontext')]\n",
    "    BWnocontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWnocontext')]\n",
    "    task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "\n",
    "    all_con_data=[]\n",
    "    all_con_data_mean=[]\n",
    "    for task_num,task_name in enumerate(task_data_dict.keys()):\n",
    "            task_data=task_data_dict[task_name]\n",
    "            row=[task_name]\n",
    "            #print(row)\n",
    "            row_2=[task_name]\n",
    "            for event in event_list:\n",
    "                #print(event)\n",
    "                event_epoch_list=task_data[event]\n",
    "                aon_vHp_con=[]\n",
    "                for event_epoch in event_epoch_list:\n",
    "                        #print(row,event, event_epoch) \n",
    "                        print(event_epoch.events.shape[0])\n",
    "                        if event_epoch.events.shape[0] <5:\n",
    "                            \n",
    "                            print(f\"Skipping {event} for {task_name} due to insufficient events\")\n",
    "                            continue\n",
    "                        fmin=1\n",
    "                        fmax=100\n",
    "                        fs=2000\n",
    "                        freqs = np.arange(fmin,fmax)\n",
    "                        n_cycles = freqs/3\n",
    "                        \n",
    "                        con = mne_connectivity.spectral_connectivity_epochs(event_epoch, method='coh', sfreq=int(fs),\n",
    "                                                            mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                                            cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False)\n",
    "                        \n",
    "                        ########TRYING HIGHPASS FILTERING FOR ARTIFACT REMOVAL################\n",
    "                        # epoch_highpass = event_epoch.copy().filter(l_freq = 1, h_freq=None, filter_length = \"0.7s\" )\n",
    "                        # con = mne_connectivity.spectral_connectivity_epochs(epoch_highpass, method='coh', sfreq=int(fs),\n",
    "                        #                                     mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                        #                                     cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        coh = con.get_data(output='dense')\n",
    "                        indices = con.names\n",
    "                        \n",
    "\n",
    "                        for i in range(coh.shape[0]):\n",
    "                            for j in range(coh.shape[1]):\n",
    "                                if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                                    coherence= coh[i,j,:,:]\n",
    "                                    if tanh_norm:\n",
    "                                        coherence=np.arctanh(coherence)\n",
    "                                    aon_vHp_con.append(coherence)\n",
    "                row.append(np.mean(aon_vHp_con, axis=0))\n",
    "                row_2.append(np.mean(aon_vHp_con))\n",
    "            all_con_data.append(row)                    \n",
    "            all_con_data_mean.append(row_2)\n",
    "    # Convert all_con_data to a DataFrame for easier manipulation\n",
    "    all_con_data_df = pd.DataFrame(all_con_data, columns=['task'] + event_list)\n",
    "    all_con_data_df.to_pickle(savepath+'marked_coherence_spectrogram_before_after_door_dig_truncated_{}{}_251125.pkl'.format(int(time_window*fs), suffix))\n",
    "\n",
    "coherogram_pkl(time_window=time_window, fs=fs, tanh_norm=tanh_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bf37a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##############\n",
    "\n",
    "time_window = 1\n",
    "fs = 2000  # Sampling frequency\n",
    "tanh_norm = True\n",
    "\n",
    "###############\n",
    "\n",
    "if tanh_norm:\n",
    "    suffix ='_normalized'\n",
    "else:\n",
    "    suffix ='_non-normalized'\n",
    "\n",
    "all_con_data_df=pd.read_pickle(savepath+'marked_coherence_spectrogram_before_after_door_dig_truncated_{}{}_251125.pkl'.format(int(time_window*fs), suffix))\n",
    "event_list=['mne_epoch_door_before','mne_epoch_dig_before','mne_epoch_dig_after']\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "times=np.arange(0, time_window, 1/fs)\n",
    "fig, axs=plt.subplots(2,3, figsize=(15,10), sharey=True)\n",
    "vmin = all_con_data_df[event_list].applymap(np.min).min().min()\n",
    "vmax = all_con_data_df[event_list].applymap(np.max).max().max()\n",
    "event_names=['Before Door','Before Dig','After Dig']\n",
    "for i, event in enumerate(event_list):\n",
    "    axs[0,i].imshow(all_con_data_df[event][0], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[0,i].set_xlabel('Time (s)')\n",
    "    axs[0,i].set_ylabel('Frequency (Hz)')\n",
    "    axs[0,i].set_title(event_names[i])\n",
    "\n",
    "    axs[1,i].imshow(all_con_data_df[event][1], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[1,i].set_xlabel('Time (s)')\n",
    "    axs[1,i].set_ylabel('Frequency (Hz)')\n",
    "    axs[1,i].set_title(event_names[i])\n",
    "    axs[0,0].text(-0.3, 0.5, 'Context', transform=axs[0,0].transAxes, fontsize=14, verticalalignment='center', rotation=90)\n",
    "    axs[1,0].text(-0.3, 0.5, 'No Context', transform=axs[1,0].transAxes, fontsize=14, verticalalignment='center', rotation=90)\n",
    "    # Add a colorbar\n",
    "cbar = fig.colorbar(axs[0,0].images[0], ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "cbar.set_label('Coherence (A.U.)', fontsize=12)\n",
    "fig.savefig(savepath+f'marked_coherence_spectrogram_before_after_door_dig_{int(time_window*fs/2)}ms{suffix}_231125.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5445a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbd74a32",
   "metadata": {},
   "source": [
    "## End of Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405c0869",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#fig = test_epoch.compute_psd(fmin=1, fmax=100, method='multitaper').plot(average=True, amplitude=False, picks=\"data\")\n",
    "test_epoch.plot_image(combine=\"mean\", picks=aon_indices)\n",
    "test_epoch.plot_image(combine=\"mean\", picks=vHp_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7176730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02171d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mne_all_channels_data = mne.io.RawArray(all_channels_data, info)\n",
    "mne_all_channels_data.plot(scalings='auto', show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f183ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_epochs.plot(n_epochs=10, events=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be07ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_epochs[\"1\"].plot_image(combine=\"mean\", colorbar=True, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffcf7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_epochs[\"1\"].plot_tfr(fmin=4, fmax=100, method='multitaper', show=True, picks=data_channels, spatial_colors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79794ad",
   "metadata": {},
   "source": [
    "## Plotting TFR of raw data (complete experiment) to check for noise in each trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ef5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfr= mne_all_channels_data.compute_tfr(method='morlet', freqs=np.arange(1, 100), n_cycles=np.arange(1, 100)/3,picks=data_channels, decim=1, n_jobs=1, verbose=True)\n",
    "print(tfr.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf313826",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfr.plot([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16f3b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the TFR for each channel\n",
    "n_channels = all_ch_data_tfr.shape[1]\n",
    "n_freqs = all_ch_data_tfr.shape[2]\n",
    "n_times = all_ch_data_tfr.shape[3]\n",
    "\n",
    "fig, axes = plt.subplots(n_channels, 1, figsize=(12, 3 * n_channels), sharex=True)\n",
    "if n_channels == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ch in range(n_channels):\n",
    "    ax = axes[ch]\n",
    "    im = ax.imshow(\n",
    "        all_ch_data_tfr[0, ch], \n",
    "        aspect='auto', \n",
    "        origin='lower',\n",
    "        extent=[times[0], times[-1], freqs[0], freqs[-1]],\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    ax.set_title(f\"TFR - {all_channels[ch]}\")\n",
    "    ax.set_ylabel(\"Frequency (Hz)\")\n",
    "    fig.colorbar(im, ax=ax, orientation='vertical', label='Power')\n",
    "\n",
    "axes[-1].set_xlabel(\"Time (s)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lfp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
