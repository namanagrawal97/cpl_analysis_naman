{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Importing packages and the functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import getpass\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import functions\n",
    "import lfp_pre_processing_functions\n",
    "import power_functions\n",
    "import coherence_functions\n",
    "import spectrogram_plotting_functions\n",
    "import plotting_styles\n",
    "import scipy.stats\n",
    "import mne_connectivity\n",
    "import mne\n",
    "importlib.reload(functions) #loads our custom made functions.py file\n",
    "importlib.reload(spectrogram_plotting_functions)\n",
    "importlib.reload(plotting_styles)\n",
    "\n",
    "linestyle = plotting_styles.linestyles\n",
    "colors = plotting_styles.colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Loading the data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code fetches the current 'user' by using getpass. Then it sets the basepath, loads the files and specifies the savepath. Note that the basepath, files and savepath need to be changed depending on where you have kept the files and where you want the results to be stored. In this case, I have set it up to be in a particular folder in my Dropbox account, which is stored locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch the current user\n",
    "user= (getpass.getuser())\n",
    "print(\"Hello\", user)\n",
    "\n",
    "if user == 'CPLab':\n",
    "    base='D:\\\\Dropbox\\\\CPLab'\n",
    "else:\n",
    "    base='C:\\\\Users\\\\{}\\\\Dropbox\\\\CPLab'.format(user)\n",
    "#Set the basepath, savepath and load the data files\n",
    "files = glob.glob(base+'\\\\all_data_mat_250825\\\\*.mat')\n",
    "savepath = base+'\\\\results\\\\'\n",
    "print(\"Base path:\", base)\n",
    "print(\"Save path:\", savepath)\n",
    "print(files)\n",
    "\n",
    "\n",
    "all_bands_dict = {'total':[1,100], 'beta':[12,30], 'gamma':[30,80], 'theta':[4,12]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting LFP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keyboard_dict={'98':'b','119':'w','120':'nc','49':'1','48':'0'} #specifying the map of keyboard annotations to their meanings.\n",
    "all_bands={'total':[1,100],'beta':[12,30], 'gamma':[30,80], 'theta':[4,12]}\n",
    "importlib.reload(lfp_pre_processing_functions) #Reloading the lfp_pre_processing_functions module to ensure we have the latest version\n",
    "#files=[f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230615_dk6_BW_context_day1.mat', f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230626_dk6_BW_nocontext_day1.mat'] #This is just for testing purposes\n",
    "time_window=1\n",
    "fs=2000\n",
    "#Initializing a few empty things to store data\n",
    "events_codes_all = {}\n",
    "compiled_data_all_epochs = []\n",
    "compiled_data_list=[]\n",
    "compiled_shuffled_data_list = []\n",
    "baseline_lfp_all = []\n",
    "normalization_comparison_all = []\n",
    "for file in files: #Looping through data files\n",
    "    \n",
    "    ## Get the date, mouse_id and task from the file name\n",
    "    base_name = os.path.basename(file)\n",
    "    base_name, _ = os.path.splitext(base_name)\n",
    "    date, mouse_id, task=lfp_pre_processing_functions.exp_params(base_name) #Using a custom made function [see functions.py]\n",
    "    print(date, mouse_id, task)\n",
    "    if task == 'nocontextday2' or task == 'nocontextos2':\n",
    "        task = 'nocontext'\n",
    "    if task =='nocontext':\n",
    "        continue\n",
    "    f=h5py.File(file, 'r')  ## Open the data file\n",
    "    channels = list(f.keys()) ## Extract channels list from the data file\n",
    "    print(channels)\n",
    "    if not any(\"AON\" in channel or \"vHp\" in channel for channel in channels):\n",
    "        continue\n",
    "    events,reference_electrode=lfp_pre_processing_functions.get_keyboard_and_ref_channels(f,channels)\n",
    "\n",
    "    events_codes=np.array(events['codes'][0]) #saving the keyboard annotations of the events (door open, door close etc.)\n",
    "    events_times=np.array(events['times'][0]) #saving when the events happened\n",
    "    events_codes_all[base_name] = events_codes #saving the codes in a dictionary to be analyzed later for events other than the ones in our keyboard_dict map\n",
    "    \n",
    "    #Generating epochs from events (epochs are basically start of a trial and end of a trial)\n",
    "    epochs=lfp_pre_processing_functions.generate_epochs_with_first_event(events_codes, events_times)\n",
    "\n",
    "    # task Start time\n",
    "    first_event=events_times[0]\n",
    "    #finding global start and end time of all channels, since they start and end recordings at different times\n",
    "    global_start_time, global_end_time=lfp_pre_processing_functions.find_global_start_end_times(f,channels)\n",
    "    \n",
    "    ## Reference electrode finding and padding\n",
    "    reference_time = np.array(reference_electrode['times']).flatten()\n",
    "    reference_value = np.array(reference_electrode['values']).flatten()\n",
    "    padd_ref_data,padded_ref_time=lfp_pre_processing_functions.pad_raw_data_raw_time(reference_value,reference_time,global_start_time,global_end_time,sampling_rate=2000)\n",
    "\n",
    "\n",
    "    for channeli in channels:\n",
    "        if \"AON\" in channeli or  \"vHp\" in channeli :\n",
    "            \n",
    "            channel_id=channeli\n",
    "            # Extracting raw data and time\n",
    "            data_all=f[channeli]\n",
    "            raw_data=np.array(data_all['values']).flatten()\n",
    "            raw_time = np.array(data_all['times']).flatten()\n",
    "            sampling_rate = 2000\n",
    "            print(channel_id)\n",
    "            print(raw_data.shape, raw_time.shape, sampling_rate)\n",
    "            \n",
    "            padded_data,padded_time=lfp_pre_processing_functions.pad_raw_data_raw_time(raw_data,raw_time,global_start_time,global_end_time,sampling_rate)\n",
    "            subtracted_data = padded_data - padd_ref_data\n",
    "            raw_data=subtracted_data\n",
    "            notch_filtered_data = lfp_pre_processing_functions.iir_notch(raw_data, sampling_rate, 60)\n",
    "            \n",
    "            data_before, time, baseline_mean, baseline_std=lfp_pre_processing_functions.baseline_data_normalization(notch_filtered_data, raw_time, first_event, sampling_rate)\n",
    "            first_event_index=np.where(raw_time>first_event)[0][0]\n",
    "\n",
    "            baseline_row=[mouse_id,task,channel_id,np.array(data_before)]\n",
    "            baseline_lfp_all.append(baseline_row)\n",
    "            normalized_data=notch_filtered_data\n",
    "\n",
    "            #Saving non-normalized data and normalized data for plotting\n",
    "            normalization_row=[mouse_id,task,channel_id,[notch_filtered_data[first_event_index:first_event_index+30*sampling_rate]],np.mean(data_before),np.std(data_before),[normalized_data[first_event_index:first_event_index+30*sampling_rate]]]\n",
    "            normalization_comparison_all.append(normalization_row)\n",
    "\n",
    "\n",
    "            for i,epochi in enumerate(epochs):\n",
    "                \n",
    "                compiled_data = pd.DataFrame() # Initializing a dataframe to store the data of a single epoch\n",
    "                compiled_shuffled_data = pd.DataFrame() # Initializing a dataframe to store the shuffled data of a single epoch\n",
    "                door_timestamp = epochi[0][0]\n",
    "                trial_type = epochi[0][1]\n",
    "                dig_type = epochi[1, 1]\n",
    "                dig_timestamp = epochi[1, 0]\n",
    "                print(door_timestamp,trial_type,dig_timestamp,dig_type)\n",
    "                \n",
    "                \n",
    "                data_complete_trial=lfp_pre_processing_functions.extract_complete_trial_data(notch_filtered_data,time,door_timestamp,dig_timestamp,sampling_rate,time_window)\n",
    "                data_trial_before, data_trial_after=lfp_pre_processing_functions.extract_event_data(notch_filtered_data,time,door_timestamp,sampling_rate,time_window)\n",
    "                data_dig_before, data_dig_after=lfp_pre_processing_functions.extract_event_data(notch_filtered_data,time,dig_timestamp,sampling_rate,time_window)\n",
    "                data_door_around=np.append(data_trial_before, data_trial_after)\n",
    "                data_dig_around=np.append(data_dig_before, data_dig_after)\n",
    "                epoch_data = [data_complete_trial, data_trial_before, data_trial_after, data_dig_before, data_dig_after, data_door_around, data_dig_around]\n",
    "                epoch_data = [lfp_pre_processing_functions.zscore_event_data(x, baseline_std) for x in epoch_data]\n",
    "                shuffled_epoch_data = [np.random.permutation(x) for x in epoch_data]  # Shuffle the epoch data\n",
    "                compiled_data = dict(rat=mouse_id, date=date, task=task, channel=channel_id, trial=i, timestamps=[door_timestamp, dig_timestamp],\n",
    "                                     side=keyboard_dict.get(str(int(trial_type)), ''), correct=keyboard_dict.get(str(int(dig_type)), ''), time=time,\n",
    "                                     **dict(zip(['complete_trial', 'pre_door', 'post_door', 'pre_dig', 'post_dig', 'around_door', 'around_dig'], epoch_data)))\n",
    "                compiled_shuffled_data = dict(rat=mouse_id, date=date, task=task, channel=channel_id, trial=i, timestamps=[door_timestamp, dig_timestamp],\n",
    "                                     side=keyboard_dict.get(str(int(trial_type)), ''), correct=keyboard_dict.get(str(int(dig_type)), ''), time=time,\n",
    "                                     **dict(zip(['complete_trial', 'pre_door', 'post_door', 'pre_dig', 'post_dig', 'around_door', 'around_dig'], shuffled_epoch_data)))\n",
    "                compiled_data_list.append(compiled_data)\n",
    "                compiled_shuffled_data_list.append(compiled_shuffled_data)\n",
    "def combine_and_save_data(data_list, name):\n",
    "    compiled_data_all_epochs = []\n",
    "    compiled_data_all_epochs.extend(data_list)\n",
    "    compiled_data_all_epochs = pd.DataFrame(compiled_data_all_epochs)\n",
    "    compiled_data_all_epochs= compiled_data_all_epochs[compiled_data_all_epochs['task']!='nocontext']\n",
    "    compiled_data_all_epochs.to_pickle(savepath+'{}.pkl'.format(name))\n",
    "\n",
    "combine_and_save_data(compiled_data_list, f'compiled_data_all_epochs_truncated_{int(time_window*fs)}')\n",
    "combine_and_save_data(compiled_shuffled_data_list, f'compiled_shuffled_data_all_epochs_truncated_{int(time_window*fs)}')\n",
    "\n",
    "baseline_lfp_all = pd.DataFrame(baseline_lfp_all, columns=['rat', 'task', 'channel', 'data'])\n",
    "baseline_lfp_all.to_pickle(savepath+'baseline_lfp_all.pkl')\n",
    "normalization_comparison_all = pd.DataFrame(normalization_comparison_all, columns=['rat', 'task', 'channel', 'non_normalized_data', 'baseline_mean', 'baseline_std', 'normalized_data'])   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Waveform Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Rat 1-100Hz around door and digging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_data_all_epochs=pd.read_pickle(savepath+'compiled_data_all_epochs.pkl')\n",
    "waveform_data_all = compiled_data_all_epochs.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform_data_all['channel'] = waveform_data_all['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "event_dictionary = {'around_door':'Before and After door open', 'around_dig': 'Before and After Digging'}\n",
    "all_bands_dict = {'total':[1,100], 'beta':[12,30], 'gamma':[30,80], 'theta':[4,12]}\n",
    "rat_list=['dk5']\n",
    "for rat in rat_list:\n",
    "    writer=pd.ExcelWriter(os.path.join(savepath, f'{rat}_waveform_data.xlsx'), engine='xlsxwriter')\n",
    "    \n",
    "    waveform_data = waveform_data_all[waveform_data_all['rat'] == rat]\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(20, 10))\n",
    "    fig.suptitle(f'{rat} LFP (1-100Hz)', fontsize=20)\n",
    "    \n",
    "    subfigs = fig.subfigures(2, 1)\n",
    "    subfigs=subfigs.flatten()\n",
    "    for subfig in subfigs:\n",
    "        subfig.patch.set_edgecolor('black')\n",
    "        subfig.patch.set_linewidth(2)\n",
    "\n",
    "    areas=['AON','vHp']\n",
    "    for outerind, area in enumerate(areas):\n",
    "        subfig=subfigs[outerind]\n",
    "        axs = subfig.subplots(1, 2)\n",
    "        subfig.suptitle(f'{area}', fontsize=16)\n",
    "        waveform_data_area = waveform_data[waveform_data['channel'] == area]\n",
    "        waveform_data_area = waveform_data_area.reset_index(drop=True)\n",
    "\n",
    "        for innerind, col in enumerate(['around_door', 'around_dig']):\n",
    "            data = np.array(waveform_data_area[col].tolist())  # Ensure data is a numpy array\n",
    "            ax = axs[innerind]  # Correct indexing for axs\n",
    "            ax.set_title(f'{event_dictionary[col]}', fontsize=16)            \n",
    "            sheet_dict={}\n",
    "            for task in (['BWcontext', 'BWnocontext']):\n",
    "                task_data = data[waveform_data_area['task'] == task]\n",
    "                \n",
    "                if len(task_data) > 0:\n",
    "                    task_data = np.array([functions.freq_band(row, all_bands_dict['total'][0], all_bands_dict['total'][1], 2000) for row in task_data])\n",
    "                    data_mean = np.mean(task_data, axis=0)\n",
    "                    data_sem = scipy.stats.sem(task_data, axis=0)\n",
    "                    time_axis = np.linspace(-0.7, 0.7, len(data_mean))\n",
    "                    ax.plot(time_axis, data_mean, color=plotting_styles.colors[task])\n",
    "                    ax.fill_between(time_axis, data_mean - data_sem, data_mean + data_sem, alpha=0.2, color=plotting_styles.colors[task])\n",
    "                    sheet_dict[f'{task}_mean'] = data_mean\n",
    "                    sheet_dict[f'{task}_sem'] = data_sem\n",
    "            sheet_dict['time'] = time_axis\n",
    "            sheet_df=pd.DataFrame(sheet_dict)\n",
    "            sheet_df.to_excel(writer, sheet_name=f'{area}_{col}', index=False)\n",
    "            ax.vlines(0, ax.get_ylim()[0], ax.get_ylim()[1], color='k', linestyle='--')\n",
    "            ax.set_xlabel('Time (s)', fontsize=14)\n",
    "            ax.set_ylabel('Amplitude (uV)', fontsize=14)\n",
    "            ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "            #ax.tick_params(axis='both', which='minor', labelsize=10)\n",
    "    #writer.close()\n",
    "    #fig.savefig(os.path.join(savepath,f' LFP_total_waveform_{rat}'), dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All rats alls bands around door and digging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "compiled_data_all_epochs = pd.read_pickle(savepath+'compiled_data_all_epochs.pkl')\n",
    "rat_list=list(np.unique(compiled_data_all_epochs['rat']))\n",
    "window = [-2, 2]  # Set the window for the waveform\n",
    "\n",
    "#band = 'total'  # Insert the band of interest\n",
    "tasks = ['BWcontext', 'BWnocontext']\n",
    "areas=['AON','vHp']\n",
    "compiled_data_all_epochs['around_door'] = compiled_data_all_epochs['pre_door'].apply(lambda x: x.tolist()) + compiled_data_all_epochs['post_door'].apply(lambda x: x.tolist())\n",
    "compiled_data_all_epochs['around_dig'] = compiled_data_all_epochs['pre_dig'].apply(lambda x: x.tolist()) + compiled_data_all_epochs['post_dig'].apply(lambda x: x.tolist())\n",
    "print(np.array(compiled_data_all_epochs['around_door'][0]).shape, np.array(compiled_data_all_epochs['around_dig'][0]).shape)\n",
    "all_bands_dict = {'total':[1,100], 'beta':[12,30], 'gamma':[30,80], 'theta':[4,12]}\n",
    "\n",
    "for rati in rat_list:\n",
    "    rat_dict = {}\n",
    "    rat_data = compiled_data_all_epochs[compiled_data_all_epochs['rat'] == rati]\n",
    "    rat_data['channel']=rat_data['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "    rat_data = rat_data.reset_index(drop=True)\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(10, 10))    \n",
    "    subfigs = fig.subfigures(2, 1)\n",
    "    subfigs=subfigs.flatten()\n",
    "    subfigs[1].set_facecolor('0.85')\n",
    "    fig.suptitle(f'{rati}')\n",
    "    \n",
    "    for outerind, area in enumerate(areas):\n",
    "        subfig=subfigs[outerind]\n",
    "        axs = subfig.subplots(4, 2)\n",
    "        \n",
    "        rat_data_area = rat_data[rat_data['channel'] == area]\n",
    "        rat_data_area = rat_data_area.reset_index(drop=True)   \n",
    "    \n",
    "        for i, band in enumerate(all_bands_dict.keys()):\n",
    "            rat_data_band=rat_data_area.__deepcopy__()\n",
    "            for col in (['around_door', 'around_dig']):\n",
    "                rat_data_band[col] = rat_data_area[col].apply(lambda x: functions.freq_band(x, all_bands_dict[band][0], all_bands_dict[band][1], 2000))\n",
    "\n",
    "            rat_data_band_grouped = rat_data_band.groupby(['task', 'channel'])\n",
    "            for (task, channel), group in rat_data_band_grouped:\n",
    "                group=group.reset_index(drop=True)\n",
    "                print(group.shape)\n",
    "                #group['around_dig']=np.concatenate([group['pre_dig'], group['post_dig']], axis=1)\n",
    "                for j, col in enumerate(['around_door', 'around_dig']):\n",
    "                    data = np.array(group[col])\n",
    "                    data_mean = np.mean(data, axis=0)\n",
    "                    data_sem = scipy.stats.sem(data, axis=0)\n",
    "                    time_axis = np.linspace(-0.7, 0.7, len(data_mean))\n",
    "                    ax = axs[i, j]\n",
    "                    ax.set_title(f'{band} {channel} {col}')\n",
    "                    ax.plot(time_axis, data_mean, color=plotting_styles.colors[task])\n",
    "                    ax.fill_between(time_axis, data_mean - data_sem, data_mean + data_sem, alpha=0.2, color=plotting_styles.colors[task])\n",
    "                    ax.vlines(0, ax.get_ylim()[0], ax.get_ylim()[1], color='k', linestyle='--')\n",
    "    #fig.savefig(os.path.join(savepath,f' LFP_waveform{rati}'), dpi=300)\n",
    "    plt.show()\n",
    "    #plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaged across rats single band "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform_data = compiled_data_all_epochs.copy()\n",
    "waveform_data['channel'] = waveform_data['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "event_dictionary = {'around_door':'Before and After door open', 'around_dig': 'Before and After Digging'}\n",
    "fig = plt.figure(constrained_layout=True, figsize=(20, 10))\n",
    "fig.suptitle(f'raw LFP averaged across rats', fontsize=20)\n",
    "\n",
    "subfigs = fig.subfigures(2, 1)\n",
    "subfigs=subfigs.flatten()\n",
    "for subfig in subfigs:\n",
    "    subfig.patch.set_edgecolor('black')\n",
    "    subfig.patch.set_linewidth(0.5)\n",
    "areas=['AON','vHp']\n",
    "for outerind, area in enumerate(areas):\n",
    "    subfig=subfigs[outerind]\n",
    "    axs = subfig.subplots(1, 2)\n",
    "    subfig.suptitle(f'{area}', fontsize= 16) \n",
    "    waveform_data_area = waveform_data[waveform_data['channel'] == area]\n",
    "    waveform_data_area = waveform_data_area.reset_index(drop=True)\n",
    "\n",
    "    for innerind, col in enumerate(['around_door', 'around_dig']):\n",
    "        data = np.array(waveform_data_area[col].tolist())  # Ensure data is a numpy array\n",
    "        ax = axs[innerind]  # Correct indexing for axs\n",
    "        ax.set_title(f'{event_dictionary[col]}', fontsize=14)\n",
    "        for task in (['BWcontext', 'BWnocontext']):\n",
    "            task_data = data[waveform_data_area['task'] == task]\n",
    "            if len(task_data) > 0:\n",
    "            \n",
    "                data_mean = np.mean(task_data, axis=0)\n",
    "                data_sem = scipy.stats.sem(task_data, axis=0)\n",
    "                time_axis = np.linspace(-2, 2, len(data_mean))\n",
    "                \n",
    "                ax.plot(time_axis, data_mean, color=plotting_styles.colors[task])\n",
    "                ax.fill_between(time_axis, data_mean - data_sem, data_mean + data_sem, alpha=0.2, color=plotting_styles.colors[task])\n",
    "        ax.vlines(0, ax.get_ylim()[0], ax.get_ylim()[1], color='k', linestyle='--')\n",
    "        ax.set_xlabel('Time (s)', fontsize=14)\n",
    "        ax.set_ylabel('Amplitude (uV)', fontsize=14)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "        ax.tick_params(axis='both', which='minor', labelsize=10)\n",
    "#fig.savefig(os.path.join(savepath,f' LFP_raw_waveform_averaged'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaged across rats all bands (To be Deleted later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform_data = compiled_data_all_epochs.copy()\n",
    "waveform_data['channel'] = waveform_data['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "waveform_data = waveform_data.reset_index(drop=True)\n",
    "fig = plt.figure(constrained_layout=True, figsize=(20, 10))\n",
    "subfigs = fig.subfigures(2, 1)\n",
    "subfigs=subfigs.flatten()\n",
    "subfigs[1].set_facecolor('0.85')\n",
    "fig.suptitle(f'Waveform')\n",
    "\n",
    "for i, band in enumerate(all_bands_dict.keys()):\n",
    "    print(band)\n",
    "\n",
    "waveform_data_grouped = waveform_data.groupby(['task', 'channel'])\n",
    "for outerind, area in enumerate(areas):\n",
    "    subfig=subfigs[outerind]\n",
    "    axs = subfig.subplots(4, 2)\n",
    "    waveform_data_area = waveform_data[waveform_data['channel'] == area]\n",
    "    waveform_data_area = waveform_data_area.reset_index(drop=True)\n",
    "    \n",
    "    for i, band in enumerate(all_bands_dict.keys()):\n",
    "        for col in (['around_door', 'around_dig']):\n",
    "            waveform_data_area[col+'_'+band] = waveform_data_area[col].apply(lambda x: functions.freq_band(x, all_bands_dict[band][0], all_bands_dict[band][1], 2000))\n",
    "\n",
    "        data = waveform_data_area[[f'around_door_{band}', f'around_dig_{band}']]\n",
    "        data_mean = data.groupby(waveform_data_area['task']).mean() \n",
    "        data_sem = data.groupby(waveform_data_area['task']).sem()\n",
    "        time_axis = np.linspace(-2, 2, len(data_mean.columns))\n",
    "        for j, task in enumerate(tasks):\n",
    "            ax = axs[i, j]\n",
    "            ax.set_title(f'{band} {task}')\n",
    "            ax.plot(time_axis, data_mean.loc[task], color=plotting_styles.colors[task])\n",
    "            ax.fill_between(time_axis, data_mean.loc[task] - data_sem.loc[task], data_mean.loc[task] + data_sem.loc[task], alpha=0.2, color=plotting_styles.colors[task])\n",
    "            ax.vlines(0, ax.get_ylim()[0], ax.get_ylim()[1], color='k', linestyle='--')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting waveform using MNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 1\n",
    "fs =2000\n",
    "lfp_data = pd.read_pickle(savepath +f'marked_mne_epochs_array_{int(time_window*fs)}.pkl')\n",
    "\n",
    "test_epoch = lfp_data['mne_epoch_door_before'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "\n",
    "epochs_filtered = test_epoch.copy().filter(\n",
    "    l_freq=4, \n",
    "    h_freq=12, \n",
    "    method='iir',\n",
    "    iir_params={'order': 4, 'ftype': 'butter'}\n",
    ")\n",
    "\n",
    "def get_epoch_lfp_single(test_epoch, time_window):\n",
    "    channels = list(test_epoch.ch_names)\n",
    "    aon_channels = [channel for channel in channels if \"AON\" in channel]\n",
    "    vhp_channels = [channel for channel in channels if \"vHp\" in channel]\n",
    "    test_epoch_aon = test_epoch.get_data(picks=aon_channels)\n",
    "    test_epoch_vhp = test_epoch.get_data(picks=vhp_channels)\n",
    "    test_epoch_aon_mean = list(np.mean(test_epoch_aon, axis=(0,1)))\n",
    "    test_epoch_vhp_mean = list(np.mean(test_epoch_vhp, axis = (0,1)))\n",
    "    times = np.linspace(0, time_window, len(test_epoch_aon_mean))\n",
    "    return test_epoch_aon_mean, test_epoch_vhp_mean, times\n",
    "\n",
    "def get_epoch_lfp_single(test_epoch, time_window, trial_num):\n",
    "    channels = list(test_epoch.ch_names)\n",
    "    aon_channels = [channel for channel in channels if \"AON\" in channel]\n",
    "    vhp_channels = [channel for channel in channels if \"vHp\" in channel]\n",
    "    test_epoch_aon = test_epoch.get_data(picks=aon_channels)\n",
    "    test_epoch_vhp = test_epoch.get_data(picks=vhp_channels)\n",
    "    test_epoch_aon = list(test_epoch_aon[trial_num,0,:])\n",
    "    test_epoch_vhp = list(test_epoch_vhp[trial_num,0,:])\n",
    "    times = list(np.linspace(0, time_window, len(test_epoch_aon)))\n",
    "    return test_epoch_aon, test_epoch_vhp, times\n",
    "\n",
    "\n",
    "total_aon, total_vhp, total_time = get_epoch_lfp_single(test_epoch, time_window=1, trial_num=0)\n",
    "beta_aon, beta_vhp, beta_time = get_epoch_lfp_single(epochs_filtered, time_window=1, trial_num=0)\n",
    "\n",
    "total_aon_single, total_vhp_single, total_time_single = get_epoch_lfp_single(test_epoch, time_window=1, trial_num=0)\n",
    "beta_aon_single, beta_vhp_single, beta_time_single = get_epoch_lfp_single(epochs_filtered, time_window=1, trial_num=0)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize =(15,5))\n",
    "\n",
    "ax.plot(total_time_single, total_aon_single, label ='total', color = 'blue')\n",
    "ax.plot(total_time_single, total_vhp_single, label =\"beta\", color='orange')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window=1\n",
    "fs=2000\n",
    "trial_num =4\n",
    "## Potential Candidates - 3, \n",
    "lfp_data = pd.read_pickle(savepath + f'marked_mne_epochs_array_{int(time_window*fs)}.pkl')\n",
    "\n",
    "dk6_bwcontext = lfp_data[(lfp_data['rat_id']=='dk6') & (lfp_data['task']=='BWcontext')]\n",
    "dk6_bwnocontext = lfp_data[(lfp_data['rat_id']=='dk6') & (lfp_data['task']=='BWnocontext')]\n",
    "\n",
    "\n",
    "event_dict = {\n",
    "    'mne_epoch_door_before': 'Before Door',\n",
    "    'mne_epoch_dig_before': 'Before Dig',\n",
    "    'mne_epoch_dig_after': 'After Dig'\n",
    "}\n",
    "all_bands_dict = {\n",
    "    'total': [1, 100],\n",
    "    'theta': [4, 12],\n",
    "    'beta': [12, 30],\n",
    "    'gamma': [30, 80]\n",
    "}\n",
    "time_window = 1.0\n",
    "\n",
    "def filter_epoch(epoch, l_freq, h_freq):\n",
    "    \"\"\"Apply bandpass filter to epoch\"\"\"\n",
    "    return epoch.copy().filter(\n",
    "        l_freq=l_freq,\n",
    "        h_freq=h_freq,\n",
    "        method='iir',\n",
    "        iir_params={'order': 4, 'ftype': 'butter'}\n",
    "    )\n",
    "    \n",
    "aon_fig, aon_axs = plt.subplots(4, 3, figsize=(15, 16), sharey='row')\n",
    "aon_fig.suptitle('AON LFP')\n",
    "aon_writer = pd.ExcelWriter(savepath + f'lfp_events_aon_{int(time_window*fs/2)}ms.xlsx')\n",
    "\n",
    "vhp_fig, vhp_axs = plt.subplots(4, 3, figsize=(15, 16), sharey='row')\n",
    "vhp_fig.suptitle('vHC LFP')\n",
    "vhp_writer = pd.ExcelWriter(savepath + f'lfp_events_vhp_{int(time_window*fs/2)}ms.xlsx')\n",
    "\n",
    "bwcontext_fig, bwcontext_axs = plt.subplots(4, 3, figsize=(15, 16), sharey='row')\n",
    "bwcontext_fig.suptitle('Context LFP')\n",
    "bwcontext_writer = pd.ExcelWriter(savepath + f'lfp_events_bwcontext_{int(time_window*fs/2)}ms.xlsx')\n",
    "\n",
    "bwnocontext_fig, bwnocontext_axs = plt.subplots(4, 3, figsize=(15, 16), sharey='row')\n",
    "bwnocontext_fig.suptitle('No Context LFP')\n",
    "bwnocontext_writer = pd.ExcelWriter(savepath + f'lfp_events_bwnocontext_{int(time_window*fs/2)}ms.xlsx')\n",
    "\n",
    "for event_idx, event in enumerate(event_dict.keys()):\n",
    "    aon_dict = {}\n",
    "    vhp_dict = {}\n",
    "    \n",
    "    bwcontext_dict = {}\n",
    "    bwnocontext_dict={}\n",
    "    \n",
    "    bwcontext_epoch_object = dk6_bwcontext[event].iloc[0]\n",
    "    bwnocontext_epoch_object = dk6_bwnocontext[event].iloc[0]\n",
    "    \n",
    "    print(bwcontext_epoch_object)\n",
    "    \n",
    "    for band_idx, (band_name, (l_freq, h_freq)) in enumerate(all_bands_dict.items()):\n",
    "        print(f\"  Processing {band_name} band ({l_freq}-{h_freq} Hz)\")\n",
    "        bwcontext_epoch_object_filtered = filter_epoch(bwcontext_epoch_object, l_freq, h_freq)\n",
    "        bwnocontext_epoch_object_filtered = filter_epoch(bwnocontext_epoch_object, l_freq, h_freq) \n",
    "        \n",
    "        aon_bwcontext,vhp_bwcontext,time_bwcontext = get_epoch_lfp_single(bwcontext_epoch_object_filtered, time_window, trial_num)\n",
    "        aon_bwnocontext,vhp_bwnocontext,time_bwnocontext = get_epoch_lfp_single(bwnocontext_epoch_object_filtered, time_window, trial_num)\n",
    "        \n",
    "        #==============Plot AON====================================\n",
    "        \n",
    "        ax_aon = aon_axs[band_idx, event_idx]\n",
    "        ax_aon.plot(time_bwcontext, aon_bwcontext, label='Context', linewidth=2, color='blue')\n",
    "        ax_aon.plot(time_bwnocontext, aon_bwnocontext, label='No Context', linewidth=2, color='Orange')\n",
    "        \n",
    "        if event_idx == 0:\n",
    "            ax_aon.set_ylabel(f'{band_name.capitalize()}\\n({l_freq}-{h_freq} Hz)\\nAmplitude (mV)', fontsize=10)\n",
    "        if band_idx == 0:\n",
    "            ax_aon.set_title(f'{event_dict[event]}', fontsize=11, fontweight='bold')\n",
    "        if band_idx == 3:\n",
    "            ax_aon.set_xlabel('Time (s)', fontsize=10)\n",
    "        ax_aon.legend(loc='best', fontsize=8)\n",
    "\n",
    "        #==============Plot vHp====================================\n",
    "        \n",
    "        ax_vhp = vhp_axs[band_idx, event_idx]\n",
    "        ax_vhp.plot(time_bwcontext, vhp_bwcontext, label='Context', linewidth=2, color='blue')\n",
    "        ax_vhp.plot(time_bwnocontext, vhp_bwnocontext, label='No Context', linewidth=2, color='Orange')\n",
    "        \n",
    "        if event_idx == 0:\n",
    "            ax_vhp.set_ylabel(f'{band_name.capitalize()}\\n({l_freq}-{h_freq} Hz)\\nAmplitude (mV)', fontsize=10)\n",
    "        if band_idx == 0:\n",
    "            ax_vhp.set_title(f'{event_dict[event]}', fontsize=11, fontweight='bold')\n",
    "        if band_idx == 3:\n",
    "            ax_vhp.set_xlabel('Time (s)', fontsize=10)\n",
    "        ax_vhp.legend(loc='best', fontsize=8)\n",
    "\n",
    "        #====================Plotting BW Context ==================\n",
    "        ax_bwcontext = bwcontext_axs[band_idx, event_idx]\n",
    "        ax_bwcontext.plot(time_bwcontext, aon_bwcontext, label='AON', linewidth=2, color='blue')\n",
    "        ax_bwcontext.plot(time_bwcontext, vhp_bwcontext, label='vHp', linewidth=2, color='Orange')\n",
    "        \n",
    "        if event_idx == 0:\n",
    "            ax_bwcontext.set_ylabel(f'{band_name.capitalize()}\\n({l_freq}-{h_freq} Hz)\\nAmplitude (mV)', fontsize=10)\n",
    "        if band_idx == 0:\n",
    "            ax_bwcontext.set_title(f'{event_dict[event]}', fontsize=11, fontweight='bold')\n",
    "        if band_idx == 3:\n",
    "            ax_bwcontext.set_xlabel('Time (s)', fontsize=10)\n",
    "        ax_bwcontext.legend(loc='best', fontsize=8)\n",
    "        #====================Plotting BW No Context ==================\n",
    "        ax_bwnocontext = bwnocontext_axs[band_idx, event_idx]\n",
    "        ax_bwnocontext.plot(time_bwnocontext, aon_bwnocontext, label='AON', linewidth=2, color='blue')\n",
    "        ax_bwnocontext.plot(time_bwnocontext, vhp_bwnocontext, label='vHp', linewidth=2, color='Orange')\n",
    "        \n",
    "        if event_idx == 0:\n",
    "            ax_bwnocontext.set_ylabel(f'{band_name.capitalize()}\\n({l_freq}-{h_freq} Hz)\\nAmplitude (mV)', fontsize=10)\n",
    "        if band_idx == 0:\n",
    "            ax_bwnocontext.set_title(f'{event_dict[event]}', fontsize=11, fontweight='bold')\n",
    "        if band_idx == 3:\n",
    "            ax_bwnocontext.set_xlabel('Time (s)', fontsize=10)\n",
    "        ax_bwnocontext.legend(loc='best', fontsize=8)\n",
    "\n",
    "        #=================Saving in AON -vHp CSV===================\n",
    "        if time_bwcontext==time_bwnocontext:\n",
    "            aon_dict['time'] = time_bwcontext\n",
    "        else:\n",
    "            print(\"WTF Why are times different\")\n",
    "            break\n",
    "        aon_dict[f'bwcontext_{band_name}'] = aon_bwcontext\n",
    "        aon_dict[f'bwnocontext_{band_name}'] = aon_bwnocontext\n",
    "        \n",
    "        if time_bwcontext==time_bwnocontext:\n",
    "            vhp_dict['time'] = time_bwcontext\n",
    "        else:\n",
    "            print(\"WTF Why are times different\")\n",
    "            break\n",
    "        vhp_dict[f'bwcontext_{band_name}'] = vhp_bwcontext\n",
    "        vhp_dict[f'bwnocontext_{band_name}'] = vhp_bwnocontext\n",
    "        \n",
    "        aon_df = pd.DataFrame(aon_dict)\n",
    "        vhp_df = pd.DataFrame(vhp_dict)\n",
    "        \n",
    "        aon_df.to_excel(aon_writer, sheet_name=event_dict[event])\n",
    "        vhp_df.to_excel(vhp_writer, sheet_name=event_dict[event])\n",
    "\n",
    "        #=================Saving in Context - NoContext CSV===================\n",
    "        if time_bwcontext==time_bwnocontext:\n",
    "            bwcontext_dict['time'] = time_bwcontext\n",
    "        else:\n",
    "            print(\"WTF Why are times different\")\n",
    "            break\n",
    "        bwcontext_dict[f'aon_{band_name}'] = aon_bwcontext\n",
    "        bwcontext_dict[f'vhp_{band_name}'] = vhp_bwcontext\n",
    "        \n",
    "        if time_bwcontext==time_bwnocontext:\n",
    "            vhp_dict['time'] = time_bwcontext\n",
    "        else:\n",
    "            print(\"WTF Why are times different\")\n",
    "            break\n",
    "        bwnocontext_dict[f'aon_{band_name}'] = aon_bwnocontext\n",
    "        bwnocontext_dict[f'vhp_{band_name}'] = vhp_bwnocontext\n",
    "                \n",
    "        bwcontext_df = pd.DataFrame(bwcontext_dict)\n",
    "        bwnocontext_df = pd.DataFrame(bwnocontext_dict)\n",
    "        \n",
    "        bwcontext_df.to_excel(bwcontext_writer, sheet_name=event_dict[event])\n",
    "        bwnocontext_df.to_excel(bwnocontext_writer, sheet_name=event_dict[event])\n",
    "\n",
    "aon_writer.close()\n",
    "vhp_writer.close()\n",
    "bwcontext_writer.close()\n",
    "bwnocontext_writer.close()\n",
    "\n",
    "aon_fig.savefig(savepath+f'lfp_events_aon_{int(time_window*fs/2)}ms.png', format='png', dpi=300, bbox_inches='tight')\n",
    "vhp_fig.savefig(savepath+f'lfp_events_vhp_{int(time_window*fs/2)}ms.png', format='png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "bwcontext_fig.savefig(savepath+f'lfp_events_bwcontext_{int(time_window*fs/2)}ms.png', format='png', dpi=300, bbox_inches='tight')\n",
    "bwnocontext_fig.savefig(savepath+f'lfp_events_bwnocontext_{int(time_window*fs/2)}ms.png', format='png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aon_fig, aon_axs = plt.subplots(4, 3, figsize=(15, 16))\n",
    "vhp_fig, vhp_axs = plt.subplots(4, 3, figsize=(15, 16))\n",
    "\n",
    "# Process each frequency band\n",
    "for band_idx, (band_name, (l_freq, h_freq)) in enumerate(all_bands_dict.items()):\n",
    "    print(f\"  Processing {band_name} band ({l_freq}-{h_freq} Hz)\")\n",
    "    \n",
    "    # Process each event type\n",
    "    for event_idx, (col, event_name) in enumerate(event_dict.items()):\n",
    "        \n",
    "        # Filter and process context data\n",
    "        if len(dk6_bwcontext) > 0:\n",
    "            context_filtered = filter_epoch(dk6_bwcontext[col], l_freq, h_freq)\n",
    "\n",
    "            context_results = get_epoch_lfp_single(context_filtered, time_window)\n",
    "            \n",
    "            # Extract components correctly\n",
    "            context_times = np.array(context_results.iloc[0][2])\n",
    "            context_aon_list = [result[0] for result in context_results]\n",
    "            context_vhp_list = [result[1] for result in context_results]\n",
    "            \n",
    "            context_aon_means = np.array(context_aon_list)\n",
    "            context_vhp_means = np.array(context_vhp_list)\n",
    "            \n",
    "            context_aon_avg = np.mean(context_aon_means, axis=0)\n",
    "            context_aon_sem = np.std(context_aon_means, axis=0) / np.sqrt(len(context_aon_means))\n",
    "            \n",
    "            context_vhp_avg = np.mean(context_vhp_means, axis=0)\n",
    "            context_vhp_sem = np.std(context_vhp_means, axis=0) / np.sqrt(len(context_vhp_means))\n",
    "        \n",
    "        # Filter and process no context data\n",
    "        if len(dk6_bwnocontext) > 0:\n",
    "            nocontext_filtered = filter_epoch(dk6_bwnocontext[col], l_freq, h_freq)\n",
    "\n",
    "            nocontext_results = get_epoch_lfp_single(nocontext_filtered, time_window)\n",
    "            \n",
    "            # Extract components correctly\n",
    "            nocontext_times = np.array(nocontext_results.iloc[0][2])\n",
    "            nocontext_aon_list = [result[0] for result in nocontext_results]\n",
    "            nocontext_vhp_list = [result[1] for result in nocontext_results]\n",
    "            \n",
    "            nocontext_aon_means = np.array(nocontext_aon_list)\n",
    "            nocontext_vhp_means = np.array(nocontext_vhp_list)\n",
    "            \n",
    "            nocontext_aon_avg = np.mean(nocontext_aon_means, axis=0)\n",
    "            nocontext_aon_sem = np.std(nocontext_aon_means, axis=0) / np.sqrt(len(nocontext_aon_means))\n",
    "            \n",
    "            nocontext_vhp_avg = np.mean(nocontext_vhp_means, axis=0)\n",
    "            nocontext_vhp_sem = np.std(nocontext_vhp_means, axis=0) / np.sqrt(len(nocontext_vhp_means))\n",
    "        \n",
    "        # ============ AON PLOTS ============\n",
    "        ax_aon = aon_axs[band_idx, event_idx]\n",
    "        \n",
    "        if len(dk6_bwcontext) > 0:\n",
    "            ax_aon.plot(context_times, context_aon_avg, label='Context', linewidth=2, color='blue')\n",
    "            ax_aon.fill_between(\n",
    "                context_times,\n",
    "                context_aon_avg - context_aon_sem,\n",
    "                context_aon_avg + context_aon_sem,\n",
    "                alpha=0.3,\n",
    "                color='blue'\n",
    "            )\n",
    "        \n",
    "        if len(dk6_bwnocontext) > 0:\n",
    "            ax_aon.plot(nocontext_times, nocontext_aon_avg, label='No Context', linewidth=2, color='orange')\n",
    "            ax_aon.fill_between(\n",
    "                nocontext_times,\n",
    "                nocontext_aon_avg - nocontext_aon_sem,\n",
    "                nocontext_aon_avg + nocontext_aon_sem,\n",
    "                alpha=0.3,\n",
    "                color='orange'\n",
    "            )\n",
    "        \n",
    "        # Formatting for AON\n",
    "        if event_idx == 0:\n",
    "            ax_aon.set_ylabel(f'{band_name.capitalize()}\\n({l_freq}-{h_freq} Hz)\\nAmplitude (μV)', fontsize=10)\n",
    "        if band_idx == 0:\n",
    "            ax_aon.set_title(f'{event_name}', fontsize=11, fontweight='bold')\n",
    "        if band_idx == 3:\n",
    "            ax_aon.set_xlabel('Time (s)', fontsize=10)\n",
    "        \n",
    "        ax_aon.legend(loc='best', fontsize=8)\n",
    "        ax_aon.grid(True, alpha=0.3)\n",
    "        ax_aon.axvline(x=0, color='k', linestyle='--', alpha=0.5, linewidth=1)\n",
    "        \n",
    "        # ============ vHp PLOTS ============\n",
    "        ax_vhp = vhp_axs[band_idx, event_idx]\n",
    "        \n",
    "        if len(dk6_bwcontext) > 0:\n",
    "            ax_vhp.plot(context_times, context_vhp_avg, label='Context', linewidth=2, color='blue')\n",
    "            ax_vhp.fill_between(\n",
    "                context_times,\n",
    "                context_vhp_avg - context_vhp_sem,\n",
    "                context_vhp_avg + context_vhp_sem,\n",
    "                alpha=0.3,\n",
    "                color='blue'\n",
    "            )\n",
    "        \n",
    "        if len(dk6_bwnocontext) > 0:\n",
    "            ax_vhp.plot(nocontext_times, nocontext_vhp_avg, label='No Context', linewidth=2, color='orange')\n",
    "            ax_vhp.fill_between(\n",
    "                nocontext_times,\n",
    "                nocontext_vhp_avg - nocontext_vhp_sem,\n",
    "                nocontext_vhp_avg + nocontext_vhp_sem,\n",
    "                alpha=0.3,\n",
    "                color='orange'\n",
    "            )\n",
    "        \n",
    "        # Formatting for vHp\n",
    "        if event_idx == 0:\n",
    "            ax_vhp.set_ylabel(f'{band_name.capitalize()}\\n({l_freq}-{h_freq} Hz)\\nAmplitude (μV)', fontsize=10)\n",
    "        if band_idx == 0:\n",
    "            ax_vhp.set_title(f'{event_name}', fontsize=11, fontweight='bold')\n",
    "        if band_idx == 3:\n",
    "            ax_vhp.set_xlabel('Time (s)', fontsize=10)\n",
    "        \n",
    "        ax_vhp.legend(loc='best', fontsize=8)\n",
    "        ax_vhp.grid(True, alpha=0.3)\n",
    "        ax_vhp.axvline(x=0, color='k', linestyle='--', alpha=0.5, linewidth=1)\n",
    "\n",
    "# Add overall titles and adjust layout\n",
    "aon_fig.suptitle(f'Rat {rat} - AON LFP Analysis (Frequency Bands)', fontsize=14, fontweight='bold', y=0.995)\n",
    "vhp_fig.suptitle(f'Rat {rat} - vHp LFP Analysis (Frequency Bands)', fontsize=14, fontweight='bold', y=0.995)\n",
    "\n",
    "aon_fig.tight_layout()\n",
    "vhp_fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "lfp_data = pd.read_pickle(savepath + f'marked_mne_epochs_array_{int(time_window*fs)}.pkl')\n",
    "rat_list = np.unique(lfp_data['rat_id'])\n",
    "print(f\"Processing {len(rat_list)} rats: {rat_list}\")\n",
    "\n",
    "# Define events and parameters\n",
    "event_dict = {\n",
    "    'mne_epoch_door_before': 'Before Door',\n",
    "    'mne_epoch_dig_before': 'Before Dig',\n",
    "    'mne_epoch_dig_after': 'After Dig'\n",
    "}\n",
    "all_bands_dict = {\n",
    "    'total': [1, 100],\n",
    "    'theta': [4, 12],\n",
    "    'beta': [12, 30],\n",
    "    'gamma': [30, 80]\n",
    "}\n",
    "time_window = 1.0\n",
    "\n",
    "def filter_epoch(epoch, l_freq, h_freq):\n",
    "    \"\"\"Apply bandpass filter to epoch\"\"\"\n",
    "    return epoch.copy().filter(\n",
    "        l_freq=l_freq,\n",
    "        h_freq=h_freq,\n",
    "        method='iir',\n",
    "        iir_params={'order': 4, 'ftype': 'butter'}\n",
    "    )\n",
    "\n",
    "# Process each rat\n",
    "for rat in rat_list:\n",
    "    print(f\"Processing rat: {rat}\")\n",
    "    \n",
    "    # Filter data for current rat\n",
    "    rat_data = lfp_data[lfp_data['rat_id'] == rat].copy()\n",
    "    \n",
    "    # Separate data by task and reset index\n",
    "    bwcontext_data = rat_data[rat_data['task'] == 'BWcontext'].copy().reset_index(drop=True)\n",
    "    bwnocontext_data = rat_data[rat_data['task'] == 'BWnocontext'].copy().reset_index(drop=True)\n",
    "    \n",
    "    # Skip if no data for this rat\n",
    "    if len(bwcontext_data) == 0 and len(bwnocontext_data) == 0:\n",
    "        print(f\"  No data for rat {rat}, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Create separate figures for AON and vHp with 4 rows (one per band)\n",
    "    aon_fig, aon_axs = plt.subplots(4, 3, figsize=(15, 16))\n",
    "    vhp_fig, vhp_axs = plt.subplots(4, 3, figsize=(15, 16))\n",
    "    \n",
    "    # Process each frequency band\n",
    "    for band_idx, (band_name, (l_freq, h_freq)) in enumerate(all_bands_dict.items()):\n",
    "        print(f\"  Processing {band_name} band ({l_freq}-{h_freq} Hz)\")\n",
    "        \n",
    "        # Process each event type\n",
    "        for event_idx, (col, event_name) in enumerate(event_dict.items()):\n",
    "            \n",
    "            # Filter and process context data\n",
    "            if len(bwcontext_data) > 0:\n",
    "                context_filtered = bwcontext_data[col].apply(\n",
    "                    lambda epoch: filter_epoch(epoch, l_freq, h_freq)\n",
    "                )\n",
    "                context_results = context_filtered.apply(\n",
    "                    lambda epoch: get_epoch_lfp_single(epoch, time_window)\n",
    "                )\n",
    "                \n",
    "                # Extract components correctly\n",
    "                context_times = np.array(context_results.iloc[0][2])\n",
    "                context_aon_list = [result[0] for result in context_results]\n",
    "                context_vhp_list = [result[1] for result in context_results]\n",
    "                \n",
    "                context_aon_means = np.array(context_aon_list)\n",
    "                context_vhp_means = np.array(context_vhp_list)\n",
    "                \n",
    "                context_aon_avg = np.mean(context_aon_means, axis=0)\n",
    "                context_aon_sem = np.std(context_aon_means, axis=0) / np.sqrt(len(context_aon_means))\n",
    "                \n",
    "                context_vhp_avg = np.mean(context_vhp_means, axis=0)\n",
    "                context_vhp_sem = np.std(context_vhp_means, axis=0) / np.sqrt(len(context_vhp_means))\n",
    "            \n",
    "            # Filter and process no context data\n",
    "            if len(bwnocontext_data) > 0:\n",
    "                nocontext_filtered = bwnocontext_data[col].apply(\n",
    "                    lambda epoch: filter_epoch(epoch, l_freq, h_freq)\n",
    "                )\n",
    "                nocontext_results = nocontext_filtered.apply(\n",
    "                    lambda epoch: get_epoch_lfp_single(epoch, time_window)\n",
    "                )\n",
    "                \n",
    "                # Extract components correctly\n",
    "                nocontext_times = np.array(nocontext_results.iloc[0][2])\n",
    "                nocontext_aon_list = [result[0] for result in nocontext_results]\n",
    "                nocontext_vhp_list = [result[1] for result in nocontext_results]\n",
    "                \n",
    "                nocontext_aon_means = np.array(nocontext_aon_list)\n",
    "                nocontext_vhp_means = np.array(nocontext_vhp_list)\n",
    "                \n",
    "                nocontext_aon_avg = np.mean(nocontext_aon_means, axis=0)\n",
    "                nocontext_aon_sem = np.std(nocontext_aon_means, axis=0) / np.sqrt(len(nocontext_aon_means))\n",
    "                \n",
    "                nocontext_vhp_avg = np.mean(nocontext_vhp_means, axis=0)\n",
    "                nocontext_vhp_sem = np.std(nocontext_vhp_means, axis=0) / np.sqrt(len(nocontext_vhp_means))\n",
    "            \n",
    "            # ============ AON PLOTS ============\n",
    "            ax_aon = aon_axs[band_idx, event_idx]\n",
    "            \n",
    "            if len(bwcontext_data) > 0:\n",
    "                ax_aon.plot(context_times, context_aon_avg, label='Context', linewidth=2, color='blue')\n",
    "                ax_aon.fill_between(\n",
    "                    context_times,\n",
    "                    context_aon_avg - context_aon_sem,\n",
    "                    context_aon_avg + context_aon_sem,\n",
    "                    alpha=0.3,\n",
    "                    color='blue'\n",
    "                )\n",
    "            \n",
    "            if len(bwnocontext_data) > 0:\n",
    "                ax_aon.plot(nocontext_times, nocontext_aon_avg, label='No Context', linewidth=2, color='orange')\n",
    "                ax_aon.fill_between(\n",
    "                    nocontext_times,\n",
    "                    nocontext_aon_avg - nocontext_aon_sem,\n",
    "                    nocontext_aon_avg + nocontext_aon_sem,\n",
    "                    alpha=0.3,\n",
    "                    color='orange'\n",
    "                )\n",
    "            \n",
    "            # Formatting for AON\n",
    "            if event_idx == 0:\n",
    "                ax_aon.set_ylabel(f'{band_name.capitalize()}\\n({l_freq}-{h_freq} Hz)\\nAmplitude (μV)', fontsize=10)\n",
    "            if band_idx == 0:\n",
    "                ax_aon.set_title(f'{event_name}', fontsize=11, fontweight='bold')\n",
    "            if band_idx == 3:\n",
    "                ax_aon.set_xlabel('Time (s)', fontsize=10)\n",
    "            \n",
    "            ax_aon.legend(loc='best', fontsize=8)\n",
    "            ax_aon.grid(True, alpha=0.3)\n",
    "            ax_aon.axvline(x=0, color='k', linestyle='--', alpha=0.5, linewidth=1)\n",
    "            \n",
    "            # ============ vHp PLOTS ============\n",
    "            ax_vhp = vhp_axs[band_idx, event_idx]\n",
    "            \n",
    "            if len(bwcontext_data) > 0:\n",
    "                ax_vhp.plot(context_times, context_vhp_avg, label='Context', linewidth=2, color='blue')\n",
    "                ax_vhp.fill_between(\n",
    "                    context_times,\n",
    "                    context_vhp_avg - context_vhp_sem,\n",
    "                    context_vhp_avg + context_vhp_sem,\n",
    "                    alpha=0.3,\n",
    "                    color='blue'\n",
    "                )\n",
    "            \n",
    "            if len(bwnocontext_data) > 0:\n",
    "                ax_vhp.plot(nocontext_times, nocontext_vhp_avg, label='No Context', linewidth=2, color='orange')\n",
    "                ax_vhp.fill_between(\n",
    "                    nocontext_times,\n",
    "                    nocontext_vhp_avg - nocontext_vhp_sem,\n",
    "                    nocontext_vhp_avg + nocontext_vhp_sem,\n",
    "                    alpha=0.3,\n",
    "                    color='orange'\n",
    "                )\n",
    "            \n",
    "            # Formatting for vHp\n",
    "            if event_idx == 0:\n",
    "                ax_vhp.set_ylabel(f'{band_name.capitalize()}\\n({l_freq}-{h_freq} Hz)\\nAmplitude (μV)', fontsize=10)\n",
    "            if band_idx == 0:\n",
    "                ax_vhp.set_title(f'{event_name}', fontsize=11, fontweight='bold')\n",
    "            if band_idx == 3:\n",
    "                ax_vhp.set_xlabel('Time (s)', fontsize=10)\n",
    "            \n",
    "            ax_vhp.legend(loc='best', fontsize=8)\n",
    "            ax_vhp.grid(True, alpha=0.3)\n",
    "            ax_vhp.axvline(x=0, color='k', linestyle='--', alpha=0.5, linewidth=1)\n",
    "    \n",
    "    # Add overall titles and adjust layout\n",
    "    aon_fig.suptitle(f'Rat {rat} - AON LFP Analysis (Frequency Bands)', fontsize=14, fontweight='bold', y=0.995)\n",
    "    vhp_fig.suptitle(f'Rat {rat} - vHp LFP Analysis (Frequency Bands)', fontsize=14, fontweight='bold', y=0.995)\n",
    "    \n",
    "    aon_fig.tight_layout()\n",
    "    vhp_fig.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Power Spectra Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Baseline Power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotting_styles\n",
    "from mne.time_frequency import psd_array_multitaper\n",
    "importlib.reload(plotting_styles)\n",
    "importlib.reload(power_functions)\n",
    "linestyle = plotting_styles.linestyles\n",
    "colors = plotting_styles.colors\n",
    "baseline_lfp_all = pd.read_pickle(savepath+'baseline_lfp_all.pkl')\n",
    "df= baseline_lfp_all.__deepcopy__()\n",
    "df['channel']=df['channel'].apply(lambda x:'AON' if 'AON' in x else 'vHp')\n",
    "channel_experiment_group=df.groupby(['task','channel'])\n",
    "channel_dict = {'BWcontext_AON': 'context AON', 'BWcontext_vHp': 'context vHp',\n",
    "                'BWnocontext_AON': 'No context AON', 'BWnocontext_vHp': 'No context vHp'}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "mean_dict={}\n",
    "for channel, data in channel_experiment_group:\n",
    "    print(channel)\n",
    "    data_array=np.vstack(data['data'].to_numpy())\n",
    "    print(data_array.shape)\n",
    "    data_array_welch = np.array([power_functions.apply_welch_transform(row) for row in data_array]) # Applying Welch's method to each row of data_array\n",
    "    print(data_array_welch.shape)\n",
    "    freqs = np.linspace(0,1000,num=int(data_array_welch.shape[1]))  # Assuming the frequency range is 0-1000 Hz\n",
    "    print(freqs.shape)\n",
    "\n",
    "    data_array_welch_mean = np.mean(data_array_welch, axis=0)\n",
    "    data_array_welch_std = np.std(data_array_welch, axis=0)\n",
    "    print(data_array_welch_mean.shape, data_array_welch_std.shape)\n",
    "    mean_dict[channel[0] + '_' + channel[1] + '_mean'] = data_array_welch_mean\n",
    "    mean_dict[channel[0] + '_' + channel[1] + '_std'] = data_array_welch_std\n",
    "    \n",
    "    ax.plot(freqs,data_array_welch_mean, linestyle=linestyle[channel[1]], color=colors[channel[0]], label=f'{channel[0]} {channel[1]}')\n",
    "    ax.fill_between(freqs,data_array_welch_mean-data_array_welch_std,data_array_welch_mean+data_array_welch_std, alpha=0.1, color=colors[channel[0]])\n",
    "    #ax.set_yscale('log')\n",
    "    ax.set_xlim(0,100)\n",
    "    ax.legend(loc='upper right', fontsize=20)\n",
    "    ax.set_title('Baseline Power Spectral Density', fontsize=20)\n",
    "    ax.set_xlabel('Frequency (Hz)', fontsize=20)\n",
    "    ax.set_ylabel('Power (V^2/Hz)', fontsize=20)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "#    ax.set_yscale('log')\n",
    "mean_dict['frequency']=freqs\n",
    "mean_df=pd.DataFrame(mean_dict)\n",
    "#mean_df.to_csv(savepath+'baseline_power_truncated.csv')\n",
    "#plt.savefig(savepath+'baseline_power_truncated.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "# Calculate multitaper PSD for each group and plot\n",
    "fig_mt, ax_mt = plt.subplots(figsize=(15, 10))\n",
    "mt_mean_dict = {}\n",
    "for channel, data in channel_experiment_group:\n",
    "    data_array = np.vstack(data['data'].to_numpy())\n",
    "    # Multitaper PSD: average across trials\n",
    "    psds = []\n",
    "    for row in data_array:\n",
    "        psd, freqs_mt = psd_array_multitaper(row, sfreq=2000,bandwidth=2, fmin=0, fmax=100, adaptive=True, normalization='full', verbose=0)\n",
    "        psds.append(psd)\n",
    "    psds = np.array(psds)\n",
    "    psd_mean = psds.mean(axis=0)\n",
    "    psd_std = psds.std(axis=0)\n",
    "    mt_mean_dict[channel[0] + '_' + channel[1] + '_mean'] = psd_mean\n",
    "    mt_mean_dict[channel[0] + '_' + channel[1] + '_std'] = psd_std\n",
    "    ax_mt.plot(freqs_mt, psd_mean, linestyle=linestyle[channel[1]], color=colors[channel[0]], label=f'{channel[0]}_{channel[1]}')\n",
    "    ax_mt.fill_between(freqs_mt, psd_mean-psd_std, psd_mean+psd_std, alpha=0.1, color=colors[channel[0]])\n",
    "ax_mt.set_xlim(0, 100)\n",
    "handles, labels = ax_mt.get_legend_handles_labels()\n",
    "ax_mt.legend(handles, [channel_dict[l] for l in labels], loc='upper right', fontsize=20)\n",
    "ax_mt.set_title('Baseline Power Spectral Density (Multitaper)', fontsize=20)\n",
    "ax_mt.set_xlabel('Frequency (Hz)', fontsize=20)\n",
    "ax_mt.set_ylabel('Power (V^2/Hz)', fontsize=20)\n",
    "ax_mt.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "mt_mean_dict['frequency'] = freqs_mt\n",
    "mt_mean_df = pd.DataFrame(mt_mean_dict)\n",
    "mt_mean_df.to_csv(savepath+'baseline_psd_multitaper.csv')\n",
    "plt.savefig(savepath+'baseline_psd_multitaper.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close(fig_mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Prepare data for ANOVA: for each frequency, compare power between tasks\n",
    "# We'll do this for both AON and vHp channels\n",
    "\n",
    "results = {'frequency': [], 'AON_F': [], 'AON_p': [], 'vHp_F': [], 'vHp_p': []}\n",
    "tasks = ['BWcontext', 'BWnocontext']\n",
    "def make_welch_data_dfs(data, task, channel):\n",
    "    data_task_channel = data[(data['task'] == task) & (data['channel'] == channel)]\n",
    "    data_array = np.vstack(data_task_channel['data'].to_numpy())\n",
    "    data_array_welch = np.array([power_functions.apply_welch_transform(row) for row in data_array])  # Applying Welch's method to each row of data_array\n",
    "    return data_array_welch\n",
    "\n",
    "aon_context_vals= make_welch_data_dfs(df, 'BWcontext', 'AON')\n",
    "aon_nocontext_vals= make_welch_data_dfs(df, 'BWnocontext', 'AON')\n",
    "vHp_context_vals= make_welch_data_dfs(df, 'BWcontext', 'vHp')\n",
    "vHp_nocontext_vals= make_welch_data_dfs(df, 'BWnocontext', 'vHp')\n",
    "for freq in range(aon_context_vals.shape[1]):\n",
    "    aon_f, aon_p = f_oneway(aon_context_vals[:, freq], aon_nocontext_vals[:, freq])\n",
    "    vHp_f, vHp_p = f_oneway(vHp_context_vals[:, freq], vHp_nocontext_vals[:, freq])\n",
    "    \n",
    "    results['frequency'].append(freq)\n",
    "    results['AON_F'].append(aon_f)\n",
    "    results['AON_p'].append(aon_p)\n",
    "    results['vHp_F'].append(vHp_f)\n",
    "    results['vHp_p'].append(vHp_p)\n",
    "    # Convert results to DataFrame and filter for frequency 1 to 100\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df[(results_df['frequency'] >= 1) & (results_df['frequency'] <= 100)]\n",
    "results_df.to_csv(savepath + 'anova_psd_per_frequency_1_100.csv', index=False)\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# For each frequency, extract power for each task and channel\n",
    "for i, freq in enumerate(mean_df['frequency']):\n",
    "    # For ANOVA, we need the raw values, not the means, so we go back to the original data\n",
    "    # Get all AON/vHp power values at this frequency for each task\n",
    "    aon_mask = (data['channel'] == 'AON')\n",
    "    vhp_mask = (data['channel'] == 'vHp')\n",
    "    context_mask = (data['task'] == 'BWcontext')\n",
    "    nocontext_mask = (data['task'] == 'BWnocontext')\n",
    "\n",
    "    aon_context_vals = data_array_welch[aon_mask & context_mask, i]\n",
    "    aon_nocontext_vals = data_array_welch[aon_mask & nocontext_mask, i]\n",
    "    vhp_context_vals = data_array_welch[vhp_mask & context_mask, i]\n",
    "    vhp_nocontext_vals = data_array_welch[vhp_mask & nocontext_mask, i]\n",
    "\n",
    "    # ANOVA for AON\n",
    "    if len(aon_context_vals) > 1 and len(aon_nocontext_vals) > 1:\n",
    "        F_aon, p_aon = f_oneway(aon_context_vals, aon_nocontext_vals)\n",
    "    else:\n",
    "        F_aon, p_aon = float('nan'), float('nan')\n",
    "\n",
    "    # ANOVA for vHp\n",
    "    if len(vhp_context_vals) > 1 and len(vhp_nocontext_vals) > 1:\n",
    "        F_vhp, p_vhp = f_oneway(vhp_context_vals, vhp_nocontext_vals)\n",
    "    else:\n",
    "        F_vhp, p_vhp = float('nan'), float('nan')\n",
    "\n",
    "    results['frequency'].append(freq)\n",
    "    results['AON_F'].append(F_aon)\n",
    "    results['AON_p'].append(p_aon)\n",
    "    results['vHp_F'].append(F_vhp)\n",
    "    results['vHp_p'].append(p_vhp)\n",
    "\n",
    "anova_df = pd.DataFrame(results)\n",
    "anova_df.to_csv(savepath + 'anova_psd_per_frequency.csv', index=False)\n",
    "print(anova_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaselinePower Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.time_frequency import psd_array_multitaper\n",
    "\n",
    "baseline_lfp_all = pd.read_pickle(savepath+'baseline_lfp_all.pkl') #Loading the baseline LFP data\n",
    "df= baseline_lfp_all.__deepcopy__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "importlib.reload(plotting_styles)\n",
    "importlib.reload(power_functions)\n",
    "linestyles = plotting_styles.linestyles\n",
    "colors = plotting_styles.colors\n",
    "brain_areas = ['AON','vHp']\n",
    "\n",
    "\n",
    "number_per_segment = 2000\n",
    "tukey_window = scipy.signal.get_window(('tukey', 0.2), number_per_segment)    \n",
    "all_bands_dict = {'total':[1,100], 'beta':[12,30], 'gamma':[30,80], 'theta':[4,12]}\n",
    "task_dict = {'BWcontext': 'Context', 'BWnocontext': 'No Context'}\n",
    "df['data']=df['data'].apply(lambda x:power_functions.apply_welch_transform(x))\n",
    "\n",
    "for band_name, band_values in all_bands_dict.items():\n",
    "    df[band_name+'_power']=df['data'].apply(lambda x:power_functions.get_band_power(x, band_values[0], band_values[1]))\n",
    "\n",
    "writer=pd.ExcelWriter(savepath+'baseline_power_per_band_welch.xlsx')\n",
    "fig, axs = plt.subplots(1,2, figsize=(15, 10), sharey=True)\n",
    "axs=axs.flatten()\n",
    "for i, area in enumerate(brain_areas):\n",
    "    data = df[df['channel'].str.contains(area)]\n",
    "    data_melted = data.melt(id_vars=['rat','task','channel'], value_vars=['total_power','beta_power','gamma_power','theta_power'], var_name='band', value_name='power')\n",
    "    sns.barplot(\n",
    "        data=data_melted, x='band', y='power', hue='task',\n",
    "        hue_order=['BWcontext', 'BWnocontext'], palette=colors, ax=axs[i])\n",
    "    sns.stripplot(data=data_melted, x='band', y='power', hue='task', hue_order=['BWcontext','BWnocontext'], palette=colors, dodge=True, alpha=0.5, jitter=0.2, ax=axs[i], linewidth=1, legend=False )\n",
    "#    axs[i].set_yscale('log')\n",
    "    axs[i].set_title(f'Baseline {area} Power per Band', fontsize=20)\n",
    "    axs[i].set_xlabel('Band', fontsize=20)\n",
    "    axs[i].set_ylabel('Power V^2', fontsize=20)\n",
    "    axs[i].legend(loc='upper right', fontsize=15)\n",
    "    axs[i].set_xticks(([0,1,2,3]),list(all_bands_dict.keys()))\n",
    "    axs[i].tick_params(axis='both', which='major', labelsize=15)\n",
    "    data_melted.to_excel(writer, sheet_name=area)\n",
    "writer.close()\n",
    "plt.savefig(savepath+'baseline_power_per_band_welch.png', format='png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "baseline_lfp_all = pd.read_pickle(savepath+'baseline_lfp_all.pkl') #Loading the baseline LFP data\n",
    "df= baseline_lfp_all.__deepcopy__()\n",
    "\n",
    "# Calculate multitaper PSD and band power for each row\n",
    "df['data_mt'] = df['data'].apply(lambda x: psd_array_multitaper(x, sfreq=2000, fmin=0, fmax=100, adaptive=True,bandwidth=2, normalization='full', verbose=0, max_iter=500)[0])\n",
    "\n",
    "for band_name, band_values in all_bands_dict.items():\n",
    "    # df[band_name + '_power_mt'] = df['data_mt'].apply(lambda x: psd_array_multitaper(x, sfreq=2000, fmin=band_values[0], fmax=band_values[1], adaptive=True,bandwidth=2, normalization='full', verbose=0, max_iter=500,faverage=True)[0])\n",
    "\n",
    "    df[band_name + '_power_mt'] = df['data_mt'].apply(lambda x: power_functions.get_band_power(x, band_values[0], band_values[1]))\n",
    "    epsilon = 1e-12\n",
    "    #df[band_name + '_power_mt'] = df[band_name + '_power_mt'].apply(lambda x: 10*np.log10(x + epsilon))     # Log-normalize multitaper band power, handling log(0) by adding a small epsilon\n",
    "\n",
    "    # Plot multitaper band power\n",
    "writer_mt = pd.ExcelWriter(savepath + 'baseline_power_per_band_multitaper.xlsx')\n",
    "\n",
    "fig_mt, axs_mt = plt.subplots(1, 2, figsize=(15, 10), sharey=True)\n",
    "axs_mt = axs_mt.flatten()\n",
    "for i, area in enumerate(brain_areas):\n",
    "    data_mt = df[df['channel'].str.contains(area)]\n",
    "    data_melted_mt = data_mt.melt(\n",
    "        id_vars=['rat', 'task', 'channel'],\n",
    "        value_vars=['total_power_mt', 'beta_power_mt', 'gamma_power_mt', 'theta_power_mt'],\n",
    "        var_name='band', value_name='power'\n",
    "    )\n",
    "    # Plot log-normalized multitaper band power\n",
    "    sns.barplot(\n",
    "        data=data_melted_mt, x='band', y='power', hue='task',\n",
    "        hue_order=['BWcontext', 'BWnocontext'], palette=colors, ax=axs_mt[i]\n",
    "    )\n",
    "    sns.stripplot(\n",
    "        data=data_melted_mt, x='band', y='power', hue='task',\n",
    "        hue_order=['BWcontext', 'BWnocontext'], palette=colors, dodge=True, alpha=1, jitter=0.2,\n",
    "        ax=axs_mt[i], linewidth=1, legend=False\n",
    "    )\n",
    "    axs_mt[i].set_title(f'Baseline {area} Power per band', fontsize=20)\n",
    "    axs_mt[i].set_xlabel('Band', fontsize=20)\n",
    "    axs_mt[i].set_ylabel('Power (V^2)', fontsize=20)\n",
    "    handles, labels = axs_mt[i].get_legend_handles_labels()\n",
    "    axs_mt[i].legend(handles, [task_dict[l] for l in labels], loc='upper right', fontsize=15)\n",
    "    #axs_mt[i].legend(loc='upper right', fontsize=15)\n",
    "    axs_mt[i].set_xticks([0, 1, 2, 3], list(all_bands_dict.keys()))\n",
    "    axs_mt[i].tick_params(axis='both', which='major', labelsize=15)\n",
    "    data_melted_mt.to_excel(writer_mt, sheet_name=area)\n",
    "writer_mt.close()\n",
    "plt.savefig(savepath + 'baseline_power_per_band_multitaper.png', format='png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Power Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_lfp_all = pd.read_pickle(savepath+'baseline_lfp_all.pkl')\n",
    "df= baseline_lfp_all.__deepcopy__()\n",
    "df['channel']=df['channel'].apply(lambda x:'AON' if 'AON' in x else 'vHp')\n",
    "channel_experiment_group=df.groupby(['task','channel'])\n",
    "channel_dict = {'BWcontext_AON': 'context AON', 'BWcontext_vHp': 'context vHp',\n",
    "                'BWnocontext_AON': 'No context AON', 'BWnocontext_vHp': 'No context vHp'}\n",
    "\n",
    "\"\"\"\n",
    "Doing a test run\n",
    "\n",
    "test_array = df['data'].iloc[0]\n",
    "print(test_array.shape)\n",
    "\n",
    "test_array_new = test_array.reshape((1,1,-1))\n",
    "print(test_array_new.shape)\n",
    "\n",
    "fmin = 1\n",
    "fmax = 100\n",
    "freqs = np.arange(fmin, fmax)\n",
    "n_cycles = freqs / 3.  # different number of cycles per frequency\n",
    "fs =2000\n",
    "\n",
    "tfr_array = tfr_array_morlet(test_array_new, sfreq=fs, freqs=freqs, n_cycles=n_cycles, n_jobs=-1, output='power')\n",
    "\n",
    "print(tfr_array.shape)  # Should be (n_epochs,n_channels, n_freqs, n_times)\n",
    "\n",
    "tfr_array_squeezed = tfr_array.squeeze()\n",
    "print(tfr_array_squeezed.shape)  # Should be (n_freqs, n_times)\n",
    "\n",
    "plt.imshow(tfr_array_squeezed, aspect='auto', origin='lower', extent=[-2, 0, fmin, fmax])\n",
    "plt.colorbar(label='Power')\n",
    "#for task_channel, data in channel_experiment_group:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from mne.time_frequency import tfr_array_morlet\n",
    "\n",
    "def compute_tfr(data_array, fmin=1, fmax=100, fs=2000):\n",
    "    data_array = data_array.reshape((1, 1, -1))\n",
    "    freqs = np.arange(fmin, fmax)\n",
    "    n_cycles = freqs / 3.  # different number of cycles per frequency\n",
    "    tfr_array = tfr_array_morlet(data_array, sfreq=fs, freqs=freqs, n_cycles=n_cycles, n_jobs=1, output='power')\n",
    "    tfr_array_squeezed = tfr_array.squeeze()\n",
    "    #tfr_normalized = scipy.stats.zscore(tfr_array_squeezed, axis=1)\n",
    "    tfr_normalized = 10*np.log10(tfr_array_squeezed) #dB normalization\n",
    "    return tfr_normalized\n",
    "\n",
    "df['data_tfr'] = df['data'].apply(compute_tfr)\n",
    "\n",
    "\n",
    "channel_experiment_group=df.groupby(['task','channel'])\n",
    "channel_dict = {'BWcontext_AON': 'context AON', 'BWcontext_vHp': 'context vHp',\n",
    "                'BWnocontext_AON': 'No context AON', 'BWnocontext_vHp': 'No context vHp'}\n",
    "fig, axs= plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Baseline Power Spectrograms', fontsize=20)\n",
    "axs=axs.flatten()\n",
    "for i, (task_channel, data) in enumerate(channel_experiment_group):\n",
    "    ax = axs[i]\n",
    "    print(task_channel)\n",
    "    data_array_tfr = np.array(data['data_tfr'].tolist())\n",
    "    print(data_array_tfr.shape)  # Should be (n_epochs, n_freqs, n_times)\n",
    "    \n",
    "    data_array_tfr_mean = np.mean(data_array_tfr, axis=0)\n",
    "    print(data_array_tfr_mean.shape)  # Should be (n_freqs, n_times)\n",
    "    ax.imshow(data_array_tfr_mean, aspect='auto', origin='lower', extent=[-2, 0, 1, 100])\n",
    "\n",
    "    ax.set_title(f'{task_channel[0]} {task_channel[1]}')\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Frequency (Hz)')\n",
    "fig.colorbar(ax.images[0], ax=axs, label='Power (dB)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will plot the power spectra for each rat and the mean power spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will plot the power spectra for the complete trial # [NOT USED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(20, 10), sharex=True, sharey=True)\n",
    "axs=axs.flatten()\n",
    "fig.suptitle('Power Spectral Density')\n",
    "linestyles = {'AON': '-', 'vHp': '--'}\n",
    "\n",
    "for i,rati in enumerate(rat_list):\n",
    "    rat_data=power_df[power_df['rat']==rati]\n",
    "    rat_data=rat_data.reset_index(drop=True)\n",
    "    rat_data_grouped=rat_data.groupby(['task','channel'])\n",
    "    for (task, channel),group in rat_data_grouped:\n",
    "        print(task, channel)\n",
    "        group=group.reset_index(drop=True)\n",
    "        col='complete_trial'\n",
    "        data = np.array(group[col])\n",
    "        data_mean = np.mean(data, axis=0)\n",
    "        data_sem = scipy.stats.sem(data, axis=0)\n",
    "        freq = np.linspace(0, 1000, len(data_mean))        \n",
    "        ax = axs[i]\n",
    "        ax.set_title(f'{rati}')\n",
    "        ax.plot(freq, data_mean, color=colors[task], linestyle=linestyles[channel])\n",
    "        ax.fill_between(freq, data_mean - data_sem, data_mean + data_sem, alpha=0.2, color=colors[task])\n",
    "        ax.set_xlim(0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 Average Power Spectra across all rats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 Event Power Spectra individual Rats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Events PSD Welch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "\n",
    "time_window=0.7\n",
    "fs=2000\n",
    "\n",
    "##################\n",
    "\n",
    "from mne.time_frequency import psd_array_multitaper\n",
    "\n",
    "importlib.reload(power_functions)\n",
    "compiled_data_all_epochs = pd.read_pickle(savepath+f'compiled_data_all_epochs_truncated_{int(time_window*fs)}.pkl')\n",
    "power_df=compiled_data_all_epochs.__deepcopy__()\n",
    "# number_per_segment = 700\n",
    "# tukey_window = scipy.signal.get_window(('tukey', 0.1), number_per_segment)\n",
    "columns= ['complete_trial','pre_door', 'post_door', 'pre_dig', 'post_dig', 'around_door', 'around_dig']\n",
    "\n",
    "power_df.loc[:,columns]=power_df.loc[:,columns].applymap(lambda x:power_functions.apply_welch_transform(x))\n",
    "events_dict={'pre_door':' Pre Door','post_door':'Post Door','pre_dig':'Pre Dig','post_dig':'Post Dig'}\n",
    "fig, axs=plt.subplots(1,4, figsize=(40,10), sharex=True, sharey=True)\n",
    "axs=axs.flatten()\n",
    "for ax in axs:\n",
    "    ax.tick_params(axis='y', which='both', labelleft=True)\n",
    "writer=pd.ExcelWriter(savepath+'events_power_spectral_density.xlsx')\n",
    "for i, event in enumerate(events_dict.keys()):\n",
    "\n",
    "    data = power_df[['rat','task','channel',event]]\n",
    "    data['channel']=data['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "    data_groups=data.groupby(['task','channel'])\n",
    "    mean_data_dict={}\n",
    "    for (task, channel), group in data_groups:\n",
    "        group=group.reset_index(drop=True)\n",
    "        data = np.array(group[event])\n",
    "        data_mean = np.mean(data, axis=0)\n",
    "        data_sem = scipy.stats.sem(data, axis=0)\n",
    "        mean_data_dict[task+'_'+channel+'_mean']=data_mean\n",
    "        mean_data_dict[task+'_'+channel+'_sem']=data_sem\n",
    "        freq = np.linspace(0, 1000, len(data_mean))\n",
    "        ax = axs[i]\n",
    "        ax.set_title(f'{events_dict[event]}', fontsize=20)\n",
    "        ax.plot(freq, data_mean, color=plotting_styles.colors[task], linestyle=plotting_styles.linestyles[channel])\n",
    "        ax.fill_between(freq, data_mean - data_sem, data_mean + data_sem, alpha=0.2, color=plotting_styles.colors[task])\n",
    "        ax.set_xlim(0, 100)\n",
    "        #ax.set_yscale('log')\n",
    "        ax.set_xlabel('Frequency (Hz)', fontsize=20)\n",
    "        if i == 0:\n",
    "            ax.set_ylabel('Power (V^2/Hz)', fontsize=25)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "    mean_data_dict['frequency'] = freq\n",
    "    mean_df=pd.DataFrame(mean_data_dict)\n",
    "    mean_df.to_excel(writer, sheet_name=event)\n",
    "writer.close()\n",
    "fig.savefig(savepath+f'pow_events_psd{int(time_window*fs/2)}ms.png', format='png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Events PSD MT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "time_window=0.4\n",
    "fs=2000\n",
    "###############\n",
    "\n",
    "\n",
    "importlib.reload(power_functions)\n",
    "compiled_data_all_epochs = pd.read_pickle(savepath+f'compiled_data_all_epochs_truncated_{int(time_window*fs)}.pkl')\n",
    "power_df=compiled_data_all_epochs.__deepcopy__()\n",
    "# number_per_segment = 700\n",
    "# tukey_window = scipy.signal.get_window(('tukey', 0.1), number_per_segment)\n",
    "columns= ['pre_door', 'post_door', 'pre_dig', 'post_dig', 'around_door', 'around_dig']\n",
    "# Apply multitaper PSD to each event column\n",
    "def multitaper_transform(x):\n",
    "    # x is a 1D array or list of values\n",
    "    psd, _ = psd_array_multitaper(x, sfreq=2000, fmin=0, fmax=100, adaptive=True, bandwidth=6, normalization='length', verbose=0, max_iter=1000)\n",
    "    #psd = 10 * np.log10(psd)\n",
    "    return psd\n",
    "\n",
    "power_df.loc[:, columns] = power_df.loc[:, columns].applymap(multitaper_transform)\n",
    "task_dict = {'BWcontext': 'Context', 'BWnocontext': 'No Context'}\n",
    "events_dict = {'pre_door': ' Pre Door', 'post_door': 'Post Door', 'pre_dig': 'Pre Dig', 'post_dig': 'Post Dig'}\n",
    "fig, axs = plt.subplots(1, 4, figsize=(40, 10), sharex=True, sharey=True)\n",
    "axs = axs.flatten()\n",
    "for ax in axs:\n",
    "    ax.tick_params(axis='y', which='both', labelleft=True)\n",
    "writer = pd.ExcelWriter(savepath + f'pow_events_psd_{int(time_window*fs/2)}ms.xlsx')\n",
    "for i, event in enumerate(events_dict.keys()):\n",
    "    data = power_df[['rat', 'task', 'channel', event]]\n",
    "    data['channel'] = data['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "    data_groups = data.groupby(['task', 'channel'])\n",
    "    mean_data_dict = {}\n",
    "    for (task, channel), group in data_groups:\n",
    "        group = group.reset_index(drop=True)\n",
    "        data_arr = np.array(group[event])\n",
    "        data_mean = np.mean(data_arr, axis=0)\n",
    "        data_sem = scipy.stats.sem(data_arr, axis=0)\n",
    "        mean_data_dict[task + '_' + channel + '_mean'] = data_mean\n",
    "        mean_data_dict[task + '_' + channel + '_sem'] = data_sem\n",
    "        freq = np.linspace(0, 100, len(data_mean))\n",
    "        ax = axs[i]\n",
    "        ax.set_title(f'{events_dict[event]}', fontsize=20)\n",
    "        ax.plot(freq, data_mean, color=plotting_styles.colors[task], linestyle=plotting_styles.linestyles[channel], label=f'{task_dict[task]} {channel}')\n",
    "        ax.fill_between(freq, data_mean - data_sem, data_mean + data_sem, alpha=0.2, color=plotting_styles.colors[task])\n",
    "        ax.set_xlim(0, 100)\n",
    "        # ax.set_yscale('log')\n",
    "        ax.set_xlabel('Frequency (Hz)', fontsize=20)\n",
    "        if i == 0:\n",
    "            ax.set_ylabel('Power (V^2/Hz)', fontsize=25)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "    mean_data_dict['frequency'] = freq\n",
    "    mean_df = pd.DataFrame(mean_data_dict)\n",
    "    mean_df.to_excel(writer, sheet_name=event)\n",
    "writer.close()\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, labels, loc='upper right', fontsize=20)\n",
    "fig.savefig(savepath + f'pow_events_psd_{int(time_window*fs/2)}ms.png', format='png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "con_data_df_clean = pd.read_pickle(savepath + f'marked_mne_epochs_array_df_truncated_{int(time_window*fs)}_251125.pkl')\n",
    "event_list = ['mne_epoch_door_before', 'mne_epoch_dig_before', 'mne_epoch_dig_after']\n",
    "\n",
    "def get_channel_groups(channel_names, task):\n",
    "    \"\"\"\n",
    "    Identify channel groups based on channel names and task.\n",
    "    Returns dictionary with channel indices for each group.\n",
    "    \"\"\"\n",
    "    aon_channels = [ch for ch in channel_names if 'AON' in ch]\n",
    "    vhp_channels = [ch for ch in channel_names if 'vHp' in ch]\n",
    "    \n",
    "    groups = {\n",
    "        f'{task}_AON': aon_channels,\n",
    "        f'{task}_vHp': vhp_channels\n",
    "    }\n",
    "    \n",
    "    return groups\n",
    "\n",
    "print(con_data_df_clean.columns)\n",
    "\n",
    "# Dictionary to store PSD data for each event and task combination\n",
    "# Structure: event_psd_data[event][task][group_name] = list of (n_trials, n_freqs) arrays\n",
    "event_psd_data = {event: {} for event in event_list}\n",
    "freqs_array = None  # Will store frequency values\n",
    "\n",
    "for i in range(len(con_data_df_clean)):\n",
    "    rat = con_data_df_clean.loc[i, 'rat_id']\n",
    "    task = con_data_df_clean.loc[i, 'task']\n",
    "    date = con_data_df_clean.loc[i, 'date']\n",
    "    \n",
    "    print(f\"Processing rat {rat}, task {task}, date {date} ({i+1}/{len(con_data_df_clean)})\")\n",
    "    \n",
    "    for event in event_list:\n",
    "        test_epoch = con_data_df_clean.loc[i, event]\n",
    "        \n",
    "        # Compute PSD\n",
    "        test_epoch_psd = test_epoch.compute_psd(\n",
    "            method='multitaper', \n",
    "            fmin=0, \n",
    "            fmax=100, \n",
    "            adaptive=True, \n",
    "            bandwidth=6, \n",
    "            normalization='full', \n",
    "            verbose=0,\n",
    "            exclude=['Ref']\n",
    "        )\n",
    "        \n",
    "        # Get PSD data and frequencies\n",
    "        psd_array = test_epoch_psd.get_data()  # shape: (n_trials, n_channels, n_freqs)\n",
    "        freqs = test_epoch_psd.freqs\n",
    "        channel_names = test_epoch_psd.ch_names\n",
    "        \n",
    "        # Store frequency array (same for all)\n",
    "        if freqs_array is None:\n",
    "            freqs_array = freqs\n",
    "        \n",
    "        # Identify channel groups based on task and channel names\n",
    "        channel_groups = get_channel_groups(channel_names, task)\n",
    "        \n",
    "        # Initialize task dictionary if needed\n",
    "        if task not in event_psd_data[event]:\n",
    "            event_psd_data[event][task] = {group: [] for group in channel_groups.keys()}\n",
    "        \n",
    "        # Process each channel group\n",
    "        for group_name, group_channels in channel_groups.items():\n",
    "            # Find indices of channels in this group\n",
    "            channel_indices = [idx for idx, ch in enumerate(channel_names) \n",
    "                             if ch in group_channels]\n",
    "            \n",
    "            if len(channel_indices) > 0:\n",
    "                # Get PSD data for these channels: (n_trials, n_channels_in_group, n_freqs)\n",
    "                group_psd = psd_array[:, channel_indices, :]\n",
    "                \n",
    "                # Average across channels: (n_trials, n_freqs)\n",
    "                group_psd_avg_channels = np.mean(group_psd, axis=1)\n",
    "                \n",
    "                # Store this rat's data\n",
    "                event_psd_data[event][task][group_name].append(group_psd_avg_channels)\n",
    "\n",
    "# Now aggregate across rats for each event and task\n",
    "print(\"\\nAggregating across rats...\")\n",
    "final_psd_dfs = {}\n",
    "\n",
    "for event in event_list:\n",
    "    # Dictionary to hold columns for this event\n",
    "    df_dict = {'frequency': freqs_array}  # Add frequency column first\n",
    "    \n",
    "    # Get all tasks for this event\n",
    "    for task in event_psd_data[event].keys():\n",
    "        # Process each channel group\n",
    "        for group_name in event_psd_data[event][task].keys():\n",
    "            rat_data = event_psd_data[event][task][group_name]\n",
    "            \n",
    "            if len(rat_data) > 0:\n",
    "                # rat_data is a list of (n_trials, n_freqs) arrays, one per rat\n",
    "                # Concatenate all rats' trials: (total_trials_all_rats, n_freqs)\n",
    "                all_trials = np.concatenate(rat_data, axis=0)\n",
    "                \n",
    "                # Calculate mean and SEM across all trials from all rats\n",
    "                psd_mean = np.mean(all_trials, axis=0)\n",
    "                psd_sem = stats.sem(all_trials, axis=0)\n",
    "                \n",
    "                # Add as columns\n",
    "                df_dict[f'{group_name}_mean'] = psd_mean\n",
    "                df_dict[f'{group_name}_sem'] = psd_sem\n",
    "            else:\n",
    "                df_dict[f'{group_name}_mean'] = np.full(len(freqs_array), np.nan)\n",
    "                df_dict[f'{group_name}_sem'] = np.full(len(freqs_array), np.nan)\n",
    "    \n",
    "    # Create dataframe where each row is a frequency bin\n",
    "    final_psd_dfs[event] = pd.DataFrame(df_dict)\n",
    "    print(f\"{event}: {len(final_psd_dfs[event])} frequency bins, {len(df_dict)} columns\")\n",
    "\n",
    "# Save PSD Mean/SEM to Excel\n",
    "excel_filename_psd = savepath + f'pow_events_psd_{int(time_window*fs/2)}ms.xlsx'\n",
    "print(f\"\\nSaving PSD mean/SEM to {excel_filename_psd}...\")\n",
    "\n",
    "with pd.ExcelWriter(excel_filename_psd, engine='openpyxl') as writer:\n",
    "    for event, df in final_psd_dfs.items():\n",
    "        sheet_name = event.replace('mne_epoch_', '')[:31]\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        print(f\"Saved sheet: {sheet_name} ({len(df)} rows, {len(df.columns)} columns)\")\n",
    "\n",
    "print(\"\\nPSD Mean/SEM Analysis Done!\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\n=== PSD Mean/SEM Sample ===\")\n",
    "for event, df in final_psd_dfs.items():\n",
    "    print(f\"\\n{event}:\")\n",
    "    print(df.head(10))\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== Plotting =====\n",
    "print(\"\\nCreating plots...\")\n",
    "\n",
    "# Create figure with 3 subplots in a row\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharey=True)\n",
    "\n",
    "# Event titles for subplots\n",
    "event_titles = {\n",
    "    'mne_epoch_door_before': 'Door Before',\n",
    "    'mne_epoch_dig_before': 'Dig Before',\n",
    "    'mne_epoch_dig_after': 'Dig After'\n",
    "}\n",
    "\n",
    "for idx, event in enumerate(event_list):\n",
    "    ax = axes[idx]\n",
    "    df = final_psd_dfs[event]\n",
    "    freqs = df['frequency'].values\n",
    "    \n",
    "    # Get all column names except frequency\n",
    "    data_columns = [col for col in df.columns if col != 'frequency']\n",
    "    \n",
    "    # Extract task and region from column names and plot\n",
    "    plotted_lines = []\n",
    "    for col in data_columns:\n",
    "        if '_mean' in col:\n",
    "            # Parse column name: e.g., 'BWContext_AON_mean'\n",
    "            col_name = col.replace('_mean', '')\n",
    "            \n",
    "            mean_col = col\n",
    "            sem_col = col.replace('_mean', '_sem')\n",
    "            \n",
    "            if mean_col in df.columns and sem_col in df.columns:\n",
    "                mean = df[mean_col].values\n",
    "                sem = df[sem_col].values\n",
    "                \n",
    "                # Determine color and linestyle based on column name\n",
    "                if 'BWcontext' in col_name:\n",
    "                    color = 'blue'\n",
    "                elif 'BWnocontext' in col_name or 'BWNocontext' in col_name:\n",
    "                    color = 'orange'\n",
    "                else:\n",
    "                    color = 'black'\n",
    "                \n",
    "                if 'AON' in col_name:\n",
    "                    linestyle = '-'\n",
    "                elif 'vHp' in col_name:\n",
    "                    linestyle = '--'\n",
    "                else:\n",
    "                    linestyle = '-'\n",
    "                \n",
    "                # Create label\n",
    "                label = col_name.replace('_', ' ')\n",
    "                \n",
    "                # Plot mean line\n",
    "                line = ax.plot(freqs, mean, color=color, linestyle=linestyle, \n",
    "                       linewidth=2, label=label)\n",
    "                \n",
    "                # Add shaded SEM\n",
    "                ax.fill_between(freqs, mean - sem, mean + sem, \n",
    "                               color=color, alpha=0.2)\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel('Frequency (Hz)', fontsize=12)\n",
    "    ax.set_ylabel('Power Spectral Density (mV^2/Hz)', fontsize=12)\n",
    "    ax.set_title(event_titles[event], fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim([freqs.min(), freqs.max()])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "plot_filename = savepath + f'pow_events_psd_{int(time_window*fs/2)}ms.png'\n",
    "plt.savefig(plot_filename, dpi=300, bbox_inches='tight')\n",
    "print(f\"Plot saved to {plot_filename}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAll done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Spectrograms for each each trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window=0.7\n",
    "fs=2000\n",
    "mne_epochs = pd.read_pickle(savepath+f'mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "## Test Epoch\n",
    "\n",
    "test_epoch_pre_door = mne_epochs['mne_epoch_door_before'].iloc[0]\n",
    "test_epoch_pre_dig = mne_epochs['mne_epoch_dig_before'].iloc[0]\n",
    "fmin=2.5\n",
    "fmax=100\n",
    "fs=2000\n",
    "freqs = np.arange(fmin,fmax)\n",
    "n_cycles = freqs/3\n",
    "\n",
    "power_pre_door = test_epoch_pre_door.compute_tfr(\n",
    "    method=\"morlet\", freqs=freqs, n_cycles=n_cycles, return_itc=False, average=False\n",
    ")\n",
    "\n",
    "power_pre_door_data = power_pre_door.get_data()\n",
    "print(power_pre_door_data.shape)\n",
    "power_pre_dig = test_epoch_pre_dig.compute_tfr(\n",
    "    method=\"morlet\", freqs=freqs, n_cycles=n_cycles, return_itc=False, average=False\n",
    ")\n",
    "power_pre_dig_data = power_pre_dig.get_data()\n",
    "print(power_pre_dig_data.shape)\n",
    "channel_names = power_pre_door.ch_names\n",
    "print(channel_names)\n",
    "plt.imshow(power_pre_door_data[0, 0, :, :], aspect='auto', origin='lower', extent=[-0.7, 0, fmin, fmax])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(power_pre_dig_data[0, 0, :, :], aspect='auto', origin='lower', extent=[-0.7, 0, fmin, fmax])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_of_cols = power_pre_door_data.shape[0]\n",
    "num_of_rows = power_pre_door_data.shape[1]\n",
    "\n",
    "fig, axs = plt.subplots(num_of_rows, num_of_cols, figsize=(20, 10), sharex=True, sharey=True)\n",
    "vmin_global = 0\n",
    "vmax_global = 0\n",
    "\n",
    "for i in range(num_of_rows):\n",
    "    for j in range(num_of_cols):\n",
    "        pre_door = power_pre_door_data[j,i, :, :]\n",
    "        pre_dig = power_pre_dig_data[j,i, :, :]\n",
    "        net_power = pre_dig - pre_door\n",
    "        axs[i, j].imshow(pre_door, aspect='auto', origin='lower', extent=[-0.7, 0, fmin, fmax])\n",
    "        if j == 0:\n",
    "            axs[i, j].set_ylabel(f'{channel_names[i]}', fontsize=10)\n",
    "        if i==0:\n",
    "            axs[i, j].set_title(f'trial {j}', fontsize=10)\n",
    "            \n",
    "        vmin_global = min(vmin_global, pre_door.min())\n",
    "        vmax_global = max(vmax_global, pre_door.max())\n",
    "\n",
    "# for ax in axs.flat:\n",
    "#     # Set color limits for all axes to the global min/max\n",
    "#     for im in ax.get_images():\n",
    "#         im.set_clim(vmin_global, vmax_global)\n",
    "fig.colorbar(axs[0, 0].images[0], ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne_epochs = pd.read_pickle(savepath+'mne_epochs_array_df_truncated_1400.pkl')\n",
    "def get_power_tfr(epoch):\n",
    "    fmin=2.5\n",
    "    fmax=100\n",
    "    fs=2000\n",
    "    freqs = np.arange(fmin,fmax)\n",
    "    n_cycles = freqs/3\n",
    "\n",
    "    power = epoch.compute_tfr(\n",
    "        method=\"morlet\", freqs=freqs, n_cycles=n_cycles, return_itc=False, average=False, method_kw\n",
    "        \n",
    "    )\n",
    "\n",
    "    return power\n",
    "results = []\n",
    "for row in mne_epochs.itertuples(index=False):\n",
    "    experiment, rat_id, task = row.experiment, row.rat_id, row.task\n",
    "    door_before,door_after = row.mne_epoch_door_before, row.mne_epoch_door_after\n",
    "    dig_before,dig_after = row.mne_epoch_dig_before, row.mne_epoch_dig_after\n",
    "    around_door, around_dig = row.mne_epoch_around_door, row.mne_epoch_around_dig\n",
    "\n",
    "    power_door_before = get_power_tfr(door_before)\n",
    "    power_door_after = get_power_tfr(door_after)\n",
    "    power_dig_before = get_power_tfr(dig_before)\n",
    "    power_dig_after = get_power_tfr(dig_after)\n",
    "    power_around_door = get_power_tfr(around_door)\n",
    "    power_around_dig = get_power_tfr(around_dig)\n",
    "\n",
    "    net_power = power_dig_before - power_door_before\n",
    "    channel_names = door_before.ch_names\n",
    "    new_row = [experiment, rat_id, task,power_door_before,power_door_after,power_dig_before,power_dig_after, power_around_door, power_around_dig, net_power, channel_names]\n",
    "    results.append(new_row)\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['experiment', 'rat_id', 'task', 'power_pre_door', 'power_post_door','power_pre_dig','power_post_dig','power_around_door','power_around_dig','net_power_pre_dig_pre_door', 'channel_names'])\n",
    "results_df.to_pickle(savepath + 'power_tfr_epochs_mrlt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_power_epoch = results_df['power_pre_door'].iloc[0]\n",
    "test_power_epoch_2 = results_df['power_pre_door'].iloc[1]\n",
    "def plot_power_spec_from_epochs(test_power_epoch):\n",
    "    print(test_power_epoch.ch_names)\n",
    "    aon_channels = [channel for channel in test_power_epoch.ch_names if \"AON\" in channel]\n",
    "    vhp_channels = [channel for channel in test_power_epoch.ch_names if \"vHp\" in channel]\n",
    "    print(aon_channels,vhp_channels)\n",
    "    averaged_epoch_power = test_power_epoch.average(dim='epochs')\n",
    "    averaged_epoch_power.plot(title=\"auto\", vlim = (0, None))\n",
    "    averaged_epoch_power.plot(picks=aon_channels,title=\"AON power\", combine='mean', vlim = (0, None))\n",
    "    averaged_epoch_power.plot(picks=vhp_channels, title=\"VHP power\", combine='mean', vlim = (0, None))\n",
    "    return averaged_epoch_power, aon_channels,vhp_channels\n",
    "epoch1_avg, aon_1, vhp_1 = plot_power_spec_from_epochs(test_power_epoch)\n",
    "epoch2_avg, aon_2, vhp_2 = plot_power_spec_from_epochs(test_power_epoch_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0,1400,8)\n",
    "list(np.arange(0,1600,200))\n",
    "list(np.round(np.arange(0,0.8,0.1), decimals = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_pickle(savepath+'power_tfr_epochs_mrlt.pkl')\n",
    "\n",
    "def make_averaged_power(epoch, area):\n",
    "    #print(epoch.ch_names)\n",
    "    area_channels = [channel for channel in epoch.ch_names if area in channel]\n",
    "    #print(area_channels)\n",
    "\n",
    "    if len(area_channels)==0:\n",
    "        print(\"Error\")\n",
    "        return None\n",
    "    else:\n",
    "        area_epoch = epoch.copy()\n",
    "        area_epoch.pick(area_channels)\n",
    "        averaged_epoch_power = area_epoch.average(dim='epochs')\n",
    "        print(f\"Data shape before mean: {averaged_epoch_power.shape}\")  # DEBUG\n",
    "        mean_ch_power = np.mean(averaged_epoch_power.get_data(), axis = 0)\n",
    "        print(f\"Data shape after mean: {mean_ch_power.shape}\")  # DEBUG\n",
    "        return mean_ch_power\n",
    "\n",
    "# test_averaged_epoch_power = make_averaged_power(test_power_epoch, \"vHp\")\n",
    "# print(test_averaged_epoch_power.shape)\n",
    "\n",
    "for area in [\"AON\", \"vHp\"]:\n",
    "    area_df = pd.DataFrame()\n",
    "    fig, axs = plt.subplots(2,3, figsize= (15,10))\n",
    "    fig.suptitle(f'Average {area} Power')\n",
    "    for rowi, task in enumerate([\"BWcontext\", \"BWnocontext\"]):\n",
    "        task_data=results_df[results_df['task']==task]\n",
    "        print(f\"\\nTask: {task}, Area: {area}, Rows in task_data: {len(task_data)}\")\n",
    "        #for coli, event in enumerate(['power_pre_door', 'power_pre_dig','power_post_dig']):\n",
    "        for coli, event in enumerate(['power_around_door']):\n",
    "        \n",
    "            print(coli,event, task, area)\n",
    "            event_arrays = task_data[event].apply(lambda x: make_averaged_power(x, area))\n",
    "            \n",
    "            valid_arrays = [arr for arr in event_arrays.values if arr is not None]\n",
    "            \n",
    "            print(f\"Valid arrays found: {len(valid_arrays)}\")\n",
    "            \n",
    "            if len(valid_arrays) > 0:\n",
    "                averaged_array = np.mean(np.stack(valid_arrays), axis=0)\n",
    "                print(f\"Averaged array shape: {averaged_array.shape}\")\n",
    "                \n",
    "                ax = axs[rowi, coli]\n",
    "                im = ax.imshow(X= averaged_array, cmap = 'viridis', aspect='auto', origin='lower')\n",
    "                                # Add titles and labels\n",
    "                ax.set_title(f'{event.replace(\"_\", \" \").title()}')\n",
    "                ax.set_xlabel('Time (samples)')\n",
    "                ax.set_ylabel('Frequency (Hz)')\n",
    "                # ax.set_xticks(list(np.arange(0,1600,200)))\n",
    "                # ax.set_xticklabels(list(np.round(np.arange(0,0.8,0.1), decimals = 1)))\n",
    "                # Add colorbar\n",
    "                plt.colorbar(im, ax=ax, label='Power (mV^2/Hz)')\n",
    "                \n",
    "                # Add row labels\n",
    "                if coli == 0:\n",
    "                    ax.set_ylabel(f'{task}\\nFrequency (Hz)', fontweight='bold')\n",
    "                # Add your plotting code here\n",
    "            else:\n",
    "                print(f\"WARNING: No valid data for {area}, {task}, {event}\")\n",
    "                ax = axs[rowi, coli]\n",
    "                ax.text(0.5, 0.5, 'No data', ha='center', va='center')\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(savepath+f'power_spectrogram_{area}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for resulti in results_df.itertuples(index=False):\n",
    "    net_power = resulti.net_power_pre_dig_pre_door\n",
    "    vmin=net_power.min()\n",
    "    vmax=net_power.max()\n",
    "    num_of_trials = net_power.shape[0]\n",
    "    num_of_channels = net_power.shape[1]\n",
    "    fig, axs = plt.subplots(nrows=num_of_channels, ncols=num_of_trials,figsize=(10, 5))\n",
    "    for channeli in range(num_of_channels):\n",
    "        for triali in range(num_of_trials):\n",
    "            net_power_tfr = net_power[triali, channeli, :, :]\n",
    "            ax= axs[channeli, triali]\n",
    "            ax.imshow(net_power_tfr, aspect='auto', origin='lower', extent=[-0.7, 0, 2.5, 100], vmin=vmin, vmax=vmax)\n",
    "\n",
    "    plt.colorbar(ax.images[0], ax=ax, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(power_around_dig.shape)\n",
    "\n",
    "net_power = power_dig_before - power_door_before\n",
    "\n",
    "print(net_power.shape)\n",
    "print(net_power[0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Power for 1s around digging only [NOT USED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_list=['around_dig','around_door']\n",
    "fig, axs=plt.subplots(2,2, figsize=(20,10), sharex=True, sharey=True)\n",
    "axs=axs.flatten()\n",
    "writer=pd.ExcelWriter(savepath+'events_power_spectral_density.xlsx')\n",
    "for i, event in enumerate(events_list):\n",
    "\n",
    "    data = power_df[['rat','task','channel',event]]\n",
    "    data['channel']=data['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "    data_groups=data.groupby(['task','channel'])\n",
    "    mean_data_dict={}\n",
    "    for (task, channel), group in data_groups:\n",
    "        group=group.reset_index(drop=True)\n",
    "        data = np.array(group[event])\n",
    "        data_mean = np.mean(data, axis=0)\n",
    "        data_sem = scipy.stats.sem(data, axis=0)\n",
    "        mean_data_dict[task+'_'+channel+'_mean']=data_mean\n",
    "        mean_data_dict[task+'_'+channel+'_sem']=data_sem\n",
    "        freq = np.linspace(0, 1000, len(data_mean))\n",
    "        ax = axs[i]\n",
    "        ax.set_title(f'{event}')\n",
    "        ax.plot(freq, data_mean, color=plotting_styles.colors[task], linestyle=plotting_styles.linestyles[channel])\n",
    "        ax.fill_between(freq, data_mean - data_sem, data_mean + data_sem, alpha=0.2, color=plotting_styles.colors[task])\n",
    "        ax.set_xlim(0, 100)\n",
    "        ax.set_xlabel('Frequency (Hz)')\n",
    "        ax.set_ylabel('Power uV^2/Hz')\n",
    "    mean_df=pd.DataFrame(mean_data_dict)\n",
    "    #mean_df.to_excel(writer, sheet_name=event)\n",
    "#writer.close()\n",
    "#plt.savefig(savepath+'events_power_spectral_density.png')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Events Power Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Power Boxplots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per Trial [NOT USED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "compiled_data_all_epochs=pd.read_pickle(savepath+'compiled_data_all_epochs_truncated.pkl')\n",
    "boxplot_df=compiled_data_all_epochs.__deepcopy__()\n",
    "\n",
    "boxplot_df.loc[:,['pre_door','post_door','pre_dig','post_dig']]=boxplot_df.loc[:,['pre_door','post_door','pre_dig','post_dig']].applymap(lambda x: power_functions.apply_welch_transform(x))\n",
    "new_boxplot_df=boxplot_df[['rat', 'task', 'date', 'channel','trial']].copy()\n",
    "bands_dict = {'beta': [12, 30], 'gamma': [30, 80], 'theta': [4, 12], 'total': [1, 100]}\n",
    "for col in ['pre_door','post_door','pre_dig','post_dig']:\n",
    "    for band, (band_start, band_end) in bands_dict.items():\n",
    "        new_boxplot_df[band + '_' + col] = boxplot_df[col].apply(lambda x: power_functions.get_band_power(x, band_start, band_end))\n",
    "\n",
    "new_boxplot_df['unique_id'] = new_boxplot_df['rat'] + '_' + new_boxplot_df['task']+ '_' + new_boxplot_df['date']\n",
    "\n",
    "all_boxplot_df=new_boxplot_df.__deepcopy__()\n",
    "\n",
    "all_boxplot_df['channel'] = all_boxplot_df['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "aon_channels=all_boxplot_df[all_boxplot_df['channel']=='AON']\n",
    "vhp_channels=all_boxplot_df[all_boxplot_df['channel']=='vHp']\n",
    "\n",
    "area_list= ['AON', 'vHp']\n",
    "for area in area_list:\n",
    "    area_channels = all_boxplot_df[all_boxplot_df['channel'] == area]\n",
    "    writer=pd.ExcelWriter(savepath+'events_power_per_band_{}_truncated.xlsx'.format(area), engine='xlsxwriter')\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(40, 10), sharex=True, sharey=True)\n",
    "\n",
    "    axs=axs.flatten()\n",
    "    for ax in axs:\n",
    "        ax.tick_params(axis='y', which='both', labelleft=True)\n",
    "    events_dict={'pre_door':'Pre Door', 'post_door':'Post Door', 'pre_dig':'Pre Dig', 'post_dig':'Post Dig'}\n",
    "    for i, event in enumerate(events_dict.keys()):\n",
    "        area_df=area_channels.__deepcopy__()\n",
    "        event_cols = [col for col in area_df.columns if event in col]\n",
    "        print(event_cols)\n",
    "        event_df = area_df[['rat', 'task', 'channel','trial', *event_cols]]\n",
    "        event_df_melted = pd.melt(event_df, id_vars=['rat', 'task', 'channel','trial'], var_name='band', value_name='power')\n",
    "        event_df_melted['band'] = event_df_melted['band'].apply(lambda x: x.split('_')[0])\n",
    "        ax=axs[i]\n",
    "        #sns.boxplot(data=event_df_melted, x='band', y='power', hue='task', hue_order=['BWcontext','BWnocontext'], palette=colors, showfliers=False, ax=axs[i])\n",
    "        #sns.stripplot(data=event_df_melted, x='band', y='power', hue='task', hue_order=['BWcontext','BWnocontext'], palette=colors, dodge=True, alpha=0.5, jitter=0.2, ax=axs[i], linewidth=1, legend=False )\n",
    "        sns.violinplot(x='band',y='power',hue='task',hue_order=['BWcontext','BWnocontext'],data=event_df_melted, ax=ax)\n",
    "        #sns.stripplot(x='band',y='power',hue='task',hue_order=['BWcontext','BWnocontext'],data=event_df,dodge=True,edgecolor='black',linewidth=1,jitter=True, legend=False, ax=ax, color=\".3\", size=2)\n",
    "\n",
    "        ax.set_title(f'{events_dict[event]} {area}', fontsize=20)\n",
    "        ax.set_xlabel('Band', fontsize=20)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        #axs[i].set_yscale('log')\n",
    "        #axs[i].set_ylim(1e-3, 1e3)\n",
    "        if i == 0:\n",
    "            axs[i].set_ylabel('Power (V^2)', fontsize=25)\n",
    "        event_df_melted.to_excel(writer, sheet_name=event)\n",
    "    writer.close()\n",
    "    plt.savefig(savepath+'events_power_per_band_{}_truncated.png'.format(area), format='png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per Trial Multitaper [NOT USED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from mne.time_frequency import psd_array_multitaper\n",
    "\n",
    "compiled_data_all_epochs = pd.read_pickle(savepath + 'compiled_data_all_epochs_truncated.pkl')\n",
    "boxplot_df = compiled_data_all_epochs.__deepcopy__()\n",
    "\n",
    "event_cols = ['pre_door', 'post_door', 'pre_dig', 'post_dig']\n",
    "bands_dict = {'beta': [12, 30], 'gamma': [30, 80], 'theta': [4, 12], 'total': [1, 100]}\n",
    "sfreq = 2000\n",
    "epsilon = 1e-12  # for log-normalization\n",
    "\n",
    "# Apply multitaper PSD to each event column\n",
    "for col in event_cols:\n",
    "    boxplot_df[col] = boxplot_df[col].apply(lambda x: psd_array_multitaper(x, sfreq=sfreq, fmin=0, fmax=1000, adaptive=True, normalization='full', verbose=0, max_iter=500, bandwidth=4)[0])\n",
    "\n",
    "# Calculate band power from multitaper PSD and log-normalize\n",
    "new_boxplot_df = boxplot_df[['rat', 'task', 'date', 'channel', 'trial']].copy()\n",
    "for col in event_cols:\n",
    "    for band, (band_start, band_end) in bands_dict.items():\n",
    "        new_boxplot_df[f'{band}_{col}_mt'] = boxplot_df[col].apply(lambda x: np.log10(power_functions.get_band_power(x, band_start, band_end) + epsilon))\n",
    "\n",
    "new_boxplot_df['unique_id'] = new_boxplot_df['rat'] + '_' + new_boxplot_df['task'] + '_' + new_boxplot_df['date']\n",
    "all_boxplot_df = new_boxplot_df.__deepcopy__()\n",
    "all_boxplot_df['channel'] = all_boxplot_df['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "all_boxplot_df.to_excel(savepath + 'power_per_trial_mt.xlsx', index=False)\n",
    "area_list = ['AON', 'vHp']\n",
    "for area in area_list:\n",
    "    area_channels = all_boxplot_df[all_boxplot_df['channel'] == area]\n",
    "    writer = pd.ExcelWriter(savepath + f'events_power_per_band_multitaper_log10_{area}_truncated.xlsx', engine='xlsxwriter')\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(40, 10), sharex=True, sharey=True)\n",
    "    axs = axs.flatten()\n",
    "    for ax in axs:\n",
    "        ax.tick_params(axis='y', which='both', labelleft=True)\n",
    "    events_dict = {'pre_door': 'Pre Door', 'post_door': 'Post Door', 'pre_dig': 'Pre Dig', 'post_dig': 'Post Dig'}\n",
    "    for i, event in enumerate(events_dict.keys()):\n",
    "        event_cols_mt = [f'{band}_{event}_mt' for band in bands_dict.keys()]\n",
    "        event_df = area_channels[['rat', 'task', 'channel', 'trial', *event_cols_mt]]\n",
    "        event_df_melted = pd.melt(event_df, id_vars=['rat', 'task', 'channel', 'trial'], var_name='band', value_name='power')\n",
    "        event_df_melted['band'] = event_df_melted['band'].apply(lambda x: x.split('_')[0])\n",
    "        ax = axs[i]\n",
    "        sns.violinplot(x='band', y='power', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=event_df_melted, ax=ax)\n",
    "        ax.set_title(f'{events_dict[event]} {area} (Multitaper, log10)', fontsize=20)\n",
    "        ax.set_xlabel('Band', fontsize=20)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        if i == 0:\n",
    "            ax.set_ylabel('log10 Power (V^2)', fontsize=25)\n",
    "        event_df_melted.to_excel(writer, sheet_name=event)\n",
    "    writer.close()\n",
    "    plt.savefig(savepath + f'events_power_per_band_multitaper_log10_{area}_truncated.png', format='png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean across all trials Welch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(power_functions)\n",
    "compiled_data_all_epochs=pd.read_pickle(savepath+'compiled_data_all_epochs_truncated.pkl')\n",
    "boxplot_df=compiled_data_all_epochs.__deepcopy__()\n",
    "event_list=['pre_door','post_door','pre_dig','post_dig']\n",
    "boxplot_df.loc[:,event_list]=boxplot_df.loc[:,event_list].applymap(lambda x: power_functions.apply_welch_transform(x))\n",
    "new_boxplot_df=power_functions.get_all_band_power_from_welchdf(boxplot_df, event_list)\n",
    "new_boxplot_df['channel'] = new_boxplot_df['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "new_boxplot_df['unique_id'] = new_boxplot_df['rat'] + '_' + new_boxplot_df['task']+ '_' + new_boxplot_df['date']\n",
    "print(new_boxplot_df.columns)\n",
    "# Group by unique_id and channel, then take the mean across rows for columns containing 'pre' or 'post'\n",
    "pre_post_cols = [col for col in new_boxplot_df.columns if ('pre' in col or 'post' in col)]\n",
    "mean_data_list = []\n",
    "\n",
    "mean_boxplot_df=new_boxplot_df.__deepcopy__()\n",
    "unique_id_list=list(np.unique(mean_boxplot_df['unique_id']))\n",
    "mean_data_list=[]\n",
    "\n",
    "for unique_id in unique_id_list:\n",
    "    unique_id_df=mean_boxplot_df[mean_boxplot_df['unique_id']==unique_id]\n",
    "    unique_id_df_grouped=unique_id_df.groupby(['channel'])\n",
    "    for channel, group in unique_id_df_grouped:\n",
    "        print(channel)\n",
    "        group=group.reset_index(drop=True)\n",
    "        columns = [col for col in group.columns if 'pre' in col or 'post' in col]\n",
    "        print(columns)\n",
    "        rat_id=group['rat'].iloc[0]\n",
    "        task_id=group['task'].iloc[0]\n",
    "        date_id=group['date'].iloc[0]\n",
    "        channel_id=group['channel'].iloc[0]\n",
    "        mean_data_dict={}\n",
    "        for col in columns:\n",
    "            data=np.array(group[col])\n",
    "            data_mean=np.mean(data,axis=0)\n",
    "            data_sem=scipy.stats.sem(data,axis=0)\n",
    "            mean_data_dict[col+'_mean']=data_mean\n",
    "            mean_data_dict[col+'_sem']=data_sem\n",
    "        mean_data_dict['rat']=rat_id\n",
    "        mean_data_dict['task']=task_id\n",
    "        mean_data_dict['date']=date_id\n",
    "        mean_data_dict['channel']=channel_id\n",
    "        mean_data_list.append(mean_data_dict)\n",
    "mean_df=pd.DataFrame(mean_data_list)\n",
    "mean_df.drop(columns=['date'], inplace=True)\n",
    "\n",
    "\n",
    "mean_df_melted=pd.melt(mean_df, id_vars=['rat','task','channel'], var_name='band', value_name='power')\n",
    "mean_df_melted['band name']=mean_df_melted['band'].apply(lambda x: x.split('_')[0])\n",
    "mean_df_melted['event']=mean_df_melted['band'].apply(lambda x: x.split('_')[1:3])\n",
    "mean_df_melted['event']=mean_df_melted['event'].apply(lambda x: '_'.join(x))\n",
    "mean_df_melted['type']=mean_df_melted['band'].apply(lambda x: x.split('_')[-1])\n",
    "cols = list(mean_df_melted.columns)\n",
    "cols.append(cols.pop(cols.index('power')))\n",
    "mean_df_melted = mean_df_melted[cols]\n",
    "mean_df_melted.drop(columns=['band'], inplace=True)\n",
    "mean_df_melted=mean_df_melted[mean_df_melted['band name']!= 'total'] #Remove total band if it exists\n",
    "mean_df_melted_grouped=mean_df_melted.groupby(['event'])\n",
    "writer=pd.ExcelWriter(savepath+'mean_across_trials_power_truncated.xlsx')\n",
    "for event, group in mean_df_melted_grouped:\n",
    "    print(event)\n",
    "    group=group.reset_index(drop=True)\n",
    "    group.to_excel(writer, sheet_name=event[0])\n",
    "writer.close()\n",
    "arealist=['AON','vHp']\n",
    "for area in arealist:\n",
    "    fig,axs=plt.subplots(1,4,figsize=(40,10), sharey=True)\n",
    "    axs=axs.flatten()\n",
    "    for ax in axs:\n",
    "        ax.tick_params(axis='y', which='both', labelleft=True)\n",
    "    events_dict={'pre_door':'Pre Door', 'post_door':'Post Door', 'pre_dig':'Pre Dig', 'post_dig':'Post Dig'}\n",
    "    for i,event in enumerate(events_dict.keys()):\n",
    "        ax=axs[i]\n",
    "        ## Plotting AON mean power\n",
    "        plotting_df=mean_df_melted[(mean_df_melted['channel'].str.contains(area)) & (mean_df_melted['type']=='mean') & (mean_df_melted['event']==event)] \n",
    "        # Remove outliers using the IQR method for each band name\n",
    "        def remove_outliers_iqr(df, value_col='power', group_col='band name'):\n",
    "            def iqr_filter(group):\n",
    "                q1 = group[value_col].quantile(0.25)\n",
    "                q3 = group[value_col].quantile(0.75)\n",
    "                iqr = q3 - q1\n",
    "                lower = q1 - 1.5 * iqr\n",
    "                upper = q3 + 1.5 * iqr\n",
    "                return group[(group[value_col] >= lower) & (group[value_col] <= upper)]\n",
    "            return df.groupby(group_col, group_keys=False).apply(iqr_filter)\n",
    "\n",
    "        plotting_df = remove_outliers_iqr(plotting_df)\n",
    "        sns.boxplot(x='band name',y='power',hue='task',data=plotting_df,ax=ax, showfliers=False)\n",
    "        sns.stripplot(x='band name',y='power',hue='task',data=plotting_df,dodge=True,edgecolor='black',linewidth=1,jitter=True, legend=False, ax=ax, color=\".3\", size=2)\n",
    "        #ax.set_yscale('log')\n",
    "        ax.set_xlabel('Band', fontsize=20)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        ax.legend(loc='upper left', fontsize=15)\n",
    "        if i == 0:\n",
    "            ax.set_ylabel('Power (V^2)', fontsize=25)\n",
    "        ax.set_title(f'{area} power {events_dict[event]}', fontsize=20)\n",
    "    fig.savefig(savepath+f'mean_power_across_trials_{area}_truncated.png', format='png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "#mean_df=pd.DataFrame(mean_data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multitaper Mean across trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "\n",
    "time_window = 1\n",
    "fs = 2000\n",
    "\n",
    "##############\n",
    "\n",
    "from mne.time_frequency import psd_array_multitaper\n",
    "\n",
    "importlib.reload(power_functions)\n",
    "\n",
    "compiled_data_all_epochs = pd.read_pickle(savepath+f'compiled_data_all_epochs_truncated_{int(time_window*fs)}.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_df=compiled_data_all_epochs.__deepcopy__()\n",
    "# number_per_segment = 700\n",
    "# tukey_window = scipy.signal.get_window(('tukey', 0.1), number_per_segment)\n",
    "columns= ['pre_door', 'post_door', 'pre_dig', 'post_dig', 'around_door', 'around_dig']\n",
    "# Apply multitaper PSD to each event column\n",
    "def multitaper_transform(x):\n",
    "    # x is a 1D array or list of values\n",
    "    psd, _ = psd_array_multitaper(x, sfreq=2000, fmin=0, fmax=1000, adaptive=True, bandwidth=6, normalization='full', verbose=0, max_iter=1000)\n",
    "    return psd\n",
    "\n",
    "event_list=['pre_door','post_door','pre_dig','post_dig']\n",
    "power_df.loc[:, columns] = power_df.loc[:, columns].applymap(multitaper_transform)\n",
    "new_boxplot_df = power_functions.get_all_band_power_from_mt(power_df, event_list)\n",
    "#new_boxplot_df['channel'] = new_boxplot_df['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp') #TRIAL\n",
    "new_boxplot_df['unique_id'] = new_boxplot_df['rat'] + '_' + new_boxplot_df['task'] + '_' + new_boxplot_df['date']\n",
    "print(new_boxplot_df.columns)\n",
    "\n",
    "pre_post_cols = [col for col in new_boxplot_df.columns if ('pre' in col or 'post' in col)]\n",
    "mean_data_list = []\n",
    "\n",
    "mean_boxplot_df = new_boxplot_df.__deepcopy__()\n",
    "unique_id_list = list(np.unique(mean_boxplot_df['unique_id']))\n",
    "mean_data_list = []\n",
    "\n",
    "for unique_id in unique_id_list:\n",
    "    unique_id_df = mean_boxplot_df[mean_boxplot_df['unique_id'] == unique_id]\n",
    "    unique_id_df_grouped = unique_id_df.groupby(['channel'])\n",
    "    for channel, group in unique_id_df_grouped:\n",
    "        print(channel)\n",
    "        group = group.reset_index(drop=True)\n",
    "        columns = [col for col in group.columns if 'pre' in col or 'post' in col]\n",
    "        print(columns)\n",
    "        rat_id = group['rat'].iloc[0]\n",
    "        task_id = group['task'].iloc[0]\n",
    "        date_id = group['date'].iloc[0]\n",
    "        channel_id = group['channel'].iloc[0]\n",
    "        mean_data_dict = {}\n",
    "        for col in columns:\n",
    "            data = np.array(group[col])\n",
    "            data_mean = np.mean(data, axis=0)\n",
    "            data_sem = scipy.stats.sem(data, axis=0)\n",
    "            mean_data_dict[col + '_mean'] = data_mean\n",
    "            mean_data_dict[col + '_sem'] = data_sem\n",
    "        mean_data_dict['rat'] = rat_id\n",
    "        mean_data_dict['task'] = task_id\n",
    "        mean_data_dict['date'] = date_id\n",
    "        mean_data_dict['channel'] = channel_id\n",
    "        mean_data_list.append(mean_data_dict)\n",
    "mean_df = pd.DataFrame(mean_data_list)\n",
    "mean_df.drop(columns=['date'], inplace=True)\n",
    "\n",
    "\n",
    "def remove_outliers_iqr(df, value_col='power', group_col='band name'):\n",
    "    def iqr_filter(group):\n",
    "        q1 = group[value_col].quantile(0.25)\n",
    "        q3 = group[value_col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower = q1 - 1.5 * iqr\n",
    "        upper = q3 + 1.5 * iqr\n",
    "        return group[(group[value_col] >= lower) & (group[value_col] <= upper)]\n",
    "    return df.groupby(group_col, group_keys=False).apply(iqr_filter)\n",
    "\n",
    "mean_df_melted = pd.melt(mean_df, id_vars=['rat', 'task', 'channel'], var_name='band', value_name='power')\n",
    "mean_df_melted['band name'] = mean_df_melted['band'].apply(lambda x: x.split('_')[0])\n",
    "mean_df_melted['event'] = mean_df_melted['band'].apply(lambda x: x.split('_')[1:3])\n",
    "mean_df_melted['event'] = mean_df_melted['event'].apply(lambda x: '_'.join(x))\n",
    "mean_df_melted['type'] = mean_df_melted['band'].apply(lambda x: x.split('_')[-1])\n",
    "cols = list(mean_df_melted.columns)\n",
    "cols.append(cols.pop(cols.index('power')))\n",
    "mean_df_melted = mean_df_melted[cols]\n",
    "mean_df_melted.drop(columns=['band'], inplace=True)\n",
    "#mean_df_melted=mean_df_melted[mean_df_melted['band name']!= 'total'] #Remove total band if it exists\n",
    "mean_df_melted_grouped = mean_df_melted.groupby(['event'])\n",
    "\n",
    "writer_mt = pd.ExcelWriter(savepath + f'pow_events_perband_{int(time_window*fs/2)}ms.xlsx')\n",
    "for event, group in mean_df_melted_grouped:\n",
    "    print(event)\n",
    "    group = group.reset_index(drop=True)\n",
    "    group.to_excel(writer_mt, sheet_name=event[0])\n",
    "writer_mt.close()\n",
    "arealist = ['AON', 'vHp']\n",
    "fig = plt.figure(figsize=(40, 20), layout='constrained')\n",
    "fig.suptitle('Power per band (Multitaper)', fontsize=30)\n",
    "subfigs = fig.subfigures(nrows=2, ncols=1)\n",
    "\n",
    "for area_num,area in enumerate(arealist):\n",
    "    axs = subfigs[area_num].subplots(nrows=1, ncols=4, sharey=True)\n",
    "    axs = axs.flatten()\n",
    "    for ax in axs:\n",
    "        ax.tick_params(axis='y', which='both', labelleft=True)\n",
    "    events_dict = {'pre_door': 'Pre Door', 'post_door': 'Post Door', 'pre_dig': 'Pre Dig', 'post_dig': 'Post Dig'}\n",
    "    for i, event in enumerate(events_dict.keys()):\n",
    "        ax = axs[i]\n",
    "        plotting_df = mean_df_melted[\n",
    "            (mean_df_melted['channel'].str.contains(area)) &\n",
    "            (mean_df_melted['type'] == 'mean') &\n",
    "            (mean_df_melted['event'] == event)\n",
    "        ]\n",
    "\n",
    "        plotting_df = remove_outliers_iqr(plotting_df) ## REMOVE OUTLIERS\n",
    "        band_order = ['theta', 'beta', 'gamma','total']\n",
    "        sns.barplot(x='band name', y='power', hue='task', data=plotting_df, order=band_order, ax=ax)\n",
    "        sns.stripplot(x='band name', y='power', hue='task', data=plotting_df, order=band_order, dodge=True, palette=colors, jitter=True, legend=False, ax=ax, linewidth=1, alpha=0.8)\n",
    "        ax.set_xlabel('Band', fontsize=20)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        task_dict = {'BWcontext': 'Context', 'BWnocontext': 'No Context'}\n",
    "        ax.legend(handles, [task_dict[l] for l in labels], loc='upper right', fontsize=15)\n",
    "        #ax.legend(loc='upper left', fontsize=15)\n",
    "        if i == 0:\n",
    "            ax.set_ylabel('Power (mV^2)', fontsize=25)\n",
    "        else:\n",
    "            ax.set_ylabel('')\n",
    "        ax.set_title(f'{area} power {events_dict[event]}', fontsize=20)\n",
    "fig.savefig(savepath + f'pow_events_perband_{int(time_window*fs/2)}ms.png', format='png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Power using MNE Epochs [NOT MEAN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df_clean = pd.read_pickle(savepath + f'marked_mne_epochs_array_df_truncated_{int(time_window*fs)}_251125.pkl')\n",
    "\n",
    "test_epoch = con_data_df_clean['mne_epoch_dig_before'].iloc[0]\n",
    "test_epoch_array=test_epoch.get_data()\n",
    "print(test_epoch_array.shape)\n",
    "test_epoch_psd = test_epoch.compute_psd(method='multitaper', fmin=0, fmax=100, adaptive=True, bandwidth=6, normalization='full', verbose=0, max_iter=1000, exclude=['Ref'])\n",
    "print(test_epoch_psd.get_data().shape)\n",
    "\n",
    "def multitaper_transform(x):\n",
    "    # x is a 1D array or list of values\n",
    "    psd, _ = psd_array_multitaper(x, sfreq=2000, fmin=0, fmax=1000, adaptive=True, bandwidth=6, normalization='full', verbose=0, max_iter=1000)\n",
    "    return psd\n",
    "\n",
    "single_data = test_epoch_array[0,0,:]\n",
    "psd_single_data = multitaper_transform(single_data)\n",
    "\n",
    "print(test_epoch_psd.get_data()[0,0,:])\n",
    "print(psd_single_data)\n",
    "\n",
    "\n",
    "mne_psd_array=test_epoch_psd.get_data()\n",
    "mne_psd_array_mean=np.mean(mne_psd_array, axis=0)\n",
    "\n",
    "# Create dataframe with channel names and PSD arrays\n",
    "mne_psd_array_df = pd.DataFrame({\n",
    "    'channel': test_epoch_psd.ch_names,\n",
    "    'pre_dig': list(mne_psd_array_mean),\n",
    "    'rat_id':'dk3'\n",
    "})\n",
    "\n",
    "new_boxplot_df = power_functions.get_all_band_power_from_mt(mne_psd_array_df, ['pre_dig'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Power using MNE Epochs [USED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "con_data_df_clean = pd.read_pickle(savepath + f'marked_mne_epochs_array_df_truncated_{int(time_window*fs)}_251125.pkl')\n",
    "event_list = ['mne_epoch_door_before', 'mne_epoch_dig_before', 'mne_epoch_dig_after']\n",
    "\n",
    "# Define frequency bands\n",
    "freq_bands = {\n",
    "        'theta': (4, 12),\n",
    "        'beta': (12, 30),\n",
    "        'gamma': (30, 80),\n",
    "        'total': (0, 100),\n",
    "}\n",
    "\n",
    "def calculate_band_power(psd, freqs, fmin, fmax):\n",
    "    \"\"\"Calculate average power in a frequency band\"\"\"\n",
    "    freq_mask = (freqs >= fmin) & (freqs <= fmax)\n",
    "    if np.sum(freq_mask) == 0:\n",
    "        return np.nan\n",
    "    return np.mean(psd[freq_mask])\n",
    "\n",
    "print(con_data_df_clean.columns)\n",
    "\n",
    "# Dictionary to store dataframes for each event\n",
    "event_dataframes = {event: [] for event in event_list}\n",
    "\n",
    "for i in range(len(con_data_df_clean)):\n",
    "    rat = con_data_df_clean.loc[i, 'rat_id']\n",
    "    task = con_data_df_clean.loc[i, 'task']\n",
    "    date = con_data_df_clean.loc[i, 'date']\n",
    "    \n",
    "    print(f\"Processing rat {rat}, task {task}, date {date} ({i+1}/{len(con_data_df_clean)})\")\n",
    "    \n",
    "    for event in event_list:\n",
    "        test_epoch = con_data_df_clean.loc[i, event]\n",
    "        \n",
    "        # Compute PSD\n",
    "        test_epoch_psd = test_epoch.compute_psd(\n",
    "            method='multitaper', \n",
    "            fmin=0, \n",
    "            fmax=100, \n",
    "            adaptive=True, \n",
    "            bandwidth=6, \n",
    "            normalization='full', \n",
    "            verbose=0,\n",
    "            exclude=['Ref']\n",
    "        )\n",
    "        \n",
    "        # Get PSD data and frequencies\n",
    "        psd_array = test_epoch_psd.get_data()\n",
    "        freqs = test_epoch_psd.freqs\n",
    "        channel_names = test_epoch_psd.ch_names\n",
    "        \n",
    "        # Create rows with band power\n",
    "        band_rows = []\n",
    "        for trial in range(psd_array.shape[0]):\n",
    "            for ch_idx, channel in enumerate(channel_names):\n",
    "                psd = psd_array[trial, ch_idx, :]\n",
    "                \n",
    "                # Calculate power for each frequency band\n",
    "                for band_name, (fmin, fmax) in freq_bands.items():\n",
    "                    power = calculate_band_power(psd, freqs, fmin, fmax)\n",
    "                    \n",
    "                    band_rows.append({\n",
    "                        'rat': rat,\n",
    "                        'date': date,\n",
    "                        'task': task,\n",
    "                        'trial_num': trial,\n",
    "                        'channel': channel,\n",
    "                        'band': band_name,\n",
    "                        'power': power\n",
    "                    })\n",
    "        \n",
    "        # Add to the event's dataframe list\n",
    "        event_dataframes[event].extend(band_rows)\n",
    "\n",
    "# Combine all rows for each event into dataframes\n",
    "print(\"\\nCreating final dataframes...\")\n",
    "final_dfs = {}\n",
    "for event in event_list:\n",
    "    df = pd.DataFrame(event_dataframes[event])\n",
    "    final_dfs[event] = df\n",
    "    print(f\"{event}: {len(df)} rows\")\n",
    "\n",
    "# Save to Excel with multiple sheets\n",
    "excel_filename = savepath + f'pow_events_perband_pertrial_{int(time_window*fs/2)}ms.xlsx'\n",
    "print(f\"\\nSaving to {excel_filename}...\")\n",
    "\n",
    "with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:\n",
    "    for event, df in final_dfs.items():\n",
    "        # Clean up sheet name (Excel has 31 char limit)\n",
    "        sheet_name = event.replace('mne_epoch_', '')[:31]\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        print(f\"Saved sheet: {sheet_name} ({len(df)} rows)\")\n",
    "\n",
    "print(\"\\nDone!\")\n",
    "\n",
    "# Display sample from each dataframe\n",
    "for event, df in final_dfs.items():\n",
    "    print(f\"\\n{event} - First 10 rows:\")\n",
    "    print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a mean across channels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_boxplot_df=boxplot_df.__deepcopy__()\n",
    "unique_id_list=list(np.unique(mean_boxplot_df['unique_id']))\n",
    "mean_data_list=[]\n",
    "sem_data_list=[]\n",
    "mean_boxplot_df['channel']=mean_boxplot_df['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "mean_boxplot_df_grouped=mean_boxplot_df.groupby(['task', 'channel', 'trial'])\n",
    "for (task, channel, trial), group in mean_boxplot_df_grouped:\n",
    "    print(task, channel, trial)\n",
    "    group=group.reset_index(drop=True)\n",
    "    columns=group.columns[-21:-1]\n",
    "    data_array=np.array(group[columns])\n",
    "    data_mean=np.mean(data_array, axis=0)\n",
    "    data_sem=scipy.stats.sem(data_array, axis=0)\n",
    "    print(data_mean)\n",
    "    print(data_sem)\n",
    "    mean_data_dict = {col: data_mean[idx] for idx, col in enumerate(columns)}\n",
    "    sem_data_dict = {col: data_sem[idx] for idx, col in enumerate(columns)}\n",
    "    mean_data_dict['task'] = task\n",
    "    mean_data_dict['channel'] = channel\n",
    "    mean_data_dict['trial'] = trial\n",
    "    sem_data_dict['task'] = task\n",
    "    sem_data_dict['channel'] = channel\n",
    "    sem_data_dict['trial'] = trial\n",
    "    mean_data_list.append(mean_data_dict)\n",
    "    sem_data_list.append(sem_data_dict)\n",
    "\n",
    "mean_df = pd.DataFrame(mean_data_list)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for task in ['BWcontext', 'BWnocontext']:\n",
    "    task_data = mean_df[(mean_df['task'] == task) & (mean_df['channel'] == 'AON')]\n",
    "    ax.plot(task_data['trial'], task_data['total_complete_trial'], label=task, marker='o')\n",
    "ax.set_xlabel('Trial Number')\n",
    "ax.set_ylabel('AON Power in total complete trial')\n",
    "ax.set_title('AON Power in total complete trial across Trials')\n",
    "ax.set_xticks(np.arange(0, 20, 1))\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean in groups of 5 trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_df_grouped=boxplot_df.groupby(['unique_id'])\n",
    "mean_data_list=[]\n",
    "for unique_id, group in boxplot_df_grouped:\n",
    "    print(unique_id)\n",
    "    num_of_trials=len(group['trial'].unique())\n",
    "    print(num_of_trials)\n",
    "    group=group.reset_index(drop=True)\n",
    "    print()\n",
    "    for channel in group['channel'].unique():\n",
    "        i=0\n",
    "        group_channel=group[group['channel']==channel]\n",
    "        group_channel=group_channel.reset_index(drop=True)\n",
    "        \n",
    "        while i < 16:\n",
    "            print(i)\n",
    "            group_trial = group_channel[(group_channel['trial'] >= i) & (group_channel['trial'] < i + 4)]\n",
    "            group_trial_data_array = np.array(group_trial.loc[:, 'beta_pre_door':'total_around_dig'])\n",
    "            data_mean= group_trial_data_array.mean(axis=0)\n",
    "            row = {**group_channel.iloc[0][['rat', 'task', 'channel', 'unique_id']].to_dict(),\n",
    "                   **{'trial': f'{i}-{i + 4}'},\n",
    "                   **dict(zip(group_trial.loc[:, 'beta_pre_door':'total_around_dig'].columns, data_mean))}\n",
    "            mean_data_list.append(row)\n",
    "\n",
    "            i=i+4\n",
    "mean_df = pd.DataFrame(mean_data_list)\n",
    "mean_df_melted=pd.melt(mean_df, id_vars=['rat','task','channel','trial', 'unique_id'], var_name='band_event', value_name='power')\n",
    "mean_df_melted['band name']=mean_df_melted['band_event'].apply(lambda x: x.split('_')[0])\n",
    "mean_df_melted['event']=mean_df_melted['band_event'].apply(lambda x: x.split('_')[1:3])\n",
    "mean_df_melted['event']=mean_df_melted['event'].apply(lambda x: '_'.join(x))\n",
    "mean_df_melted_grouped=mean_df_melted.groupby(['event'])\n",
    "writer=pd.ExcelWriter(savepath+'power_boxplot_average_per_4_trials.xlsx')\n",
    "for event, group in mean_df_melted_grouped:\n",
    "    print(event)\n",
    "    group=group.reset_index(drop=True)\n",
    "    group.drop(columns=['band_event','event'], inplace=True)\n",
    "    group.to_excel(writer, sheet_name=event[0])\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Power Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import spectrogram\n",
    "\n",
    "power_spec_df = compiled_data_all_epochs.__deepcopy__()\n",
    "print(power_spec_df.iloc[0,-2].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_spec_df.iloc[:, -2:] = power_spec_df.iloc[:, -2:].applymap(lambda x: spectrogram(x, fs=2000, nperseg=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(power_spec_df.iloc[0,-2][1])\n",
    "for col in ['around_door','around_dig']:\n",
    "\n",
    "    power_spec_df[col+'_f'] = power_spec_df[col].apply(lambda x: x[0])\n",
    "    power_spec_df[col+'_t'] = power_spec_df[col].apply(lambda x: x[1])\n",
    "    power_spec_df[col+'_sxx'] = power_spec_df[col].apply(lambda x: x[2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_spec_df['channel'] = power_spec_df['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "power_spec_df_grouped = power_spec_df.groupby(['task', 'channel'])\n",
    "for (task, channel), group in power_spec_df_grouped:\n",
    "    group = group.reset_index(drop=True)\n",
    "    for col in ['around_door', 'around_dig']:\n",
    "        data = np.array(group[col + '_sxx'])\n",
    "        data_mean = np.mean(data, axis=0)\n",
    "        print(data_mean.shape)\n",
    "        freq = group[col + '_f'].iloc[0]\n",
    "        time = group[col + '_t'].iloc[0]\n",
    "        time_adjusted=np.linspace(-2,2,len(time))\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(20, 10), constrained_layout=True)\n",
    "        im = ax.pcolormesh(time_adjusted, freq, data_mean, shading='gouraud', vmin=0, vmax=0.5)\n",
    "        fig.colorbar(im, ax=ax)\n",
    "        ax.set_title(f'{task} {channel} {col}')\n",
    "        ax.set_ylim(0, 100)\n",
    "        # ax.set_xticks(np.arange(-2, 3, 1))  # Set x-ticks from -2 to 2 seconds\n",
    "        # ax.set_xticklabels(np.arange(-2, 3, 1))  # Set x-tick labels from -2 to 2 seconds\n",
    "        ax.vlines(0, 0, 100, color='red', linestyle='--')\n",
    "        ax.set_xlabel('Time (s)', fontsize=20)\n",
    "\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        ax.set_ylabel('Frequency (Hz)', fontsize=20)\n",
    "        i = i + 1\n",
    "        fig.savefig(savepath + f'power_mean_spectrogram_{task}_{channel}_{col}.png', dpi=300)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Coherence Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Coherence functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating static data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "importlib.reload(coherence_functions)\n",
    "# --- Basic Parameters ---\n",
    "sfreq = 1000  # Sampling frequency in Hz\n",
    "n_epochs = 20  # Number of trials\n",
    "n_times = 2000  # Number of time points per trial (2 seconds of data)\n",
    "times = np.arange(n_times) / sfreq  # Time vector for one epoch\n",
    "n_signals = 3  # We'll create 3 channels\n",
    "\n",
    "# We will test connectivity in the beta band\n",
    "freq_of_interest = 20.0  # 20 Hz\n",
    "# --- Generate Data for Static Connectivity ---\n",
    "\n",
    "# Initialize data array: (n_epochs, n_signals, n_times)\n",
    "static_data = np.random.randn(n_epochs, n_signals, n_times) * 0.1  # Add background noise\n",
    "\n",
    "# Create the shared 20 Hz sine wave component\n",
    "shared_signal = np.sin(2 * np.pi * freq_of_interest * times)\n",
    "\n",
    "# Add the shared signal to the first two channels for all epochs\n",
    "static_data[:, 0, :] += shared_signal\n",
    "static_data[:, 1, :] += shared_signal\n",
    "\n",
    "print(\"Shape of static_data:\", static_data.shape)\n",
    "ch_names=['AON', 'vHp', 'PFC']  # Example channel names\n",
    "info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types='eeg')\n",
    "static_data_mne=mne.EpochsArray(static_data, info)\n",
    "print(static_data_mne)\n",
    "# Plot the static_data for each channel in the first epoch\n",
    "fig, axs = plt.subplots(n_signals, 1, figsize=(12, 6), sharex=True)\n",
    "for i, ch in enumerate(ch_names):\n",
    "    axs[i].plot(times, static_data[0, i, :], label=f'Channel: {ch}')\n",
    "    axs[i].set_ylabel('Amplitude')\n",
    "    axs[i].legend(loc='upper right')\n",
    "axs[-1].set_xlabel('Time (s)')\n",
    "plt.suptitle('Static Data Example (Epoch 0)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "coherence_band_sce = coherence_functions.convert_epoch_to_coherence(static_data_mne)\n",
    "print(coherence_band_sce)\n",
    "coherence_band_time=coherence_functions.convert_epoch_to_coherence_time(static_data_mne)\n",
    "print(coherence_band_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncating LFP data and loading it into MNE arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import mne_connectivity\n",
    "import sys\n",
    "importlib.reload(lfp_pre_processing_functions)\n",
    "#files=[f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230616_dk6_BW_context_day2.mat']\n",
    "event_data_df=[]\n",
    "con_data_df=[]\n",
    "\n",
    "con_data_df_shuffled=[]\n",
    "shuffled_event_data_df=[]\n",
    "events_codes_all = {}\n",
    "random_baseline_data=[]\n",
    "baseline_lfp_all=[]\n",
    "\n",
    "time_window = 0.7\n",
    "fs= 2000\n",
    "\n",
    "files_short=[files[10]] ### TEST CHANGE THIS \n",
    "\n",
    "\n",
    "for file_num,file in enumerate(files):\n",
    "    #if 'dk1' in file:\n",
    "        \n",
    "        #print(file)\n",
    "        base_name = os.path.basename(file)\n",
    "        base_name, _ = os.path.splitext(base_name)\n",
    "\n",
    "        date, rat_id, task = lfp_pre_processing_functions.exp_params(base_name)\n",
    "        print(date, rat_id, task)\n",
    "        if task == 'nocontextday2' or task == 'nocontextos2':\n",
    "            task = 'nocontext'\n",
    "        if task =='nocontext':\n",
    "            continue\n",
    "        # if rat_id=='dk1': #REMOVING DK1 TEMPORARLILY . PLEASE CHANGE LATER\n",
    "        #     continue\n",
    "        f = h5py.File(file, 'r')\n",
    "        channels = list(f.keys())\n",
    "        #print(channels)\n",
    "         \n",
    "        if not any(\"AON\" in channel or \"vHp\" in channel for channel in channels):\n",
    "            print(\"No AON or vHp channels in this file\")\n",
    "            continue\n",
    "\n",
    "        events,reference_electrode=lfp_pre_processing_functions.get_keyboard_and_ref_channels(f,channels)\n",
    "\n",
    "    #finding global start and end time of all channels, since they start and end recordings at different times\n",
    "        global_start_time, global_end_time=lfp_pre_processing_functions.find_global_start_end_times(f,channels)\n",
    "        \n",
    "        ## Reference electrode finding and padding\n",
    "        reference_time = np.array(reference_electrode['times']).flatten()\n",
    "        reference_value = np.array(reference_electrode['values']).flatten()\n",
    "        padd_ref_data,padded_ref_time=lfp_pre_processing_functions.pad_raw_data_raw_time(reference_value,reference_time,global_start_time,global_end_time,sampling_rate=2000)\n",
    "\n",
    "        events_codes = np.array(events['codes'][0])\n",
    "        events_times = np.array(events['times'][0])\n",
    "        events_codes_all[base_name] = events_codes\n",
    "        epochs = lfp_pre_processing_functions.generate_epochs_with_first_event(events_codes, events_times)\n",
    "        #epochs = functions.generate_specific_num_of_epochs_with_first_event(events_codes, events_times,5)\n",
    "        aon_lfp_channels=[x for x in channels if 'AON' in x ]\n",
    "        vHp_lfp_channels=[x for x in channels if 'vHp' in x ]\n",
    "        all_channels=np.concatenate((aon_lfp_channels,vHp_lfp_channels))\n",
    "        #print(all_channels)\n",
    "        \n",
    "        mne_baseline_data=np.zeros((1,len(all_channels),4000))\n",
    "        mne_epoch_door_before=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_door_after=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_dig_before=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_dig_after=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_around_door=np.zeros((len(epochs),len(all_channels),int(time_window*fs)*2))\n",
    "        mne_epoch_around_dig=np.zeros((len(epochs),len(all_channels),int(time_window*fs)*2))\n",
    "        \n",
    "        mne_baseline_data_shuffled=np.zeros((1,len(all_channels),4000))\n",
    "        mne_epoch_door_before_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_door_after_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_dig_before_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_dig_after_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_around_door_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs*2)))\n",
    "        mne_epoch_around_dig_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs*2)))\n",
    "\n",
    "        print(f'File {rat_id} {task} {date} has {len(epochs)} epochs and {len(all_channels)} channels')\n",
    "\n",
    "\n",
    "        first_event = events_times[0]\n",
    "        \n",
    "        for channel_num,channeli in enumerate(all_channels):\n",
    "            if \"AON\" in channeli or \"vHp\" in channeli:\n",
    "                channel_id = channeli\n",
    "                data_all = f[channeli]\n",
    "                raw_data = np.array(data_all['values']).flatten()\n",
    "                raw_time = np.array(data_all['times']).flatten()\n",
    "                sampling_rate = int(1 / data_all['interval'][0][0])\n",
    "                #print(raw_data.shape, raw_time.shape, sampling_rate)\n",
    "                padded_data,padded_time=lfp_pre_processing_functions.pad_raw_data_raw_time(raw_data,raw_time,global_start_time,global_end_time,sampling_rate)\n",
    "                subtracted_data = padded_data - padd_ref_data\n",
    "                raw_data=subtracted_data\n",
    "                notch_data = lfp_pre_processing_functions.iir_notch(raw_data, sampling_rate, 60) ###CHANGE notch_data to notch_filtered_data\n",
    "\n",
    "                print(notch_data.nbytes)\n",
    "                notch_data_detrended = scipy.signal.detrend(notch_data)\n",
    "                notch_filtered_data=lfp_pre_processing_functions.sosbandpass(notch_data_detrended, fs=2000, start_freq=1,end_freq=100, order=8) ###CHANGE THIS FOR NOT BANDBASS FILTERTING\n",
    "                print(notch_filtered_data.nbytes)\n",
    "                \n",
    "                data_before, time, baseline_mean, baseline_std=lfp_pre_processing_functions.baseline_data_normalization(notch_filtered_data, raw_time, first_event, sampling_rate)\n",
    "                first_event_index=np.where(raw_time>first_event)[0][0]\n",
    "\n",
    "                mne_baseline_data[0,channel_num,:]=list(data_before)\n",
    "                mne_baseline_data_shuffled[0,channel_num,:]=list(np.random.permutation(data_before))\n",
    "                total = notch_filtered_data\n",
    "\n",
    "                \n",
    "                for i, epochi in enumerate(epochs):\n",
    "                    door_timestamp = epochi[0][0]\n",
    "                    trial_type = epochi[0][1]\n",
    "                    dig_type = epochi[1, 1]\n",
    "                    #print(dig_type)\n",
    "                    dig_timestamp = epochi[1, 0]\n",
    "                    #print(door_timestamp, trial_type, dig_timestamp, dig_type)\n",
    "                    data_trial_before, data_trial_after=lfp_pre_processing_functions.extract_event_data(notch_filtered_data,time,door_timestamp,sampling_rate,truncation_time=time_window)\n",
    "                    data_dig_before, data_dig_after=lfp_pre_processing_functions.extract_event_data(notch_filtered_data,time,dig_timestamp,sampling_rate,truncation_time=time_window)\n",
    "                    data_around_door=np.concatenate((data_trial_before, data_trial_after))\n",
    "                    data_around_dig=np.concatenate((data_dig_before, data_dig_after))\n",
    "\n",
    "                    epoch_data = [data_trial_before, data_trial_after, data_dig_before, data_dig_after, data_around_door, data_around_dig]\n",
    "                    event_data_list = [lfp_pre_processing_functions.zscore_event_data(x, baseline_std) for x in epoch_data]\n",
    "\n",
    "                    mne_epoch_door_before[i,channel_num,:]=list(event_data_list[0][-int(time_window*fs):])\n",
    "                    mne_epoch_door_after[i,channel_num,:]=list(event_data_list[1][:int(time_window*fs)])\n",
    "                    mne_epoch_dig_before[i,channel_num,:]=list(event_data_list[2][-int(time_window*fs):])\n",
    "                    mne_epoch_dig_after[i,channel_num,:]=list(event_data_list[3][:int(time_window*fs)])\n",
    "                    mid_point = int(len(event_data_list[4]) / 2)\n",
    "                    mne_epoch_around_door[i,channel_num,:]=list(event_data_list[4][mid_point-int(time_window*fs):mid_point+int(time_window*fs)])\n",
    "                    mne_epoch_around_dig[i,channel_num,:]=list(event_data_list[5][mid_point-int(time_window*fs):mid_point+int(time_window*fs)])\n",
    "\n",
    "                    mne_epoch_door_before_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[0][-int(time_window*fs):]))\n",
    "                    mne_epoch_door_after_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[1][:int(time_window*fs)]))\n",
    "                    mne_epoch_dig_before_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[2][-int(time_window*fs):]))\n",
    "                    mne_epoch_dig_after_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[3][:int(time_window*fs)]))\n",
    "                    mne_epoch_around_door_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[4][mid_point-int(time_window*fs):mid_point+int(time_window*fs)]))\n",
    "                    mne_epoch_around_dig_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[5][mid_point-int(time_window*fs):mid_point+int(time_window*fs)]))\n",
    "\n",
    "        if len(all_channels)>0:\n",
    "            fs=2000\n",
    "            freqs = np.arange(1,100)\n",
    "            n_cycles = freqs/3\n",
    "            info = mne.create_info(ch_names=list(all_channels), sfreq=fs, ch_types='eeg')\n",
    "            mne_baseline = mne.EpochsArray(mne_baseline_data, info)\n",
    "            mne_epoch_door_before = mne.EpochsArray(mne_epoch_door_before, info)\n",
    "            mne_epoch_door_after= mne.EpochsArray(mne_epoch_door_after, info)\n",
    "            mne_epoch_dig_before = mne.EpochsArray(mne_epoch_dig_before, info)\n",
    "            mne_epoch_dig_after = mne.EpochsArray(mne_epoch_dig_after, info)\n",
    "            mne_epoch_around_door = mne.EpochsArray(mne_epoch_around_door, info)\n",
    "            mne_epoch_around_dig = mne.EpochsArray(mne_epoch_around_dig, info)\n",
    "            \n",
    "            row_list=[file_num,date,rat_id,task,mne_baseline,mne_epoch_door_before,mne_epoch_door_after,mne_epoch_dig_before,mne_epoch_dig_after,mne_epoch_around_door,mne_epoch_around_dig]\n",
    "            \n",
    "            mne_baseline_shuffled = mne.EpochsArray(mne_baseline_data_shuffled, info)\n",
    "            mne_epoch_door_before_shuffled = mne.EpochsArray(mne_epoch_door_before_shuffled, info)\n",
    "            mne_epoch_door_after_shuffled = mne.EpochsArray(mne_epoch_door_after_shuffled, info)\n",
    "            mne_epoch_dig_before_shuffled = mne.EpochsArray(mne_epoch_dig_before_shuffled, info)\n",
    "            mne_epoch_dig_after_shuffled = mne.EpochsArray(mne_epoch_dig_after_shuffled, info)\n",
    "            mne_epoch_around_door_shuffled = mne.EpochsArray(mne_epoch_around_door_shuffled, info)\n",
    "            mne_epoch_around_dig_shuffled = mne.EpochsArray(mne_epoch_around_dig_shuffled, info)\n",
    "            row_list_shuffled=[file_num,date,rat_id,task,mne_baseline_shuffled,mne_epoch_door_before_shuffled,mne_epoch_door_after_shuffled,mne_epoch_dig_before_shuffled,mne_epoch_dig_after_shuffled,mne_epoch_around_door_shuffled,mne_epoch_around_dig_shuffled]\n",
    "            shuffled_event_data_df.append(row_list_shuffled)\n",
    "\n",
    "            con_data_df.append(row_list)\n",
    "            con_data_df_shuffled.append(row_list_shuffled)\n",
    "\n",
    "\n",
    "con_data_df=pd.DataFrame(con_data_df, columns=['experiment','date','rat_id','task','mne_baseline','mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after','mne_epoch_around_door','mne_epoch_around_dig'])\n",
    "con_data_df.to_pickle(savepath+f'mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "con_data_df_shuffled=pd.DataFrame(con_data_df_shuffled, columns=['experiment','date','rat_id','task','mne_baseline','mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after','mne_epoch_around_door','mne_epoch_around_dig'])\n",
    "con_data_df_shuffled.to_pickle(savepath+f'mne_epochs_array_df_shuffled_truncated_{int(time_window*fs)}.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy import signal\n",
    "\n",
    "# Before filter\n",
    "f_before, psd_before = signal.welch(notch_data, fs=2000, nperseg=1024)\n",
    "\n",
    "# After filter\n",
    "f_after, psd_after = signal.welch(notch_data_detrended, fs=2000, nperseg=1024)\n",
    "fig,axs=plt.subplots(2,1, figsize = (10,5), sharey=True)\n",
    "axs=axs.flatten()\n",
    "axs[0].semilogy(f_before, psd_before, label='Before filter')\n",
    "axs[1].semilogy(f_after, psd_after, label='After filter')\n",
    "# .xlabel('Frequency (Hz)')\n",
    "# plt.ylabel('Power Spectral Density')\n",
    "axs[0].set_xlim([0, 10])  # Focus on your filter range\n",
    "axs[1].set_xlim([0, 10])  # Focus on your filter range\n",
    "\n",
    "# plt.axvline(1, color='r', linestyle='--', label='Start freq (1 Hz)')\n",
    "# plt.axvline(100, color='g', linestyle='--', label='End freq (100 Hz)')\n",
    "# plt.legend()\n",
    "# plt.title('Frequency Domain')\n",
    "# plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Baseline Coherence Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(coherence_functions)\n",
    "time_window = 1\n",
    "fs = 2000\n",
    "con_data_df = pd.read_pickle(savepath + f'marked_mne_epochs_array_df_truncated_{int(time_window*fs)}_251125.pkl')\n",
    "#con_data_df=pd.read_pickle(savepath+f'mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n",
    "baseline_df=con_data_df.__deepcopy__()\n",
    "baseline_df['mne_baseline']=baseline_df['mne_baseline'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_density(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax= plt.subplots(1, 1, figsize=(10, 5), sharex=True, sharey=True)\n",
    "writer=pd.ExcelWriter(savepath+'coh_baseline_density_normalized.xlsx')\n",
    "baseline_df_grouped=baseline_df.groupby(['task'])\n",
    "task_dict={'BWcontext':'Context','BWnocontext':'No Context'}\n",
    "baseline_dict={}\n",
    "for (task, group) in baseline_df_grouped:\n",
    "    print(task[0])\n",
    "    group=group.reset_index(drop=True)\n",
    "    data = np.array(group['mne_baseline'].tolist())\n",
    "    data_mean = np.mean(data, axis=0)\n",
    "    data_sem = scipy.stats.sem(data, axis=0)\n",
    "    freq = np.linspace(0, 100, len(data_mean))\n",
    "    ax.plot(freq, data_mean, label=task_dict[task[0]])\n",
    "    ax.fill_between(freq, data_mean - data_sem, data_mean + data_sem, alpha=0.2)\n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.set_title(f'Baseline AON-vHp Coherence Density')\n",
    "    ax.set_xlabel('Frequency (Hz)')\n",
    "    ax.set_ylabel('Coherence (Z-transformed)')\n",
    "    ax.legend()\n",
    "    baseline_dict[f'{task[0]}_mean'] = data_mean\n",
    "    baseline_dict[f'{task[0]}_sem'] = data_sem\n",
    "baseline_dict['frequency'] = freq\n",
    "mean_df = pd.DataFrame(baseline_dict)\n",
    "mean_df.to_excel(writer, sheet_name='mean_coherence_density')\n",
    "writer.close()\n",
    "\n",
    "fig.savefig(savepath+'coh_baseline_density_normalized.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Baseline Coherence Boxplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### channel pair averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(coherence_functions)\n",
    "time_window = 1\n",
    "fs = 2000\n",
    "con_data_df = pd.read_pickle(savepath + f'marked_mne_epochs_array_df_truncated_{int(time_window*fs)}_251125.pkl')\n",
    "baseline_df=con_data_df.__deepcopy__()\n",
    "task_dict={'BWcontext':'Context','BWnocontext':'No Context'}\n",
    "bands_dict = {'beta': [12, 30], 'gamma': [30, 80],'theta':[4,12], 'total': [1, 100]}\n",
    "for col in ['mne_baseline']:\n",
    "    print(col)\n",
    "    for band, (band_start, band_end) in bands_dict.items():\n",
    "        baseline_df[band + '_' + col] = baseline_df[col].apply(lambda x: coherence_functions.convert_epoch_to_coherence_baseline(x, band_start=band_start, band_end=band_end))\n",
    "baseline_df.drop(columns=['mne_baseline', 'mne_epoch_door_before', 'mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after','mne_epoch_around_door','mne_epoch_around_dig'], inplace=True)\n",
    "baseline_df_melted=pd.melt(baseline_df, id_vars=['experiment','rat_id','task','date'], var_name='band', value_name='coherence')\n",
    "baseline_df_melted['band']=baseline_df_melted['band'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "\n",
    "####Plotting coherence per band\n",
    "fig, axs= plt.subplots(1, 1, figsize=(20, 10), sharex=True, sharey=True)\n",
    "sns.barplot(x='band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=baseline_df_melted, legend=True, ax=axs)\n",
    "sns.stripplot(x='band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=baseline_df_melted, dodge=True, edgecolor='black', linewidth=1, jitter=True, legend=False, ax=axs)\n",
    "axs.set_title('Baseline Coherence per band', fontsize=20)\n",
    "axs.set_ylabel('Coherence (Z-transformed)', fontsize=20)\n",
    "axs.set_xlabel('')\n",
    "axs.tick_params(axis='both', which='major', labelsize=20)\n",
    "handles, labels = axs.get_legend_handles_labels()\n",
    "axs.legend(handles, [task_dict[l] for l in labels], loc='upper left', fontsize=15)\n",
    "fig.savefig(savepath+'coh_baseline_perband_avgchannelpair_normalized.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "###Writing the baseline coherence per band to excel\n",
    "writer=pd.ExcelWriter(savepath+'coh_baseline_perband_avgchannelpair_normalized.xlsx')\n",
    "baseline_df_melted.to_excel(writer)\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel Pair separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "\n",
    "time_window = 1\n",
    "fs = 2000\n",
    "tanh_norm = True\n",
    "###########\n",
    "\n",
    "if tanh_norm:\n",
    "    suffix ='normalized'\n",
    "else:\n",
    "    suffix = 'nonnormalized'\n",
    "importlib.reload(coherence_functions)\n",
    "\n",
    "con_data_df = pd.read_pickle(savepath + f'marked_mne_epochs_array_{int(time_window*fs)}.pkl')\n",
    "columns_to_process = ['mne_epoch_door_before', 'mne_epoch_door_after', 'mne_baseline']\n",
    "baseline_df = coherence_functions.convert_baseline_to_coherence_mt_expanded(con_data_df[columns_to_process + ['rat_id', 'task']],rat_ids=con_data_df['rat_id'], tasks=con_data_df['task'],columns_to_process=['mne_baseline'], tanh_norm=tanh_norm)\n",
    "baseline_df.drop(columns=['event_type'], inplace=True)\n",
    "baseline_df.to_excel(savepath+f'coh_baseline_perband_channelpair_{suffix}.xlsx')\n",
    "\n",
    "####Plotting coherence per band\n",
    "fig, axs= plt.subplots(1, 1, figsize=(20, 10), sharex=True, sharey=True)\n",
    "sns.barplot(x='frequency_band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=baseline_df, legend=True, ax=axs)\n",
    "sns.stripplot(x='frequency_band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=baseline_df, dodge=True, edgecolor='black', linewidth=1, jitter=True, legend=False, ax=axs)\n",
    "axs.set_title('Baseline Coherence per band', fontsize=20)\n",
    "axs.set_ylabel('Coherence (Z-transformed)', fontsize=20)\n",
    "axs.set_xlabel('')\n",
    "axs.tick_params(axis='both', which='major', labelsize=20)\n",
    "handles, labels = axs.get_legend_handles_labels()\n",
    "axs.legend(handles, [task_dict[l] for l in labels], loc='upper left', fontsize=15)\n",
    "fig.savefig(savepath+f'coh_baseline_perband_channelpair_{suffix}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Coherence around events [door before, door after, dig before, dig after]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating AON-vHp connectivity Spectrogram from Epochs Array and Saving if as a pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "\n",
    "time_window = 1\n",
    "fs = 2000  # Sampling frequency\n",
    "tanh_norm = True\n",
    "###############\n",
    "\n",
    "import numpy as np\n",
    "import mne\n",
    "\n",
    "def randomize_timepoints(epochs, seed=None):\n",
    "    \"\"\"Shuffle time points independently for each channel and epoch.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    data = epochs.get_data()\n",
    "    randomized_data = data.copy()\n",
    "    \n",
    "    for epoch_idx in range(randomized_data.shape[0]):\n",
    "        for channel_idx in range(randomized_data.shape[1]):\n",
    "            rng.shuffle(randomized_data[epoch_idx, channel_idx, :])\n",
    "    \n",
    "    return mne.EpochsArray(randomized_data, epochs.info, \n",
    "                          events=epochs.events, tmin=epochs.tmin)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def coherogram_pkl(time_window, fs, tanh_norm, shuffle=False):\n",
    "    if tanh_norm:\n",
    "        suffix ='_normalized'\n",
    "    else:\n",
    "        suffix ='_non-normalized'\n",
    "\n",
    "    if shuffle:\n",
    "        shuffled = 'shuffled'\n",
    "    else:\n",
    "        shuffled =''\n",
    "\n",
    "    con_data_df_clean = pd.read_pickle(savepath + f'marked_mne_epochs_array_df_truncated_{int(time_window*fs)}_251125.pkl')\n",
    "    event_list=['mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after']\n",
    "\n",
    "    print(event_list)\n",
    "    BWcontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWcontext')]\n",
    "    BWnocontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWnocontext')]\n",
    "    task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "\n",
    "    all_con_data=[]\n",
    "    all_con_data_mean=[]\n",
    "    for task_num,task_name in enumerate(task_data_dict.keys()):\n",
    "            task_data=task_data_dict[task_name]\n",
    "            row=[task_name]\n",
    "            #print(row)\n",
    "            row_2=[task_name]\n",
    "            for event in event_list:\n",
    "                #print(event)\n",
    "                event_epoch_list=task_data[event]\n",
    "                aon_vHp_con=[]\n",
    "                for event_epoch in event_epoch_list:\n",
    "                        #print(row,event, event_epoch) \n",
    "                        if event_epoch.events.shape[0] <5:\n",
    "                            print(f\"Skipping {event} for {task_name} due to insufficient events\")\n",
    "                            continue\n",
    "                        fmin=1\n",
    "                        fmax=100\n",
    "                        fs=2000\n",
    "                        freqs = np.arange(fmin,fmax)\n",
    "                        n_cycles = freqs/3\n",
    "                        \n",
    "                        if shuffle:\n",
    "                            event_epoch = randomize_timepoints(event_epoch, seed=42) ### TURN ON FOR RANDOMIZING\n",
    "\n",
    "                        con = mne_connectivity.spectral_connectivity_epochs(event_epoch, method='coh', sfreq=int(fs),\n",
    "                                                            mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                                            cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False)\n",
    "                        \n",
    "                        coh = con.get_data(output='dense')\n",
    "                        indices = con.names\n",
    "                        \n",
    "\n",
    "                        for i in range(coh.shape[0]):\n",
    "                            for j in range(coh.shape[1]):\n",
    "                                if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                                    coherence= coh[i,j,:,:]\n",
    "                                    if tanh_norm:\n",
    "                                        coherence=np.arctanh(coherence)\n",
    "                                    aon_vHp_con.append(coherence)\n",
    "                row.append(np.mean(aon_vHp_con, axis=0))\n",
    "                row_2.append(np.mean(aon_vHp_con))\n",
    "            all_con_data.append(row)                    \n",
    "            all_con_data_mean.append(row_2)\n",
    "    # Convert all_con_data to a DataFrame for easier manipulation\n",
    "    all_con_data_df = pd.DataFrame(all_con_data, columns=['task'] + event_list)\n",
    "    all_con_data_df.to_pickle(savepath+'coherence_spectrogram_before_after_door_dig_truncated_{}{}{}.pkl'.format(int(time_window*fs), suffix, shuffled))\n",
    "\n",
    "coherogram_pkl(time_window=time_window, fs=fs, tanh_norm=tanh_norm, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "\n",
    "time_window = 1\n",
    "fs = 2000  # Sampling frequency\n",
    "tanh_norm = True\n",
    "shuffle = False\n",
    "###############\n",
    "\n",
    "if tanh_norm:\n",
    "    suffix ='_normalized'\n",
    "else:\n",
    "    suffix ='_non-normalized'\n",
    "\n",
    "if shuffle:\n",
    "    shuffled = '_shuffled'\n",
    "else:\n",
    "    shuffled =''\n",
    "\n",
    "all_con_data_df=pd.read_pickle(savepath+'coherence_spectrogram_before_after_door_dig_truncated_{}{}{}.pkl'.format(int(time_window*fs), suffix, shuffled))\n",
    "event_list=['mne_epoch_door_before','mne_epoch_dig_before','mne_epoch_dig_after']\n",
    "\n",
    "times=np.arange(0, time_window, 1/fs)\n",
    "fig, axs=plt.subplots(2,3, figsize=(15,10), sharey=True)\n",
    "vmin = all_con_data_df[event_list].applymap(np.min).min().min()\n",
    "vmax = all_con_data_df[event_list].applymap(np.max).max().max()\n",
    "event_names=['Before Door','Before Dig','After Dig']\n",
    "\n",
    "writer = pd.ExcelWriter(savepath + 'coh_events_spectrogram_averaged{}_{}ms{}.xlsx'.format(suffix,int(time_window*fs/2), shuffled))\n",
    "\n",
    "\n",
    "for i, event in enumerate(event_list):\n",
    "    axs[0,i].imshow(all_con_data_df[event][0], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[0,i].set_xlabel('Time (s)')\n",
    "    axs[0,i].set_ylabel('Frequency (Hz)')\n",
    "    axs[0,i].set_title(event_names[i])\n",
    "\n",
    "    axs[1,i].imshow(all_con_data_df[event][1], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[1,i].set_xlabel('Time (s)')\n",
    "    axs[1,i].set_ylabel('Frequency (Hz)')\n",
    "    axs[1,i].set_title(event_names[i])\n",
    "    axs[0,0].text(-0.3, 0.5, 'Context', transform=axs[0,0].transAxes, fontsize=14, verticalalignment='center', rotation=90)\n",
    "    axs[1,0].text(-0.3, 0.5, 'No Context', transform=axs[1,0].transAxes, fontsize=14, verticalalignment='center', rotation=90)\n",
    "    \n",
    "    print(all_con_data_df[event][0].shape)\n",
    "    \n",
    "    freqs = [f'{int(freq)}Hz' for freq in np.linspace(1, 100, all_con_data_df[event][0].shape[0])]\n",
    "    freqs.insert(0, 'Frequency (Hz) / Time (s)')\n",
    "    print(len(freqs))\n",
    "    time_points = [f'{np.round(t, 3)}s' for t in np.linspace(0, time_window, all_con_data_df[event][0].shape[1])]\n",
    "\n",
    "    df_context = pd.DataFrame(all_con_data_df[event][0])\n",
    "    df_context.loc[-1] = time_points  # Add time points as the first row\n",
    "    df_context.index = df_context.index + 1  # Shift index\n",
    "    df_context = df_context.sort_index()\n",
    "    df_context.insert(0, 'Frequency (Hz)/ Time (s)', freqs)\n",
    "    df_context.to_excel(writer, sheet_name=f'{event_names[i]}_Context', index=False)\n",
    "\n",
    "    df_nocontext = pd.DataFrame(all_con_data_df[event][1])\n",
    "    df_nocontext.loc[-1] = time_points  # Add time points as the first row\n",
    "    df_nocontext.index = df_nocontext.index + 1  # Shift index\n",
    "    df_nocontext = df_nocontext.sort_index()\n",
    "    df_nocontext.insert(0, 'Frequency (Hz)/ Time (s)', freqs)\n",
    "    df_nocontext.to_excel(writer, sheet_name=f'{event_names[i]}_NoContext', index=False)\n",
    "    \n",
    "    # Add a colorbar\n",
    "writer.close()\n",
    "cbar = fig.colorbar(axs[0,0].images[0], ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "cbar.set_label('Z-Coherence (A.U.)', fontsize=12)\n",
    "fig.savefig(savepath+f'coh_events_spectrogram_averaged{suffix}_{int(time_window*fs/2)}ms{shuffled}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Coherence around events [around door and around dig]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating AON-vHp connectivity around door and dig and saving it in a pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "\n",
    "time_window = 0.4\n",
    "fs = 2000  # Sampling frequency\n",
    "tanh_norm = True\n",
    "###############\n",
    "def coherogram_pkl(time_window, fs, tanh_norm):\n",
    "    if tanh_norm:\n",
    "        suffix ='_normalized'\n",
    "    else:\n",
    "        suffix ='_non-normalized'\n",
    "\n",
    "\n",
    "    con_data_df_clean=pd.read_pickle(savepath+'mne_epochs_array_df_truncated_{}.pkl'.format(int(time_window*fs)))\n",
    "\n",
    "    event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "\n",
    "    print(event_list)\n",
    "    BWcontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWcontext')]\n",
    "    BWnocontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWnocontext')]\n",
    "    task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "\n",
    "    all_con_data=[]\n",
    "    all_con_data_mean=[]\n",
    "    for task_num,task_name in enumerate(task_data_dict.keys()):\n",
    "            task_data=task_data_dict[task_name]\n",
    "            row=[task_name]\n",
    "            #print(row)\n",
    "            row_2=[task_name]\n",
    "            for event in event_list:\n",
    "                #print(event)\n",
    "                event_epoch_list=task_data[event]\n",
    "                aon_vHp_con=[]\n",
    "                for event_epoch in event_epoch_list:\n",
    "                        #print(row,event, event_epoch) \n",
    "                        if event_epoch.events.shape[0] <5:\n",
    "                            print(f\"Skipping {event} for {task_name} due to insufficient events\")\n",
    "                            continue\n",
    "                        fmin=1\n",
    "                        fmax=100\n",
    "                        fs=2000\n",
    "                        freqs = np.arange(fmin,fmax)\n",
    "                        n_cycles = freqs/3\n",
    "\n",
    "\n",
    "                        con = mne_connectivity.spectral_connectivity_epochs(event_epoch, method='coh', sfreq=int(fs),\n",
    "                                                            mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                                            cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False)\n",
    "                        coh = con.get_data(output='dense')\n",
    "                        indices = con.names\n",
    "                        \n",
    "\n",
    "                        for i in range(coh.shape[0]):\n",
    "                            for j in range(coh.shape[1]):\n",
    "                                if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                                    coherence= coh[i,j,:,:]\n",
    "                                    if tanh_norm:\n",
    "                                        coherence=np.arctanh(coherence)\n",
    "                                    aon_vHp_con.append(coherence)\n",
    "                row.append(np.mean(aon_vHp_con, axis=0))\n",
    "                row_2.append(np.mean(aon_vHp_con))\n",
    "            all_con_data.append(row)                    \n",
    "            all_con_data_mean.append(row_2)\n",
    "    # Convert all_con_data to a DataFrame for easier manipulation\n",
    "    all_con_data_df = pd.DataFrame(all_con_data, columns=['task'] + event_list)\n",
    "    all_con_data_df.to_pickle(savepath+'coherence_spectrogram_around_door_dig_truncated_{}{}.pkl'.format(int(time_window*fs), suffix))\n",
    "\n",
    "coherogram_pkl(time_window=time_window, fs=fs, tanh_norm=tanh_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected range check\n",
    "print(\"Expected coherence ranges:\")\n",
    "print(\"Raw coherence: 0 to 1\")\n",
    "print(\"Fisher Z-transform: 0 to infinity (practically -3 to 3)\")\n",
    "\n",
    "# Check if your values are reasonable\n",
    "all_data = np.concatenate([all_con_data_df[event][i].flatten() \n",
    "                          for event in event_list for i in range(2)])\n",
    "\n",
    "if np.any(all_data < -5) or np.any(all_data > 5):\n",
    "    print(\"WARNING: Unusually extreme Fisher Z values detected!\")\n",
    "    print(\"Consider checking your coherence calculation.\")\n",
    "\n",
    "# Check the actual data values\n",
    "print(\"Data statistics:\")\n",
    "for event in event_list:\n",
    "    for i, task in enumerate(['Context', 'No Context']):\n",
    "        data = all_con_data_df[event][i]\n",
    "        print(f\"{task} - {event}:\")\n",
    "        print(f\"  Min: {np.min(data):.3f}\")\n",
    "        print(f\"  Max: {np.max(data):.3f}\")\n",
    "        print(f\"  Mean: {np.mean(data):.3f}\")\n",
    "        print(f\"  Std: {np.std(data):.3f}\")\n",
    "        print(f\"  Median: {np.median(data):.3f}\")\n",
    "        print(f\"  25th percentile: {np.percentile(data, 25):.3f}\")\n",
    "        print(f\"  75th percentile: {np.percentile(data, 75):.3f}\")\n",
    "\n",
    "print(f\"\\nGlobal vmin: {vmin:.3f}\")\n",
    "print(f\"Global vmax: {vmax:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coherence spectrogram [NOT USED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "time_window = 0.4\n",
    "fs = 2000  # Sampling frequency\n",
    "tanh_norm = True\n",
    "###############\n",
    "\n",
    "def plot_coherogram(time_window, fs, tanh_norm):\n",
    "    if tanh_norm:\n",
    "        suffix ='_normalized'\n",
    "    else:\n",
    "        suffix ='_non-normalized'\n",
    "\n",
    "\n",
    "    all_con_data_df=pd.read_pickle(savepath+'coherence_spectrogram_around_door_dig_truncated_{}{}.pkl'.format(int(time_window*fs),suffix))\n",
    "    event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "    fs=2000\n",
    "    times=np.arange(-1*time_window, time_window, 1/fs)\n",
    "    fig, axs=plt.subplots(2,2, figsize=(20,10), sharey=True)\n",
    "    vmin = all_con_data_df[event_list].applymap(np.min).min().min()\n",
    "    vmax = all_con_data_df[event_list].applymap(np.max).max().max()\n",
    "    event_names=['Around Door','Around Dig']\n",
    "    writer = pd.ExcelWriter(savepath + 'coh_events_spectrogram_averaged_normalized_{}{}.xlsx'.format(int(time_window*fs),suffix))\n",
    "\n",
    "    for i, event in enumerate(event_list):\n",
    "        axs[0,i].imshow(all_con_data_df[event][0], extent=[times[0], times[-1], 1, 100],\n",
    "                    aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        axs[0,i].set_xlabel('')\n",
    "\n",
    "        axs[0,i].set_ylabel('Frequency (Hz)', fontsize=20)\n",
    "        axs[0,i].set_title(event_names[i], fontsize=20)\n",
    "        axs[0,i].vlines(0, 0, 100, color='k', linestyle='--')\n",
    "        axs[0,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "        axs[1,i].imshow(all_con_data_df[event][1], extent=[times[0], times[-1], 1, 100],\n",
    "                    aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        axs[1,i].set_xlabel('Time (s)', fontsize=20)\n",
    "        axs[1,i].set_ylabel('Frequency (Hz)', fontsize=20)\n",
    "        axs[1,i].set_title(event_names[i], fontsize=20)\n",
    "        axs[1,i].vlines(0, 0, 100, color='k', linestyle='--')\n",
    "\n",
    "        axs[0,0].text(-0.2, 0.5, 'Context', transform=axs[0,0].transAxes, fontsize=18, verticalalignment='center', rotation=90)\n",
    "        axs[1,0].text(-0.2, 0.5, 'No Context', transform=axs[1,0].transAxes, fontsize=18, verticalalignment='center', rotation=90)\n",
    "        axs[0,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "        axs[1,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "        axs[0,i].set_xticks(np.arange(-1*time_window, time_window+0.1, time_window))  # Set x-ticks from -1 to 1 seconds\n",
    "        axs[0,i].set_xticklabels(np.arange(-1*time_window, time_window+0.1, time_window))  # Set x-tick labels from -1 to 1 seconds\n",
    "        axs[1,i].set_xticks(np.arange(-1*time_window, time_window+0.1, time_window))  # Set x-ticks from -1 to 1 seconds\n",
    "        axs[1,i].set_xticklabels(np.arange(-1*time_window, time_window+0.1, time_window))  # Set x-tick labels from -1 to 1 seconds\n",
    "\n",
    "        print(all_con_data_df[event][0].shape)\n",
    "        \n",
    "        freqs = [f'{int(freq)}Hz' for freq in np.linspace(1, 100, all_con_data_df[event][0].shape[0])]\n",
    "        freqs.insert(0, 'Frequency (Hz) / Time (s)')\n",
    "        print(len(freqs))\n",
    "        time_points = [f'{np.round(t, 3)}s' for t in np.linspace(-1*time_window, time_window, all_con_data_df[event][0].shape[1])]\n",
    "\n",
    "        df_context = pd.DataFrame(all_con_data_df[event][0])\n",
    "        df_context.loc[-1] = time_points  # Add time points as the first row\n",
    "        df_context.index = df_context.index + 1  # Shift index\n",
    "        df_context = df_context.sort_index()\n",
    "        df_context.insert(0, 'Frequency (Hz)/ Time (s)', freqs)\n",
    "        df_context.to_excel(writer, sheet_name=f'{event_names[i]}_Context', index=False)\n",
    "    \n",
    "        df_nocontext = pd.DataFrame(all_con_data_df[event][1])\n",
    "        df_nocontext.loc[-1] = time_points  # Add time points as the first row\n",
    "        df_nocontext.index = df_nocontext.index + 1  # Shift index\n",
    "        df_nocontext = df_nocontext.sort_index()\n",
    "        df_nocontext.insert(0, 'Frequency (Hz)/ Time (s)', freqs)\n",
    "        df_nocontext.to_excel(writer, sheet_name=f'{event_names[i]}_NoContext', index=False)\n",
    "\n",
    "    writer.close()\n",
    "    # Add a colorbar\n",
    "    cbar = fig.colorbar(axs[0,0].images[0], ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "    cbar.set_label('Coherence', loc='center', fontsize=20, labelpad=10)\n",
    "    cbar.ax.tick_params(labelsize=20)  # Set colorbar tick label size\n",
    "\n",
    "    fig.savefig(savepath + f'\\\\aon_vhp_coherence_event_spectrogram_{int(time_window*fs)}{suffix}.png',format='png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_coherogram(time_window=time_window, fs=fs, tanh_norm=tanh_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Coherogram for each experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "time_window = 1\n",
    "fs = 2000  # Sampling frequency\n",
    "tanh_norm = True\n",
    "###############\n",
    "def coherogram_perexperiment_pkl(time_window, fs, tanh_norm):\n",
    "\n",
    "    if tanh_norm:\n",
    "        suffix ='normalized'\n",
    "    else:\n",
    "        suffix ='nonnormalized'\n",
    "\n",
    "    con_data_df_clean = pd.read_pickle(savepath + f'marked_mne_epochs_array_df_truncated_{int(time_window*fs)}_251125.pkl')\n",
    "\n",
    "    event_list=['mne_epoch_door_before','mne_epoch_dig_before','mne_epoch_dig_after']\n",
    "\n",
    "    print(event_list)\n",
    "\n",
    "    test_list = [con_data_df_clean.iloc[0]]\n",
    "    mean_con_data=pd.DataFrame()\n",
    "    def epoch_coherogram(epoch, fmin=1, fmax=100, fs=2000):\n",
    "            print(epoch.events.shape)\n",
    "        # if epoch.events.shape[0] < 5:\n",
    "        #     print(\"Not enough events in the epoch\")\n",
    "        #     return None\n",
    "        # else:\n",
    "            freqs = np.arange(fmin, fmax)\n",
    "            n_cycles = freqs / 3\n",
    "            con = mne_connectivity.spectral_connectivity_epochs(epoch, method='coh', sfreq=int(fs),\n",
    "                                                                mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                                                cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False)\n",
    "            coh = con.get_data(output='dense')\n",
    "            indices = con.names\n",
    "            aon_vHp_con = []\n",
    "            for i in range(coh.shape[0]):\n",
    "                for j in range(coh.shape[1]):\n",
    "                    if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                        coherence= coh[i,j,:,:]\n",
    "                        if tanh_norm:\n",
    "                            coherence=np.arctanh(coherence)\n",
    "                        aon_vHp_con.append(coherence)\n",
    "            \n",
    "            mean_con = np.mean(aon_vHp_con, axis=0)\n",
    "            return mean_con\n",
    "    mean_con_data['mne_epoch_door_before'] = con_data_df_clean['mne_epoch_door_before'].apply(epoch_coherogram)\n",
    "    mean_con_data['mne_epoch_dig_before'] = con_data_df_clean['mne_epoch_dig_before'].apply(epoch_coherogram)\n",
    "    mean_con_data['mne_epoch_dig_after'] = con_data_df_clean['mne_epoch_dig_after'].apply(epoch_coherogram)\n",
    "\n",
    "    mean_con_data['experiment'] = con_data_df_clean['experiment']\n",
    "    mean_con_data['date'] = con_data_df_clean['date']\n",
    "    mean_con_data['task'] = con_data_df_clean['task']\n",
    "    mean_con_data['rat_id'] = con_data_df_clean['rat_id']\n",
    "    mean_con_data.dropna(inplace=True)\n",
    "    mean_con_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    mean_con_data.to_pickle(savepath + f'marked_coherence_around_events_mean_{int(time_window*fs)}.pkl')\n",
    "\n",
    "coherogram_perexperiment_pkl(time_window=time_window, fs=fs, tanh_norm=tanh_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "time_window = 1\n",
    "fs = 2000  # Sampling frequency\n",
    "tanh_norm = True\n",
    "#################\n",
    "\n",
    "event_of_interest_dict = {'mne_epoch_door_before':'door_before','mne_epoch_dig_before':'dig_before','mne_epoch_dig_after':'dig_after'}\n",
    "\n",
    "event_of_interest = 'mne_epoch_dig_after'\n",
    "\n",
    "def plot_coherogram_perexperiment(time_window, fs, tanh_norm):\n",
    "    if tanh_norm:\n",
    "        suffix ='normalized'\n",
    "    else:\n",
    "        suffix ='nonnormalized'\n",
    "\n",
    "    mean_con_data=pd.read_pickle(savepath + f'marked_coherence_around_events_mean_{int(time_window*fs)}.pkl')\n",
    "    vmin = mean_con_data[event_of_interest].apply(np.min).min()\n",
    "    vmax = mean_con_data[event_of_interest].apply(np.max).max()\n",
    "\n",
    "    BWcontext_data=mean_con_data[(mean_con_data['task']=='BWcontext')]\n",
    "    BWnocontext_data=mean_con_data[(mean_con_data['task']=='BWnocontext')]\n",
    "    task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "    rat_ids, rat_nums = np.unique(BWcontext_data['rat_id'], return_counts=True)\n",
    "    print(rat_ids, rat_nums)\n",
    "    rat_nums_max = rat_nums.max()\n",
    "    print(rat_nums_max)\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    for group_name, group_df in task_data_dict.items():\n",
    "        writer = pd.ExcelWriter(savepath + f'coh_events_spectrogram_perexp_{group_name}_{event_of_interest_dict[event_of_interest]}_{int(time_window*fs/2)}.xlsx')\n",
    "\n",
    "        print(f\"Plotting group: {group_name}\")\n",
    "        group_dict = {'BWcontext': 'Context', 'BWnocontext': 'No Context'}\n",
    "        rat_ids, rat_nums = np.unique(group_df['rat_id'], return_counts=True)\n",
    "        rat_nums_max = rat_nums.max()\n",
    "\n",
    "        num_of_rows = 4 # Each row should be a rats\n",
    "        num_of_cols = rat_nums_max # Each column should be the max number of experiments for a rat\n",
    "\n",
    "        fig, axs = plt.subplots(num_of_rows, num_of_cols, figsize=(25, 10), sharex=True, sharey=True)\n",
    "        dk1_count = 0\n",
    "        dk3_count = 0\n",
    "        dk5_count = 0\n",
    "        dk6_count = 0\n",
    "        for i, (idx, row) in enumerate(group_df.iterrows()):\n",
    "            rat_id = row['rat_id']\n",
    "            data = np.array(row[event_of_interest])\n",
    "            if rat_id == 'dk1':\n",
    "                ax=axs[0, dk1_count]\n",
    "                dk1_count += 1\n",
    "            elif rat_id == 'dk3':\n",
    "                ax=axs[1, dk3_count]\n",
    "                dk3_count += 1\n",
    "            elif rat_id == 'dk5':\n",
    "                ax=axs[2, dk5_count]\n",
    "                dk5_count += 1\n",
    "            elif rat_id == 'dk6':\n",
    "                ax=axs[3, dk6_count]\n",
    "                dk6_count += 1\n",
    "            im = ax.imshow(data, extent=[0, time_window, 1, 100], aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "            ax.set_title(f\"{row['rat_id']} {row['date']}\")\n",
    "\n",
    "            ##### Writing to excel\n",
    "\n",
    "            freqs = [f'{int(freq)}Hz' for freq in np.linspace(1, 100, data.shape[0])]\n",
    "            freqs.insert(0, 'Frequency (Hz) / Time (s)')\n",
    "            print(len(freqs))\n",
    "            time_points = [f'{np.round(t, 3)}s' for t in np.linspace(0, time_window, data.shape[1])]\n",
    "\n",
    "            df_towrite = pd.DataFrame(data)\n",
    "            df_towrite.loc[-1] = time_points  # Add time points as the first row\n",
    "            df_towrite.index = df_towrite.index + 1  # Shift index\n",
    "            df_towrite = df_towrite.sort_index()\n",
    "            df_towrite.insert(0, 'Frequency (Hz)/ Time (s)', freqs)\n",
    "            df_towrite.to_excel(writer, sheet_name=f'{group_dict[group_name]}_{rat_id}_{row[\"date\"]}', index=False)\n",
    "\n",
    "        for j in range(i + 1, len(axs)):\n",
    "            fig.delaxes(axs[j])\n",
    "        fig.suptitle(f\"{group_dict[group_name]} {suffix} {event_of_interest_dict[event_of_interest]}\", fontsize=16)\n",
    "        fig.colorbar(im, ax=axs, orientation='vertical', fraction=0.02, label=f'{suffix} Coherence(A.U.)')\n",
    "        fig.savefig(savepath + f'coh_events_spectrogram_perexp_{group_name}_{event_of_interest_dict[event_of_interest]}_{int(time_window*fs/2)}ms.png', dpi=300, bbox_inches='tight')\n",
    "        #plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.show()\n",
    "        writer.close()\n",
    "plot_coherogram_perexperiment(time_window=time_window, fs=fs, tanh_norm=tanh_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Coherograms of single trials [NOT USED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 0.7  # seconds\n",
    "fs = 2000  # Sampling frequency\n",
    "single_epochs_df=pd.read_pickle(savepath+f'behavior_coherence_single_epochs_mne_truncated_{int(time_window*fs)}.pkl')\n",
    "unique_id_list = single_epochs_df['unique_id'].unique()\n",
    "\n",
    "print(unique_id_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_id_list = unique_id_list[0:1]\n",
    "for unique_id in unique_id_list:\n",
    "    unique_id_df = single_epochs_df[single_epochs_df['unique_id'] == unique_id]\n",
    "    trial_nums = len(unique_id_df['trial'].unique())\n",
    "    fig, axs = plt.subplots(8, trial_nums, figsize=(20, 10), sharex=True)\n",
    "    fig.suptitle(f'Unique ID: {unique_id} - AON-vHp Coherence Around Dig', fontsize=16)\n",
    "    for trial_idi in unique_id_df['trial'].unique():\n",
    "        trial_df = unique_id_df[unique_id_df['trial'] == trial_idi]\n",
    "        mne_epoch_around_dig = trial_df['around_dig'].iloc[0]\n",
    "        fmin=1\n",
    "        fmax=100\n",
    "        freqs = np.arange(fmin, fmax)\n",
    "        n_cycles = freqs / 3\n",
    "        con = mne_connectivity.spectral_connectivity_epochs(mne_epoch_around_dig, method='coh', sfreq=int(fs),\n",
    "                                                mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                                cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False, n_jobs=-1)\n",
    "        coh = con.get_data(output='dense')\n",
    "        indices = con.names\n",
    "        aon_vHp_con = []\n",
    "        channel_pair =0\n",
    "        for i in range(coh.shape[0]):\n",
    "            for j in range(coh.shape[1]):\n",
    "                if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                    coherence= coh[i,j,:,:]\n",
    "                    coherence=np.arctanh(coherence)\n",
    "                    aon_vHp_con.append(coherence)\n",
    "                    axs[channel_pair, trial_idi].imshow(coherence, extent=[-time_window, time_window, 1, 100], aspect='auto', origin='lower')\n",
    "                    if channel_pair == 0:\n",
    "                        axs[channel_pair, trial_idi].set_title(f'Trial {trial_idi}')\n",
    "                    if trial_idi == 0:\n",
    "                        axs[channel_pair, trial_idi].set_ylabel(f'{indices[i]}-{indices[j]}')\n",
    "                    channel_pair += 1\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherence Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(coherence_functions)\n",
    "time_window = 1\n",
    "fs = 2000\n",
    "\n",
    "# test_epoch = con_data_df['mne_epoch_door_before'].iloc[0]\n",
    "# fmin=1\n",
    "# fmax = 100\n",
    "# tanh_norm = True\n",
    "# con=mne_connectivity.spectral_connectivity_epochs(test_epoch, method='coh', sfreq=int(2000), fmin=fmin, fmax=fmax,faverage=False, mode='multitaper',mt_bandwidth = 2.8,mt_adaptive=True, mt_low_bias=True, verbose=False)\n",
    "# coh = con.get_data(output='dense')\n",
    "# #print(coh)\n",
    "# indices = con.names\n",
    "# #print(indices)\n",
    "# aon_vhp_con=[]\n",
    "# print(coh.shape)\n",
    "# for i in range(coh.shape[0]):\n",
    "#     for j in range(coh.shape[1]):\n",
    "#         #print(i,j)\n",
    "#         if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "#             print('AON and vHp found')\n",
    "#             coherence = coh[i,j,:]\n",
    "#             if tanh_norm:\n",
    "#                 coherence=np.arctanh(coherence)  # Convert to Fisher Z-score\n",
    "#             aon_vhp_con.append(coherence)\n",
    "# aon_vhp_con_mean = np.mean(aon_vhp_con, axis=0)\n",
    "\n",
    "test_epoch_1 = coherence_functions.convert_event_epoch_to_coherence_density_mt(test_epoch, shuffle=False)\n",
    "print(test_epoch_1)\n",
    "\n",
    "shuffled_dict = {True : 'shuffled', False : 'real'}\n",
    "event_dict = {'mne_epoch_door_before' : 'Before Door', 'mne_epoch_dig_before' : 'Before Dig','mne_epoch_dig_after' : 'After Dig'}\n",
    "csd_event_dict = {}\n",
    "writer = pd.ExcelWriter(savepath+'coh_events_density.xlsx')\n",
    "for shuffle in shuffled_dict.keys():\n",
    "    print(shuffle)\n",
    "    con_data_df = pd.read_pickle(savepath + f'marked_mne_epochs_array_{int(time_window*fs)}.pkl')\n",
    "    for event_name in event_dict.keys():\n",
    "        print(event_name)\n",
    "        con_data_df[event_name]=con_data_df[event_name].apply(lambda x: coherence_functions.convert_event_epoch_to_coherence_density_mt(x, shuffle=shuffle))\n",
    "        con_data_df_grouped=con_data_df.groupby(['task'])\n",
    "        task_dict={'BWcontext':'Context','BWnocontext':'No Context'}\n",
    "        baseline_dict={}\n",
    "        for (task, group) in con_data_df_grouped:\n",
    "            print(task[0])\n",
    "            group=group.reset_index(drop=True)\n",
    "            data = np.array(group[event_name].tolist())\n",
    "            data_mean = np.mean(data, axis=0)\n",
    "            data_sem = scipy.stats.sem(data, axis=0)\n",
    "            freq = np.linspace(0, 100, len(data_mean))\n",
    "            csd_event_dict[f'{event_dict[event_name]}_{task[0]}_{shuffled_dict[shuffle]}_mean'] = data_mean\n",
    "            csd_event_dict[f'{event_dict[event_name]}_{task[0]}_{shuffled_dict[shuffle]}_sem'] = data_sem\n",
    "csd_event_dict['frequency'] = freq\n",
    "mean_df = pd.DataFrame(csd_event_dict)\n",
    "mean_df.to_excel(writer)\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Coherence Boxplots "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Aon-vHp connectivity per band and storing it in pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "time_window = 0.7\n",
    "fs = 2000  # Sampling frequency\n",
    "############\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "importlib.reload(coherence_functions)\n",
    "\n",
    "\n",
    "con_data_df_clean=pd.read_pickle(savepath+f'mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "single_baseline_epoch=con_data_df_clean['mne_epoch_door_before'].iloc[0]\n",
    "theta_band=[4,8]\n",
    "\n",
    "theta_coherence=coherence_functions.convert_epoch_to_coherence_time(single_baseline_epoch)\n",
    "print(theta_coherence)\n",
    "\n",
    "print(coherence_functions.convert_epoch_to_coherence_mt(single_baseline_epoch, tanh_norm=True))\n",
    "\n",
    "\n",
    "def convert_epoch_to_coherence_mt_per_channel(epoch, tanh_norm=True, fmin=1, fmax=100, fs=2000):\n",
    "    band_dict={'beta':[12,30],'gamma':[30,80],'total':[1,100], 'theta':[4,12]}\n",
    "    coherence_dict={}\n",
    "    coherence_channel_dict={}\n",
    "    for band in band_dict.keys():\n",
    "        \n",
    "        fmin=band_dict[band][0]\n",
    "        fmax=band_dict[band][1]\n",
    "        freqs = np.arange(fmin,fmax)\n",
    "        #print(n_cycles)\n",
    "        con=mne_connectivity.spectral_connectivity_epochs(epoch, method='coh', sfreq=int(2000), fmin=fmin, fmax=fmax,faverage=True, mode='multitaper',mt_bandwidth = 2.8,mt_adaptive=True, mt_low_bias=True, verbose=False, n_jobs=-1)\n",
    "        coh = con.get_data(output='dense')\n",
    "        #print(coh)\n",
    "        indices = con.names\n",
    "        #print(indices)\n",
    "        aon_vhp_con=[]\n",
    "        print(coh.shape)\n",
    "        channel_dict={}\n",
    "        for i in range(coh.shape[0]):\n",
    "            for j in range(coh.shape[1]):\n",
    "                #print(i,j)\n",
    "                if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                    print('AON and vHp found')\n",
    "                    coherence = coh[i,j,:]\n",
    "                    if tanh_norm:\n",
    "                        coherence=np.arctanh(coherence)  # Convert to Fisher Z-score\n",
    "                    channel_dict[f'{indices[i]}-{indices[j]}']=coherence\n",
    "                    \n",
    "                    aon_vhp_con.append(np.mean(coherence))\n",
    "                    #print('freqs averaged',coh[i,j,0,:].shape)\n",
    "                    #print(coh[0,i,j,:])\n",
    "                else:\n",
    "                    continue\n",
    "        if aon_vhp_con==[]:\n",
    "            print('no coherence found')\n",
    "        else:\n",
    "            #print(aon_vhp_con)\n",
    "            aon_vhp_con_mean=np.mean(aon_vhp_con, axis=0)\n",
    "            #print(aon_vhp_con_mean, 'coherenece')\n",
    "            coherence_dict[band]=aon_vhp_con_mean\n",
    "            coherence_channel_dict[band]=channel_dict\n",
    "    return coherence_dict, coherence_channel_dict\n",
    "\n",
    "single_baseline_epoch=con_data_df_clean['mne_epoch_door_before'].iloc[0]\n",
    "band_coherence, channel_coherence=convert_epoch_to_coherence_mt_per_channel(single_baseline_epoch, tanh_norm=True)\n",
    "print(band_coherence)\n",
    "print(channel_coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importlib.reload(coherence_functions)\n",
    "def epoch_coherence_channelpair_multiple(time_window, fs, tanh_norm=True):\n",
    "    \"\"\"\n",
    "    Process multiple epoch columns and return coherence DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    time_window : float\n",
    "        Time window in seconds\n",
    "    fs : int\n",
    "        Sampling frequency\n",
    "    tanh_norm : bool\n",
    "        Whether to apply Fisher Z-transformation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Coherence DataFrame with event types\n",
    "    \n",
    "    \"\"\"\n",
    "    if tanh_norm:\n",
    "        suffix ='normalized'\n",
    "    else:\n",
    "        suffix ='nonnormalized'\n",
    "\n",
    "    con_data_df_clean = pd.read_pickle(savepath + f'marked_mne_epochs_array_df_truncated_{int(time_window*fs)}_251125.pkl')\n",
    "    #con_data_df_shuffled = pd.read_pickle(savepath + f'mne_epochs_array_df_shuffled_truncated_{int(time_window*fs)}.pkl')\n",
    "    columns_to_process = ['mne_epoch_door_before', 'mne_epoch_door_after', 'mne_epoch_dig_before', 'mne_epoch_dig_after']\n",
    "\n",
    "    coherence_df = coherence_functions.convert_epochs_to_coherence_mt_expanded(\n",
    "        \n",
    "        con_data_df_clean[columns_to_process + ['rat_id', 'task']],  # Include necessary columns\n",
    "        con_data_df_clean['rat_id'], \n",
    "        con_data_df_clean['task'],\n",
    "        columns_to_process,\n",
    "        tanh_norm=tanh_norm\n",
    "    )\n",
    "    shuffled_coherence_df = coherence_functions.convert_epochs_to_coherence_mt_expanded(\n",
    "        con_data_df_clean[columns_to_process + ['rat_id', 'task']],  # Include necessary columns\n",
    "        con_data_df_clean['rat_id'], \n",
    "        con_data_df_clean['task'],\n",
    "        columns_to_process,\n",
    "        tanh_norm=tanh_norm,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    coherence_df.to_pickle(savepath + f'coherence_channelpair_{int(time_window*fs)}_{suffix}.pkl')\n",
    "    shuffled_coherence_df.to_pickle(savepath + f'coherence_channelpair_shuffled_{int(time_window*fs)}_{suffix}.pkl')\n",
    "\n",
    "def plot_coherence_channelpair(time_window, fs, tanh_norm=True):\n",
    "    if tanh_norm:\n",
    "        suffix ='normalized'\n",
    "    else:\n",
    "        suffix ='nonnormalized'\n",
    "    coherence_df = pd.read_pickle(savepath + f'coherence_channelpair_{int(time_window*fs)}_{suffix}.pkl')\n",
    "    shuffled_coherence_df = pd.read_pickle(savepath + f'coherence_channelpair_shuffled_{int(time_window*fs)}_{suffix}.pkl')\n",
    "    generate_events_boxplots(time_window, fs, suffix, coherence_df)\n",
    "    generate_events_boxplots(time_window, fs, suffix+'_shuffled', shuffled_coherence_df)\n",
    "\n",
    "\n",
    "def generate_events_boxplots(time_window, fs, suffix, coherence_df):\n",
    "    event_dict = {\n",
    "        'mne_epoch_door_before': 'Door Before',\n",
    "        'mne_epoch_door_after': 'Door After',\n",
    "        'mne_epoch_dig_before': 'Dig Before',\n",
    "        'mne_epoch_dig_after': 'Dig After'\n",
    "    }\n",
    "    coherence_df['event_type'] = coherence_df['event_type'].map(event_dict)\n",
    "    \n",
    "    vmin = coherence_df['coherence'].min()\n",
    "    vmax = coherence_df['coherence'].max()\n",
    "    print(f\"Global vmin: {vmin}, vmax: {vmax}\")\n",
    "\n",
    "    event_types = coherence_df['event_type'].unique()\n",
    "    num_event_types = len(event_types)\n",
    "    writer=pd.ExcelWriter(savepath + f'\\\\coh_events_perband_channelpair_{suffix}_{int(time_window*fs/2)}ms.xlsx')\n",
    "\n",
    "    fig, axs = plt.subplots(1, num_event_types, figsize=(40,10), sharey=True)\n",
    "    task_dict = {'BWcontext': 'Context', 'BWnocontext': 'No Context'}\n",
    "    band_order = ['theta', 'beta','theta+beta','gamma', 'total']\n",
    "    \n",
    "    for i, event_type in enumerate(event_types):\n",
    "        ax=axs[i] if num_event_types > 1 else axs\n",
    "        event_data_df_melted = coherence_df[coherence_df['event_type'] == event_type]\n",
    "        event_data_df_melted['band'] = pd.Categorical(event_data_df_melted['frequency_band'], categories=band_order, ordered=True)\n",
    "        sns.barplot(x='band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=event_data_df_melted, order=band_order, legend=True, ax=axs[i])\n",
    "        sns.stripplot(x='band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=event_data_df_melted, order=band_order, dodge=True, edgecolor='black', linewidth=1, jitter=True, legend=False, ax=axs[i])\n",
    "        #axs[i].set_xticklabels(['Total', 'Theta', 'Beta', 'Gamma'])\n",
    "        handles, labels = axs[i].get_legend_handles_labels()\n",
    "        axs[i].legend(handles, [task_dict[l] for l in labels], loc='upper right', fontsize=15)\n",
    "        axs[i].set_title(f'{event_type}', fontsize=20)\n",
    "        if i == 0:\n",
    "            axs[i].set_ylabel(f'Coherence ({suffix})', fontsize=20)\n",
    "        else:\n",
    "            axs[i].set_ylabel('')\n",
    "        axs[i].set_xlabel('')\n",
    "        axs[i].tick_params(axis='both', which='major', labelsize=20)\n",
    "        event_data_df_melted.drop(columns=['event_type'], inplace=True)\n",
    "        event_data_df_melted.rename(columns={'frequency_band': 'band', 'epoch_idx': 'experiment'}, inplace=True)\n",
    "        event_data_df_melted.to_excel(writer, sheet_name=event_type)\n",
    "\n",
    "    writer.close()\n",
    "    plt.suptitle(f'AON-vHp Coherence per Channel Pair ({suffix})', fontsize=18)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    fig.savefig(savepath + f'coh_events_perband_channelpair_{suffix}_{int(time_window*fs/2)}ms.png', dpi=300)\n",
    "    plt.show()\n",
    "# Usage example:\n",
    "time_window = 1\n",
    "fs = 2000\n",
    "tanh_norm = True\n",
    "\n",
    "# Specify which columns to process\n",
    "\n",
    "# Process multiple columns\n",
    "epoch_coherence_channelpair_multiple(\n",
    "    time_window=time_window, \n",
    "    fs=fs, \n",
    "    tanh_norm=tanh_norm\n",
    ")\n",
    "plot_coherence_channelpair(\n",
    "    time_window=time_window, \n",
    "    fs=fs, \n",
    "    tanh_norm=tanh_norm\n",
    ")  \n",
    "\n",
    "# print(\"Coherence DataFrame shape:\", coherence_df.shape)\n",
    "# print(\"Coherence DataFrame columns:\", coherence_df.columns.tolist())\n",
    "# print(\"Sample output:\")\n",
    "# print(coherence_df.head())\n",
    "# print(\"\\nEvent types:\")\n",
    "# print(coherence_df['event_type'].unique())\n",
    "# print(\"\\nFrequency bands:\")\n",
    "# print(coherence_df['frequency_band'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "\n",
    "time_window = 1\n",
    "fs=2000\n",
    "tanh_norm = True\n",
    "##############\n",
    "\n",
    "importlib.reload(coherence_functions)\n",
    "\n",
    "\n",
    "def coherence_boxplot_pkl(time_window, fs, tanh_norm):\n",
    "\n",
    "    importlib.reload(coherence_functions)\n",
    "\n",
    "    if tanh_norm:\n",
    "        suffix ='normalized'\n",
    "    else:\n",
    "        suffix ='nonnormalized'\n",
    "\n",
    "    con_data_df_clean = pd.read_pickle(savepath + f'marked_mne_epochs_array_df_truncated_{int(time_window*fs)}_251125.pkl')\n",
    "\n",
    "    con_data_df_clean['coherence_door_before']=con_data_df_clean['mne_epoch_door_before'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_mt(x, tanh_norm=tanh_norm))\n",
    "    con_data_df_clean['coherence_door_after']=con_data_df_clean['mne_epoch_door_after'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_mt(x, tanh_norm=tanh_norm))\n",
    "    con_data_df_clean['coherence_dig_before']=con_data_df_clean['mne_epoch_dig_before'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_mt(x, tanh_norm=tanh_norm))\n",
    "    con_data_df_clean['coherence_dig_after']=con_data_df_clean['mne_epoch_dig_after'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_mt(x, tanh_norm=tanh_norm))\n",
    "    con_data_df_clean.drop(columns=['mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after'], inplace=True)\n",
    "    con_data_df_clean.to_pickle(savepath+f'coherence_boxplot_mt_{int(fs*time_window)}_{suffix}.pkl')\n",
    "\n",
    "    con_data_df_shuffled=pd.read_pickle(savepath + f'marked_mne_epochs_array_df_truncated_{int(time_window*fs)}_251125.pkl')\n",
    "    \n",
    "    con_data_df_shuffled['coherence_door_before']=con_data_df_shuffled['mne_epoch_door_before'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_mt(x,tanh_norm=tanh_norm, shuffle=True))\n",
    "    con_data_df_shuffled['coherence_door_after']=con_data_df_shuffled['mne_epoch_door_after'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_mt(x,tanh_norm=tanh_norm, shuffle=True))\n",
    "    con_data_df_shuffled['coherence_dig_before']=con_data_df_shuffled['mne_epoch_dig_before'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_mt(x,tanh_norm=tanh_norm, shuffle=True))\n",
    "    con_data_df_shuffled['coherence_dig_after']=con_data_df_shuffled['mne_epoch_dig_after'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_mt(x,tanh_norm=tanh_norm, shuffle=True))\n",
    "    con_data_df_shuffled.drop(columns=['mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after'], inplace=True)\n",
    "    con_data_df_shuffled.to_pickle(savepath+f'coherence_boxplot_mt_shuffled_{int(fs*time_window)}_{suffix}.pkl')\n",
    "\n",
    "coherence_boxplot_pkl(time_window=time_window, fs=fs, tanh_norm=tanh_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "\n",
    "time_window = 1\n",
    "fs=2000\n",
    "tanh_norm = True\n",
    "###################\n",
    "def plot_coherence_boxplot(time_window, fs, tanh_norm):\n",
    "    importlib.reload(coherence_functions)\n",
    "    if tanh_norm:\n",
    "        suffix ='normalized'\n",
    "    else:\n",
    "        suffix ='nonnormalized'\n",
    "    print(suffix)\n",
    "\n",
    "    con_data_df_clean=pd.read_pickle(savepath+f'coherence_boxplot_mt_{int(fs*time_window)}_{suffix}.pkl')\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(40, 10), sharey=True)\n",
    "    fig.suptitle(f'Coherence {time_window}s', fontsize=24)\n",
    "    axs = axs.flatten()\n",
    "    for ax in axs:\n",
    "        ax.tick_params(axis='y', which='both', labelleft=True)\n",
    "    writer=pd.ExcelWriter(savepath + f'\\\\coh_events_perband_averaged_{suffix}_{int(time_window*fs/2)}ms.xlsx')\n",
    "    events_dict={'coherence_door_before':'Pre Door', 'coherence_door_after': 'Post Door', 'coherence_dig_before':'Pre Dig', 'coherence_dig_after':'Post Dig'}\n",
    "    task_dict={'BWcontext':'Context','BWnocontext':'No Context'}\n",
    "    band_order = ['theta', 'beta','theta+beta', 'gamma', 'total']\n",
    "    for i, event in enumerate(events_dict.keys()):\n",
    "        event_data = con_data_df_clean[event]\n",
    "        event_data_df = pd.DataFrame(event_data.tolist())\n",
    "        event_data_df.reset_index(drop=True, inplace=True)\n",
    "        event_data_df['rat_id'] = con_data_df_clean['rat_id'].reset_index(drop=True)\n",
    "        event_data_df['task'] = con_data_df_clean['task'].reset_index(drop=True)\n",
    "        event_data_df_melted = pd.melt(event_data_df, id_vars=['rat_id', 'task'], value_vars=band_order, var_name='band', value_name='coherence')\n",
    "        sns.barplot(x='band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=event_data_df_melted, legend=True, ax=axs[i])\n",
    "        sns.stripplot(x='band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=event_data_df_melted, dodge=True, edgecolor='black', linewidth=1, jitter=True, legend=False, ax=axs[i])\n",
    "        #axs[i].set_xticklabels(['Total', 'Theta', 'Beta', 'Gamma'])\n",
    "        handles, labels = axs[i].get_legend_handles_labels()\n",
    "        axs[i].legend(handles, [task_dict[l] for l in labels], loc='upper right', fontsize=15)\n",
    "        axs[i].set_title(f'{events_dict[event]}', fontsize=20)\n",
    "        if i == 0:\n",
    "            axs[i].set_ylabel(f'Coherence ({suffix})', fontsize=20)\n",
    "        else:\n",
    "            axs[i].set_ylabel('')\n",
    "        axs[i].set_xlabel('')\n",
    "        axs[i].tick_params(axis='both', which='major', labelsize=20)\n",
    "        event_data_df_melted.to_excel(writer, sheet_name=event)\n",
    "    writer.close()\n",
    "    fig.savefig(savepath+f'coh_events_perband_averaged_{suffix}_{int(time_window*fs/2)}ms.png', format='png',dpi=300, bbox_inches='tight')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \"\"\"Shuffled coherence boxplot per band\"\"\"\n",
    "\n",
    "\n",
    "    con_data_df_shuffled=pd.read_pickle(savepath+f'coherence_boxplot_mt_shuffled_{int(fs*time_window)}_{suffix}.pkl')\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(40, 10), sharey=True)\n",
    "    fig.suptitle(f'Shuffled Coherence {time_window}s', fontsize=24)\n",
    "    axs = axs.flatten()\n",
    "    for ax in axs:\n",
    "        ax.tick_params(axis='y', which='both', labelleft=True)\n",
    "    writer=pd.ExcelWriter(savepath + f'\\\\coh_events_perband_averaged_{suffix}_{int(time_window*fs/2)}ms_shuffled.xlsx')\n",
    "    events_dict={'coherence_door_before':'Pre Door', 'coherence_door_after': 'Post Door', 'coherence_dig_before':'Pre Dig', 'coherence_dig_after':'Post Dig'}\n",
    "    for i, event in enumerate(events_dict.keys()):\n",
    "        event_data = con_data_df_shuffled[event]\n",
    "        event_data_df = pd.DataFrame(event_data.tolist())\n",
    "        event_data_df.reset_index(drop=True, inplace=True)\n",
    "        event_data_df['rat_id'] = con_data_df_shuffled['rat_id'].reset_index(drop=True)\n",
    "        event_data_df['task'] = con_data_df_shuffled['task'].reset_index(drop=True)\n",
    "        event_data_df_melted = pd.melt(event_data_df, id_vars=['rat_id', 'task'], value_vars=band_order, var_name='band', value_name='coherence')\n",
    "        sns.barplot(x='band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=event_data_df_melted, legend=True, ax=axs[i])\n",
    "        sns.stripplot(x='band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=event_data_df_melted, dodge=True, edgecolor='black', linewidth=1, jitter=True, legend=False, ax=axs[i])\n",
    "        #axs[i].set_xticklabels(['Total', 'Theta', 'Beta', 'Gamma'])\n",
    "        handles, labels = axs[i].get_legend_handles_labels()\n",
    "        axs[i].legend(handles, [task_dict[l] for l in labels], loc='upper right', fontsize=15)\n",
    "\n",
    "        axs[i].set_title(f'{events_dict[event]}', fontsize=20)\n",
    "        if i == 0:\n",
    "            axs[i].set_ylabel(f'Coherence ({suffix})', fontsize=20)\n",
    "        else:\n",
    "            axs[i].set_ylabel('')\n",
    "        axs[i].set_xlabel('')\n",
    "        axs[i].tick_params(axis='both', which='major', labelsize=20)\n",
    "        event_data_df_melted.to_excel(writer, sheet_name=event)\n",
    "    writer.close()\n",
    "    fig.savefig(savepath+f'coh_events_perband_averaged_{suffix}_{int(time_window*fs/2)}ms_shuffled.png', format='png',dpi=300, bbox_inches='tight')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_coherence_boxplot(time_window=time_window, fs=fs, tanh_norm=tanh_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.anova import AnovaRM\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "\n",
    "con_data_df_clean=pd.read_pickle(savepath+f'coherence_boxplot_mt_shuffled_{int(fs*time_window)}.pkl')\n",
    "fig, axs = plt.subplots(1, 4, figsize=(40, 10), sharey=True)\n",
    "axs = axs.flatten()\n",
    "for ax in axs:\n",
    "    ax.tick_params(axis='y', which='both', labelleft=True)\n",
    "writer=pd.ExcelWriter(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\coherence_boxplot_mt_shuffled_{int(fs*time_window)}.xlsx')\n",
    "events_dict={'coherence_door_before':'Pre Door', 'coherence_door_after': 'Post Door', 'coherence_dig_before':'Pre Dig', 'coherence_dig_after':'Post Dig'}\n",
    "for i, event in enumerate(events_dict.keys()):\n",
    "    event_data = con_data_df_clean[event]\n",
    "    event_data_df = pd.DataFrame(event_data.tolist())\n",
    "    event_data_df.reset_index(drop=True, inplace=True)\n",
    "    event_data_df['rat_id'] = con_data_df_clean['rat_id'].reset_index(drop=True)\n",
    "    event_data_df['task'] = con_data_df_clean['task'].reset_index(drop=True)\n",
    "    event_data_df_melted = pd.melt(event_data_df, id_vars=['rat_id', 'task'], value_vars=['total', 'theta', 'beta', 'gamma'], var_name='band', value_name='coherence')\n",
    "    sns.barplot(x='band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=event_data_df_melted, legend=True, ax=axs[i])\n",
    "    sns.stripplot(x='band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=event_data_df_melted, dodge=True, edgecolor='black', linewidth=1, jitter=True, legend=False, ax=axs[i])\n",
    "    #axs[i].set_xticklabels(['Total', 'Theta', 'Beta', 'Gamma'])\n",
    "    axs[i].legend(title='Task', fontsize=20, title_fontsize=20, loc='upper right')\n",
    "    \n",
    "    axs[i].set_title(f'{events_dict[event]}', fontsize=20)\n",
    "    if i == 0:\n",
    "        axs[i].set_ylabel('Coherence (A.U.)', fontsize=20)\n",
    "    else:\n",
    "        axs[i].set_ylabel('')\n",
    "    axs[i].set_xlabel('')\n",
    "    axs[i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    event_data_df_melted.to_excel(writer, sheet_name=event)\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    # Prepare data for repeated measures ANOVA\n",
    "    # Each rat_id is a subject, band is within-subject, task is between-subject\n",
    "    anova_results = {}\n",
    "    posthoc_results = {}\n",
    "\n",
    "    # Only keep rats that have both tasks for proper repeated measures\n",
    "    rats_with_both = event_data_df_melted.groupby('rat_id')['task'].nunique()\n",
    "    rats_with_both = rats_with_both[rats_with_both == 2].index.tolist()\n",
    "    filtered_df = event_data_df_melted[event_data_df_melted['rat_id'].isin(rats_with_both)]\n",
    "\n",
    "    # Pivot to wide format for repeated measures ANOVA\n",
    "    for band in ['total', 'theta', 'beta', 'gamma']:\n",
    "        band_df = filtered_df[filtered_df['band'] == band]\n",
    "        # ANOVA: repeated measures on band, between on task\n",
    "        # For each rat, we need both tasks\n",
    "        # We'll use a mixed-effects model for repeated measures\n",
    "        model = ols('coherence ~ C(task)', data=band_df).fit()\n",
    "        aov_table = sm.stats.anova_lm(model, typ=2)\n",
    "        anova_results[band] = aov_table\n",
    "\n",
    "        # Posthoc: LSD (least significant difference) test\n",
    "        mc = MultiComparison(band_df['coherence'], band_df['task'])\n",
    "        posthoc = mc.tukeyhsd()  # Tukey is more conservative, but LSD is not directly available in statsmodels\n",
    "        posthoc_results[band] = posthoc.summary()\n",
    "        print(f\"ANOVA results for {band} band in {events_dict[event]}\")\n",
    "        print(aov_table)\n",
    "        print(f\"Posthoc (Tukey HSD) results for {band} band:\")\n",
    "        print(posthoc.summary())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherogram and Boxplots together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs=2000\n",
    "for time_window in [1]:\n",
    "    for tanh_norm in [True]:\n",
    "\n",
    "        # coherogram_pkl(time_window=time_window, fs=fs, tanh_norm=tanh_norm)\n",
    "        # plot_coherogram(time_window=time_window, fs=fs, tanh_norm=tanh_norm)\n",
    "        \n",
    "        # coherence_boxplot_pkl(time_window=time_window, fs=fs, tanh_norm=tanh_norm)\n",
    "        # plot_coherence_boxplot(time_window=time_window, fs=fs, tanh_norm=tanh_norm)\n",
    "\n",
    "        # coherogram_perexperiment_pkl(time_window=time_window, fs=fs, tanh_norm=tanh_norm)\n",
    "        # plot_coherogram_perexperiment(time_window=time_window, fs=fs, tanh_norm=tanh_norm)\n",
    "\n",
    "        epoch_coherence_channelpair_multiple(time_window=time_window, fs=fs, tanh_norm=tanh_norm)\n",
    "        plot_coherence_channelpair(time_window=time_window, fs=fs, tanh_norm=tanh_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting AON-vHp connectivity separated by Bands ## [NOT USED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df_clean=pd.read_pickle(savepath+'coherence_boxplot_per_event_per_band_single_value.pkl')\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20, 10))\n",
    "axs = axs.flatten()\n",
    "writer = pd.ExcelWriter(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\coherence_band_per_event.xlsx')\n",
    "\n",
    "bands = ['total', 'theta', 'beta', 'gamma']\n",
    "events = ['coherence_door_before', 'coherence_door_after', 'coherence_dig_before', 'coherence_dig_after']\n",
    "\n",
    "for i, band in enumerate(bands):\n",
    "    band_data = []\n",
    "    for event in events:\n",
    "        event_data = con_data_df_clean[event]\n",
    "        event_data_df = pd.DataFrame(event_data.tolist())\n",
    "        event_data_df.reset_index(drop=True, inplace=True)\n",
    "        event_data_df['rat_id'] = con_data_df_clean['rat_id'].reset_index(drop=True)\n",
    "        event_data_df['task'] = con_data_df_clean['task'].reset_index(drop=True)\n",
    "        event_data_df['event'] = event\n",
    "        event_data_df['band'] = band\n",
    "        event_data_df['coherence'] = event_data_df[band]\n",
    "        band_data.append(event_data_df[['rat_id', 'task', 'event', 'band', 'coherence']])\n",
    "    \n",
    "    band_data_df = pd.concat(band_data, ignore_index=True)\n",
    "    sns.boxplot(x='event', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=band_data_df, showfliers=False, legend=False, ax=axs[i])\n",
    "    sns.stripplot(x='event', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=band_data_df, dodge=True, edgecolor='black', linewidth=1, jitter=True, legend=False, ax=axs[i])\n",
    "    axs[i].set_xticklabels(['Door Before', 'Door After', 'Dig Before', 'Dig After'], rotation=0)\n",
    "    axs[i].set_title(band.capitalize())\n",
    "    axs[i].set_ylabel('Coherence')\n",
    "    axs[i].set_xlabel('')\n",
    "    band_data_df.to_excel(writer, sheet_name=band)\n",
    "\n",
    "writer.close()\n",
    "\n",
    "# Create custom legend handles and labels\n",
    "from matplotlib.lines import Line2D\n",
    "colors = {'BWnocontext': '#ff7f0e', 'BWcontext': '#1f77b4'}\n",
    "\n",
    "handles = [\n",
    "    Line2D([0], [0], color=colors['BWcontext'], marker='o', linestyle='', markersize=10, label='BWcontext'),\n",
    "    Line2D([0], [0], color=colors['BWnocontext'], marker='o', linestyle='', markersize=10, label='BWnocontext')\n",
    "]\n",
    "\n",
    "# Add the custom legend to the figure\n",
    "fig.legend(handles=handles, loc='upper right', bbox_to_anchor=(1.1, 1), title='Task')\n",
    "fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\coherence_band_per_event.png', dpi=600)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same boxplot as above but for a single band ## [NOT USED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df_clean=pd.read_pickle(savepath+'coherence_boxplot_per_event_per_band_single_value.pkl')\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "bands = ['beta']\n",
    "events = ['coherence_door_before', 'coherence_door_after', 'coherence_dig_before', 'coherence_dig_after']\n",
    "\n",
    "for i, band in enumerate(bands):\n",
    "    band_data = []\n",
    "    for event in events:\n",
    "        event_data = con_data_df_clean[event]\n",
    "        event_data_df = pd.DataFrame(event_data.tolist())\n",
    "        event_data_df.reset_index(drop=True, inplace=True)\n",
    "        event_data_df['rat_id'] = con_data_df_clean['rat_id'].reset_index(drop=True)\n",
    "        event_data_df['task'] = con_data_df_clean['task'].reset_index(drop=True)\n",
    "        event_data_df['event'] = event\n",
    "        event_data_df['band'] = band\n",
    "        event_data_df['coherence'] = event_data_df[band]\n",
    "        band_data.append(event_data_df[['rat_id', 'task', 'event', 'band', 'coherence']])\n",
    "    \n",
    "    band_data_df = pd.concat(band_data, ignore_index=True)\n",
    "    sns.boxplot(x='event', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=band_data_df, showfliers=False, legend=False, ax=axs)\n",
    "    sns.stripplot(x='event', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=band_data_df, dodge=True, edgecolor='black', linewidth=1, jitter=True, legend=False, ax=axs)\n",
    "    axs.set_xticklabels(['Pre Door', 'Post Door', 'Pre Dig', 'Post Dig'], rotation=0)\n",
    "    axs.set_title(band.capitalize()+' Band Coherence between AON and vHp', fontsize=20)\n",
    "    \n",
    "    axs.set_ylabel('Coherence', fontsize=20)\n",
    "    axs.set_xlabel('Behavior Events', fontsize=20)\n",
    "    axs.tick_params(axis='both', which='major', labelsize=20)\n",
    "    axs.tick_params(axis='both', which='minor', labelsize=20)\n",
    "    #axs.legend(title='', fontsize=20, loc='upper right' )\n",
    "# # Create custom legend handles and labels\n",
    "from matplotlib.lines import Line2D\n",
    "colors = {'BWnocontext': '#ff7f0e', 'BWcontext': '#1f77b4'}\n",
    "\n",
    "handles = [\n",
    "    Line2D([0], [0], color=colors['BWcontext'], marker='o', linestyle='', markersize=10, label='Context'),\n",
    "    Line2D([0], [0], color=colors['BWnocontext'], marker='o', linestyle='', markersize=10, label='No Context')\n",
    "]\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add the custom legend to the figure\n",
    "fig.legend(handles=handles, loc='upper right', bbox_to_anchor=(0.4, 0.95), title='', fontsize=20, ncol=1)\n",
    "fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\coherence_beta_band_per_event.png', dpi=600, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase Based Connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating phase coherograms for each experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "time_window = 0.7\n",
    "fs=2000\n",
    "#############\n",
    "\n",
    "con_data_df_clean=pd.read_pickle(savepath+'mne_epochs_array_df_truncated_{}.pkl'.format(int(time_window*fs)))\n",
    "\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "\n",
    "print(event_list)\n",
    "\n",
    "test_list = [con_data_df_clean.iloc[0]]\n",
    "mean_con_data=pd.DataFrame()\n",
    "def epoch_coherogram(epoch, fmin=1, fmax=100, fs=2000):\n",
    "    print(epoch.events.shape)\n",
    "    if epoch.events.shape[0] < 5:\n",
    "        print(\"Not enough events in the epoch\")\n",
    "        return None\n",
    "    else:\n",
    "        freqs = np.arange(fmin, fmax)\n",
    "        n_cycles = freqs / 3\n",
    "        con = mne_connectivity.spectral_connectivity_epochs(epoch, method='plv', sfreq=int(fs),\n",
    "                                                            mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                                            cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False)\n",
    "        coh = con.get_data(output='dense')\n",
    "        indices = con.names\n",
    "        aon_vHp_con = []\n",
    "        for i in range(coh.shape[0]):\n",
    "            for j in range(coh.shape[1]):\n",
    "                if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                    coherence= coh[i,j,:,:]\n",
    "                    #coherence=np.arctanh(coherence)\n",
    "                    aon_vHp_con.append(coherence)\n",
    "        \n",
    "        mean_con = np.mean(aon_vHp_con, axis=0)\n",
    "        return mean_con\n",
    "test_pli = epoch_coherogram(test_list[0]['mne_epoch_around_door'])\n",
    "plt.imshow(test_pli, extent=[-0.7, 0.7, 1, 100], aspect='auto', origin='lower', cmap='jet')\n",
    "plt.colorbar()\n",
    "\n",
    "mean_con_data['around_dig_mean_con'] = con_data_df_clean['mne_epoch_around_dig'].apply(epoch_coherogram)\n",
    "mean_con_data['around_door_mean_con'] = con_data_df_clean['mne_epoch_around_door'].apply(epoch_coherogram)\n",
    "\n",
    "mean_con_data['experiment'] = con_data_df_clean['experiment']\n",
    "mean_con_data['task'] = con_data_df_clean['task']\n",
    "mean_con_data['rat_id'] = con_data_df_clean['rat_id']\n",
    "mean_con_data['date'] = con_data_df_clean['date']\n",
    "\n",
    "mean_con_data.dropna(inplace=True)\n",
    "mean_con_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin = mean_con_data['around_dig_mean_con'].apply(np.min).min()\n",
    "vmax = mean_con_data['around_dig_mean_con'].apply(np.max).max()\n",
    "\n",
    "BWcontext_data=mean_con_data[(mean_con_data['task']=='BWcontext')]\n",
    "BWnocontext_data=mean_con_data[(mean_con_data['task']=='BWnocontext')]\n",
    "task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for group_name, group_df in task_data_dict.items():\n",
    "    \n",
    "    rat_ids, rat_nums = np.unique(group_df['rat_id'], return_counts=True)\n",
    "    rat_nums_max = rat_nums.max()\n",
    "\n",
    "    num_of_rows = 4 # Each row should be a rats\n",
    "    num_of_cols = rat_nums_max # Each column should be the max number of experiments for a rat\n",
    "\n",
    "    fig, axs = plt.subplots(num_of_rows, num_of_cols, figsize=(25, 10), sharex=True, sharey=True)\n",
    "    dk1_count = 0\n",
    "    dk3_count = 0\n",
    "    dk5_count = 0\n",
    "    dk6_count = 0\n",
    "    for i, (idx, row) in enumerate(group_df.iterrows()):\n",
    "        rat_id = row['rat_id']\n",
    "        data = np.array(row['around_dig_mean_con'])\n",
    "        if rat_id == 'dk1':\n",
    "            ax=axs[0, dk1_count]\n",
    "            dk1_count += 1\n",
    "        elif rat_id == 'dk3':\n",
    "            ax=axs[1, dk3_count]\n",
    "            dk3_count += 1\n",
    "        elif rat_id == 'dk5':\n",
    "            ax=axs[2, dk5_count]\n",
    "            dk5_count += 1\n",
    "        elif rat_id == 'dk6':\n",
    "            ax=axs[3, dk6_count]\n",
    "            dk6_count += 1\n",
    "        im = ax.imshow(data, extent=[-1*time_window, time_window, 1, 100], aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        ax.set_title(f\"{row['rat_id']} {row['date']}\")\n",
    "        ax.axvline(0, color='k', linestyle='--', linewidth=2)\n",
    "        ax.axhline(12, color='green', linestyle='--')\n",
    "        ax.axhline(30, color='green', linestyle='--')\n",
    "    \n",
    "    \n",
    "    # fig, axs = plt.subplots(group_df.shape[0] // 5 + int(group_df.shape[0] % 5 != 0), 5, figsize=(25, 10), sharex=True, sharey=True)\n",
    "    # axs = axs.flatten()\n",
    "    # for i, (idx, row) in enumerate(group_df.iterrows()):\n",
    "    #     data = np.array(row['around_dig_mean_con'])\n",
    "    #     ax = axs[i]\n",
    "    #     im = ax.imshow(data, extent=[-0.7, 0.7, 1, 100], aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    #     ax.set_title(f\"{row['rat_id']} {row['experiment']}\")\n",
    "    #     ax.axvline(0, color='k', linestyle='--', linewidth=2)\n",
    "    #     ax.axhline(12, color='green', linestyle='--')\n",
    "    #     ax.axhline(30, color='green', linestyle='--')\n",
    "    # for j in range(i + 1, len(axs)):\n",
    "    #     fig.delaxes(axs[j])\n",
    "    fig.suptitle(f\"{group_name} AON-vHp PLV Around Dig\", fontsize=16)\n",
    "    fig.colorbar(im, ax=axs, orientation='vertical', fraction=0.02)\n",
    "    fig.savefig(savepath + f'plv_around_dig_{group_name}.png', dpi=300, bbox_inches='tight')\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average PLI around door and dig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df_clean=pd.read_pickle(savepath+'mne_epochs_array_df_truncated_1400.pkl')\n",
    "\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "\n",
    "print(event_list)\n",
    "BWcontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWcontext')]\n",
    "BWnocontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWnocontext')]\n",
    "task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "\n",
    "\n",
    "all_con_data=[]\n",
    "all_con_data_mean=[]\n",
    "for task_num,task_name in enumerate(task_data_dict.keys()):\n",
    "        task_data=task_data_dict[task_name]\n",
    "        row=[task_name]\n",
    "         #print(row)\n",
    "        row_2=[task_name]\n",
    "        for event in event_list:\n",
    "            #print(event)\n",
    "            event_epoch_list=task_data[event]\n",
    "            aon_vHp_con=[]\n",
    "            for event_epoch in event_epoch_list:\n",
    "                    #print(row,event, event_epoch) \n",
    "                    if event_epoch.events.shape[0] <5:\n",
    "                        print(f\"Skipping {event} for {task_name} due to insufficient events\")\n",
    "                        continue\n",
    "                    fmin=1\n",
    "                    fmax=100\n",
    "                    fs=2000\n",
    "                    freqs = np.arange(fmin,fmax)\n",
    "                    n_cycles = freqs/3\n",
    "                    # con= mne_connectivity.spectral_connectivity_time(event_epoch, method='coh', sfreq=int(fs), average=False,\n",
    "                    #                                      mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                    #                                      n_cycles=n_cycles, verbose=False, fmin=1, fmax=100, faverage=False)\n",
    "                    # coh = con.get_data(output='dense')\n",
    "                    # indices = con.names\n",
    "                    # print(coh.shape, indices)a\n",
    "                    # for i in range(coh.shape[0]):\n",
    "                    #     for j in range(coh.shape[1]):\n",
    "                    #         if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                    #             coherence= coh[i,j,:]\n",
    "                    #             coherence=np.arctanh(coherence)\n",
    "                    #             aon_vHp_con.append(coherence)\n",
    "\n",
    "                    con = mne_connectivity.spectral_connectivity_epochs(event_epoch, method='pli', sfreq=int(fs),\n",
    "                                                         mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                                         cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False)\n",
    "                    coh = con.get_data(output='dense')\n",
    "                    indices = con.names\n",
    "                    \n",
    "\n",
    "                    for i in range(coh.shape[0]):\n",
    "                        for j in range(coh.shape[1]):\n",
    "                            if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                                coherence= coh[i,j,:,:]\n",
    "                                #coherence=np.arctanh(coherence)\n",
    "                                aon_vHp_con.append(coherence)\n",
    "            row.append(np.mean(aon_vHp_con, axis=0))\n",
    "            row_2.append(np.mean(aon_vHp_con))\n",
    "        all_con_data.append(row)                    \n",
    "        all_con_data_mean.append(row_2)\n",
    "# Convert all_con_data to a DataFrame for easier manipulation\n",
    "all_con_data_df = pd.DataFrame(all_con_data, columns=['task'] + event_list)\n",
    "all_con_data_df.to_pickle(savepath+'pli_coherogram_around_door_dig_truncated.pkl')\n",
    "fs=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_con_data_df=pd.read_pickle(savepath+'pli_coherogram_around_door_dig_truncated.pkl')\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "fs=2000\n",
    "times=np.arange(-0.7, 0.7, 1/fs)\n",
    "fig, axs=plt.subplots(2,2, figsize=(20,10), sharey=True)\n",
    "fig.suptitle('AON-vHp Phase Lag Index Around Door and Dig', fontsize=20)\n",
    "vmin = all_con_data_df[event_list].applymap(np.min).min().min()\n",
    "vmax = all_con_data_df[event_list].applymap(np.max).max().max()\n",
    "event_names=['Around Door','Around Dig']\n",
    "for i, event in enumerate(event_list):\n",
    "    axs[0,i].imshow(all_con_data_df[event][0], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=0, vmax=1)\n",
    "    axs[0,i].set_xlabel('')\n",
    "    axs[0,i].set_ylabel('Frequency (Hz)', fontsize=20)\n",
    "    axs[0,i].set_title(event_names[i], fontsize=20)\n",
    "    axs[0,i].vlines(0, 0, 100, color='k', linestyle='--')\n",
    "    axs[0,i].hlines(12, times[0], times[-1], color='green', linestyle='--')\n",
    "    axs[0,i].hlines(30, times[0], times[-1], color='green', linestyle='--')\n",
    "    \n",
    "    axs[1,i].imshow(all_con_data_df[event][1], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=0, vmax=1)\n",
    "    axs[1,i].set_xlabel('Time (s)', fontsize=20)\n",
    "    axs[1,i].set_ylabel('Frequency (Hz)', fontsize=20)\n",
    "    axs[1,i].set_title(event_names[i], fontsize=20)\n",
    "    axs[1,i].vlines(0, 0, 100, color='k', linestyle='--')\n",
    "    axs[1,i].hlines(12, times[0], times[-1], color='green', linestyle='--')\n",
    "    axs[1,i].hlines(30, times[0], times[-1], color='green', linestyle='--')\n",
    "    \n",
    "    axs[0,0].text(-0.2, 0.5, 'Context', transform=axs[0,0].transAxes, fontsize=18, verticalalignment='center', rotation=90)\n",
    "    axs[1,0].text(-0.2, 0.5, 'No Context', transform=axs[1,0].transAxes, fontsize=18, verticalalignment='center', rotation=90)\n",
    "    axs[0,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    axs[1,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    # axs[0,i].set_xticks(np.arange(-2, 3, 1))  # Set x-ticks from -2 to 2 seconds\n",
    "    # axs[0,i].set_xticklabels(np.arange(-2, 3, 1))  # Set x-tick labels from -2 to 2 seconds\n",
    "    # axs[1,i].set_xticks(np.arange(-2, 3, 1))  # Set x-ticks from -2 to 2 seconds\n",
    "    # axs[1,i].set_xticklabels(np.arange(-2, 3, 1))  # Set x-tick labels from -2 to 2 seconds\n",
    "\n",
    "    # Add a colorbar\n",
    "cbar = fig.colorbar(axs[0,0].images[0], ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "cbar.set_label('PLI', loc='center', fontsize=20, labelpad=10)\n",
    "cbar.ax.tick_params(labelsize=20)  # Set colorbar tick label size\n",
    "\n",
    "fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\aon_vhp_pli_coherogram.png',format='png', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase Slope Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 0.7\n",
    "fs=2000\n",
    "con_data_df_clean=pd.read_pickle(savepath+f'mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "\n",
    "print(event_list)\n",
    "\n",
    "test_list = [con_data_df_clean.iloc[0]]\n",
    "mean_con_data=pd.DataFrame()\n",
    "#epoch_psi(epoch, fmin=1, fmax=100, fs=2000):\n",
    "def epoch_psi(epoch, fmin, fmax, fs=2000):\n",
    "    print(epoch.events.shape)\n",
    "    aon_indices = [i for i, ch in enumerate(epoch.ch_names) if 'AON' in ch]\n",
    "    vHp_indices = [i for i, ch in enumerate(epoch.ch_names) if 'vHp' in ch]\n",
    "    indices = mne_connectivity.seed_target_indices(aon_indices, vHp_indices)\n",
    "\n",
    "    if epoch.events.shape[0] < 5:\n",
    "        print(\"Not enough events in the epoch\")\n",
    "        # Return empty arrays or np.nan to avoid TypeError\n",
    "        return [], []\n",
    "    else:\n",
    "        freqs = np.arange(fmin, fmax)\n",
    "        n_cycles = freqs / 3\n",
    "        con = mne_connectivity.phase_slope_index(\n",
    "            epoch, indices=indices, sfreq=int(fs),\n",
    "            mode='cwt_morlet', cwt_freqs=freqs,\n",
    "            cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax\n",
    "        )\n",
    "        coh = con.get_data()\n",
    "        print(coh.shape)\n",
    "        indices = con.names\n",
    "\n",
    "        mean_con = np.mean(coh, axis=0)\n",
    "        mean_con = list(mean_con[0, :])\n",
    "        all_cons = np.array([coh[i, 0, :] for i in range(coh.shape[0])])\n",
    "        return mean_con, all_cons\n",
    "\n",
    "epoch = test_list[0]['mne_epoch_around_door']\n",
    "mean_con, all_cons = epoch_psi(epoch, fmin=12, fmax=30, fs=2000)\n",
    "\n",
    "def generate_simulated_epoch(n_channels=4, n_times=2000, n_events=10, sfreq=2000):\n",
    "    ch_names = ['AON_1', 'AON_2', 'vHp_1', 'vHp_2']\n",
    "    ch_types = ['eeg'] * n_channels\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "    data = np.random.randn(n_events, n_channels, n_times)  # Random data for simulation\n",
    "    events = np.array([[i, 0, 1] for i in range(n_events)])  # Dummy events\n",
    "    epoch = mne.EpochsArray(data, info, events)\n",
    "    return epoch\n",
    "\n",
    "simulated_epoch = generate_simulated_epoch()\n",
    "mean_con, all_cons = epoch_psi(simulated_epoch)\n",
    "plt.plot(mean_con)\n",
    "\n",
    "psi_data_df = pd.DataFrame()\n",
    "psi_data_df['around_dig_mean_con'], psi_data_df['around_dig_all_cons'] = zip(*con_data_df_clean['mne_epoch_around_dig'].apply(epoch_psi))\n",
    "psi_data_df['around_door_mean_con'], psi_data_df['around_door_all_cons'] = zip(*con_data_df_clean['mne_epoch_around_door'].apply(epoch_psi))\n",
    "psi_data_df['experiment'] = con_data_df_clean['experiment']\n",
    "psi_data_df['task'] = con_data_df_clean['task']\n",
    "psi_data_df['rat_id'] = con_data_df_clean['rat_id']\n",
    "psi_data_df.dropna(inplace=True)\n",
    "psi_data_df = psi_data_df[psi_data_df['around_dig_mean_con'].apply(lambda x: len(x) > 0) & psi_data_df['around_door_mean_con'].apply(lambda x: len(x) > 0)]\n",
    "psi_data_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "BWcontext_data = psi_data_df[(psi_data_df['task'] == 'BWcontext')]\n",
    "BWnocontext_data = psi_data_df[(psi_data_df['task'] == 'BWnocontext')]\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Stack the lists vertically and compute the mean across axis 0\n",
    "bwcontext_stacked = np.vstack(BWcontext_data['around_dig_mean_con'].values)\n",
    "bwcontext_mean = np.mean(bwcontext_stacked, axis=0)\n",
    "\n",
    "bwnocontext_stacked = np.vstack(BWnocontext_data['around_dig_mean_con'].values)\n",
    "bwnocontext_mean = np.mean(bwnocontext_stacked, axis=0)\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(20, 10))\n",
    "ax.plot(times, bwnocontext_mean, label=' No Context', color='orange')\n",
    "ax.plot(times, bwcontext_mean, label='Context', color='blue')\n",
    "ax.set_title('AON-vHp PSI Beta Band Around Dig', fontsize=20)\n",
    "plt.axhline(0, color='k', linestyle='--')\n",
    "plt.axvline(0, color='k', linestyle='-', linewidth=2)\n",
    "ax.set_xlabel('Time (s)', fontsize=20)\n",
    "ax.set_ylabel('Phase Slope Index', fontsize=20)\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, labels, loc='upper left', fontsize=15)\n",
    "fig.savefig(savepath + 'aon_vhp_psi_around_dig.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Stack the lists vertically and compute the mean across axis 0\n",
    "bwcontext_stacked = np.vstack(BWcontext_data['around_dig_all_cons'].values)\n",
    "bwcontext_mean = np.mean(bwcontext_stacked, axis=0)\n",
    "bw_context_sem = np.std(bwcontext_stacked, axis=0) / np.sqrt(bwcontext_stacked.shape[0])\n",
    "\n",
    "bwnocontext_stacked = np.vstack(BWnocontext_data['around_dig_all_cons'].values)\n",
    "bwnocontext_mean = np.mean(bwnocontext_stacked, axis=0)\n",
    "bwnocontext_sem = np.std(bwnocontext_stacked, axis=0) / np.sqrt(bwnocontext_stacked.shape[0])\n",
    "fig,ax=plt.subplots(figsize=(20, 10))\n",
    "ax.plot(times, bwnocontext_mean, label=' No Context', color='orange')\n",
    "ax.fill_between(times, bwnocontext_mean - bwnocontext_sem, bwnocontext_mean + bwnocontext_sem, color='orange', alpha=0.3)\n",
    "ax.plot(times, bwcontext_mean, label='Context', color='blue')\n",
    "ax.fill_between(times, bwcontext_mean - bw_context_sem, bwcontext_mean + bw_context_sem, color='blue', alpha=0.3)\n",
    "ax.set_title('AON-vHp PSI Beta Band Around Dig', fontsize=20)\n",
    "plt.axhline(0, color='k', linestyle='--')\n",
    "plt.axvline(0, color='k', linestyle='-', linewidth=2)\n",
    "ax.set_xlabel('Time (s)', fontsize=20)\n",
    "ax.set_ylabel('Phase Slope Index', fontsize=20)\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, labels, loc='upper left', fontsize=15)\n",
    "fig.savefig(savepath + 'aon_vhp_psi_around_dig.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making a cumulative figure with all bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "time_window = 0.7\n",
    "fs=2000\n",
    "times=np.arange(-0.7, 0.7, 1/fs)\n",
    "##############\n",
    "\n",
    "\n",
    "bands_list =[(4,8), (12,30), (30,80)]  # Theta, Beta, Gamma\n",
    "band_names = ['theta', 'beta', 'gamma']\n",
    "\n",
    "real_con_data = pd.read_pickle(savepath+f'mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n",
    "shuffled_con_data = pd.read_pickle(savepath+f'mne_epochs_array_df_shuffled_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "def clean_and_merge_data(psi_data_df, con_data_df_clean):\n",
    "    psi_data_df['experiment'] = con_data_df_clean['experiment']\n",
    "    psi_data_df['task'] = con_data_df_clean['task']\n",
    "    psi_data_df['rat_id'] = con_data_df_clean['rat_id']\n",
    "    psi_data_df.dropna(inplace=True)\n",
    "    #psi_data_df = psi_data_df[psi_data_df['around_dig_mean_con'].apply(lambda x: len(x) > 0)]\n",
    "    psi_data_df.reset_index(drop=True, inplace=True)\n",
    "    return psi_data_df\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(60, 10), sharey='row', sharex=True)\n",
    "fig.suptitle('AON-vHp Phase Slope Index Around Dig', fontsize=24)\n",
    "for band_idx, (fmin, fmax) in enumerate(bands_list):\n",
    "    print(f\"Processing band: {band_names[band_idx]} ({fmin}-{fmax} Hz)\")\n",
    "    real_data_psi = pd.DataFrame()\n",
    "    real_data_psi['around_dig_mean_con'], real_data_psi['around_dig_all_cons'] = zip(*real_con_data['mne_epoch_around_dig'].apply(lambda x: epoch_psi(x, fmin=fmin, fmax=fmax)))\n",
    "\n",
    "    shuffled_data_psi = pd.DataFrame()\n",
    "    shuffled_data_psi['around_dig_mean_con'], shuffled_data_psi['around_dig_all_cons'] = zip(*shuffled_con_data['mne_epoch_around_dig'].apply(lambda x: epoch_psi(x, fmin=fmin, fmax=fmax)))\n",
    "\n",
    "    real_data_psi = clean_and_merge_data(real_data_psi, real_con_data)\n",
    "    shuffled_data_psi = clean_and_merge_data(shuffled_data_psi, shuffled_con_data)\n",
    "\n",
    "    for i, data in enumerate([real_data_psi, shuffled_data_psi]):\n",
    "        print(f\"Processing {'real' if i == 0 else 'shuffled'} data\")\n",
    "        BWcontext_data = data[(data['task'] == 'BWcontext')]\n",
    "        BWnocontext_data = data[(data['task'] == 'BWnocontext')]\n",
    "        \n",
    "        bwcontext_stacked = np.vstack(BWcontext_data['around_dig_all_cons'].values)\n",
    "        bwcontext_mean = np.mean(bwcontext_stacked, axis=0)\n",
    "        bw_context_sem = np.std(bwcontext_stacked, axis=0) / np.sqrt(bwcontext_stacked.shape[0])\n",
    "\n",
    "        bwnocontext_stacked = np.vstack(BWnocontext_data['around_dig_all_cons'].values)\n",
    "        bwnocontext_mean = np.mean(bwnocontext_stacked, axis=0)\n",
    "        bwnocontext_sem = np.std(bwnocontext_stacked, axis=0) / np.sqrt(bwnocontext_stacked.shape[0])\n",
    "        \n",
    "        ax = axs[band_idx, i]\n",
    "        ax.plot(times, bwnocontext_mean, label=' No Context', color='grey')\n",
    "        ax.fill_between(times, bwnocontext_mean - bwnocontext_sem, bwnocontext_mean + bwnocontext_sem, color='grey', alpha=0.3)\n",
    "        ax.plot(times, bwcontext_mean, label='Context', color='black')\n",
    "        ax.fill_between(times, bwcontext_mean - bw_context_sem, bwcontext_mean + bw_context_sem, color='black', alpha=0.3)\n",
    "        \n",
    "        ax.set_title(f'{band_names[band_idx].capitalize()} {\"Real\" if i == 0 else \"Shuffled\"}', fontsize=16)\n",
    "        ax.axhline(0, color='blue', linestyle='--')\n",
    "        ax.axvline(0, color='red', linestyle='-', linewidth=2)\n",
    "        if band_idx == 2:\n",
    "            ax.set_xlabel('Time (s)', fontsize=14)\n",
    "        else:\n",
    "            ax.set_xlabel('')\n",
    "        ax.set_ylabel('Phase Slope Index', fontsize=14)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        ax.legend(handles, labels, loc='upper left', fontsize=12)\n",
    "        \n",
    "fig.savefig(savepath + 'aon_vhp_psi_around_dig_bands_real_vs_shuffled.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSI for each frequency point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(type(low_fs), low_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 0.7\n",
    "fs=2000\n",
    "con_data_df_clean=pd.read_pickle(savepath+f'mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "\n",
    "print(event_list)\n",
    "\n",
    "test_list = [con_data_df_clean.iloc[0]]\n",
    "mean_con_data=pd.DataFrame()\n",
    "#epoch_psi(epoch, fmin=1, fmax=100, fs=2000):\n",
    "def epoch_psi(epoch, fs=2000):\n",
    "    print(epoch.events.shape)\n",
    "    aon_indices = [i for i, ch in enumerate(epoch.ch_names) if 'AON' in ch]\n",
    "    vHp_indices = [i for i, ch in enumerate(epoch.ch_names) if 'vHp' in ch]\n",
    "    indices = mne_connectivity.seed_target_indices(aon_indices, vHp_indices)\n",
    "    print(indices)\n",
    "    if epoch.events.shape[0] < 5:\n",
    "        print(\"Not enough events in the epoch\")\n",
    "        # Return empty arrays or np.nan to avoid TypeError\n",
    "        return [], []\n",
    "    else:\n",
    "        low_fs = np.arange(1, 100,1)\n",
    "        high_fs = np.arange(2, 101,1)\n",
    "        \n",
    "        for bandi,(fmin, fmax) in enumerate(zip(low_fs, high_fs)):\n",
    "            \n",
    "            print(f\"{bandi}:Processing frequency band: {fmin}-{fmax} Hz\")\n",
    "            freqs = np.arange(fmin, fmax)\n",
    "            n_cycles = freqs / 3\n",
    "            con = mne_connectivity.phase_slope_index(\n",
    "                epoch, indices=indices, sfreq=int(fs),\n",
    "                mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax\n",
    "            )\n",
    "            coh = con.get_data()\n",
    "            print(coh.shape)\n",
    "            indices = con.names\n",
    "\n",
    "            # mean_con = np.mean(coh, axis=0)\n",
    "            # mean_con = list(mean_con[0, :])\n",
    "            # all_cons = np.array([coh[i, 0, :] for i in range(coh.shape[0])])\n",
    "#    return mean_con, all_cons\n",
    "\n",
    "epoch = test_list[0]['mne_epoch_around_door']\n",
    "epoch_psi(epoch, fs=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_simulated_epoch(n_channels=4, n_times=2000, n_events=10, sfreq=2000):\n",
    "    ch_names = ['AON_1', 'AON_2', 'vHp_1', 'vHp_2']\n",
    "    ch_types = ['eeg'] * n_channels\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "    data = np.random.randn(n_events, n_channels, n_times)  # Random data for simulation\n",
    "    events = np.array([[i, 0, 1] for i in range(n_events)])  # Dummy events\n",
    "    epoch = mne.EpochsArray(data, info, events)\n",
    "    return epoch\n",
    "\n",
    "simulated_epoch = generate_simulated_epoch()\n",
    "mean_con, all_cons = epoch_psi(simulated_epoch)\n",
    "plt.plot(mean_con)\n",
    "\n",
    "psi_data_df = pd.DataFrame()\n",
    "psi_data_df['around_dig_mean_con'], psi_data_df['around_dig_all_cons'] = zip(*con_data_df_clean['mne_epoch_around_dig'].apply(epoch_psi))\n",
    "psi_data_df['around_door_mean_con'], psi_data_df['around_door_all_cons'] = zip(*con_data_df_clean['mne_epoch_around_door'].apply(epoch_psi))\n",
    "psi_data_df['experiment'] = con_data_df_clean['experiment']\n",
    "psi_data_df['task'] = con_data_df_clean['task']\n",
    "psi_data_df['rat_id'] = con_data_df_clean['rat_id']\n",
    "psi_data_df.dropna(inplace=True)\n",
    "psi_data_df = psi_data_df[psi_data_df['around_dig_mean_con'].apply(lambda x: len(x) > 0) & psi_data_df['around_door_mean_con'].apply(lambda x: len(x) > 0)]\n",
    "psi_data_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "BWcontext_data = psi_data_df[(psi_data_df['task'] == 'BWcontext')]\n",
    "BWnocontext_data = psi_data_df[(psi_data_df['task'] == 'BWnocontext')]\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Stack the lists vertically and compute the mean across axis 0\n",
    "bwcontext_stacked = np.vstack(BWcontext_data['around_dig_mean_con'].values)\n",
    "bwcontext_mean = np.mean(bwcontext_stacked, axis=0)\n",
    "\n",
    "bwnocontext_stacked = np.vstack(BWnocontext_data['around_dig_mean_con'].values)\n",
    "bwnocontext_mean = np.mean(bwnocontext_stacked, axis=0)\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(20, 10))\n",
    "ax.plot(times, bwnocontext_mean, label=' No Context', color='orange')\n",
    "ax.plot(times, bwcontext_mean, label='Context', color='blue')\n",
    "ax.set_title('AON-vHp PSI Beta Band Around Dig', fontsize=20)\n",
    "plt.axhline(0, color='k', linestyle='--')\n",
    "plt.axvline(0, color='k', linestyle='-', linewidth=2)\n",
    "ax.set_xlabel('Time (s)', fontsize=20)\n",
    "ax.set_ylabel('Phase Slope Index', fontsize=20)\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, labels, loc='upper left', fontsize=15)\n",
    "fig.savefig(savepath + 'aon_vhp_psi_around_dig.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Stack the lists vertically and compute the mean across axis 0\n",
    "bwcontext_stacked = np.vstack(BWcontext_data['around_dig_all_cons'].values)\n",
    "bwcontext_mean = np.mean(bwcontext_stacked, axis=0)\n",
    "bw_context_sem = np.std(bwcontext_stacked, axis=0) / np.sqrt(bwcontext_stacked.shape[0])\n",
    "\n",
    "bwnocontext_stacked = np.vstack(BWnocontext_data['around_dig_all_cons'].values)\n",
    "bwnocontext_mean = np.mean(bwnocontext_stacked, axis=0)\n",
    "bwnocontext_sem = np.std(bwnocontext_stacked, axis=0) / np.sqrt(bwnocontext_stacked.shape[0])\n",
    "fig,ax=plt.subplots(figsize=(20, 10))\n",
    "ax.plot(times, bwnocontext_mean, label=' No Context', color='orange')\n",
    "ax.fill_between(times, bwnocontext_mean - bwnocontext_sem, bwnocontext_mean + bwnocontext_sem, color='orange', alpha=0.3)\n",
    "ax.plot(times, bwcontext_mean, label='Context', color='blue')\n",
    "ax.fill_between(times, bwcontext_mean - bw_context_sem, bwcontext_mean + bw_context_sem, color='blue', alpha=0.3)\n",
    "ax.set_title('AON-vHp PSI Beta Band Around Dig', fontsize=20)\n",
    "plt.axhline(0, color='k', linestyle='--')\n",
    "plt.axvline(0, color='k', linestyle='-', linewidth=2)\n",
    "ax.set_xlabel('Time (s)', fontsize=20)\n",
    "ax.set_ylabel('Phase Slope Index', fontsize=20)\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, labels, loc='upper left', fontsize=15)\n",
    "fig.savefig(savepath + 'aon_vhp_psi_around_dig.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=test_list[0]['mne_epoch_around_door']\n",
    "epoch.ch_names\n",
    "\n",
    "print(aon_indices, vHp_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_con_data['around_dig_mean_con'] = con_data_df_clean['mne_epoch_around_dig'].apply(epoch_coherogram)\n",
    "mean_con_data['around_door_mean_con'] = con_data_df_clean['mne_epoch_around_door'].apply(epoch_coherogram)\n",
    "\n",
    "mean_con_data['experiment'] = con_data_df_clean['experiment']\n",
    "mean_con_data['task'] = con_data_df_clean['task']\n",
    "mean_con_data['rat_id'] = con_data_df_clean['rat_id']\n",
    "mean_con_data.dropna(inplace=True)\n",
    "mean_con_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference in coherence between BWContext and BWnOContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_con_data_df_net=all_con_data_df.__deepcopy__()\n",
    "all_con_data_df_net.set_index('task', inplace=True)\n",
    "all_con_data_df_net.loc['difference'] = all_con_data_df_net.loc['BWcontext'] - all_con_data_df_net.loc['BWnocontext']\n",
    "all_con_data_df_net.reset_index(inplace=True)\n",
    "\n",
    "fs=2000\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "times=np.arange(-2, 2, 1/fs)\n",
    "fig, axs=plt.subplots(1,2, figsize=(20,10), sharey=True)\n",
    "fig.suptitle('Difference in Coherence between BW Context and BW No Context')\n",
    "axs=axs.flatten()\n",
    "vmin = all_con_data_df_net[event_list].applymap(np.min).min().min()\n",
    "vmax = all_con_data_df_net[event_list].applymap(np.max).max().max()\n",
    "event_names=['Around Door','Around Dig']\n",
    "for i, event in enumerate(event_list):\n",
    "    axs[i].imshow(all_con_data_df_net[event][2], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[i].set_xlabel('Time (s)')\n",
    "    axs[i].set_ylabel('Frequency (Hz)')\n",
    "    axs[i].set_title(event_names[i])\n",
    "    axs[i].vlines(0, 0, 100, color='k', linestyle='--')\n",
    "\n",
    "cbar = fig.colorbar(axs[0].images[0], ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "cbar.set_label('Coherence')\n",
    "#fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\aon_vhp_coherence_event_spectrogram.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase Difference manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs=2000\n",
    "time_window = 1\n",
    "con_data_df_clean=pd.read_pickle(savepath+f'marked_mne_epochs_array_{int(time_window*fs)}.pkl')\n",
    "\n",
    "\n",
    "for row in con_data_df_clean.itertuples(index=False):\n",
    "    experiment = row.experiment\n",
    "    rat_id = row.rat_id\n",
    "    task = row.task\n",
    "    mne_epoch = row.mne_epoch_door_before\n",
    "    data_around_dig = row.mne_epoch_around_dig\n",
    "    data_before_dig = row.mne_epoch_dig_before\n",
    "    data_after_dig = row.mne_epoch_dig_after\n",
    "    data_before_door = row.mne_epoch_door_before\n",
    "    data_after_door = row.mne_epoch_door_after\n",
    "\n",
    "    event_of_interest = data_before_dig ### CHANGE THIS TO THE DESIRED EVENT\n",
    "\n",
    "    print(f'Processing Rat: {rat_id}, Experiment: {experiment}, Task: {task}')\n",
    "    print(event_of_interest.get_data().shape)  # Should be (n_epochs, n_channels,n_times)\n",
    "    single_data = event_of_interest.get_data()[0, 0, :]  # Get data for the first channel\n",
    "    print(single_data.shape)  # Should be (n_times,)\n",
    "    fs=2000\n",
    "    l_freq =12\n",
    "    h_freq = 30\n",
    "    iir_filter = mne.filter.create_filter(single_data, sfreq=fs,l_freq=l_freq, h_freq=h_freq, method='iir', verbose=False)\n",
    "    event_of_interest.filter(l_freq=l_freq, h_freq=h_freq, method='iir', iir_params=iir_filter, verbose=False)\n",
    "    event_of_interest.apply_hilbert(envelope=False, n_jobs=1, verbose=False)\n",
    "\n",
    "    aon_indices = [i for i, ch in enumerate(event_of_interest.ch_names) if 'AON' in ch]\n",
    "    vhp_indices = [i for i, ch in enumerate(event_of_interest.ch_names) if 'vHp' in ch]\n",
    "    aon_channels = [event_of_interest.ch_names[i] for i in aon_indices]\n",
    "    vhp_channels = [event_of_interest.ch_names[i] for i in vhp_indices]\n",
    "    print(aon_indices, vhp_indices, aon_channels, vhp_channels)\n",
    "    aon_vhp_pairs = [(aon_ch, vhp_ch) for aon_ch in aon_channels for vhp_ch in vhp_channels]\n",
    "    print(aon_vhp_pairs)\n",
    "    num_of_cols = event_of_interest.get_data().shape[0]\n",
    "    num_of_rows = len(aon_vhp_pairs)\n",
    "\n",
    "    fig, axs = plt.subplots(num_of_rows, num_of_cols, subplot_kw={'projection': 'polar'},figsize=(40, 10))\n",
    "    fig.suptitle(f'AON-vHp Phase Difference Around Dig for Rat: {rat_id}, Experiment: {experiment}, Task: {task}', fontsize=20)\n",
    "    for i, (aon_ch, vhp_ch) in enumerate(aon_vhp_pairs):\n",
    "        for j in range(num_of_cols):\n",
    "            ax = axs[i, j]\n",
    "            ax.set_xticklabels([])          # remove theta labels\n",
    "            ax.set_yticklabels([])          # remove radial labels\n",
    "            # or hide ticks entirely:\n",
    "            #ax.xaxis.set_ticks([])\n",
    "            ax.yaxis.set_ticks([])\n",
    "\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(f'{aon_ch} - {vhp_ch}', fontsize=10)\n",
    "            aon_index = aon_indices[aon_channels.index(aon_ch)]\n",
    "            vhp_index = vhp_indices[vhp_channels.index(vhp_ch)]\n",
    "            aon_epoch_data = np.angle(event_of_interest.get_data()[j,aon_index, :])\n",
    "            vhp_epoch_data = np.angle(event_of_interest.get_data()[j,vhp_index, :])\n",
    "            #print(aon_index, vhp_index, aon_epoch_data.shape, vhp_epoch_data.shape)\n",
    "            phase_diff = aon_epoch_data - vhp_epoch_data\n",
    "            ispc = np.abs(np.mean(np.exp(1j * phase_diff)))\n",
    "            pli = abs(np.mean(np.sign(np.imag(np.exp(1j * phase_diff)))))\n",
    "            phase_diff_wrapped = np.mod(phase_diff, 2 * np.pi)\n",
    "            ax.hist(phase_diff_wrapped, bins=50, color='red', alpha=0.7, density=True)\n",
    "            if i== 0:\n",
    "                ax.set_title(f'trial {j}\\nispc:{ispc:.2f} pli:{pli:.2f}', fontsize=10)\n",
    "            else:\n",
    "                ax.set_title(f'ispc:{ispc:.2f} pli:{pli:.2f}', fontsize=10)\n",
    "    fig.savefig(savepath + f'{task}_{rat_id}_{experiment}_phase_difference_aon_vhp_before_dig.png', dpi=100, bbox_inches='tight')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GC measures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "time_window = 0.7\n",
    "fs=2000\n",
    "##############\n",
    "\n",
    "\n",
    "\n",
    "con_data_df_clean=pd.read_pickle(savepath+f'mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n",
    "#con_data_df=pd.DataFrame(con_data_df, columns=['rat_id','task','mne_baseline','mne_epoch_door_before','mne_epoch_door_after',\n",
    "                                               #'mne_epoch_dig_before','mne_epoch_dig_after', 'mne_epoch_around_door', 'mne_epoch_around_dig'])\n",
    "\n",
    "def calculate_net_gc(mne_data):\n",
    "        \n",
    "        mne_data=mne_data.resample(500)\n",
    "        \n",
    "        aon_signals=[\n",
    "        idx\n",
    "        for idx, ch_info in enumerate(mne_data.info[\"chs\"])\n",
    "        if \"AON\" in ch_info[\"ch_name\"]\n",
    "        ]\n",
    "        print(aon_signals)\n",
    "        vhp_signals=[\n",
    "            idx\n",
    "            for idx, ch_info in enumerate(mne_data.info[\"chs\"])\n",
    "            if \"vHp\" in ch_info[\"ch_name\"]\n",
    "        ]\n",
    "        print(vhp_signals)\n",
    "\n",
    "        indices_aon_vhp = (np.array([aon_signals]), np.array([vhp_signals]))\n",
    "        indices_vhp_aon = (np.array([vhp_signals]), np.array([aon_signals]))\n",
    "        print(\"indices_aon_vhp:\", indices_aon_vhp, \"indices_vhp_aon:\", indices_vhp_aon)\n",
    "        gc_ab = mne_connectivity.spectral_connectivity_epochs(\n",
    "        mne_data,\n",
    "        method=[\"gc\"],\n",
    "        indices=indices_aon_vhp,\n",
    "        fmin=2.5,\n",
    "        fmax=100,\n",
    "        rank=None,\n",
    "        gc_n_lags=50,\n",
    "        )\n",
    "        freqs = gc_ab.freqs\n",
    "\n",
    "        gc_ba = mne_connectivity.spectral_connectivity_epochs(\n",
    "            mne_data,\n",
    "            method=[\"gc\"],\n",
    "            indices=indices_vhp_aon,\n",
    "            fmin=2.5,\n",
    "            fmax=100,\n",
    "            rank=None,\n",
    "            gc_n_lags=50,\n",
    "        )\n",
    "        freqs = gc_ba.freqs\n",
    "\n",
    "        net_gc = gc_ab.get_data() - gc_ba.get_data()\n",
    "        return gc_ab.get_data()[0], gc_ba.get_data()[0],net_gc[0], freqs\n",
    "\n",
    "def calculate_gc_indch(mne_data):\n",
    "    mne_data = mne_data.resample(500)\n",
    "\n",
    "    aon_signals = [\n",
    "        idx\n",
    "        for idx, ch_info in enumerate(mne_data.info[\"chs\"])\n",
    "        if \"AON\" in ch_info[\"ch_name\"]\n",
    "    ]\n",
    "    vhp_signals = [\n",
    "        idx\n",
    "        for idx, ch_info in enumerate(mne_data.info[\"chs\"])\n",
    "        if \"vHp\" in ch_info[\"ch_name\"]\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for aon_idx in aon_signals:\n",
    "        for vhp_idx in vhp_signals:\n",
    "            indices_aon_vhp = (np.array([[aon_idx]]), np.array([[vhp_idx]]))\n",
    "            indices_vhp_aon = (np.array([[vhp_idx]]), np.array([[aon_idx]]))\n",
    "            print(\"indices_aon_vhp:\", indices_aon_vhp, \"indices_vhp_aon:\", indices_vhp_aon)\n",
    "            gc_ab = mne_connectivity.spectral_connectivity_epochs(\n",
    "                mne_data,\n",
    "                method=[\"gc\"],\n",
    "                indices=indices_aon_vhp,\n",
    "                fmin=2.5,\n",
    "                fmax=100,\n",
    "                rank=None,\n",
    "                gc_n_lags=50,\n",
    "            )\n",
    "            gc_ba = mne_connectivity.spectral_connectivity_epochs(\n",
    "                mne_data,\n",
    "                method=[\"gc\"],\n",
    "                indices=indices_vhp_aon,\n",
    "                fmin=2.5,\n",
    "                fmax=100,\n",
    "                rank=None,\n",
    "                gc_n_lags=50,\n",
    "            )\n",
    "            freqs = gc_ab.freqs\n",
    "            net_gc = gc_ab.get_data()[0] - gc_ba.get_data()[0]\n",
    "\n",
    "            aon_ch = mne_data.info[\"chs\"][aon_idx][\"ch_name\"]\n",
    "            vhp_ch = mne_data.info[\"chs\"][vhp_idx][\"ch_name\"]\n",
    "\n",
    "            results.append({\n",
    "                \"channelpair\": (aon_ch, vhp_ch),\n",
    "                \"gc_aon_vhp\": gc_ab.get_data()[0],\n",
    "                \"gc_vhp_aon\": gc_ba.get_data()[0],\n",
    "                \"net_gc\": net_gc,\n",
    "                \"freqs\": freqs\n",
    "            })\n",
    "    return results\n",
    "\n",
    "test_mne = con_data_df_clean.iloc[0]['mne_epoch_dig_after']\n",
    "test_netgc = calculate_net_gc(test_mne)\n",
    "test_indch = calculate_gc_indch(test_mne)\n",
    "print(test_indch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gc_columns = ['mne_epoch_door_before', 'mne_epoch_door_after', 'mne_epoch_dig_before', 'mne_epoch_dig_after', 'mne_epoch_around_door', 'mne_epoch_around_dig']\n",
    "\n",
    "aon_vhp_gc_results = []\n",
    "vhp_aon_gc_results = []\n",
    "net_gc_results = []\n",
    "\n",
    "for idx, row in con_data_df_clean.iterrows():\n",
    "\n",
    "    for col in gc_columns:\n",
    "        gc_list = calculate_gc_indch(row[col])\n",
    "\n",
    "        for pair_idx, gc_dict in enumerate(gc_list):\n",
    "            aon_vhp_gc_results.append({\n",
    "                'rat_id': row['rat_id'],\n",
    "                'task': row['task'],\n",
    "                'experiment': row['experiment'],\n",
    "                'date': row['date'],\n",
    "                'epoch_type': col,\n",
    "                'channelpair_idx': pair_idx,\n",
    "                'channelpair': gc_dict['channelpair'],\n",
    "                'gc_values': gc_dict['gc_aon_vhp'],\n",
    "                'freqs': gc_dict['freqs']\n",
    "            })\n",
    "            vhp_aon_gc_results.append({\n",
    "                'rat_id': row['rat_id'],\n",
    "                'task': row['task'],\n",
    "                'experiment': row['experiment'],\n",
    "                'date': row['date'],\n",
    "                'epoch_type': col,\n",
    "                'channelpair_idx': pair_idx,\n",
    "                'channelpair': gc_dict['channelpair'],\n",
    "                'gc_values': gc_dict['gc_vhp_aon'],\n",
    "                'freqs': gc_dict['freqs']\n",
    "            })\n",
    "            net_gc_results.append({\n",
    "                'rat_id': row['rat_id'],\n",
    "                'task': row['task'],\n",
    "                'experiment': row['experiment'],\n",
    "                'date': row['date'],\n",
    "                'epoch_type': col,\n",
    "                'channelpair_idx': pair_idx,\n",
    "                'channelpair': gc_dict['channelpair'],\n",
    "                'gc_values': gc_dict['net_gc'],\n",
    "                'freqs': gc_dict['freqs']\n",
    "            })\n",
    "        # aon_vhp_gc_results.append(row_result_aon_vhp)\n",
    "        # vhp_aon_gc_results.append(row_result_vhp_aon)\n",
    "        # net_gc_results.append(row_result_net_gc)\n",
    "\n",
    "# Convert to DataFrames\n",
    "aon_vhp_gc_df = pd.DataFrame(aon_vhp_gc_results)\n",
    "vhp_aon_gc_df = pd.DataFrame(vhp_aon_gc_results)\n",
    "net_gc_df = pd.DataFrame(net_gc_results)\n",
    "\n",
    "# Expand the DataFrame so each AON-vHp channel pair has its own column for each epoch type\n",
    "\n",
    "# Example: save to disk\n",
    "aon_vhp_gc_df.to_pickle(savepath + f'vhp_aon_gc_shuffled.pkl')\n",
    "vhp_aon_gc_df.to_pickle(savepath + f'aon_vhp_gc_shuffled.pkl')\n",
    "net_gc_df.to_pickle(savepath + f'net_gc_shuffled.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 0.7\n",
    "fs=2000\n",
    "aon_vhp_gc_df=pd.read_pickle(savepath + f'aon_vhp_gc.pkl')\n",
    "vhp_aon_gc_df=pd.read_pickle(savepath + f'vhp_aon_gc.pkl')\n",
    "net_gc_df=pd.read_pickle(savepath + f'net_gc.pkl')\n",
    "\n",
    "def calculate_gc_per_band(gc_array,freqs_array, bands_dict):\n",
    "    freqs_array = np.array(freqs_array)  # Convert freqs_array to numpy array\n",
    "    print(len(gc_array))\n",
    "    gc_bands_dict={}\n",
    "    for band in bands_dict.keys():\n",
    "        band_indices=np.where((freqs_array>=bands_dict[band][0]) & (freqs_array<=bands_dict[band][1]))\n",
    "        gc_band=gc_array[band_indices]\n",
    "        gc_bands_dict[band]=np.sum(gc_band)*(freqs_array[1]-freqs_array[0])\n",
    "        #gc_bands_dict[band]=(np.sum(gc_band)*0.5)/len(gc_band)\n",
    "    return gc_bands_dict\n",
    "\n",
    "test_row = aon_vhp_gc_df.iloc[0]\n",
    "row_list= [test_row]\n",
    "bands_dict = {\n",
    "    'theta': (4, 8),\n",
    "    'beta': (12, 30),\n",
    "    'gamma': (30, 100),\n",
    "    'theta+early_beta': (4, 20),\n",
    "    'total': (2.5, 100)\n",
    "}\n",
    "\n",
    "for row in row_list:\n",
    "    gc_values = row['gc_values']\n",
    "    freqs = row['freqs']\n",
    "    gc_bands = calculate_gc_per_band(gc_values, freqs, bands_dict)\n",
    "    print(row['channelpair'], row['epoch_type'], gc_bands)\n",
    "    for band in gc_bands.keys():\n",
    "        row[f'{band}'] = gc_bands[band]\n",
    "    print(row)\n",
    "events_dict = {\n",
    "    'mne_epoch_door_before': 'door_before',\n",
    "    'mne_epoch_door_after': 'door_after',\n",
    "    'mne_epoch_dig_before': 'dig_before',\n",
    "    'mne_epoch_dig_after': 'dig_after',\n",
    "    'mne_epoch_around_door': 'around_door',\n",
    "    'mne_epoch_around_dig': 'around_dig'\n",
    "}\n",
    "gc_df_dict = {'AON to vHp': aon_vhp_gc_df, 'vHp to AON': vhp_aon_gc_df, 'Net GC': net_gc_df}\n",
    "writer = pd.ExcelWriter(savepath + 'gc_band_results.xlsx')\n",
    "for i, gc_df in enumerate(gc_df_dict.values()):\n",
    "    band_results = []\n",
    "\n",
    "    for idx, row in gc_df.iterrows():\n",
    "        gc_values = row['gc_values']\n",
    "        freqs = row['freqs']\n",
    "        gc_bands = calculate_gc_per_band(gc_values, freqs, bands_dict)\n",
    "        for band in gc_bands.keys():\n",
    "            row[f'{band}'] = gc_bands[band]\n",
    "        row['event'] = events_dict[row['epoch_type']]\n",
    "        row.drop(['gc_values', 'freqs', 'epoch_type'], inplace=True)\n",
    "        band_results.append(row)\n",
    "    band_df = pd.DataFrame(band_results)\n",
    "    band_df_melted = band_df.melt(id_vars=['rat_id', 'task', 'experiment', 'date', 'event', 'channelpair'],\n",
    "                                  value_vars=list(bands_dict.keys()), var_name='band', value_name='gc_value')\n",
    "    band_df_melted.to_excel(writer, sheet_name=list(gc_df_dict.keys())[i], index=False)\n",
    "    \n",
    "    ########### Plotting ###########\n",
    "    \n",
    "\n",
    "    fig, axs=plt.subplots(3,2, sharex=False, sharey=True, figsize=(15,10))\n",
    "    axs=axs.flatten()\n",
    "    fig.suptitle(f'{list(gc_df_dict.keys())[i]} Granger causality per band')\n",
    "    for axi, event in enumerate(events_dict.values()):\n",
    "        print(axi, event)\n",
    "        ax=axs[axi]\n",
    "        ax.axhline(0, color='black', lw=1)\n",
    "        sns.barplot(x='band',y='gc_value',hue='task',hue_order=['BWcontext','BWnocontext'],data=band_df_melted[band_df_melted['event']==event], ax=ax)\n",
    "        #sns.stripplot(x='band',y='gc_value',hue='task',hue_order=['BWcontext','BWnocontext'],data=band_df_melted,dodge=True,edgecolor='black',linewidth=1,jitter=True, legend=False, ax=ax)\n",
    "        ax.set_title(f\"{event}\", fontsize=10)\n",
    "        ax.set_xlabel('')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(savepath+f'gc_events_perband_{list(gc_df_dict.keys())[i]}_{int(time_window*fs/2)}ms.png')\n",
    "    plt.show()\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net_gc_df=pd.DataFrame()\n",
    "aon_vhp_gc_df = pd.DataFrame()\n",
    "vhp_aon_gc_df = pd.DataFrame()\n",
    "\n",
    "for col in ['rat_id', 'task', 'experiment', 'date']:\n",
    "    net_gc_df[col] = con_data_df_clean[col]\n",
    "    aon_vhp_gc_df[col] = con_data_df_clean[col]\n",
    "    vhp_aon_gc_df[col] = con_data_df_clean[col]\n",
    "for event in ['door_before', 'door_after', 'dig_before', 'dig_after', 'around_door', 'around_dig']:\n",
    "    mne_col = f'mne_epoch_{event}' if event not in ['around_door', 'around_dig'] else f'mne_epoch_{event}'\n",
    "    aon_vhp_gc_df[event] = con_data_df_clean[mne_col].apply(lambda x: calculate_gc_indch(x)[0])\n",
    "    vhp_aon_gc_df[event] = con_data_df_clean[mne_col].apply(lambda x: calculate_gc_indch(x)[1])\n",
    "    net_gc_df[event] = con_data_df_clean[mne_col].apply(lambda x: calculate_gc_indch(x)[2])\n",
    "\n",
    "\n",
    "net_gc_df['freqs'] = con_data_df_clean['mne_epoch_dig_after'].apply(lambda x: calculate_gc_indch(x)[3])\n",
    "net_gc_df['freqs_door'] = con_data_df_clean['mne_epoch_door_after'].apply(lambda x: calculate_gc_indch(x)[3])\n",
    "net_gc_df=pd.DataFrame(net_gc_df, columns=['rat_id','task','experiment','date','door_before','door_after','dig_before','dig_after','around_door','around_dig', 'freqs', 'freqs_door'])\n",
    "\n",
    "aon_vhp_gc_df['freqs'] = con_data_df_clean['mne_epoch_dig_after'].apply(lambda x: calculate_gc_indch(x)[3])\n",
    "aon_vhp_gc_df['freqs_door'] = con_data_df_clean['mne_epoch_door_after'].apply(lambda x: calculate_gc_indch(x)[3])\n",
    "aon_vhp_gc_df=pd.DataFrame(aon_vhp_gc_df, columns=['rat_id','task','experiment','date','door_before','door_after','dig_before','dig_after','around_door',\n",
    "                                                'around_dig', 'freqs', 'freqs_door'])\n",
    "\n",
    "vhp_aon_gc_df['freqs'] = con_data_df_clean['mne_epoch_dig_after'].apply(lambda x: calculate_net_gc(x)[3])\n",
    "vhp_aon_gc_df['freqs_door'] = con_data_df_clean['mne_epoch_door_after'].apply(lambda x: calculate_net_gc(x)[3])\n",
    "vhp_aon_gc_df=pd.DataFrame(vhp_aon_gc_df, columns=['rat_id','task','experiment','date','door_before','door_after','dig_before','dig_after','around_door',\n",
    "                                                    'around_dig', 'freqs', 'freqs_door']) \n",
    "\n",
    "\n",
    "net_gc_df.to_pickle(savepath+f'net_gc_events_density_{int(time_window*fs/2)}ms.pkl')\n",
    "aon_vhp_gc_df.to_pickle(savepath+f'aon_vhp_gc_events_density_{int(time_window*fs/2)}ms.pkl')\n",
    "vhp_aon_gc_df.to_pickle(savepath+f'vhp_aon_gc_events_density_{int(time_window*fs/2)}ms.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "time_window = 0.7\n",
    "fs=2000\n",
    "##############\n",
    "\n",
    "\n",
    "import scipy.stats\n",
    "gc_dict = {'net': 'Net', 'aon_vhp': 'AON -> vHP', 'vhp_aon': 'vHP -> AON'}\n",
    "\n",
    "for gc_type in gc_dict.keys():\n",
    "    gc_data_df=pd.read_pickle(savepath+f'{gc_type}_gc_events_density_{int(time_window*fs/2)}ms.pkl')\n",
    "\n",
    "    gc_data_df_bwcontext=gc_data_df[gc_data_df['task']=='BWcontext']\n",
    "    gc_data_df_bwnocontext=gc_data_df[gc_data_df['task']=='BWnocontext']\n",
    "    writer=pd.ExcelWriter(savepath+f'{gc_type}_gc_events_density_{int(time_window*fs/2)}ms.xlsx')\n",
    "\n",
    "    fig,axs=plt.subplots(3,2, sharex=True, sharey=True, figsize=(15,10))\n",
    "    axs=axs.flatten()\n",
    "    fig.suptitle(f'Context vs No Context {gc_dict[gc_type]} Granger Causality')\n",
    "    events_dict={'door_before': 'Door Before','door_after': 'Door After','dig_before': 'Dig Before','dig_after': 'Dig After','around_door': 'Around Door','around_dig': 'Around Dig'}\n",
    "    for i,event in enumerate(events_dict.keys()):\n",
    "        ax=axs[i]\n",
    "        bwcontext_mean=np.mean(gc_data_df_bwcontext[event], axis=0)\n",
    "        bwnocontext_mean=np.mean(gc_data_df_bwnocontext[event], axis=0)\n",
    "        bwcontext_sem=scipy.stats.sem(gc_data_df_bwcontext[event], axis=0)\n",
    "        bwnocontext_sem=scipy.stats.sem(gc_data_df_bwnocontext[event], axis=0)\n",
    "        \n",
    "        freqs=np.linspace(2.5,100,len(bwcontext_mean))\n",
    "        \n",
    "        mean_dict={'frequency':freqs,'bwcontext':bwcontext_mean,'bwnocontext':bwnocontext_mean,'bwcontext_sem':bwcontext_sem,'bwnocontext_sem':bwnocontext_sem}\n",
    "        mean_df=pd.DataFrame(mean_dict)\n",
    "        mean_df.to_excel(writer, sheet_name=event)\n",
    "\n",
    "\n",
    "        ax.plot(freqs, bwcontext_mean, linewidth=2, label='Context')\n",
    "        ax.fill_between(freqs, bwcontext_mean - bwcontext_sem, bwcontext_mean + bwcontext_sem, alpha=0.2)\n",
    "        ax.plot(freqs, gc_data_df_bwnocontext[event].mean(), linewidth=2, label='No Context')\n",
    "        ax.fill_between(freqs, bwnocontext_mean - bwnocontext_sem, bwnocontext_mean + bwnocontext_sem, alpha=0.2)    \n",
    "        ax.plot((freqs[0], freqs[-1]), (0, 0), linewidth=2, linestyle=\"--\", color=\"k\")\n",
    "        ax.axvspan(4,12, alpha=0.2, color='red', label='Theta Range')\n",
    "        ax.axvspan(12,30, alpha=0.2, color='green', label='Beta Range')\n",
    "        ax.axvspan(30,80, alpha=0.2, color='grey', label='Gamma Range')\n",
    "        ax.set_title(f\"{events_dict[event]}\", fontsize=8)\n",
    "        ax.legend(loc='upper right', fontsize=8)\n",
    "    writer.close()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(savepath+f'{gc_type}_gc_events_density_{int(time_window*fs/2)}ms.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "#%matplotlib qt\n",
    "bands_dict={'total':[2.5,100],'theta': [4,12],'beta':[12,30],'gamma':[30,80], 'beta+theta':[4,30], 'early_beta':[12,20], 'late_beta':[20,30]}\n",
    "\n",
    "gc_dict = {'net': 'Net', 'aon_vhp': 'AON -> vHP', 'vhp_aon': 'vHP -> AON'}\n",
    "\n",
    "for gc_type in gc_dict.keys():\n",
    "    gc_data_df=pd.read_pickle(savepath+f'{gc_type}_gc_events_density_{int(time_window*fs/2)}ms.pkl')\n",
    "    gc_data_df_bwcontext=gc_data_df[gc_data_df['task']=='BWcontext']\n",
    "    gc_data_df_bwnocontext=gc_data_df[gc_data_df['task']=='BWnocontext']\n",
    "    def calculate_gc_per_band(gc_array,freqs_array, bands_dict):\n",
    "        freqs_array = np.array(freqs_array)  # Convert freqs_array to numpy array\n",
    "        print(len(gc_array))\n",
    "        gc_bands_dict={}\n",
    "        for band in bands_dict.keys():\n",
    "            band_indices=np.where((freqs_array>=bands_dict[band][0]) & (freqs_array<=bands_dict[band][1]))\n",
    "            gc_band=gc_array[band_indices]\n",
    "            gc_bands_dict[band]=np.sum(gc_band)*(freqs_array[1]-freqs_array[0])\n",
    "            #gc_bands_dict[band]=(np.sum(gc_band)*0.5)/len(gc_band)\n",
    "        return gc_bands_dict\n",
    "\n",
    "    test_gc=gc_data_df_bwcontext['door_before'].iloc[0]\n",
    "    test_freqs=gc_data_df_bwcontext['freqs'].iloc[0]\n",
    "    test_gc_band=calculate_gc_per_band(test_gc,test_freqs, bands_dict)\n",
    "    print(test_gc_band)\n",
    "\n",
    "    gc_cols = ['door_before', 'door_after', 'dig_before', 'dig_after','around_door','around_dig']\n",
    "    gc_data_df_bands = []\n",
    "\n",
    "    for index, row in gc_data_df.iterrows():\n",
    "        rat_id = row['rat_id']\n",
    "        task = row['task']\n",
    "        for gc_col in gc_cols:\n",
    "            if gc_col=='around_door_truncated' or gc_col=='around_dig_truncated':\n",
    "                freqs = row['freqs_trunc']\n",
    "            elif gc_col=='around_door' or gc_col=='around_dig':\n",
    "                freqs = row['freqs_door']\n",
    "            else:\n",
    "                freqs = row['freqs']\n",
    "            gc_values = calculate_gc_per_band(row[gc_col], freqs, bands_dict)\n",
    "            for band, gc_value in gc_values.items():\n",
    "                gc_data_df_bands.append({\n",
    "                    'rat_id': rat_id,\n",
    "                    'task': task,\n",
    "                    'event': gc_col,\n",
    "                    'band': band,\n",
    "                    'gcvalue': gc_value\n",
    "                })\n",
    "\n",
    "    gc_data_df_bands = pd.DataFrame(gc_data_df_bands)\n",
    "    gc_data_df_bands=gc_data_df_bands[gc_data_df_bands['task']!='nocontext']\n",
    "    print(gc_data_df_bands)\n",
    "    writer=pd.ExcelWriter(savepath+f'gc_events_perband_{int(time_window*fs/2)}ms.xlsx')\n",
    "    fig, axs=plt.subplots(3,2, sharex=False, sharey=True, figsize=(15,10))\n",
    "    axs=axs.flatten()\n",
    "    fig.suptitle('Average Net AON -> vHp granger causality per band')\n",
    "    for i, event in enumerate(gc_cols):\n",
    "        print(i, event)\n",
    "        ax=axs[i]\n",
    "        gc_event=gc_data_df_bands[gc_data_df_bands['event']==event]\n",
    "        gc_event.to_excel(writer, sheet_name=event)\n",
    "        ax.axhline(0, color='black', lw=1)\n",
    "        sns.boxplot(x='band',y='gcvalue',hue='task',hue_order=['BWcontext','BWnocontext'],data=gc_event,showfliers=False, ax=ax)\n",
    "        sns.stripplot(x='band',y='gcvalue',hue='task',hue_order=['BWcontext','BWnocontext'],data=gc_event,dodge=True,edgecolor='black',linewidth=1,jitter=True, legend=False, ax=ax)\n",
    "        ax.set_title(f\"{event}\", fontsize=10)\n",
    "        ax.set_xlabel('')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(savepath+f'gc_events_perband_{int(time_window*fs/2)}ms.png')\n",
    "    plt.show()\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making GC Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs=2000\n",
    "time_window=1\n",
    "\n",
    "\n",
    "### Test Case\n",
    "con_data_df_clean = pd.read_pickle(savepath + f'marked_mne_epochs_array_{int(time_window*fs)}.pkl')\n",
    "\n",
    "test_epoch = con_data_df_clean['mne_epoch_dig_before'].iloc[0]\n",
    "test_epoch = test_epoch.resample(500)\n",
    "fmin=2.5\n",
    "fmax=100\n",
    "freqs = np.arange(fmin,fmax)\n",
    "n_cycles = freqs/3\n",
    "\n",
    "###Specifying the Indices for AON and vHp channels\n",
    "aon_signals=[\n",
    "idx\n",
    "for idx, ch_info in enumerate(test_epoch.info[\"chs\"])\n",
    "if \"AON\" in ch_info[\"ch_name\"]\n",
    "]\n",
    "print(aon_signals)\n",
    "vhp_signals=[\n",
    "    idx\n",
    "    for idx, ch_info in enumerate(test_epoch.info[\"chs\"])\n",
    "    if \"vHp\" in ch_info[\"ch_name\"]\n",
    "]\n",
    "print(vhp_signals)\n",
    "\n",
    "indices_aon_vhp = (np.array([aon_signals]), np.array([vhp_signals]))\n",
    "print(indices_aon_vhp)\n",
    "import itertools\n",
    "\n",
    "indices_pairs = list(itertools.product(aon_signals, vhp_signals))\n",
    "indices = (\n",
    "    np.array([pair[0] for pair in indices_pairs]),\n",
    "    np.array([pair[1] for pair in indices_pairs])\n",
    ")\n",
    "print(indices)\n",
    "# indices = [([aon], [vhp]) for aon in aon_signals for vhp in vhp_signals]\n",
    "# print(indices)\n",
    "\n",
    "\n",
    "con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n",
    "                                        mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                        cwt_n_cycles=n_cycles, verbose=True, fmin=fmin, fmax=fmax, faverage=False, gc_n_lags=50)\n",
    "coh = con.get_data()\n",
    "indices = con.names\n",
    "aon_vHp_con = []\n",
    "print(coh.shape, indices)\n",
    "\n",
    "plt.imshow(coh[0, :, :], extent=[0, 1, 1, 100], aspect='auto', origin='lower', cmap='jet')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 8, Task: BWnocontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk3, Experiment: 9, Task: BWnocontext\n",
      "(array([[0, 1, 2, 3]]), array([[4, 5]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 10, Task: BWnocontext\n",
      "(array([[0]]), array([[1, 2]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk3, Experiment: 11, Task: BWnocontext\n",
      "(array([[0]]), array([[1, 2]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk5, Experiment: 12, Task: BWcontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk6, Experiment: 13, Task: BWcontext\n",
      "(array([[0]]), array([[1, 2]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk5, Experiment: 14, Task: BWcontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk6, Experiment: 15, Task: BWcontext\n",
      "(array([[0, 1, 2]]), array([[3, 4]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 16, Task: BWcontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 17, Task: BWcontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk5, Experiment: 18, Task: BWnocontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk6, Experiment: 19, Task: BWnocontext\n",
      "(array([[0, 1, 2, 3]]), array([[4, 5]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 20, Task: BWcontext\n",
      "(array([[0, 1, 2, 3]]), array([[4, 5]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk5, Experiment: 21, Task: BWnocontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk6, Experiment: 22, Task: BWnocontext\n",
      "(array([[0]]), array([[1, 2]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk3, Experiment: 30, Task: BWcontext\n",
      "(array([[0, 1, 2, 3]]), array([[4]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk3, Experiment: 31, Task: BWcontext\n",
      "(array([[0, 1, 2]]), array([[3, 4]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk5, Experiment: 32, Task: BWnocontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk5, Experiment: 33, Task: BWnocontext\n",
      "(array([[0, 1, 2, 3]]), array([[4, 5]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 34, Task: BWcontext\n",
      "(array([[0, 1, 2, 3]]), array([[4, 5]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 35, Task: BWcontext\n",
      "(array([[0, 1, 2, 3]]), array([[4, 5]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk3, Experiment: 36, Task: BWcontext\n",
      "(array([[0, 1, 2, 3]]), array([[4, 5]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk3, Experiment: 37, Task: BWcontext\n",
      "(array([[0, 1, 2, 3]]), array([[4, 5]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk5, Experiment: 38, Task: BWcontext\n",
      "(array([[0, 1, 2, 3]]), array([[4, 5]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 39, Task: BWnocontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 40, Task: BWnocontext\n",
      "(array([[0, 1, 2, 3]]), array([[4, 5]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk5, Experiment: 41, Task: BWcontext\n",
      "(array([[0, 1, 2, 3]]), array([[4, 5]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 8, Task: BWnocontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk3, Experiment: 9, Task: BWnocontext\n",
      "(array([[0, 1, 2, 3]]), array([[4, 5]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 10, Task: BWnocontext\n",
      "(array([[0]]), array([[1, 2]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk3, Experiment: 11, Task: BWnocontext\n",
      "(array([[0]]), array([[1, 2]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk5, Experiment: 12, Task: BWcontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk6, Experiment: 13, Task: BWcontext\n",
      "(array([[0]]), array([[1, 2]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk5, Experiment: 14, Task: BWcontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk6, Experiment: 15, Task: BWcontext\n",
      "(array([[0, 1, 2]]), array([[3, 4]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 16, Task: BWcontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 17, Task: BWcontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk5, Experiment: 18, Task: BWnocontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk6, Experiment: 19, Task: BWnocontext\n",
      "(array([[0, 1, 2, 3]]), array([[4, 5]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 20, Task: BWcontext\n",
      "(array([[0, 1, 2, 3]]), array([[4, 5]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk5, Experiment: 21, Task: BWnocontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk6, Experiment: 22, Task: BWnocontext\n",
      "(array([[0]]), array([[1, 2]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk3, Experiment: 30, Task: BWcontext\n",
      "(array([[0, 1, 2, 3]]), array([[4]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk3, Experiment: 31, Task: BWcontext\n",
      "(array([[0, 1, 2]]), array([[3, 4]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk5, Experiment: 32, Task: BWnocontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk5, Experiment: 33, Task: BWnocontext\n",
      "(array([[0, 1, 2, 3]]), array([[4, 5]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 34, Task: BWcontext\n",
      "(array([[0, 1, 2, 3]]), array([[4, 5]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 35, Task: BWcontext\n",
      "(array([[0, 1, 2, 3]]), array([[4, 5]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk3, Experiment: 36, Task: BWcontext\n",
      "(array([[0, 1, 2, 3]]), array([[4, 5]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk3, Experiment: 37, Task: BWcontext\n",
      "(array([[0, 1, 2, 3]]), array([[4, 5]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk5, Experiment: 38, Task: BWcontext\n",
      "(array([[0, 1, 2, 3]]), array([[4, 5]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 39, Task: BWnocontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 40, Task: BWnocontext\n",
      "(array([[0, 1, 2, 3]]), array([[4, 5]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk5, Experiment: 41, Task: BWcontext\n",
      "(array([[0, 1, 2, 3]]), array([[4, 5]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 8, Task: BWnocontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk3, Experiment: 9, Task: BWnocontext\n",
      "(array([[0, 1, 2, 3]]), array([[4, 5]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 10, Task: BWnocontext\n",
      "(array([[0]]), array([[1, 2]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk3, Experiment: 11, Task: BWnocontext\n",
      "(array([[0]]), array([[1, 2]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk5, Experiment: 12, Task: BWcontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk6, Experiment: 13, Task: BWcontext\n",
      "(array([[0]]), array([[1, 2]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk5, Experiment: 14, Task: BWcontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk6, Experiment: 15, Task: BWcontext\n",
      "(array([[0, 1, 2]]), array([[3, 4]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 16, Task: BWcontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:53: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 17, Task: BWcontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_14380\\322386956.py:47: RuntimeWarning: fmin=2.500 Hz corresponds to 2.500 < 5 cycles based on the epoch length 1.000 sec, need at least 2.000 sec epochs or fmin=5.000. Spectrum estimate will be unreliable.\n",
      "  con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m freqs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(fmin,fmax)\n\u001b[0;32m     45\u001b[0m n_cycles \u001b[38;5;241m=\u001b[39m freqs\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[1;32m---> 47\u001b[0m con_aon_vhp \u001b[38;5;241m=\u001b[39m \u001b[43mmne_connectivity\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspectral_connectivity_epochs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevent_of_interest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msfreq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices_aon_vhp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcwt_morlet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwt_freqs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwt_n_cycles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_cycles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfaverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgc_n_lags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m con_vhp_aon \u001b[38;5;241m=\u001b[39m mne_connectivity\u001b[38;5;241m.\u001b[39mspectral_connectivity_epochs(\n\u001b[0;32m     54\u001b[0m     event_of_interest, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgc\u001b[39m\u001b[38;5;124m'\u001b[39m, sfreq\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(fs), indices\u001b[38;5;241m=\u001b[39mindices_vhp_aon,\n\u001b[0;32m     55\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcwt_morlet\u001b[39m\u001b[38;5;124m'\u001b[39m, cwt_freqs\u001b[38;5;241m=\u001b[39mfreqs, cwt_n_cycles\u001b[38;5;241m=\u001b[39mn_cycles,\n\u001b[0;32m     56\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, fmin\u001b[38;5;241m=\u001b[39mfmin, fmax\u001b[38;5;241m=\u001b[39mfmax, faverage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, gc_n_lags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, rank\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     57\u001b[0m )\n\u001b[0;32m     59\u001b[0m coh_aon_vhp \u001b[38;5;241m=\u001b[39m con_aon_vhp\u001b[38;5;241m.\u001b[39mget_data()[\u001b[38;5;241m0\u001b[39m, :, :]\n",
      "File \u001b[1;32m<decorator-gen-302>:10\u001b[0m, in \u001b[0;36mspectral_connectivity_epochs\u001b[1;34m(data, names, method, indices, sfreq, mode, fmin, fmax, fskip, faverage, tmin, tmax, mt_bandwidth, mt_adaptive, mt_low_bias, cwt_freqs, cwt_n_cycles, gc_n_lags, rank, block_size, n_jobs, verbose)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\CPLab\\anaconda3\\envs\\lfp\\lib\\site-packages\\mne_connectivity\\spectral\\epochs.py:1210\u001b[0m, in \u001b[0;36mspectral_connectivity_epochs\u001b[1;34m(data, names, method, indices, sfreq, mode, fmin, fmax, fskip, faverage, tmin, tmax, mt_bandwidth, mt_adaptive, mt_low_bias, cwt_freqs, cwt_n_cycles, gc_n_lags, rank, block_size, n_jobs, verbose)\u001b[0m\n\u001b[0;32m   1207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1208\u001b[0m     \u001b[38;5;66;03m# compute all scores at once\u001b[39;00m\n\u001b[0;32m   1209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method[method_i] \u001b[38;5;129;01min\u001b[39;00m _multivariate_methods:\n\u001b[1;32m-> 1210\u001b[0m         \u001b[43mconn_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_con\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices_use\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1211\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1212\u001b[0m         conn_method\u001b[38;5;241m.\u001b[39mcompute_con(\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, n_cons), n_epochs)\n",
      "File \u001b[1;32mc:\\Users\\CPLab\\anaconda3\\envs\\lfp\\lib\\site-packages\\mne_connectivity\\spectral\\epochs_multivariate.py:710\u001b[0m, in \u001b[0;36m_GCEstBase.compute_con\u001b[1;34m(self, indices, ranks, n_epochs)\u001b[0m\n\u001b[0;32m    706\u001b[0m con_idcs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39mseed_idcs, \u001b[38;5;241m*\u001b[39mtarget_idcs]\n\u001b[0;32m    708\u001b[0m C \u001b[38;5;241m=\u001b[39m csd[np\u001b[38;5;241m.\u001b[39mix_(times, freqs, con_idcs, con_idcs)]\n\u001b[1;32m--> 710\u001b[0m C_bar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_csd_svd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_idcs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_rank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_rank\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    711\u001b[0m n_signals \u001b[38;5;241m=\u001b[39m seed_rank \u001b[38;5;241m+\u001b[39m target_rank\n\u001b[0;32m    712\u001b[0m con_seeds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(seed_rank)\n",
      "File \u001b[1;32mc:\\Users\\CPLab\\anaconda3\\envs\\lfp\\lib\\site-packages\\mne_connectivity\\spectral\\epochs_multivariate.py:765\u001b[0m, in \u001b[0;36m_GCEstBase._csd_svd\u001b[1;34m(self, csd, seed_idcs, seed_rank, target_rank)\u001b[0m\n\u001b[0;32m    763\u001b[0m C_bar_bb \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmatmul(U_bar_bb\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), np\u001b[38;5;241m.\u001b[39mmatmul(C_bb, U_bar_bb))\n\u001b[0;32m    764\u001b[0m C_bar_ba \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmatmul(U_bar_bb\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), np\u001b[38;5;241m.\u001b[39mmatmul(C_ba, U_bar_aa))\n\u001b[1;32m--> 765\u001b[0m C_bar \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    766\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mC_bar_aa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC_bar_ab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mC_bar_ba\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC_bar_bb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    769\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m C_bar\n",
      "File \u001b[1;32mc:\\Users\\CPLab\\anaconda3\\envs\\lfp\\lib\\site-packages\\numpy\\lib\\function_base.py:5618\u001b[0m, in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   5616\u001b[0m     values \u001b[38;5;241m=\u001b[39m ravel(values)\n\u001b[0;32m   5617\u001b[0m     axis \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 5618\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "con_data_df_clean = pd.read_pickle(savepath + f'marked_mne_epochs_array_{int(time_window*fs)}.pkl')\n",
    "\n",
    "# Extract all columns at once using vectorized operations\n",
    "experiments = con_data_df_clean['experiment'].values\n",
    "rat_ids = con_data_df_clean['rat_id'].values\n",
    "tasks = con_data_df_clean['task'].values\n",
    "events_dict = {'mne_epoch_door_before':'gc_door_before','mne_epoch_dig_before':'gc_dig_before','mne_epoch_dig_after':'gc_dig_after'}\n",
    "    \n",
    "for event in events_dict.keys():\n",
    "\n",
    "    events_of_interest = con_data_df_clean[event].values  ### CHANGE THIS TO THE DESIRED EVENT\n",
    "\n",
    "    all_gc_data = []\n",
    "\n",
    "    # Iterate only through the length, using vectorized column access\n",
    "    for i in range(len(con_data_df_clean)):\n",
    "        experiment = experiments[i]\n",
    "        rat_id = rat_ids[i]\n",
    "        task = tasks[i]\n",
    "        event_of_interest = events_of_interest[i]\n",
    "        \n",
    "        print(f'Processing Rat: {rat_id}, Experiment: {experiment}, Task: {task}')\n",
    "        \n",
    "        aon_signals = [\n",
    "            idx\n",
    "            for idx, ch_info in enumerate(event_of_interest.info[\"chs\"])\n",
    "            if \"AON\" in ch_info[\"ch_name\"]\n",
    "        ]\n",
    "        \n",
    "        vhp_signals = [\n",
    "            idx\n",
    "            for idx, ch_info in enumerate(event_of_interest.info[\"chs\"])\n",
    "            if \"vHp\" in ch_info[\"ch_name\"]\n",
    "        ]\n",
    "        \n",
    "        indices_aon_vhp = (np.array([aon_signals]), np.array([vhp_signals]))\n",
    "        print(indices_aon_vhp)\n",
    "        indices_vhp_aon = (np.array([vhp_signals]), np.array([aon_signals]))\n",
    "        \n",
    "        event_of_interest = event_of_interest.resample(500)\n",
    "        \n",
    "        fmin=2.5\n",
    "        fmax=100\n",
    "        freqs = np.arange(fmin,fmax)\n",
    "        n_cycles = freqs/3\n",
    "\n",
    "        con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(\n",
    "            event_of_interest, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n",
    "            mode='cwt_morlet', cwt_freqs=freqs, cwt_n_cycles=n_cycles, \n",
    "            verbose=False, fmin=fmin, fmax=fmax, faverage=False, gc_n_lags=5, rank=None\n",
    "        )\n",
    "        \n",
    "        con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(\n",
    "            event_of_interest, method='gc', sfreq=int(fs), indices=indices_vhp_aon,\n",
    "            mode='cwt_morlet', cwt_freqs=freqs, cwt_n_cycles=n_cycles,\n",
    "            verbose=False, fmin=fmin, fmax=fmax, faverage=False, gc_n_lags=5, rank=None\n",
    "        )\n",
    "        \n",
    "        coh_aon_vhp = con_aon_vhp.get_data()[0, :, :]\n",
    "        coh_vhp_aon = con_vhp_aon.get_data()[0, :, :]\n",
    "        \n",
    "        all_gc_data.append([experiment, rat_id, task, coh_aon_vhp, coh_vhp_aon])\n",
    "\n",
    "    all_gc_data_df = pd.DataFrame(all_gc_data, columns=['experiment', 'rat_id', 'task', 'gc_aon_vhp', 'gc_vhp_aon'])\n",
    "    all_gc_data_df.to_pickle(savepath+f'{events_dict[event]}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making 2 plots of BWcontext and BWNocontext with Net GC in each subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gc_data_df.to_pickle(savepath+'gc_dig_before.pkl')\n",
    "\n",
    "all_gc_data_df['net_gc'] = all_gc_data_df['gc_aon_vhp'] - all_gc_data_df['gc_vhp_aon']\n",
    "\n",
    "vmin = all_gc_data_df['net_gc'].apply(np.min).min()\n",
    "vmax = all_gc_data_df['net_gc'].apply(np.max).max()\n",
    "\n",
    "BWcontext_data=all_gc_data_df[(all_gc_data_df['task']=='BWcontext')]\n",
    "BWnocontext_data=all_gc_data_df[(all_gc_data_df['task']=='BWnocontext')]\n",
    "task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "\n",
    "for group_name, group_df in task_data_dict.items():\n",
    "    fig, axs = plt.subplots(group_df.shape[0] // 5 + int(group_df.shape[0] % 5 != 0), 5, figsize=(25, 10), sharex=True, sharey=True)\n",
    "    axs = axs.flatten()\n",
    "    for i, (idx, row) in enumerate(group_df.iterrows()):\n",
    "        data = np.array(row['net_gc'])\n",
    "        ax = axs[i]\n",
    "        im = ax.imshow(data, extent=[0,1, 1, 100], aspect='auto', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        ax.set_title(f\"{row['rat_id']} {row['experiment']}\")\n",
    "        ax.axvline(0, color='k', linestyle='--', linewidth=2)\n",
    "        ax.axhline(12, color='green', linestyle='--')\n",
    "        ax.axhline(30, color='green', linestyle='--')\n",
    "    for j in range(i + 1, len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "    fig.suptitle(f\"{group_name} AON -> vHp net GC\", fontsize=16)\n",
    "    fig.colorbar(im, ax=axs, orientation='vertical', fraction=0.02)\n",
    "    #fig.savefig(savepath + f'coherence_around_dig_{group_name}.png', dpi=300, bbox_inches='tight')\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "gc_list = ['gc_aon_vhp', 'gc_vhp_aon', 'net_gc']\n",
    "for gc in gc_list:\n",
    "    bw_context_mean=BWcontext_data[gc].mean()\n",
    "    bw_nocontext_mean=BWnocontext_data[gc].mean()\n",
    "    #print(bw_context_mean.shape, bw_nocontext_mean.shape)\n",
    "    vmin = min(bw_context_mean.min(), bw_nocontext_mean.min())\n",
    "    vmax = max(bw_context_mean.max(), bw_nocontext_mean.max())\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    fig.suptitle(f'Average {gc} Around Dig')\n",
    "    axs[0].imshow(bw_context_mean, extent=[0, 1, 1, 100], aspect='auto', cmap='jet',origin='lower', vmin=vmin, vmax=vmax)\n",
    "    axs[0].set_title('Context')\n",
    "    axs[1].imshow(bw_nocontext_mean, extent=[0, 1, 1, 100], aspect='auto', cmap='jet',origin='lower', vmin=vmin, vmax=vmax)\n",
    "    axs[1].set_title('No Context')\n",
    "    for ax in axs:\n",
    "        ax.axvline(0, color='k', linestyle='--', linewidth=2)\n",
    "        ax.axhline(12, color='green', linestyle='--')\n",
    "        ax.axhline(30, color='green', linestyle='--')\n",
    "    fig.colorbar(axs[0].images[0], ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gc_data_df = pd.read_pickle(savepath+'gc_around_dig.pkl')\n",
    "\n",
    "for row in all_gc_data_df.itertuples(index=False):\n",
    "    experiment = row.experiment\n",
    "    rat_id = row.rat_id\n",
    "    task = row.task\n",
    "    gc_aon_vhp = row.gc_aon_vhp\n",
    "    gc_vhp_aon = row.gc_vhp_aon\n",
    "    net_gc = gc_aon_vhp - gc_vhp_aon\n",
    "    vmin = min(gc_aon_vhp.min(), gc_vhp_aon.min(), net_gc.min())\n",
    "    vmax = max(gc_aon_vhp.max(), gc_vhp_aon.max(), net_gc.max())\n",
    "    print(f'Processing Rat: {rat_id}, Experiment: {experiment}, Task: {task}')\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "    fig.suptitle(f'Granger Causality Rat: {rat_id}, Experiment: {experiment}, Task: {task}', fontsize=20)\n",
    "\n",
    "    im = axs[0].imshow(gc_aon_vhp, extent=[-0.7, 0.7, 1, 100], aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[0].set_title('AON -> vHp')\n",
    "    axs[0].set_xlabel('Time (s)')\n",
    "    axs[0].set_ylabel('Frequency (Hz)')\n",
    "    \n",
    "    axs[1].imshow(gc_vhp_aon, extent=[-0.7, 0.7, 1, 100], aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[1].set_title('vHp -> AON')\n",
    "    axs[1].set_xlabel('Time (s)')\n",
    "\n",
    "    axs[2].imshow(net_gc, extent=[-0.7, 0.7, 1, 100], aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[2].set_title('Difference (AON -> vHp) - (vHp -> AON)')\n",
    "    axs[2].set_xlabel('Time (s)')\n",
    "    axs[2].set_ylabel('Frequency (Hz)')\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.axvline(0, color='k', linestyle='--', linewidth=2)\n",
    "        ax.axhline(12, color='green', linestyle='--')\n",
    "        ax.axhline(30, color='green', linestyle='--')\n",
    "    # Create a common colorbar for all three subplots\n",
    "    # Remove the previous imshow call for axs[2] above, use this one for colorbar\n",
    "    cbar = fig.colorbar(im, ax=axs, orientation='vertical', fraction=0.02)\n",
    "    #plt.tight_layout()\n",
    "    #plt.savefig(savepath + f'{task}_{rat_id}_{experiment}_gc_around_dig.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df_clean=pd.read_pickle(savepath+'mne_epochs_array_df.pkl')\n",
    "\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "\n",
    "print(event_list)\n",
    "BWcontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWcontext')]\n",
    "BWnocontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWnocontext')]\n",
    "task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "\n",
    "rat_list=np.unique(con_data_df_clean['rat_id'])\n",
    "print(rat_list)\n",
    "all_con_data=[]\n",
    "all_con_data_mean=[]\n",
    "for task_num,task_name in enumerate(task_data_dict.keys()):\n",
    "        task_data=task_data_dict[task_name]\n",
    "    #print(task_name)\n",
    "    # for rat_num, rat_name in enumerate(rat_list):\n",
    "    #     rat_task_data=task_data[task_data['rat_id']==rat_name]\n",
    "        row=[task_name]\n",
    "    #     #print(row)\n",
    "        row_2=[task_name]\n",
    "        for event in event_list:\n",
    "            #print(event)\n",
    "            event_epoch_list=task_data[event]\n",
    "            aon_vHp_con=[]\n",
    "            for event_epoch in event_epoch_list:\n",
    "                    #print(row,event, event_epoch) \n",
    "                    fmin=1\n",
    "                    fmax=100\n",
    "                    freqs = np.arange(fmin,fmax)\n",
    "                    n_cycles = freqs/2\n",
    "                    \n",
    "                    ###Specifying the Indices for AON and vHp channels\n",
    "                    aon_signals=[\n",
    "                    idx\n",
    "                    for idx, ch_info in enumerate(event_epoch.info[\"chs\"])\n",
    "                    if \"AON\" in ch_info[\"ch_name\"]\n",
    "                    ]\n",
    "                    print(aon_signals)\n",
    "                    vhp_signals=[\n",
    "                        idx\n",
    "                        for idx, ch_info in enumerate(event_epoch.info[\"chs\"])\n",
    "                        if \"vHp\" in ch_info[\"ch_name\"]\n",
    "                    ]\n",
    "                    print(vhp_signals)\n",
    "\n",
    "                    indices_aon_vhp = (np.array([aon_signals]), np.array([vhp_signals]))\n",
    "                    indices_vhp_aon = (np.array([vhp_signals]), np.array([aon_signals]))      \n",
    "                    gc_ab = mne_connectivity.spectral_connectivity_epochs(event_epoch, method=[\"gc\"], indices=indices_aon_vhp, fmin=2.5, fmax=100, rank=None,gc_n_lags=20)\n",
    "                    gc_ba= mne_connectivity.spectral_connectivity_epochs(event_epoch, method=[\"gc\"], indices=indices_vhp_aon, fmin=2.5, fmax=100, rank=None,gc_n_lags=20)\n",
    "                    net_gc= gc_ab.get_data() - gc_ba.get_data()\n",
    "                    print(net_gc.shape)\n",
    "\n",
    "                    coh = net_gc[0]\n",
    "                    #coh=np.abs(coh)\n",
    "                    print(coh.shape)\n",
    "                    indices = coh.names\n",
    "                    print(indices)\n",
    "\n",
    "                    for i in range(coh.shape[0]):\n",
    "                        for j in range(coh.shape[1]):\n",
    "                            if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                                aon_vHp_con.append(coh[i,j,:,:])\n",
    "            row.append(np.mean(aon_vHp_con, axis=0))\n",
    "            row_2.append(np.mean(aon_vHp_con))\n",
    "        all_con_data.append(row)                    \n",
    "        all_con_data_mean.append(row_2)\n",
    "# Convert all_con_data to a DataFrame for easier manipulation\n",
    "all_con_data_df = pd.DataFrame(all_con_data, columns=['task'] + event_list)\n",
    "#all_con_data_df.to_pickle(savepath+'coherence_spectrogram_around_door_dig.pkl')\n",
    "fs=2000\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "fs=2000\n",
    "times=np.arange(-2, 2, 1/fs)\n",
    "fig, axs=plt.subplots(2,2, figsize=(20,10), sharey=True)\n",
    "vmin = all_con_data_df[event_list].applymap(np.min).min().min()\n",
    "vmax = all_con_data_df[event_list].applymap(np.max).max().max()\n",
    "event_names=['Around Door','Around Dig']\n",
    "for i, event in enumerate(event_list):\n",
    "    axs[0,i].imshow(all_con_data_df[event][0], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[0,i].set_xlabel('')\n",
    "\n",
    "    axs[0,i].set_ylabel('Frequency (Hz)', fontsize=20)\n",
    "    axs[0,i].set_title(event_names[i], fontsize=20)\n",
    "    axs[0,i].vlines(0, 0, 100, color='k', linestyle='--')\n",
    "    axs[0,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    axs[1,i].imshow(all_con_data_df[event][1], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[1,i].set_xlabel('Time (s)', fontsize=20)\n",
    "    axs[1,i].set_ylabel('Frequency (Hz)', fontsize=20)\n",
    "    axs[1,i].set_title(event_names[i], fontsize=20)\n",
    "    axs[1,i].vlines(0, 0, 100, color='k', linestyle='--')\n",
    "\n",
    "    axs[0,0].text(-0.2, 0.5, 'Context', transform=axs[0,0].transAxes, fontsize=18, verticalalignment='center', rotation=90)\n",
    "    axs[1,0].text(-0.2, 0.5, 'No Context', transform=axs[1,0].transAxes, fontsize=18, verticalalignment='center', rotation=90)\n",
    "    axs[0,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    axs[1,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    axs[0,i].set_xticks(np.arange(-2, 3, 1))  # Set x-ticks from -2 to 2 seconds\n",
    "    axs[0,i].set_xticklabels(np.arange(-2, 3, 1))  # Set x-tick labels from -2 to 2 seconds\n",
    "    axs[1,i].set_xticks(np.arange(-2, 3, 1))  # Set x-ticks from -2 to 2 seconds\n",
    "    axs[1,i].set_xticklabels(np.arange(-2, 3, 1))  # Set x-tick labels from -2 to 2 seconds\n",
    "\n",
    "    # Add a colorbar\n",
    "cbar = fig.colorbar(axs[0,0].images[0], ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "cbar.set_label('Coherence', loc='center', fontsize=20, labelpad=10)\n",
    "cbar.ax.tick_params(labelsize=20)  # Set colorbar tick label size\n",
    "\n",
    "#fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\aon_vhp_coherence_event_spectrogram.png',format='png', dpi=600, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating complex coherence values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated Coherence with Quiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "time_window = 1\n",
    "fs=2000\n",
    "#################\n",
    "con_data_df_clean = pd.read_pickle(savepath + f'marked_mne_epochs_array_{int(time_window*fs)}.pkl')\n",
    "#con_data_df_clean=pd.read_pickle(savepath+f'mne_epochs_array_df_shuffled_truncated_{int(time_window*fs)}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "event_list=['mne_epoch_door_before', 'mne_epoch_dig_before','mne_epoch_dig_after','mne_epoch_around_door','mne_epoch_around_dig']\n",
    "fs=2000\n",
    "\n",
    "shuffled = True\n",
    "\n",
    "\n",
    "importlib.reload(lfp_pre_processing_functions)\n",
    "print(event_list)\n",
    "BWcontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWcontext')]\n",
    "BWnocontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWnocontext')]\n",
    "task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "\n",
    "rat_list=np.unique(con_data_df_clean['rat_id'])\n",
    "print(rat_list)\n",
    "all_coh_abs_data=[]\n",
    "all_coh_abs_data_mean=[]\n",
    "all_coh_phase_data=[]\n",
    "all_coh_phase_data_mean=[]\n",
    "ind_rows_df=[]\n",
    "for task_num,task_name in enumerate(task_data_dict.keys()):\n",
    "        task_data=task_data_dict[task_name]\n",
    "    #print(task_name)\n",
    "    # for rat_num, rat_name in enumerate(rat_list):\n",
    "    #     rat_task_data=task_data[task_data['rat_id']==rat_name]\n",
    "        row_coh_abs=[task_name]\n",
    "        row_coh_abs_mean=[task_name]\n",
    "        row_coh_phase=[task_name]\n",
    "        row_coh_phase_mean=[task_name]\n",
    "\n",
    "        for event in event_list:\n",
    "            #print(event)\n",
    "            event_epoch_list=task_data[event]\n",
    "            rat_id_list=task_data['rat_id']\n",
    "            exp_list=task_data['experiment']\n",
    "            \n",
    "            aon_vhp_coh_abs=[]\n",
    "            aon_vhp_coh_phase=[]\n",
    "            for rowi,event_epoch in enumerate(event_epoch_list):\n",
    "                    #print(row,event, event_epoch) \n",
    "                    rat_id=rat_id_list.iloc[rowi]\n",
    "                    experiment=exp_list.iloc[rowi]\n",
    "                    print(f'Processing Rat: {rat_id}, Experiment: {experiment}, Task: {task_name}, Event: {event}')\n",
    "                    fmin=1\n",
    "                    fmax=100\n",
    "                    freqs = np.arange(fmin,fmax)\n",
    "                    n_cycles = freqs/3\n",
    "\n",
    "                    if shuffled:\n",
    "                        event_epoch = lfp_pre_processing_functions.randomize_timepoints(event_epoch)\n",
    "                        suffix = \"_shuffled\"\n",
    "                    else:\n",
    "                        event_epoch=event_epoch\n",
    "                        suffix = \"\"\n",
    "                    con = mne_connectivity.spectral_connectivity_epochs(event_epoch, method='cohy', sfreq=int(fs),\n",
    "                                                         mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                                         cwt_n_cycles=n_cycles, verbose=False, fmin=1, fmax=100, faverage=False)\n",
    "                    coh = con.get_data(output='dense')\n",
    "                    coh_abs = np.abs(coh)\n",
    "                    coh_phase = np.angle(coh)\n",
    "\n",
    "                    indices = con.names\n",
    "                    print(indices)\n",
    "                    print(coh.shape)\n",
    "                    print(coh_abs.shape)\n",
    "                    print(coh_phase.shape)\n",
    "\n",
    "                    for i in range(coh.shape[0]):\n",
    "                        for j in range(coh.shape[1]):\n",
    "                            if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                                coherence_abs=coh_abs[i,j,:,:]\n",
    "                                coherence_abs=np.arctanh(coherence_abs)  # Apply Fisher transformation\n",
    "                                aon_vhp_coh_abs.append(coherence_abs)\n",
    "                                aon_vhp_coh_phase.append(coh_phase[i,j,:,:])\n",
    "                                ind_row =[rat_id, experiment, task_name, event, f'{indices[j]}-{indices[i]}', coh_phase[i,j,:,:]]\n",
    "\n",
    "                                ind_rows_df.append(ind_row)\n",
    "\n",
    "            row_coh_abs.append(np.mean(aon_vhp_coh_abs, axis=0))\n",
    "            row_coh_abs_mean.append(np.mean(aon_vhp_coh_abs))\n",
    "            row_coh_phase.append(np.mean(aon_vhp_coh_phase, axis=0))\n",
    "            row_coh_phase_mean.append(np.mean(aon_vhp_coh_phase))\n",
    "        all_coh_abs_data.append(row_coh_abs)\n",
    "        all_coh_abs_data_mean.append(row_coh_abs_mean)\n",
    "        all_coh_phase_data.append(row_coh_phase)\n",
    "        all_coh_phase_data_mean.append(row_coh_phase_mean)\n",
    "        \n",
    "ind_rows_df = pd.DataFrame(ind_rows_df, columns=['rat_id', 'experiment', 'task', 'event', 'channel_pair', 'coherence_phase'])\n",
    "\n",
    "\n",
    "ind_rows_df.to_pickle(savepath+f'coherence_phase_individual_epochs_around_door_dig{suffix}_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "all_coh_abs_data_df = pd.DataFrame(all_coh_abs_data, columns=['task'] + event_list)\n",
    "all_coh_abs_data_df.to_pickle(savepath+f'coherence_abs_spectrogram_around_door_dig{suffix}_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "all_coh_phase_data_df = pd.DataFrame(all_coh_phase_data, columns=['task'] + event_list)\n",
    "all_coh_phase_data_df.to_pickle(savepath+f'coherence_phase_spectrogram_around_door_dig{suffix}_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "\n",
    "fs=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = aon_vhp_coh_phase[0]\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 1\n",
    "fs=2000\n",
    "\n",
    "\n",
    "fmin = 12\n",
    "fmax = 30\n",
    "tmin = 0.5\n",
    "tmax = 0.9\n",
    "\n",
    "\n",
    "all_coh_phase_data_df_shuffled=pd.read_pickle(savepath+f'coherence_phase_spectrogram_around_door_dig_shuffled_truncated_{int(time_window*fs)}.pkl')\n",
    "all_coh_phase_data_df_real=pd.read_pickle(savepath+f'coherence_phase_spectrogram_around_door_dig_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract phase data for each condition\n",
    "\n",
    "truncated_phase_data_real = all_coh_phase_data_df_real['mne_epoch_around_dig'].apply(lambda x: x[fmin:fmax, int(tmin*fs):int(tmax*fs)])  # Theta band (8-12 Hz) and time window (0 to 0.4s)\n",
    "\n",
    "# phase_context = np.array(all_coh_phase_data_df.loc[all_coh_phase_data_df['task'] == 'BWcontext', 'mne_epoch_around_dig'].iloc[0]).flatten()\n",
    "# phase_nocontext = np.array(all_coh_phase_data_df.loc[all_coh_phase_data_df['task'] == 'BWnocontext', 'mne_epoch_around_dig'].iloc[0]).flatten()\n",
    "phase_context = np.array(truncated_phase_data_real[all_coh_phase_data_df_real['task'] == 'BWcontext'].iloc[0]).flatten()\n",
    "phase_nocontext = np.array(truncated_phase_data_real[all_coh_phase_data_df_real['task'] == 'BWnocontext'].iloc[0]).flatten()\n",
    "\n",
    "truncated_phase_data_shuffled = all_coh_phase_data_df_shuffled['mne_epoch_around_dig'].apply(lambda x: x[fmin:fmax, int(tmin*fs):int(tmax*fs)])  # Theta band (8-12 Hz) and time window (0 to 0.4s)\n",
    "phase_context_shuffled = np.array(truncated_phase_data_shuffled[all_coh_phase_data_df_shuffled['task'] == 'BWcontext'].iloc[0]).flatten()\n",
    "phase_nocontext_shuffled = np.array(truncated_phase_data_shuffled[all_coh_phase_data_df_shuffled['task'] == 'BWnocontext'].iloc[0]).flatten()\n",
    "\n",
    "# Convert phase to [0, 2pi]\n",
    "phase_context = np.mod(phase_context, 2 * np.pi)\n",
    "phase_nocontext = np.mod(phase_nocontext, 2 * np.pi)\n",
    "\n",
    "phase_context_shuffled = np.mod(phase_context_shuffled, 2 * np.pi)\n",
    "phase_nocontext_shuffled = np.mod(phase_nocontext_shuffled, 2 * np.pi)\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, subplot_kw={'projection': 'polar'}, figsize=(12, 6))\n",
    "axs=axs.flatten()\n",
    "colors = {'BWcontext': 'blue', 'BWnocontext': 'orange'}\n",
    "\n",
    "# Plot BW Context\n",
    "axs[0].hist(phase_context, bins=100, density=True, color=colors['BWcontext'], alpha=0.7)\n",
    "axs[0].set_title('Phase Difference Histogram\\nBW Context', fontsize=16)\n",
    "axs[0].set_xticks(np.linspace(0, 2 * np.pi, 8, endpoint=False))\n",
    "axs[0].set_xlim(0, 2 * np.pi)\n",
    "\n",
    "# Plot BW No Context\n",
    "axs[1].hist(phase_nocontext, bins=100, density=True, color=colors['BWnocontext'], alpha=0.7)\n",
    "axs[1].set_title('Phase Difference Histogram\\nBW No Context', fontsize=16)\n",
    "axs[1].set_xticks(np.linspace(0, 2 * np.pi, 8, endpoint=False))\n",
    "axs[1].set_xlim(0, 2 * np.pi)\n",
    "\n",
    "axs[2].hist(phase_context_shuffled, bins=100, density=True, color=colors['BWcontext'], alpha=0.7)\n",
    "axs[2].set_title('Phase Difference Histogram (Shuffled)\\nBW Context', fontsize=16)\n",
    "axs[2].set_xticks(np.linspace(0, 2 * np.pi, 8, endpoint=False))\n",
    "axs[2].set_xlim(0, 2 * np.pi)\n",
    "\n",
    "axs[3].hist(phase_nocontext_shuffled, bins=100, density=True, color=colors['BWnocontext'], alpha=0.7)\n",
    "axs[3].set_title('Phase Difference Histogram (Shuffled)\\nBW No Context', fontsize=16)\n",
    "axs[3].set_xticks(np.linspace(0, 2 * np.pi, 8, endpoint=False))\n",
    "axs[3].set_xlim(0, 2 * np.pi)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(savepath+f'coherence_phase_histogram_around_dig_truncated_{int(time_window*fs)}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "# Statistical test for difference in phase distributions between tasks\n",
    "\n",
    "# Use the Kolmogorov-Smirnov test to compare the two phase distributions\n",
    "ks_stat, p_value = ks_2samp(phase_context, phase_nocontext)\n",
    "print(f\"KS statistic: {ks_stat:.4f}, p-value: {p_value:.4e}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"There is a significant difference between the phase distributions of the two tasks.\")\n",
    "else:\n",
    "    print(\"No significant difference between the phase distributions of the two tasks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the polar plot but without averaging across experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 1\n",
    "fs=2000\n",
    "\n",
    "\n",
    "fmin = 30 \n",
    "fmax = 80\n",
    "tmin = 0.4\n",
    "tmax = 0.7\n",
    "\n",
    "\n",
    "# all_coh_phase_data_df_shuffled=pd.read_pickle(savepath+f'coherence_phase_spectrogram_around_door_dig_shuffled_truncated_{int(time_window*fs)}.pkl')\n",
    "# all_coh_phase_data_df_real=pd.read_pickle(savepath+f'coherence_phase_spectrogram_around_door_dig_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "all_coh_phase_data_df_shuffled=pd.read_pickle(savepath+f'coherence_phase_individual_epochs_around_door_dig_shuffled_truncated_{int(time_window*fs)}.pkl')\n",
    "all_coh_phase_data_df_real=pd.read_pickle(savepath+f'coherence_phase_individual_epochs_around_door_dig_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract phase data for each condition\n",
    "writer = pd.ExcelWriter(savepath+f'coherence_phase_analysis_around_door_dig_truncated_{int(time_window*fs)}.xlsx')\n",
    "bands_dict = {\n",
    "    'Theta': (4, 8),\n",
    "    'Beta': (12, 30),\n",
    "    'Gamma': (30, 80)\n",
    "}\n",
    "\n",
    "for band_name, (fmin, fmax) in bands_dict.items():\n",
    "    all_coh_phase_data_df_shuffled=pd.read_pickle(savepath+f'coherence_phase_individual_epochs_around_door_dig_shuffled_truncated_{int(time_window*fs)}.pkl')\n",
    "    all_coh_phase_data_df_real=pd.read_pickle(savepath+f'coherence_phase_individual_epochs_around_door_dig_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "    all_coh_phase_data_df_real['coherence_phase'] = all_coh_phase_data_df_real['coherence_phase'].apply(lambda x: x[fmin:fmax, int(tmin*fs):int(tmax*fs)])  # Theta band (8-12 Hz) and time window (0 to 0.4s)\n",
    "\n",
    "    all_coh_phase_data_df_real['coherence_phase'] = all_coh_phase_data_df_real['coherence_phase'].apply(lambda x: x.flatten())  # Theta band (8-12 Hz) and time window (0 to 0.4s)\n",
    "\n",
    "    bw_context_data = all_coh_phase_data_df_real[(all_coh_phase_data_df_real['task'] == 'BWcontext') & (all_coh_phase_data_df_real['event']=='mne_epoch_around_dig')]\n",
    "    phase_context = np.array([value for lst in bw_context_data['coherence_phase'] for value in lst]).flatten()\n",
    "\n",
    "    bw_nocontext_data = all_coh_phase_data_df_real[(all_coh_phase_data_df_real['task'] == 'BWnocontext') & (all_coh_phase_data_df_real['event']=='mne_epoch_around_dig')]\n",
    "    phase_nocontext = np.array([value for lst in bw_nocontext_data['coherence_phase'] for value in lst]).flatten()\n",
    "\n",
    "\n",
    "    all_coh_phase_data_df_shuffled['coherence_phase'] = all_coh_phase_data_df_shuffled['coherence_phase'].apply(lambda x: x[fmin:fmax, int(tmin*fs):int(tmax*fs)])  # Theta band (8-12 Hz) and time window (0 to 0.4s)\n",
    "    all_coh_phase_data_df_shuffled['coherence_phase'] = all_coh_phase_data_df_shuffled['coherence_phase'].apply(lambda x: x.flatten())  # Theta band (8-12 Hz) and time window (0 to 0.4s)\n",
    "    bw_context_data_shuffled = all_coh_phase_data_df_shuffled[(all_coh_phase_data_df_shuffled['task'] == 'BWcontext') & (all_coh_phase_data_df_shuffled['event']=='mne_epoch_around_dig')]\n",
    "    phase_context_shuffled = np.array([value for lst in bw_context_data_shuffled['coherence_phase'] for value in lst]).flatten()\n",
    "\n",
    "\n",
    "    bw_nocontext_data_shuffled = all_coh_phase_data_df_shuffled[(all_coh_phase_data_df_shuffled['task'] == 'BWnocontext') & (all_coh_phase_data_df_shuffled['event']=='mne_epoch_around_dig')]\n",
    "    phase_nocontext_shuffled = np.array([value for lst in bw_nocontext_data_shuffled['coherence_phase'] for value in lst]).flatten()\n",
    "\n",
    "\n",
    "    # Convert phase to [0, 2pi]\n",
    "    phase_context = np.mod(phase_context, 2 * np.pi)\n",
    "    phase_nocontext = np.mod(phase_nocontext, 2 * np.pi)\n",
    "\n",
    "    phase_context_shuffled = np.mod(phase_context_shuffled, 2 * np.pi)\n",
    "    phase_nocontext_shuffled = np.mod(phase_nocontext_shuffled, 2 * np.pi)\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, subplot_kw={'projection': 'polar'}, figsize=(12, 6))\n",
    "    fig.suptitle('Coherence Phase Freq - {}-{} Hz, Time - {}-{} s'.format(fmin, fmax, tmin, tmax), fontsize=16)\n",
    "    axs=axs.flatten()\n",
    "    colors = {'BWcontext': 'black', 'BWnocontext': 'grey'}\n",
    "\n",
    "    # Plot BW Context\n",
    "    axs[0].hist(phase_context, bins=100, density=True, color=colors['BWcontext'], alpha=0.7)\n",
    "    axs[0].set_title('Context', fontsize=16)\n",
    "    axs[0].set_xticks(np.linspace(0, 2 * np.pi, 8, endpoint=False))\n",
    "    axs[0].set_xlim(0, 2 * np.pi)\n",
    "    counts, edges = np.histogram(phase_context, bins=100, range=(0, 2*np.pi))\n",
    "\n",
    "    # 5. Get bin centers\n",
    "    # This is the Python/NumPy way to find the midpoint of each bin\n",
    "    locs = (edges[:-1] + edges[1:]) / 2\n",
    "    closed_locs = np.append(locs, locs[0])\n",
    "    closed_counts = np.append(counts, counts[0])\n",
    "    # 6. Plot the line (MATLAB: plot(locs, counts, 'LineWidth', 3);)\n",
    "    axs[0].plot(closed_locs, closed_counts, 'black', linewidth=1, label='Bin Centers Line') # 'r-' adds color\n",
    "\n",
    "    # Plot BW No Context\n",
    "    axs[1].hist(phase_nocontext, bins=100, density=True, color=colors['BWnocontext'], alpha=0.7)\n",
    "    axs[1].set_title('No Context', fontsize=16)\n",
    "    axs[1].set_xticks(np.linspace(0, 2 * np.pi, 8, endpoint=False))\n",
    "    axs[1].set_xlim(0, 2 * np.pi)\n",
    "\n",
    "    axs[2].hist(phase_context_shuffled, bins=100, density=True, color=colors['BWcontext'], alpha=0.7)\n",
    "    axs[2].set_title('Context(Shuffled)', fontsize=16)\n",
    "    axs[2].set_xticks(np.linspace(0, 2 * np.pi, 8, endpoint=False))\n",
    "    axs[2].set_xlim(0, 2 * np.pi)\n",
    "\n",
    "    axs[3].hist(phase_nocontext_shuffled, bins=100, density=True, color=colors['BWnocontext'], alpha=0.7)\n",
    "    axs[3].set_title('No Context(Shuffled)', fontsize=16)\n",
    "    axs[3].set_xticks(np.linspace(0, 2 * np.pi, 8, endpoint=False))\n",
    "    axs[3].set_xlim(0, 2 * np.pi)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(savepath+f'coherence_phase_polarhist_around_dig_{fmin}_{fmax}_{int(time_window*fs)}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    # Statistical test for difference in phase distributions between tasks\n",
    "\n",
    "\n",
    "    #########Plotting in a simple histogram##########\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(8, 6))\n",
    "    axs=axs.flatten()\n",
    "    fig.suptitle('Coherence Phase Freq - {}-{} Hz, Time - {}-{} s'.format(fmin, fmax, tmin, tmax), fontsize=16)\n",
    "    colors = {'BWcontext': 'black', 'BWnocontext': 'grey'}\n",
    "\n",
    "    # Convert phase values from [0, 2π] to [-π, π]\n",
    "    phase_context_centered = np.where(phase_context > np.pi, phase_context - 2*np.pi, phase_context)\n",
    "    phase_nocontext_centered = np.where(phase_nocontext > np.pi, phase_nocontext - 2*np.pi, phase_nocontext)\n",
    "    phase_context_shuffled_centered = np.where(phase_context_shuffled > np.pi, phase_context_shuffled - 2*np.pi, phase_context_shuffled)\n",
    "    phase_nocontext_shuffled_centered = np.where(phase_nocontext_shuffled > np.pi, phase_nocontext_shuffled - 2*np.pi, phase_nocontext_shuffled)\n",
    "\n",
    "    # Plotting the histograms with centered x-axis\n",
    "    axs[0].hist(phase_context_centered, bins=100, density=True, color=colors['BWcontext'], alpha=0.7)\n",
    "    axs[0].set_title('Context', fontsize=16)\n",
    "    axs[0].set_xticks(np.linspace(-np.pi, np.pi, 7))\n",
    "    axs[0].set_xticklabels(['-π', '-2π/3', '-π/3', '0', 'π/3', '2π/3', 'π'])\n",
    "    axs[0].set_xlim(-np.pi, np.pi)\n",
    "\n",
    "    axs[1].hist(phase_nocontext_centered, bins=100, density=True, color=colors['BWnocontext'], alpha=0.7)\n",
    "    axs[1].set_title('No Context', fontsize=16)\n",
    "    axs[1].set_xticks(np.linspace(-np.pi, np.pi, 7))\n",
    "    axs[1].set_xticklabels(['-π', '-2π/3', '-π/3', '0', 'π/3', '2π/3', 'π'])\n",
    "    axs[1].set_xlim(-np.pi, np.pi)\n",
    "\n",
    "    axs[2].hist(phase_context_shuffled_centered, bins=100, density=True, color=colors['BWcontext'], alpha=0.7)\n",
    "    axs[2].set_title('Context(Shuffled)', fontsize=16)\n",
    "    axs[2].set_xticks(np.linspace(-np.pi, np.pi, 7))\n",
    "    axs[2].set_xticklabels(['-π', '-2π/3', '-π/3', '0', 'π/3', '2π/3', 'π'])\n",
    "    axs[2].set_xlim(-np.pi, np.pi)\n",
    "\n",
    "    axs[3].hist(phase_nocontext_shuffled_centered, bins=100, density=True, color=colors['BWnocontext'], alpha=0.7)\n",
    "    axs[3].set_title('No Context(Shuffled)', fontsize=16)\n",
    "    axs[3].set_xticks(np.linspace(-np.pi, np.pi, 7))\n",
    "    axs[3].set_xticklabels(['-π', '-2π/3', '-π/3', '0', 'π/3', '2π/3', 'π'])\n",
    "    axs[3].set_xlim(-np.pi, np.pi)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(savepath+f'coherence_phase_histogram_around_dig_{fmin}_{fmax}_{int(time_window*fs)}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    # Statistical test for difference in phase distributions between tasks\n",
    "\n",
    "    data_dict = {'context_real': phase_context,\n",
    "                 'nocontext_real': phase_nocontext,\n",
    "                 'context_shuffled': phase_context_shuffled,\n",
    "                 'nocontext_shuffled': phase_nocontext_shuffled}\n",
    "    df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in data_dict.items()]))\n",
    "    df.to_excel(writer, sheet_name=f'{band_name}_phase_data', index=False)\n",
    "writer.close()\n",
    "\n",
    "ks_stat, p_value = ks_2samp(phase_context, phase_nocontext)\n",
    "print(f\"KS statistic: {ks_stat:.4f}, p-value: {p_value:.4e}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"There is a significant difference between the phase distributions of the two tasks.\")\n",
    "else:\n",
    "    print(\"No significant difference between the phase distributions of the two tasks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making polar plots with Context and No Context in single plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 1\n",
    "fs=2000\n",
    "\n",
    "\n",
    "fmin = 30 \n",
    "fmax = 80\n",
    "tmin = 0.4\n",
    "tmax = 0.7\n",
    "\n",
    "\n",
    "# all_coh_phase_data_df_shuffled=pd.read_pickle(savepath+f'coherence_phase_spectrogram_around_door_dig_shuffled_truncated_{int(time_window*fs)}.pkl')\n",
    "# all_coh_phase_data_df_real=pd.read_pickle(savepath+f'coherence_phase_spectrogram_around_door_dig_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "all_coh_phase_data_df_shuffled=pd.read_pickle(savepath+f'coherence_phase_individual_epochs_around_door_dig_shuffled_truncated_{int(time_window*fs)}.pkl')\n",
    "all_coh_phase_data_df_real=pd.read_pickle(savepath+f'coherence_phase_individual_epochs_around_door_dig_truncated_{int(time_window*fs)}.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract phase data for each condition\n",
    "writer = pd.ExcelWriter(savepath+f'coherence_phase_analysis_around_door_dig_truncated_{int(time_window*fs)}.xlsx')\n",
    "bands_dict = {\n",
    "    'Theta': (4, 8),\n",
    "    'Beta': (12, 30),\n",
    "    'Gamma': (30, 80)\n",
    "}\n",
    "\n",
    "tmin = 0\n",
    "tmax = 1\n",
    "\n",
    "tmin_idx = int(tmin * fs)\n",
    "tmax_idx = int(tmax * fs)\n",
    "\n",
    "events_dict = {\n",
    "'mne_epoch_door_before': 'Door Before',\n",
    "'mne_epoch_dig_before': 'Dig Before',\n",
    "'mne_epoch_dig_after': 'Dig After',\n",
    "}\n",
    "\n",
    "fig, axs = plt.subplots(3,3, subplot_kw={'projection': 'polar'}, figsize=(18, 12))\n",
    "\n",
    "for axi_row,(band_name, (fmin, fmax)) in enumerate(bands_dict.items()):\n",
    "\n",
    "    for axi_col,(event_key, event_name) in enumerate(events_dict.items()):\n",
    "\n",
    "        for data_type in ['real', 'shuffled']:\n",
    "            if data_type == 'real':\n",
    "                all_coh_phase_data_df = pd.read_pickle(savepath+f'coherence_phase_individual_epochs_around_door_dig_truncated_{int(time_window*fs)}.pkl')\n",
    "                linestyle = 'solid'  \n",
    "            else:\n",
    "                all_coh_phase_data_df = pd.read_pickle(savepath+f'coherence_phase_individual_epochs_around_door_dig_shuffled_truncated_{int(time_window*fs)}.pkl')\n",
    "                linestyle = 'dashed'\n",
    "            print(f'{axi_row} {axi_col} Processing Band: {band_name}, Event: {event_name}, Data Type: {data_type}')\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            all_coh_phase_data_df['coherence_phase'] = all_coh_phase_data_df['coherence_phase'].apply(lambda x: x[fmin:fmax, tmin_idx:tmax_idx])\n",
    "            all_coh_phase_data_df['coherence_phase'] = all_coh_phase_data_df['coherence_phase'].apply(lambda x: x.flatten())  \n",
    "\n",
    "            bw_context_data = all_coh_phase_data_df[(all_coh_phase_data_df['task'] == 'BWcontext') & (all_coh_phase_data_df['event']==event_key)]\n",
    "            phase_context = np.array([value for lst in bw_context_data['coherence_phase'] for value in lst]).flatten()\n",
    "\n",
    "            bw_nocontext_data = all_coh_phase_data_df[(all_coh_phase_data_df['task'] == 'BWnocontext') & (all_coh_phase_data_df['event']==event_key)]\n",
    "            phase_nocontext = np.array([value for lst in bw_nocontext_data['coherence_phase'] for value in lst]).flatten()\n",
    "\n",
    "            # Convert phase to [0, 2pi]\n",
    "            phase_context = np.mod(phase_context, 2 * np.pi)\n",
    "            phase_nocontext = np.mod(phase_nocontext, 2 * np.pi)\n",
    "\n",
    "            ax = axs[axi_row, axi_col]    \n",
    "            \n",
    "            counts, edges = np.histogram(phase_context, bins=100, range=(0, 2*np.pi), density=True)\n",
    "            locs = (edges[:-1] + edges[1:]) / 2\n",
    "            closed_locs = np.append(locs, locs[0])\n",
    "            closed_counts = np.append(counts, counts[0])\n",
    "            ax.fill(closed_locs, closed_counts, color='blue', alpha=0.3)  # Fill the area with black color\n",
    "            ax.plot(closed_locs, closed_counts, 'blue', linewidth=1, label=f'Context ({data_type})', linestyle=linestyle) # 'r-' adds color\n",
    "\n",
    "            counts, edges = np.histogram(phase_nocontext, bins=100, range=(0, 2*np.pi), density=True)\n",
    "            locs = (edges[:-1] + edges[1:]) / 2\n",
    "            closed_locs = np.append(locs, locs[0])\n",
    "            closed_counts = np.append(counts, counts[0])\n",
    "            ax.fill(closed_locs, closed_counts, color='orange', alpha=0.3)  # Fill the area with grey color\n",
    "            ax.plot(closed_locs, closed_counts, 'orange', linewidth=1, label=f'No Context ({data_type})', linestyle=linestyle)  #ax.set_xticks(np.linspace(0, 2 * np.pi, 8, endpoint=False))\n",
    "            ax.set_xlim(0, 2 * np.pi)\n",
    "            ax.set_rticks([])  # Remove radial ticks for clarity\n",
    "            # if axi_row == 0 and axi_col == 0:\n",
    "            #     ax.legend(loc='upper right', fontsize=8)\n",
    "\n",
    "            ax.set_title(f'{band_name} Band - {event_name}', fontsize=12)\n",
    "            #ax.set_rmax(0.5)  # Set maximum radius for better visualization\n",
    "# Create custom labels that combine color, style and condition\n",
    "legend_elements = [\n",
    "    plt.Line2D([0], [0], color='blue', linestyle='solid', label='Context (real)'),\n",
    "    plt.Line2D([0], [0], color='blue', linestyle='dashed', label='Context (shuffled)'),\n",
    "    plt.Line2D([0], [0], color='orange', linestyle='solid', label='No Context (real)'),\n",
    "    plt.Line2D([0], [0], color='orange', linestyle='dashed', label='No Context (shuffled)')\n",
    "]\n",
    "\n",
    "# Add legend at the bottom\n",
    "fig.legend(handles=legend_elements, loc='lower center', bbox_to_anchor=(0.5, 0), \n",
    "          ncol=4, fontsize=12)\n",
    "\n",
    "# Adjust subplot spacing to make room for legend\n",
    "plt.subplots_adjust(bottom=0.12)\n",
    "fig.suptitle(f'Coherence Phase Analysis {tmin}s-{tmax}s', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(savepath+f'coherence_phase_polarhist_all_events_{int(tmin*fs)}-{int(tmax*fs)}.png', dpi=300, bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "fs=2000\n",
    "times=np.arange(-0.7, 0.7, 1/fs)\n",
    "fig, axs=plt.subplots(2,2, figsize=(20,10), sharey=True)\n",
    "all_con_data_df=all_coh_abs_data_df\n",
    "aon_vhp_phase=all_coh_phase_data_df\n",
    "vmin = all_con_data_df[event_list].applymap(np.min).min().min()\n",
    "vmax = all_con_data_df[event_list].applymap(np.max).max().max()\n",
    "event_names=['Around Door','Around Dig']\n",
    "freqs=np.arange(2.5, 100, 0.5)\n",
    "for i, event in enumerate(event_list):\n",
    "    axs[0,i].imshow(all_con_data_df[event][0], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    coh_phase = aon_vhp_phase[event][0]\n",
    "    X, Y = np.meshgrid(np.linspace(times[0], times[-1], coh_phase.shape[1]), np.linspace(freqs[0], freqs[-1], coh_phase.shape[0]))\n",
    "\n",
    "    U = np.cos(coh_phase)\n",
    "    V = np.sin(coh_phase)\n",
    "    f_x = 100\n",
    "    f_y = 5\n",
    "    axs[0, i].quiver(X[2::f_y, ::f_x], Y[2::f_y, ::f_x], U[2::f_y, ::f_x], V[2::f_y, ::f_x], angles='uv', scale=40, alpha=0.7)\n",
    "\n",
    "    axs[0,i].set_xlabel('')\n",
    "\n",
    "    axs[0,i].set_ylabel('Frequency (Hz)', fontsize=20)\n",
    "    axs[0,i].set_title(event_names[i], fontsize=20)\n",
    "    axs[0,i].vlines(0, 0, 100, color='k', linestyle='--')\n",
    "    axs[0,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    \n",
    "    axs[1,i].imshow(all_con_data_df[event][1], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    coh_phase = aon_vhp_phase[event][1]\n",
    "    X, Y = np.meshgrid(np.linspace(times[0], times[-1], coh_phase.shape[1]), np.linspace(freqs[0], freqs[-1], coh_phase.shape[0]))\n",
    "\n",
    "    U = np.cos(coh_phase)\n",
    "    V = np.sin(coh_phase)\n",
    "    f_x = 100\n",
    "    f_y = 5\n",
    "    axs[1, i].quiver(X[2::f_y, ::f_x], Y[2::f_y, ::f_x], U[2::f_y, ::f_x], V[2::f_y, ::f_x], angles='uv', scale=40, alpha=0.7)\n",
    "\n",
    "    \n",
    "    axs[1,i].set_xlabel('Time (s)', fontsize=20)\n",
    "    axs[1,i].set_ylabel('Frequency (Hz)', fontsize=20)\n",
    "    axs[1,i].set_title(event_names[i], fontsize=20)\n",
    "    axs[1,i].vlines(0, 0, 100, color='k', linestyle='--')\n",
    "\n",
    "    axs[0,0].text(-0.2, 0.5, 'Context', transform=axs[0,0].transAxes, fontsize=18, verticalalignment='center', rotation=90)\n",
    "    axs[1,0].text(-0.2, 0.5, 'No Context', transform=axs[1,0].transAxes, fontsize=18, verticalalignment='center', rotation=90)\n",
    "    axs[0,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    axs[1,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    # axs[0,i].set_xticks(np.arange(-2, 3, 1))  # Set x-ticks from -2 to 2 seconds\n",
    "    # axs[0,i].set_xticklabels(np.arange(-2, 3, 1))  # Set x-tick labels from -2 to 2 seconds\n",
    "    # axs[1,i].set_xticks(np.arange(-2, 3, 1))  # Set x-ticks from -2 to 2 seconds\n",
    "    # axs[1,i].set_xticklabels(np.arange(-2, 3, 1))  # Set x-tick labels from -2 to 2 seconds\n",
    "\n",
    "    # Add a colorbar\n",
    "cbar = fig.colorbar(axs[0,0].images[0], ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "cbar.set_label('Coherence (Z-transformed)', loc='center', fontsize=20, labelpad=10)\n",
    "cbar.ax.tick_params(labelsize=20)  # Set colorbar tick label size\n",
    "\n",
    "fig.savefig(savepath+'aon_vhp_coherogram.png',format='png', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Behavior Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Behavior Correlation with Power\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_df=pd.read_pickle(savepath+'compiled_data_all_epochs.pkl')\n",
    "behavior_df.iloc[:,-5:]=behavior_df.iloc[:,-5:].applymap(lambda x: scipy.signal.welch(x, fs=2000, nperseg=2000)[1])\n",
    "\n",
    "bands_dict = {'beta': [12, 30], 'gamma': [30, 80], 'theta': [4, 12], 'total': [1, 100]}\n",
    "for col in behavior_df.columns[-7:]:\n",
    "    for band, (band_start, band_end) in bands_dict.items():\n",
    "        behavior_df[band + '_' + col] = behavior_df[col].apply(lambda x: functions.get_band_power(x, band_start, band_end))\n",
    "\n",
    "behavior_df['channel'] = behavior_df['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "\n",
    "behavior_df_grouped=behavior_df.groupby(['task', 'channel'])\n",
    "writer=pd.ExcelWriter(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\behavior_power_correlation.xlsx')\n",
    "for (task, channel), group in behavior_df_grouped:\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(20, 10), sharex=True, constrained_layout=True)\n",
    "    axs = axs.flatten()\n",
    "    group=behavior_df[(behavior_df['channel']==channel) & (behavior_df['task']==task)]\n",
    "\n",
    "    power_columns=group.columns[17:]\n",
    "    print(power_columns)\n",
    "    group_melted=pd.melt(group, id_vars=['rat', 'task', 'channel', 'correct?'], value_vars=power_columns, var_name='band_event', value_name='power')\n",
    "    group_melted['band']=group_melted['band_event'].apply(lambda x: x.split('_')[0])\n",
    "    group_melted['event']=group_melted['band_event'].apply(lambda x: x.split('_')[1:])\n",
    "    group_melted['event']=group_melted['event'].apply(lambda x: x[0]+'_'+x[1])\n",
    "    group_melted['correct?']=group_melted['correct?'].apply(lambda x: 'Incorrect' if x=='0' else 'Correct')\n",
    "    \n",
    "    group_melted.to_excel(writer, sheet_name=f'{channel}_{task}')\n",
    "\n",
    "\n",
    "    correct_counts = group_melted[group_melted['correct?'] == 'Correct'].shape[0]\n",
    "    incorrect_counts = group_melted[group_melted['correct?'] == 'Incorrect'].shape[0]\n",
    "    print(f\"Number of Corrects: {correct_counts}\")\n",
    "    print(f\"Number of Incorrects: {incorrect_counts}\")\n",
    "    events_list=['pre_door','post_door','pre_dig','post_dig']\n",
    "    for i, event in enumerate(events_list):\n",
    "        ax=axs[i]\n",
    "        sns.boxplot(x='band', y='power', hue='correct?', data=group_melted[group_melted['event']==event], showfliers=False, ax=ax)\n",
    "        #sns.stripplot(x='band', y='power', hue='correct?', data=aon_behavior_df_melted[aon_behavior_df_melted['event']==event], dodge=True, edgecolor='black', linewidth=1, jitter=True, ax=ax)\n",
    "        ax.set_title(event)\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('Power')\n",
    "        ax.legend(title='Correct?')\n",
    "    fig.suptitle(f'{channel} {task}')\n",
    "    fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\behavior_power_{channel}_{task}.png')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Behavior Correlation with Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "time_window=  1\n",
    "fs=2000\n",
    "behavior_coherence_df=pd.read_pickle(savepath+f'compiled_data_all_epochs_truncated_{int(time_window*fs)}.pkl')\n",
    "behavior_coherence_df_marked=pd.read_pickle(savepath+f'marked_mne_epochs_array_df_truncated_2000_251125.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "behavior_coherence_df['unique_id']=behavior_coherence_df['rat']+'_'+behavior_coherence_df['task']+behavior_coherence_df['date']\n",
    "behavior_coherence_df_grouped=behavior_coherence_df.groupby(['unique_id', 'trial'])\n",
    "behavior_coherence_compiled_data_df=[]\n",
    "\n",
    "for (unique_id, trial), group in behavior_coherence_df_grouped:\n",
    "    print(unique_id, trial)\n",
    "    channels_list=list(group['channel'].unique())\n",
    "    print(channels_list)\n",
    "    info=mne.create_info(ch_names=channels_list, sfreq=fs, ch_types='eeg')\n",
    "\n",
    "    mne_epoch_door_before=np.zeros((1,len(channels_list),int(time_window*fs)))\n",
    "    mne_epoch_door_after=np.zeros((1,len(channels_list),int(time_window*fs)))\n",
    "    mne_epoch_dig_before=np.zeros((1,len(channels_list),int(time_window*fs)))\n",
    "    mne_epoch_dig_after=np.zeros((1,len(channels_list),int(time_window*fs)))\n",
    "    mne_epoch_around_door=np.zeros((1,len(channels_list),int(time_window*fs)*2))\n",
    "    mne_epoch_around_dig=np.zeros((1,len(channels_list),int(time_window*fs)*2))\n",
    "\n",
    "    for channel_num, channel_id in enumerate(channels_list):\n",
    "        data=group[group['channel']==channel_id]\n",
    "        mne_epoch_door_before[0,channel_num,:]=data['pre_door'].values[0][:int(time_window*fs)]\n",
    "        mne_epoch_door_after[0,channel_num,:]=data['post_door'].values[0][:int(time_window*fs)]\n",
    "        mne_epoch_dig_before[0,channel_num,:]=data['pre_dig'].values[0][:int(time_window*fs)]\n",
    "        mne_epoch_dig_after[0,channel_num,:]=data['post_dig'].values[0][:int(time_window*fs)]\n",
    "        mid_point = int(len(data['around_door'].values[0])/2)\n",
    "        mne_epoch_around_door[0,channel_num,:]=data['around_door'].values[0][mid_point-int(time_window*fs):mid_point+int(time_window*fs)]\n",
    "        mne_epoch_around_dig[0,channel_num,:]=data['around_dig'].values[0][mid_point-int(time_window*fs):mid_point+int(time_window*fs)]\n",
    "\n",
    "    # mne_epoch_around_door_truncated = mne_epoch_around_door[:, :, 3000:5000]\n",
    "    # mne_epoch_around_dig_truncated = mne_epoch_around_dig[:, :, 3000:5000]\n",
    "    mne_epoch_door_before = mne.EpochsArray(mne_epoch_door_before, info)\n",
    "    mne_epoch_door_after= mne.EpochsArray(mne_epoch_door_after, info)\n",
    "    mne_epoch_dig_before = mne.EpochsArray(mne_epoch_dig_before, info)\n",
    "    mne_epoch_dig_after = mne.EpochsArray(mne_epoch_dig_after, info)\n",
    "    mne_epoch_around_door = mne.EpochsArray(mne_epoch_around_door, info)\n",
    "    mne_epoch_around_dig = mne.EpochsArray(mne_epoch_around_dig, info)\n",
    "    \n",
    "    behavior_coherence_compiled_data={\n",
    "        'rat': group['rat'].values[0],\n",
    "        'task': group['task'].values[0],\n",
    "        'date': group['date'].values[0],\n",
    "        'unique_id': unique_id,\n",
    "        'trial': trial,\n",
    "        'side': group['side'].values[0],\n",
    "        'correct?': group['correct'].values[0],\n",
    "        'time_to_dig': group['timestamps'].iloc[0][1] - group['timestamps'].iloc[0][0],\n",
    "        'pre_door': mne_epoch_door_before,\n",
    "        'post_door': mne_epoch_door_after,\n",
    "        'pre_dig': mne_epoch_dig_before,\n",
    "        'post_dig': mne_epoch_dig_after,\n",
    "        'around_door': mne_epoch_around_door,\n",
    "        'around_dig': mne_epoch_around_dig\n",
    "        ,'around_door_truncated': mne_epoch_around_door,\n",
    "        'around_dig_truncated': mne_epoch_around_dig}\n",
    "    \n",
    "    behavior_coherence_compiled_data_df.append(behavior_coherence_compiled_data)\n",
    "behavior_coherence_compiled_data_df=pd.DataFrame(behavior_coherence_compiled_data_df)\n",
    "behavior_coherence_compiled_data_df.to_pickle(savepath+f'behavior_coherence_single_epochs_mne_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "# def lfp_to_mne_epoch(lfp_data):\n",
    "#     fs=2000\n",
    "#     freqs = np.arange(1,100)\n",
    "#     n_cycles = freqs/2\n",
    "#     empty_array=np.zeros((1,1,len(lfp_data)))\n",
    "#     empty_array[0,0,:]=lfp_data\n",
    "#     info = mne.create_info(ch_names=['1'], sfreq=fs, ch_types='eeg')\n",
    "#     mne_epoch = mne.EpochsArray(empty_array, info)\n",
    "#     return mne_epoch\n",
    "\n",
    "# behavior_coherence_df[['pre_door','post_door', 'pre_dig', 'post_dig']]=behavior_coherence_df[['pre_door','post_door', 'pre_dig', 'post_dig']].applymap(lambda x: lfp_to_mne_epoch(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping bad epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "time_window=  1\n",
    "fs=2000\n",
    "behavior_coherence_compiled_data_df_truncated=pd.read_pickle(savepath+f'behavior_coherence_single_epochs_mne_truncated_{int(time_window*fs)}.pkl')\n",
    "print(behavior_coherence_compiled_data_df_truncated.columns)\n",
    "common_cols = ['rat', 'task', 'date', 'unique_id', 'trial', 'side', 'correct?','time_to_dig']\n",
    "\n",
    "pre_door_df = behavior_coherence_compiled_data_df_truncated[common_cols+['pre_door']]\n",
    "pre_dig_df = behavior_coherence_compiled_data_df_truncated[common_cols+['pre_dig']]\n",
    "post_dig_df = behavior_coherence_compiled_data_df_truncated[common_cols+['post_dig']]\n",
    "\n",
    "annotated_file = pd.read_csv(savepath+'con_data_df_annotation.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total number of bad epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bad_pre_door =0\n",
    "total_bad_pre_dig =0\n",
    "total_bad_post_dig =0\n",
    "\n",
    "for index, row in annotated_file.iterrows():\n",
    "    door_before_bad_epochs = len(json.loads(row['annotation_door_before']))\n",
    "    dig_before_bad_epochs = len(json.loads(row['annotation_dig_before']))\n",
    "    dig_after_bad_epochs = len(json.loads(row['annotation_dig_after']))\n",
    "    \n",
    "    total_bad_pre_door = total_bad_pre_door + door_before_bad_epochs \n",
    "    total_bad_pre_dig = total_bad_pre_dig + dig_before_bad_epochs\n",
    "    total_bad_post_dig  =   total_bad_post_dig + dig_after_bad_epochs\n",
    "\n",
    "print(total_bad_pre_door)\n",
    "print(total_bad_pre_dig)\n",
    "print(total_bad_post_dig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pre_door_df_filtered = []\n",
    "pre_dig_df_filtered = []\n",
    "post_dig_df_filtered = []\n",
    "# Iterate over rows using iterrows()\n",
    "\n",
    "\n",
    "\n",
    "for index, row in annotated_file.iterrows():\n",
    "    print(index)\n",
    "    unique_id = f\"{row['rat_id']}_{row['task']}{row['date']}\"\n",
    "    print(unique_id)\n",
    "    pre_door_exp_df = pre_door_df[pre_door_df['unique_id']==unique_id]\n",
    "    pre_dig_exp_df = pre_dig_df[pre_dig_df['unique_id']==unique_id]\n",
    "    post_dig_exp_df = post_dig_df[post_dig_df['unique_id']==unique_id]\n",
    "    \n",
    "    door_before_bad_epochs = json.loads(row['annotation_door_before'])\n",
    "    dig_before_bad_epochs = json.loads(row['annotation_dig_before'])\n",
    "    dig_after_bad_epochs = json.loads(row['annotation_dig_after'])\n",
    "    \n",
    "    pre_door_exp_df_filtered = pre_door_exp_df[~pre_door_exp_df['trial'].isin(door_before_bad_epochs)]\n",
    "    pre_dig_exp_df_filtered = pre_dig_exp_df[~pre_dig_exp_df['trial'].isin(dig_before_bad_epochs)]\n",
    "    post_dig_exp_df_filtered = post_dig_exp_df[~post_dig_exp_df['trial'].isin(dig_after_bad_epochs)]\n",
    "    \n",
    "    pre_door_df_filtered.append(pre_door_exp_df_filtered)\n",
    "    pre_dig_df_filtered.append(pre_dig_exp_df_filtered)\n",
    "    post_dig_df_filtered.append(post_dig_exp_df_filtered)\n",
    "\n",
    "    \n",
    "pre_door_df_filtered=pd.concat(pre_door_df_filtered).reset_index(drop=True)\n",
    "pre_dig_df_filtered=pd.concat(pre_dig_df_filtered).reset_index(drop=True)\n",
    "post_dig_df_filtered=pd.concat(post_dig_df_filtered).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(coherence_functions)\n",
    "importlib.reload(power_functions)\n",
    "bands_dict = {'beta': [12, 30], 'gamma': [30, 80],'theta':[4,12],'theta+beta':[4,30] ,'total': [1, 100]}\n",
    "for band, (band_start, band_end) in bands_dict.items():\n",
    "    pre_door_df_filtered[band + '_' + 'pre_door'] = pre_door_df_filtered['pre_door'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_behavior(x, band_start=band_start, band_end=band_end))\n",
    "    pre_dig_df_filtered[band + '_' + 'pre_dig'] = pre_dig_df_filtered['pre_dig'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_behavior(x, band_start=band_start, band_end=band_end))\n",
    "    post_dig_df_filtered[band + '_' + 'post_dig'] = post_dig_df_filtered['post_dig'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_behavior(x, band_start=band_start, band_end=band_end))\n",
    "\n",
    "    pre_door_df_filtered[band + '_' + 'pli_pre_door'] = pre_door_df_filtered['pre_door'].apply(lambda x: coherence_functions.convert_epoch_to_pli_behavior(x, band_start=band_start, band_end=band_end))\n",
    "    pre_dig_df_filtered[band + '_' + 'pli_pre_dig'] = pre_dig_df_filtered['pre_dig'].apply(lambda x: coherence_functions.convert_epoch_to_pli_behavior(x, band_start=band_start, band_end=band_end))\n",
    "    post_dig_df_filtered[band + '_' + 'pli_post_dig'] = post_dig_df_filtered['post_dig'].apply(lambda x: coherence_functions.convert_epoch_to_pli_behavior(x, band_start=band_start, band_end=band_end))\n",
    "\n",
    "    pre_door_df_filtered[band + '_' + 'plv_pre_door'] = pre_door_df_filtered['pre_door'].apply(lambda x: coherence_functions.convert_epoch_to_plv_behavior(x, band_start=band_start, band_end=band_end))\n",
    "    pre_dig_df_filtered[band + '_' + 'plv_pre_dig'] = pre_dig_df_filtered['pre_dig'].apply(lambda x: coherence_functions.convert_epoch_to_plv_behavior(x, band_start=band_start, band_end=band_end))\n",
    "    post_dig_df_filtered[band + '_' + 'plv_post_dig'] = post_dig_df_filtered['post_dig'].apply(lambda x: coherence_functions.convert_epoch_to_plv_behavior(x, band_start=band_start, band_end=band_end))\n",
    "\n",
    "    pre_door_df_filtered[band + '_' + 'aon_power_pre_door'] = pre_door_df_filtered['pre_door'].apply(lambda x: power_functions.convert_epoch_to_power(x, band_start=band_start, band_end=band_end, brain_area=\"AON\"))\n",
    "    pre_dig_df_filtered[band + '_' + 'aon_power_pre_dig'] = pre_dig_df_filtered['pre_dig'].apply(lambda x: power_functions.convert_epoch_to_power(x, band_start=band_start, band_end=band_end, brain_area=\"AON\"))\n",
    "    post_dig_df_filtered[band + '_' + 'aon_power_post_dig'] = post_dig_df_filtered['post_dig'].apply(lambda x: power_functions.convert_epoch_to_power(x, band_start=band_start, band_end=band_end, brain_area=\"AON\"))\n",
    "\n",
    "    pre_door_df_filtered[band + '_' + 'vhp_power_pre_door'] = pre_door_df_filtered['pre_door'].apply(lambda x: power_functions.convert_epoch_to_power(x, band_start=band_start, band_end=band_end, brain_area=\"vHp\"))\n",
    "    pre_dig_df_filtered[band + '_' + 'vhp_power_pre_dig'] = pre_dig_df_filtered['pre_dig'].apply(lambda x: power_functions.convert_epoch_to_power(x, band_start=band_start, band_end=band_end, brain_area=\"vHp\"))\n",
    "    post_dig_df_filtered[band + '_' + 'vhp_power_post_dig'] = post_dig_df_filtered['post_dig'].apply(lambda x: power_functions.convert_epoch_to_power(x, band_start=band_start, band_end=band_end, brain_area=\"vHp\"))\n",
    "\n",
    "pre_door_df_filtered.to_pickle(savepath + 'coh_beh_pre_door_df.pkl')\n",
    "pre_dig_df_filtered.to_pickle(savepath + 'coh_beh_pre_dig_df.pkl')\n",
    "post_dig_df_filtered.to_pickle(savepath + 'coh_beh_post_dig_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_door_df_filtered=pd.read_pickle(savepath + 'coh_beh_pre_door_df.pkl')\n",
    "pre_dig_df_filtered=pd.read_pickle(savepath + 'coh_beh_pre_dig_df.pkl')\n",
    "post_dig_df_filtered=pd.read_pickle(savepath + 'coh_beh_post_dig_df.pkl')\n",
    "\n",
    "pre_door_df_filtered=pre_door_df_filtered.drop('pre_door', axis=1)\n",
    "pre_dig_df_filtered=pre_dig_df_filtered.drop('pre_dig', axis=1)\n",
    "post_dig_df_filtered=post_dig_df_filtered.drop('post_dig', axis=1)\n",
    "\n",
    "pre_door_df_filtered.to_csv(savepath + 'coh_beh_pre_door_1000ms.csv')\n",
    "pre_dig_df_filtered.to_csv(savepath + 'coh_beh_pre_dig_1000ms.csv')\n",
    "post_dig_df_filtered.to_csv(savepath + 'coh_beh_post_dig_1000ms.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "\n",
    "pre_door_df_filtered=pd.read_pickle(savepath + 'coh_beh_pre_door_df.pkl')\n",
    "pre_dig_df_filtered=pd.read_pickle(savepath + 'coh_beh_pre_dig_df.pkl')\n",
    "post_dig_df_filtered=pd.read_pickle(savepath + 'coh_beh_post_dig_df.pkl')\n",
    "\n",
    "band = 'beta'\n",
    "event_of_interest = 'pre_dig'\n",
    "\n",
    "event_band = f'{band}_{event_of_interest}'\n",
    "\n",
    "\n",
    "task_experiments = pre_dig_df_filtered.groupby(['task','unique_id'])\n",
    "task_list= pre_dig_df_filtered['task'].unique()\n",
    "print(task_list)\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "for task in task_list:\n",
    "    task_data  = pre_dig_df_filtered[pre_dig_df_filtered['task']==task]\n",
    "    ax.scatter(task_data['time_to_dig'], task_data[event_band], label = task)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for task in task_list:\n",
    "    task_data = pre_dig_df_filtered[pre_dig_df_filtered['task'] == task]\n",
    "    unique_experiments = task_data['unique_id'].unique()\n",
    "\n",
    "    fig, axs = plt.subplots(4, 5, figsize=(20, 20))\n",
    "    fig.suptitle(f'{task} - Coherence vs Time to Dig', fontsize=20, y=1.02)\n",
    "    dk1_i=0\n",
    "    dk3_i=0\n",
    "    dk5_i=0\n",
    "    dk6_i=0\n",
    "    for experiment in unique_experiments:\n",
    "        print(task, experiment)    \n",
    "        task_exp_data =  task_data[task_data['unique_id'] == experiment]\n",
    "        time_to_dig = task_exp_data['time_to_dig']\n",
    "        coherence_value = task_exp_data[event_band]\n",
    "        \n",
    "        rat_id = task_exp_data['rat'].values[0]\n",
    "        if rat_id == 'dk1':\n",
    "            ax = axs[0, dk1_i]\n",
    "            dk1_i += 1\n",
    "        elif rat_id == 'dk3':\n",
    "            ax = axs[1, dk3_i]\n",
    "            dk3_i += 1\n",
    "        elif rat_id == 'dk5':\n",
    "            ax = axs[2, dk5_i]\n",
    "            dk5_i += 1\n",
    "        elif rat_id == 'dk6':\n",
    "            ax = axs[3, dk6_i]\n",
    "            dk6_i += 1\n",
    "        else:\n",
    "            continue  # Skip if rat_id is not one of the specified rats\n",
    "        ax.scatter(time_to_dig, coherence_value, label=experiment)\n",
    "        \n",
    "        ## Plotting Regression\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(time_to_dig, coherence_value)\n",
    "        ax.plot(time_to_dig, intercept + slope * time_to_dig, color='red', label=f'Fit: y={slope:.2f}x+{intercept:.2f}\\nR²={r_value**2:.2f}, p={p_value:.4f}')\n",
    "        ax.set_title(f'Rat: {rat_id}', fontsize=16)\n",
    "                \n",
    "        \n",
    "        ax.set_title(f'Rat: {rat_id}', fontsize=16)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing multivariate regression with mixed effects model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pre_dig_df_filtered\n",
    "\n",
    "context_data = data[data['task']=='BWcontext']\n",
    "nocontext_data = data[data['task']=='BWnocontext']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "import sys\n",
    "!c:\\Users\\sinha\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data\n",
    "context_data_clean = context_data.copy()\n",
    "\n",
    "# Log transform the outcome\n",
    "context_data_clean['log_time_to_dig'] = np.log(context_data_clean['time_to_dig'])\n",
    "\n",
    "# Rename problematic column names for theta+beta band\n",
    "context_data_clean = context_data_clean.rename(columns={\n",
    "    'theta+beta_pre_dig': 'theta_plus_beta_pre_dig',\n",
    "    'theta+beta_plv_pre_dig': 'theta_plus_beta_plv_pre_dig',\n",
    "    'theta+beta_pli_pre_dig': 'theta_plus_beta_pli_pre_dig',\n",
    "    'theta+beta_aon_power_pre_dig': 'theta_plus_beta_aon_power_pre_dig',\n",
    "    'theta+beta_vhp_power_pre_dig': 'theta_plus_beta_vhp_power_pre_dig'\n",
    "})\n",
    "\n",
    "# Dictionary to store results\n",
    "all_results = {}\n",
    "\n",
    "# Define bands with their proper names for column access\n",
    "bands_analysis = {\n",
    "    'beta': 'beta',\n",
    "    'gamma': 'gamma',\n",
    "    'theta': 'theta',\n",
    "    'theta+beta': 'theta_plus_beta',\n",
    "    'total': 'total'\n",
    "}\n",
    "\n",
    "# Run analysis for each band\n",
    "for band_name, band_prefix in bands_analysis.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ANALYZING {band_name.upper()} BAND - LOG TRANSFORMED\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Build formula\n",
    "    formula = f\"\"\"log_time_to_dig ~ trial + {band_prefix}_pre_dig + {band_prefix}_plv_pre_dig + \n",
    "                  {band_prefix}_pli_pre_dig + {band_prefix}_aon_power_pre_dig + \n",
    "                  {band_prefix}_vhp_power_pre_dig\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Fit model with clustered standard errors\n",
    "        model = smf.ols(formula, data=context_data_clean)\n",
    "        result = model.fit(cov_type='cluster', \n",
    "                          cov_kwds={'groups': context_data_clean['rat']})\n",
    "        \n",
    "        # Store result\n",
    "        all_results[band_name] = result\n",
    "        \n",
    "        # Print summary\n",
    "        print(result.summary())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing {band_name} band: {e}\")\n",
    "        all_results[band_name] = None\n",
    "\n",
    "# Create comprehensive summary table\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"COMPREHENSIVE SUMMARY: ALL FREQUENCY BANDS (LOG TRANSFORMED)\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for band_name, result in all_results.items():\n",
    "    if result is not None:\n",
    "        # Get the column prefix\n",
    "        band_prefix = bands_analysis[band_name]\n",
    "        vhp_var = f'{band_prefix}_vhp_power_pre_dig'\n",
    "        \n",
    "        # Extract key statistics\n",
    "        summary_data.append({\n",
    "            'Band': band_name,\n",
    "            'Trial_Beta': result.params['trial'],\n",
    "            'Trial_P': result.pvalues['trial'],\n",
    "            'VHP_Beta': result.params[vhp_var],\n",
    "            'VHP_SE': result.bse[vhp_var],\n",
    "            'VHP_P': result.pvalues[vhp_var],\n",
    "            'VHP_CI_Lower': result.conf_int().loc[vhp_var, 0],\n",
    "            'VHP_CI_Upper': result.conf_int().loc[vhp_var, 1],\n",
    "            'R_squared': result.rsquared,\n",
    "            'Adj_R_squared': result.rsquared_adj,\n",
    "            'AIC': result.aic,\n",
    "            'BIC': result.bic\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Add significance flags and effect interpretation\n",
    "summary_df['VHP_Sig'] = summary_df['VHP_P'].apply(\n",
    "    lambda x: '***' if x < 0.001 else '**' if x < 0.01 else '*' if x < 0.05 else 'ns'\n",
    ")\n",
    "summary_df['Trial_Sig'] = summary_df['Trial_P'].apply(\n",
    "    lambda x: '***' if x < 0.001 else '**' if x < 0.01 else '*' if x < 0.05 else 'ns'\n",
    ")\n",
    "\n",
    "# Calculate percent change in original scale (multiplicative effect)\n",
    "summary_df['VHP_Percent_Change'] = (np.exp(summary_df['VHP_Beta']) - 1) * 100\n",
    "\n",
    "# Display summary\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Print significant findings\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SIGNIFICANT VHP POWER EFFECTS (p < 0.05):\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "sig_findings = summary_df[summary_df['VHP_P'] < 0.05].sort_values('VHP_P')\n",
    "if len(sig_findings) > 0:\n",
    "    for _, row in sig_findings.iterrows():\n",
    "        print(f\"{row['Band'].upper()} Band:\")\n",
    "        print(f\"  Beta coefficient: {row['VHP_Beta']:.4f} {row['VHP_Sig']}\")\n",
    "        print(f\"  Standard error: {row['VHP_SE']:.4f}\")\n",
    "        print(f\"  P-value: {row['VHP_P']:.6f}\")\n",
    "        print(f\"  95% CI: [{row['VHP_CI_Lower']:.4f}, {row['VHP_CI_Upper']:.4f}]\")\n",
    "        print(f\"  Effect: {row['VHP_Percent_Change']:.1f}% change in digging time\")\n",
    "        print(f\"  (Positive = slower, Negative = faster)\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No significant VHP power effects found.\")\n",
    "\n",
    "# Print trial effects\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TRIAL EFFECTS (Learning):\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for _, row in summary_df.iterrows():\n",
    "    sig_marker = row['Trial_Sig']\n",
    "    if sig_marker != 'ns':\n",
    "        print(f\"{row['Band'].upper()}: β = {row['Trial_Beta']:.4f} {sig_marker}, p = {row['Trial_P']:.4f}\")\n",
    "\n",
    "# Model comparison\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"MODEL FIT COMPARISON:\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "best_model = summary_df.loc[summary_df['Adj_R_squared'].idxmax()]\n",
    "print(f\"Best fitting model: {best_model['Band'].upper()} band\")\n",
    "print(f\"Adjusted R²: {best_model['Adj_R_squared']:.4f}\")\n",
    "print(f\"AIC: {best_model['AIC']:.2f}\")\n",
    "\n",
    "print(\"\\nAll models ranked by Adjusted R²:\")\n",
    "print(summary_df[['Band', 'Adj_R_squared', 'AIC']].sort_values('Adj_R_squared', ascending=False).to_string(index=False))\n",
    "\n",
    "# Save results to CSV\n",
    "summary_df.to_csv('frequency_band_analysis_results.csv', index=False)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Results saved to 'frequency_band_analysis_results.csv'\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the short MNE Epochs to coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 0.7  # seconds\n",
    "fs = 2000  # Sampling frequency\n",
    "for time_window in [0.7]:\n",
    "    behavior_coherence_compiled_data_df_truncated=pd.read_pickle(savepath+f'behavior_coherence_single_epochs_mne_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "    print(behavior_coherence_compiled_data_df_truncated.head())\n",
    "    importlib.reload(coherence_functions)\n",
    "    bands_dict = {'beta': [12, 30], 'gamma': [30, 80],'theta':[4,12], 'total': [1, 100]}\n",
    "    for col in ['pre_door', 'post_door', 'pre_dig', 'post_dig', 'around_door','around_dig']:\n",
    "        print(col)\n",
    "        for band, (band_start, band_end) in bands_dict.items():\n",
    "            behavior_coherence_compiled_data_df_truncated[band + '_' + col] = behavior_coherence_compiled_data_df_truncated[col].apply(lambda x: coherence_functions.convert_epoch_to_coherence_behavior(x, band_start=band_start, band_end=band_end))\n",
    "    behavior_coherence_compiled_data_df_truncated.drop(columns=['pre_door', 'post_door', 'pre_dig', 'post_dig', 'around_door','around_dig'], inplace=True)\n",
    "    behavior_coherence_compiled_data_df_truncated.to_pickle(savepath+f'\\\\behavior_coherence_compiled_data_df_truncated_{int(time_window*fs)}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Coherence vs Time to Dig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window=  0.7\n",
    "fs=2000\n",
    "from scipy.stats import linregress\n",
    "behavior_coherence_compiled_data_df_truncated=pd.read_pickle(savepath+f'behavior_coherence_compiled_data_df_truncated_{int(time_window*fs)}.pkl')\n",
    "#behavior_coherence_compiled_data_df_truncated['beta_pre_dig'] = behavior_coherence_compiled_data_df_truncated['beta_pre_dig'] - behavior_coherence_compiled_data_df_truncated['beta_pre_door']\n",
    "#behavior_coherence_compiled_data_df_truncated['theta_pre_dig'] = behavior_coherence_compiled_data_df_truncated['theta_pre_dig'] - behavior_coherence_compiled_data_df_truncated['theta_pre_door']\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(3,2,figsize=(20, 20), sharey=True, constrained_layout=True)\n",
    "axs = axs.flatten()\n",
    "\n",
    "fig_hist, axs_hist = plt.subplots(3, 2, figsize=(20, 20), sharey=True, constrained_layout=True)\n",
    "axs_hist = axs_hist.flatten()\n",
    "\n",
    "grouped_df=behavior_coherence_compiled_data_df_truncated.groupby(['task'])\n",
    "band='theta'\n",
    "event='around_dig'\n",
    "task_dict = {'BWcontext': 'Context', 'BWnocontext': 'No Context'}\n",
    "\n",
    "events_dict ={'pre_door':'Pre Door', 'post_door':'Post Door', 'pre_dig':'Pre Dig', 'post_dig':'Post Dig', 'around_door':'Around Door', 'around_dig':'Around Dig'}\n",
    "for i, event in enumerate(events_dict.keys()):\n",
    "    ax = axs[i]\n",
    "    ax_hist = axs_hist[i]\n",
    "    print(np.where(behavior_coherence_compiled_data_df_truncated['{}_{}'.format(band,event)]==0))\n",
    "    for task, group in grouped_df:\n",
    "        print(task)\n",
    "        print('{}_{}'.format(band,event))\n",
    "        group = group[(np.abs(group['{}_{}'.format(band,event)]))>0]\n",
    "        print(group['{}_{}'.format(band,event)].mean())\n",
    "        group = group[(np.abs(group['time_to_dig'] - group['time_to_dig'].mean()) <= (3 * group['time_to_dig'].std()))] # Removing Outliers from Time\n",
    "        group = group[(np.abs(group['{}_{}'.format(band,event)] - group['{}_{}'.format(band,event)].mean()) <= (3 * group['{}_{}'.format(band,event)].std()))]  #Removing Outliers from Coherence\n",
    "        \n",
    "        #Plotting Regression\n",
    "        sns.regplot(y='time_to_dig', x='{}_{}'.format(band,event), data=group, ax=ax, label=task[0])\n",
    "        y= group['time_to_dig'].values\n",
    "        x= group['{}_{}'.format(band,event)].values\n",
    "        slope, intercept, r, p, se = linregress(x, y)\n",
    "        print(f'{task[0]}: Slope: {slope}, Intercept: {intercept}, R-squared: {r**2}, P-value: {p}')\n",
    "        \n",
    "        ## Plotting Histogram\n",
    "        sns.histplot(group['{}_{}'.format(band,event)], bins=30, kde=True, ax=ax_hist, label=task[0], color=colors[task[0]], stat='density', alpha=0.5)\n",
    "        ax_hist.axvline(group['{}_{}'.format(band,event)].mean(), color=colors[task[0]], linestyle='--', label=f'{task[0]} Mean')\n",
    "        ax_hist.axvline(group['{}_{}'.format(band,event)].median(), color=colors[task[0]], linestyle=':', label=f'{task[0]} Median')\n",
    "    \n",
    "    #Setting Histogram axis labels and title and legend\n",
    "    ax_hist.set_title(f'{events_dict[event]} - {band} Coherence Histogram', fontsize=16)\n",
    "    ax_hist.set_xlabel('Beta Coherence (Z-transformed)', fontsize=14)            \n",
    "    handles, labels = ax_hist.get_legend_handles_labels()\n",
    "    ax_hist.legend()\n",
    "    #ax_hist.legend(handles, [task_dict[l] for l in labels], title='Task', loc='upper right', fontsize=12)    \n",
    "    \n",
    "    \n",
    "    ax.set_title(f'{events_dict[event]}', fontsize=16)\n",
    "    ax.set_xlabel('Beta Coherence (Z-transformed)', fontsize=14)\n",
    "    ax.set_ylabel('Time to Dig (s)', fontsize=14)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, [task_dict[l] for l in labels], title='Task', loc='upper right', fontsize=12)\n",
    "    #ax.legend(title='Task')\n",
    "    \n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.tight_layout()\n",
    "fig.suptitle(f'{band} Coherence vs Time to Dig', fontsize=20, y=1.02)\n",
    "#fig.savefig(savepath+f'{band}_coherence_vs_time_to_dig_{int(time_window*fs)}.png', format='png', dpi=600, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherence vs Time to Dig per rat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window=  0.4\n",
    "fs=2000\n",
    "event_of_interest = 'post_dig'\n",
    "band_of_interest = 'gamma'\n",
    "\n",
    "event_band = f'{band_of_interest}_{event_of_interest}'\n",
    "\n",
    "from scipy.stats import linregress\n",
    "behavior_coherence_compiled_data_df_truncated=pd.read_pickle(savepath+f'behavior_coherence_compiled_data_df_truncated_{int(time_window*fs)}.pkl')\n",
    "task_experiments = behavior_coherence_compiled_data_df_truncated.groupby(['task','unique_id'])\n",
    "task_list= behavior_coherence_compiled_data_df_truncated['task'].unique()\n",
    "print(task_list)\n",
    "for task in task_list:\n",
    "    task_data = behavior_coherence_compiled_data_df_truncated[behavior_coherence_compiled_data_df_truncated['task'] == task]\n",
    "    unique_experiments = task_data['unique_id'].unique()\n",
    "\n",
    "    fig, axs = plt.subplots(4, 5, figsize=(20, 20))\n",
    "    fig.suptitle(f'{task} - Coherence vs Time to Dig', fontsize=20, y=1.02)\n",
    "    dk1_i=0\n",
    "    dk3_i=0\n",
    "    dk5_i=0\n",
    "    dk6_i=0\n",
    "    for experiment in unique_experiments:\n",
    "        print(task, experiment)    \n",
    "        task_exp_data =  task_data[task_data['unique_id'] == experiment]\n",
    "        time_to_dig = task_exp_data['time_to_dig']\n",
    "        coherence_value = task_exp_data[event_band]\n",
    "        \n",
    "        rat_id = task_exp_data['rat'].values[0]\n",
    "        if rat_id == 'dk1':\n",
    "            ax = axs[0, dk1_i]\n",
    "            dk1_i += 1\n",
    "        elif rat_id == 'dk3':\n",
    "            ax = axs[1, dk3_i]\n",
    "            dk3_i += 1\n",
    "        elif rat_id == 'dk5':\n",
    "            ax = axs[2, dk5_i]\n",
    "            dk5_i += 1\n",
    "        elif rat_id == 'dk6':\n",
    "            ax = axs[3, dk6_i]\n",
    "            dk6_i += 1\n",
    "        else:\n",
    "            continue  # Skip if rat_id is not one of the specified rats\n",
    "        ax.scatter(time_to_dig, coherence_value, label=experiment)\n",
    "        \n",
    "        ## Plotting Regression\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(time_to_dig, coherence_value)\n",
    "        ax.plot(time_to_dig, intercept + slope * time_to_dig, color='red', label=f'Fit: y={slope:.2f}x+{intercept:.2f}\\nR²={r_value**2:.2f}, p={p_value:.4f}')\n",
    "        ax.set_title(f'Rat: {rat_id}', fontsize=16)\n",
    "                \n",
    "        \n",
    "        ax.set_title(f'Rat: {rat_id}', fontsize=16)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Coherence through trials as experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_of_interest = 'pre_dig' \n",
    "band_of_interest = 'beta'\n",
    "time_window = 0.4  # seconds\n",
    "fs = 2000  # Sampling frequency\n",
    "\n",
    "behavior_coherence_compiled_data_df_truncated=pd.read_pickle(savepath+f'behavior_coherence_compiled_data_df_truncated_{int(time_window*fs)}.pkl')\n",
    "power_per_trial_df = pd.read_excel(savepath+'power_per_trial_mt.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "band_event = band_of_interest+'_'+event_of_interest\n",
    "\n",
    "vmin = behavior_coherence_compiled_data_df_truncated[band_event].min()\n",
    "vmax = behavior_coherence_compiled_data_df_truncated[band_event].max()\n",
    "\n",
    "coherence_bwcontext_data = behavior_coherence_compiled_data_df_truncated[behavior_coherence_compiled_data_df_truncated['task'] == 'BWcontext']\n",
    "coherence_bwnocontext_data = behavior_coherence_compiled_data_df_truncated[behavior_coherence_compiled_data_df_truncated['task'] == 'BWnocontext']\n",
    "\n",
    "power_bwcontext_data = power_per_trial_df[power_per_trial_df['task'] == 'BWcontext']\n",
    "power_bwnocontext_data = power_per_trial_df[power_per_trial_df['task'] == 'BWnocontext']\n",
    "\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "# axs = axs.flatten()\n",
    "#task_data_dict = {'BWcontext': coherence_bwcontext_data, 'BWnocontext': coherence_bwnocontext_data}\n",
    "# task_list =[ 'BWcontext', 'BWnocontext']\n",
    "# for axi,(task_name, task_data) in enumerate(task_data_dict.items()):\n",
    "#     print(f\"Task: {task_name}\")\n",
    "#     experiment_ids = task_data['unique_id'].unique()\n",
    "#     print(f\"Number of unique IDs for {task_name}: {len(experiment_ids)}\")\n",
    "#     task_data_dict = {}\n",
    "#     for experiment_idi in experiment_ids:\n",
    "#         experiment_data=task_data[task_data['unique_id'] == experiment_idi]\n",
    "#         rat_date = experiment_idi.split('_')[0] +'_'+experiment_idi.split('_')[1][-8:]\n",
    "#         task_data_dict[rat_date] = experiment_data[band_event].values\n",
    "#     task_data_dict['trials'] = np.arange(start=1,stop=21,step=1, dtype=int)  # Assuming 20 trials per unique ID\n",
    "#     task_data_df = pd.DataFrame.from_dict(task_data_dict, orient='index').T\n",
    "#     task_data_df = task_data_df.fillna(0)  # Fill NaN values with 0\n",
    "#     task_data_df = task_data_df.loc[:, (task_data_df != 0).any(axis=0)]\n",
    "#     ax = axs[axi]\n",
    "#     ax.set_title(f'{task_name} - {band_of_interest} {event_of_interest}', fontsize=16)\n",
    "#     ax.set_xlabel('Trials', fontsize=14)\n",
    "#     ax.set_ylabel(f'{band_of_interest} Coherence (Z-transformed)', fontsize=14)\n",
    "#     sns.heatmap(task_data_df.set_index('trials').T, cmap='Purples', ax=ax, cbar_kws={'label': f'{band_of_interest} Coherence (Z-transformed)'}, vmin=vmin, vmax=vmax)\n",
    "    # If you want to see the unique IDs themselves, uncomment the next line\n",
    "    # print(f\"Unique IDs: {unique_ids}\")\n",
    "aon_power_per_trial_df = power_per_trial_df[power_per_trial_df['channel'] == 'AON']\n",
    "vHp_power_per_trial_df = power_per_trial_df[power_per_trial_df['channel'] == 'vHp']\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(20, 10), sharex=True, constrained_layout=True)    \n",
    "task_list = ['BWcontext', 'BWnocontext']\n",
    "for axi, task_name in enumerate(task_list):\n",
    "    print(f\"Task: {task_name}\")\n",
    "    coherence_task_data = behavior_coherence_compiled_data_df_truncated[behavior_coherence_compiled_data_df_truncated['task'] == task_name]\n",
    "    aon_power_task_data = aon_power_per_trial_df[aon_power_per_trial_df['task'] == task_name]\n",
    "    vhp_power_task_data = vHp_power_per_trial_df[vHp_power_per_trial_df['task'] == task_name]    \n",
    "    coherence_task_dict = {}\n",
    "    aon_power_task_dict = {}\n",
    "    vhp_power_task_dict = {}\n",
    "    experiment_ids = coherence_task_data['unique_id'].unique()\n",
    "    for experiment_idi in experiment_ids:\n",
    "        \n",
    "        rat_idi = experiment_idi.split('_')[0]\n",
    "        date_idi = experiment_idi.split('_')[1][-8:]\n",
    "        rat_date = rat_idi + '_' + date_idi\n",
    "\n",
    "        coherence_experiment_data=coherence_task_data[coherence_task_data['unique_id'] == experiment_idi]\n",
    "        \n",
    "        ## Power Data\n",
    "        aon_power_experiment_data = aon_power_task_data[aon_power_task_data['unique_id'] == rat_idi+\"_\"+task_name+'_'+date_idi]\n",
    "        vhp_power_experiment_data = vhp_power_task_data[vhp_power_task_data['unique_id'] == rat_idi+\"_\"+task_name+'_'+date_idi]\n",
    "        \n",
    "        aon_power_per_trial_list=[]\n",
    "        vhp_power_per_trial_list=[]\n",
    "        for triali in range(0, 20):\n",
    "            if triali not in aon_power_experiment_data['trial'].values:\n",
    "                print(f\"Trial {triali} not found in AON power data for {experiment_idi}. Skipping...\")\n",
    "                aon_power_per_trial_list.append(0)\n",
    "            else:\n",
    "                aon_power_trial = aon_power_experiment_data[aon_power_experiment_data['trial'] == triali][f'{band_event}_mt'].values\n",
    "                aon_power_per_trial_list.append(aon_power_trial.mean())\n",
    "            \n",
    "            if triali not in vhp_power_experiment_data['trial'].values:\n",
    "                print(f\"Trial {triali} not found in vHp power data for {experiment_idi}. Skipping...\")\n",
    "                vhp_power_per_trial_list.append(0)\n",
    "            else:\n",
    "                vhp_power_trial = vhp_power_experiment_data[vhp_power_experiment_data['trial'] == triali][f'{band_event}_mt'].values\n",
    "                vhp_power_per_trial_list.append(vhp_power_trial.mean())\n",
    "        aon_power_per_trial_list = np.array(aon_power_per_trial_list)\n",
    "        vhp_power_per_trial_list = np.array(vhp_power_per_trial_list)\n",
    "        \n",
    "        coherence_task_dict[rat_date] = coherence_experiment_data[band_event].values\n",
    "        aon_power_task_dict[rat_date] = aon_power_per_trial_list\n",
    "        vhp_power_task_dict[rat_date] = vhp_power_per_trial_list\n",
    "        \n",
    "    def dict_to_df(task_data_dict):\n",
    "        # Exclude 'trials' key from min/max calculation\n",
    "        arrays = [v for k, v in task_data_dict.items() if k != 'trials']\n",
    "        vmin = np.min(np.concatenate(arrays))\n",
    "        vmax = np.max(np.concatenate(arrays))\n",
    "        task_data_dict['trials'] = np.arange(start=1,stop=21,step=1, dtype=int)\n",
    "        task_data_df = pd.DataFrame.from_dict(task_data_dict, orient='index').T\n",
    "        task_data_df = task_data_df.fillna(0)  # Fill NaN values with 0\n",
    "        #task_data_df = task_data_df.loc[:, (task_data_df != 0).any(axis=0)]\n",
    "        return task_data_df, vmin, vmax\n",
    "    task_data_dicts ={ 'Coherence' : coherence_task_dict,\n",
    "                        'AON Power': aon_power_task_dict,\n",
    "                        'vHp Power': vhp_power_task_dict}\n",
    "    for j, (task_data_name, task_data_dict) in enumerate(task_data_dicts.items()):\n",
    "        task_data_df,vmin,vmax = dict_to_df(task_data_dict)\n",
    "        print(f\"Task Data for {task_data_name}:\", vmin, vmax)\n",
    "        ax = axs[j, axi]\n",
    "        ax.set_title(f'{task_name} - {band_of_interest} {event_of_interest} - {task_data_name}', fontsize=16)\n",
    "        ax.set_xlabel('Trials', fontsize=14)\n",
    "        ax.set_ylabel(f'{task_data_name} (Z-transformed)', fontsize=14)\n",
    "        sns.heatmap(task_data_df.set_index('trials').T, cmap='Purples', ax=ax, cbar_kws={'label': f'{task_data_name} (Z-transformed)'}, vmin=vmin, vmax=vmax)\n",
    "        ax.set_xticklabels(task_data_df['trials'], rotation=45)\n",
    "fig.savefig(savepath+f'{band_of_interest}_coherence_power_vs_trials_{event_of_interest}.png', format='png', dpi=600, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing Mann Whitney U Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window=0.4\n",
    "fs=2000\n",
    "behavior_coherence_compiled_data_df_truncated=pd.read_pickle(savepath+f'behavior_coherence_compiled_data_df_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "event_of_interest = 'around_dig' \n",
    "band_of_interest = 'beta'\n",
    "\n",
    "band_event = band_of_interest+'_'+event_of_interest\n",
    "\n",
    "vmin = behavior_coherence_compiled_data_df_truncated[band_event].min()\n",
    "vmax = behavior_coherence_compiled_data_df_truncated[band_event].max()\n",
    "\n",
    "bwcontext_data = behavior_coherence_compiled_data_df_truncated[behavior_coherence_compiled_data_df_truncated['task'] == 'BWcontext']\n",
    "bwnocontext_data = behavior_coherence_compiled_data_df_truncated[behavior_coherence_compiled_data_df_truncated['task'] == 'BWnocontext']\n",
    "\n",
    "t,p = scipy.stats.mannwhitneyu(bwcontext_data[band_event], bwnocontext_data[band_event])\n",
    "print(f\"Mann - Whitney U test between BWcontext and BWNocontext for {band_event}: t={t}, p={p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherence Phase Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the short MNE Epochs to Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window=0.7\n",
    "fs=2000\n",
    "behavior_coherence_compiled_data_df_truncated=pd.read_pickle(savepath+f'behavior_coherence_single_epochs_mne_truncated_{int(time_window*fs)}.pkl')\n",
    "print(behavior_coherence_compiled_data_df_truncated.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(coherence_functions)\n",
    "bands_dict = {'beta': [12, 30]}#, 'gamma': [30, 80],'theta':[4,12], 'total': [1, 100]}\n",
    "for col in ['pre_door', 'post_door', 'pre_dig', 'post_dig', 'around_door','around_dig']:\n",
    "    print(col)\n",
    "    for band, (band_start, band_end) in bands_dict.items():\n",
    "        behavior_coherence_compiled_data_df_truncated[band + '_' + col] = behavior_coherence_compiled_data_df_truncated[col].apply(lambda x: coherence_functions.convert_epoch_to_phase_behavior(x, band_start=band_start, band_end=band_end))\n",
    "behavior_coherence_compiled_data_df_truncated.drop(columns=['pre_door', 'post_door', 'pre_dig', 'post_dig', 'around_door','around_dig'], inplace=True)\n",
    "behavior_coherence_compiled_data_df_truncated.to_pickle(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\behavior_phase_compiled_data_df_truncated_{int(time_window*fs)}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_coherence_compiled_data_df_truncated=pd.read_pickle(savepath+f'behavior_phase_compiled_data_df_truncated_{int(time_window*fs)}.pkl')\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(3,2,figsize=(20, 20), sharey=True, constrained_layout=True)\n",
    "axs = axs.flatten()\n",
    "\n",
    "grouped_df=behavior_coherence_compiled_data_df_truncated.groupby(['task'])\n",
    "band='beta'\n",
    "event='around_dig'\n",
    "events_dict ={'pre_door':'Pre Door', 'post_door':'Post Door', 'pre_dig':'Pre Dig', 'post_dig':'Post Dig', 'around_door':'Around Door', 'around_dig':'Around Dig'}\n",
    "for i, event in enumerate(events_dict.keys()):\n",
    "    ax = axs[i]\n",
    "    ax.set_title(f'{events_dict[event]}', fontsize=16)\n",
    "    print(np.where(behavior_coherence_compiled_data_df_truncated['{}_{}'.format(band,event)]==0))\n",
    "    for task, group in grouped_df:\n",
    "        print(task)\n",
    "        print('{}_{}'.format(band,event))\n",
    "        group = group[(np.abs(group['{}_{}'.format(band,event)]))>0]\n",
    "        print(group['{}_{}'.format(band,event)].mean())\n",
    "        group['{}_{}'.format(band,event)] = group['{}_{}'.format(band,event)].apply(lambda x: np.arctanh(x))\n",
    "        print(group['{}_{}'.format(band,event)].mean())\n",
    "        group = group[(np.abs(group['time_to_dig'] - group['time_to_dig'].mean()) <= (3 * group['time_to_dig'].std()))]\n",
    "        group = group[(np.abs(group['{}_{}'.format(band,event)] - group['{}_{}'.format(band,event)].mean()) <= (3 * group['{}_{}'.format(band,event)].std()))]\n",
    "        sns.regplot(x='time_to_dig', y='{}_{}'.format(band,event), data=group, ax=ax, label=task)\n",
    "    ax.set_ylabel('Beta PLI', fontsize=14)\n",
    "    ax.set_xlabel('Time to Dig (s)', fontsize=14)\n",
    "    ax.legend(title='Task')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.tight_layout()\n",
    "fig.suptitle(f'Beta pli vs Time to Dig', fontsize=20, y=1.02)\n",
    "#fig.savefig(savepath+'beta_coherence_vs_time_to_dig.png', format='png', dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Plot histogram of beta values for each event, comparing BW Context and BW No Context\n",
    "fig, axs = plt.subplots(3, 2, figsize=(20, 20), sharey=True, constrained_layout=True)\n",
    "axs = axs.flatten()\n",
    "events = list(events_dict.keys())\n",
    "for i, event in enumerate(events):\n",
    "    ax = axs[i]\n",
    "    for task in ['BWcontext', 'BWnocontext']:\n",
    "        data = behavior_coherence_compiled_data_df_truncated[\n",
    "            behavior_coherence_compiled_data_df_truncated['task'] == task\n",
    "        ]['{}_{}'.format(band, event)].dropna()\n",
    "        ax.hist(data, bins=30, alpha=0.6, label=task, density=True)\n",
    "    ax.set_title(f'{events_dict[event]}', fontsize=16)\n",
    "    ax.set_xlabel('Beta Value', fontsize=14)\n",
    "    ax.set_ylabel('Density', fontsize=14)\n",
    "    ax.legend(title='Task')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.tight_layout()\n",
    "fig.suptitle('Histogram of Beta Values per Event', fontsize=20, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Behavior Coherence Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window=  0.4\n",
    "fs=2000\n",
    "\n",
    "loaded_df=pd.read_pickle(savepath+f'\\\\behavior_coherence_compiled_data_df_truncated_{int(time_window*fs)}.pkl')\n",
    "print(loaded_df.head())\n",
    "coherence_band_event_df=loaded_df.loc[:,'beta_pre_door':]\n",
    "print(coherence_band_event_df.columns)\n",
    "group_melted=pd.melt(loaded_df, id_vars=['rat', 'task', 'date', 'trial','correct?', 'time_to_dig'], value_vars=coherence_band_event_df.columns, var_name='band_event', value_name='coherence')\n",
    "group_melted['band']=group_melted['band_event'].apply(lambda x: x.split('_')[0])\n",
    "group_melted['event']=group_melted['band_event'].apply(lambda x: x.split('_')[1:])\n",
    "group_melted['event']=group_melted['event'].apply(lambda x: x[0]+'_'+x[1])\n",
    "group_melted.drop(columns=['band_event'], inplace=True)\n",
    "group_melted['correct?']=group_melted['correct?'].apply(lambda x: 'Incorrect' if x=='0' else 'Correct')\n",
    "events_list=['pre_door','post_door','pre_dig','post_dig', 'around_door', 'around_dig']\n",
    "writer=pd.ExcelWriter(savepath+f'behavior_coherence_compiled_data_df_truncated_{int(time_window*fs)}.xlsx')\n",
    "for event in events_list:\n",
    "    event_df=group_melted[group_melted['event']==event]\n",
    "    event_df.to_excel(writer, sheet_name=event)\n",
    "writer.close()\n",
    "\n",
    "\n",
    "\n",
    "loaded_df=loaded_df.drop(columns=['around_door_truncated','around_dig_truncated'])\n",
    "writer=pd.ExcelWriter(savepath+f'beh_dig_coh_compiled_{int(time_window*fs/2)}ms.xlsx')\n",
    "loaded_df.to_excel(writer)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This plots the number of correct vs incorrect trials and the coherence. The idea is to check if the correct trials in general had a higher coherence than incorrect trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bwcontext_df=group_melted[group_melted['task']=='BWcontext']\n",
    "correct_counts = bwcontext_df[bwcontext_df['correct?'] == 'Correct'].shape[0]\n",
    "incorrect_counts = bwcontext_df[bwcontext_df['correct?'] == 'Incorrect'].shape[0]\n",
    "print(f\"Number of Corrects: {correct_counts}\", f\"Number of Incorrects: {incorrect_counts}\", 'bwcontext')\n",
    "bwnocontext_df=group_melted[group_melted['task']=='BWnocontext']\n",
    "correct_counts = bwnocontext_df[bwnocontext_df['correct?'] == 'Correct'].shape[0]\n",
    "incorrect_counts = bwnocontext_df[bwnocontext_df['correct?'] == 'Incorrect'].shape[0]\n",
    "print(f\"Number of Corrects: {correct_counts}\", f\"Number of Incorrects: {incorrect_counts}\", 'bwnocontext')\n",
    "%matplotlib inline\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20, 10), sharex=True, constrained_layout=True)\n",
    "axs = axs.flatten()\n",
    "for i, event in enumerate(events_list):\n",
    "    ax=axs[i]\n",
    "    sns.boxplot(x='band', y='coherence', hue='correct?',hue_order=['Correct', 'Incorrect'], data=bwcontext_df[group_melted['event']==event], showfliers=False, ax=ax)\n",
    "    sns.stripplot(x='band', y='coherence', hue='correct?',hue_order=['Correct', 'Incorrect'], data=bwcontext_df[group_melted['event']==event], dodge=True, edgecolor='black', linewidth=1, jitter=True, ax=ax, size=1, legend=False)\n",
    "    ax.set_title(event)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Coherence')\n",
    "    ax.legend(title='Correct?')\n",
    "fig.suptitle(f'BW Context Coherence and Correctness')\n",
    "#fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\behavior_coherence_BWcontext.png')\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20, 10), sharex=True, constrained_layout=True)\n",
    "axs = axs.flatten()\n",
    "for i, event in enumerate(events_list):\n",
    "    ax=axs[i]\n",
    "    sns.boxplot(x='band', y='coherence', hue='correct?',hue_order=['Correct', 'Incorrect'], data=bwnocontext_df[group_melted['event']==event], showfliers=False, ax=ax)\n",
    "    sns.stripplot(x='band', y='coherence', hue='correct?',hue_order=['Correct', 'Incorrect'], data=bwnocontext_df[group_melted['event']==event], dodge=True, edgecolor='black', linewidth=1, jitter=True, ax=ax, size=1, legend=False)\n",
    "    ax.set_title(event)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Coherence')\n",
    "    ax.legend(title='Correct?')\n",
    "fig.suptitle(f'BW No Context Coherence and Correctness')\n",
    "#fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\behavior_coherence_BWnocontext.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks=['BWcontext','BWnocontext']\n",
    "loaded_df['correct?']=loaded_df['correct?'].apply(lambda x: 'Correct' if x=='1' else 'Incorrect')\n",
    "correctness=['Correct','Incorrect']\n",
    "fig, axs=plt.subplots(2,2, figsize=(20,10))\n",
    "axs=axs.flatten()\n",
    "axi=0\n",
    "for task in tasks:\n",
    "    for dig_type in correctness:\n",
    "\n",
    "        ax=axs[axi]\n",
    "        task_df=loaded_df[(loaded_df['task']==task) & (loaded_df['correct?']==dig_type)]\n",
    "        events_list=['pre_door','post_door','pre_dig','post_dig']\n",
    "        bands=['total','beta','theta','gamma']\n",
    "        \n",
    "        correlation_matrix = np.zeros((len(events_list), len(bands)))\n",
    "\n",
    "        for i, event in enumerate(events_list):\n",
    "            for j, band in enumerate(bands):\n",
    "                column_name = f'{band}_{event}'\n",
    "                correlation_matrix[i, j] = task_df['time_to_dig'].corr(task_df[column_name])\n",
    "        \n",
    "        cax = ax.matshow(correlation_matrix, cmap='coolwarm')\n",
    "        fig.colorbar(cax, ax=ax)\n",
    "\n",
    "        ax.set_xticks(np.arange(len(bands)))\n",
    "        ax.set_yticks(np.arange(len(events_list)))\n",
    "        ax.set_xticklabels(bands)\n",
    "        ax.set_yticklabels(events_list)\n",
    "        ax.set_title(f'{task} {dig_type}')\n",
    "        axi=axi+1\n",
    "fig.tight_layout()\n",
    "# plt.show()\n",
    "# plt.xlabel('Bands')\n",
    "# plt.ylabel('Events')\n",
    "# plt.title('Correlation Matrix Heatmap')\n",
    "# plt.show()        \n",
    "# bwcontext_df=loaded_df[(loaded_df['task']=='BWcontext') & (loaded_df['correct?']=='0')]\n",
    "# events_list=['pre_door','post_door','pre_dig','post_dig']\n",
    "# bands=['total','beta','theta','gamma']\n",
    "\n",
    "# correlation_matrix = np.zeros((len(events_list), len(bands)))\n",
    "\n",
    "# for i, event in enumerate(events_list):\n",
    "#     for j, band in enumerate(bands):\n",
    "#         column_name = f'{band}_{event}'\n",
    "#         correlation_matrix[i, j] = bwcontext_df['time_to_dig'].corr(bwcontext_df[column_name])\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(10, 8))\n",
    "# cax = ax.matshow(correlation_matrix, cmap='coolwarm')\n",
    "# fig.colorbar(cax)\n",
    "\n",
    "# ax.set_xticks(np.arange(len(bands)))\n",
    "# ax.set_yticks(np.arange(len(events_list)))\n",
    "# ax.set_xticklabels(bands)\n",
    "# ax.set_yticklabels(events_list)\n",
    "\n",
    "# plt.xlabel('Bands')\n",
    "# plt.ylabel('Events')\n",
    "# plt.title('Correlation Matrix Heatmap')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import statsmodels\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "loaded_df=pd.read_pickle(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\behavior_coherence_compiled_data_df_truncated_{int(time_window*fs)}.pkl')\n",
    "print(loaded_df.head())\n",
    "loaded_df=loaded_df[loaded_df['rat']!='dk3']\n",
    "print(loaded_df.head())\n",
    "\n",
    "fig, ax=plt.subplots(1,1, figsize=(10,10))\n",
    "tasks=['BWcontext','BWnocontext']\n",
    "loaded_df['correct?']=loaded_df['correct?'].apply(lambda x: 'Correct' if x=='1' else 'Incorrect')\n",
    "correctness=['Correct','Incorrect']\n",
    "bwcontext_incorrect_df=loaded_df[(loaded_df['task']=='BWcontext')]\n",
    "bwnocontext_incorrect_df=loaded_df[(loaded_df['task']=='BWnocontext')]\n",
    "x=bwcontext_incorrect_df['time_to_dig']\n",
    "y=bwcontext_incorrect_df['beta_pre_dig']\n",
    "df = pd.DataFrame({'coherence': y, 'time': x})\n",
    "df.drop(df[df['coherence'] == 0].index, inplace=True)  # Remove rows with coherence = 0\n",
    "\n",
    "try:\n",
    "    correlation_coefficient, p_value = pearsonr(df['coherence'], df['time'])\n",
    "\n",
    "    print(f\"Pearson Correlation Coefficient (r): {correlation_coefficient:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "    correlation_coefficient_s, p_value_s = spearmanr(df['coherence'], df['time'])\n",
    "\n",
    "    print(f\"\\nSpearman Rank Correlation Coefficient (rs): {correlation_coefficient_s:.4f}\")\n",
    "    print(f\"P-value: {p_value_s:.4f}\")\n",
    "\n",
    "    # --- Interpretation ---\n",
    "    alpha = 0.05 # Set your significance level\n",
    "    print(f\"\\nSignificance Level (alpha): {alpha}\")\n",
    "\n",
    "    if p_value <= alpha:\n",
    "        print(\"Result: Reject the null hypothesis (H0).\")\n",
    "        print(\"Conclusion: There is a statistically significant linear relationship between the variables.\")\n",
    "    else:\n",
    "        print(\"Result: Fail to reject the null hypothesis (H0).\")\n",
    "        print(\"Conclusion: There is NOT enough evidence for a statistically significant linear relationship.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"Please ensure 'coherence_data' and 'time_data' are populated correctly with numerical lists or arrays of the same length.\")\n",
    "# --- Create a Pandas DataFrame ---\n",
    "\n",
    "print(f\"Original number of data points: {len(df)}\")\n",
    "\n",
    "# --- Calculate IQR Fences for BOTH variables ---\n",
    "Q1_coherence = df['coherence'].quantile(0.25)\n",
    "Q3_coherence = df['coherence'].quantile(0.75)\n",
    "IQR_coherence = Q3_coherence - Q1_coherence\n",
    "lower_fence_coherence = Q1_coherence - 1.5 * IQR_coherence\n",
    "upper_fence_coherence = Q3_coherence + 1.5 * IQR_coherence\n",
    "\n",
    "Q1_time = df['time'].quantile(0.25)\n",
    "Q3_time = df['time'].quantile(0.75)\n",
    "IQR_time = Q3_time - Q1_time\n",
    "lower_fence_time = Q1_time - 1.5 * IQR_time\n",
    "upper_fence_time = Q3_time + 1.5 * IQR_time\n",
    "\n",
    "\n",
    "print(\"\\n--- Outlier Fences ---\")\n",
    "print(f\"Coherence: Lower={lower_fence_coherence:.2f}, Upper={upper_fence_coherence:.2f}\")\n",
    "print(f\"Time:      Lower={lower_fence_time:.2f}, Upper={upper_fence_time:.2f}\")\n",
    "\n",
    "# --- Identify outliers (points outside fences for EITHER variable) ---\n",
    "outlier_condition = (\n",
    "    (df['coherence'] < lower_fence_coherence) | (df['coherence'] > upper_fence_coherence) |\n",
    "    (df['time'] < lower_fence_time) | (df['time'] > upper_fence_time)\n",
    ")\n",
    "\n",
    "outliers = df[outlier_condition]\n",
    "print(f\"\\nIdentified {len(outliers)} potential outliers:\")\n",
    "print(outliers)\n",
    "# --- Filter out the outliers ---\n",
    "df_filtered = df[~outlier_condition] # Use ~ to negate the condition, keeping non-outliers\n",
    "print(f\"\\nNumber of data points after removing outliers: {len(df_filtered)}\")\n",
    "\n",
    "\n",
    "# --- Recalculate Correlation on Filtered Data ---\n",
    "if len(df_filtered) > 1: # Need at least 2 points to calculate correlation\n",
    "    # Extract the filtered data columns\n",
    "    coherence_filtered = df_filtered['coherence']\n",
    "    time_filtered = df_filtered['time']\n",
    "\n",
    "    # Calculate original correlation (optional comparison)\n",
    "    try:\n",
    "      original_r, original_p = pearsonr(df['coherence'], df['time'])\n",
    "      print(f\"\\nOriginal Correlation (r): {original_r:.4f}, p-value: {original_p:.4f}\")\n",
    "    except Exception as e:\n",
    "      print(f\"\\nCould not calculate original correlation: {e}\")\n",
    "\n",
    "\n",
    "    # Calculate filtered correlation\n",
    "    try:\n",
    "      filtered_r, filtered_p = pearsonr(coherence_filtered, time_filtered)\n",
    "      print(f\"Filtered Correlation (r): {filtered_r:.4f}, p-value: {filtered_p:.4f}\")\n",
    "\n",
    "      # Interpretation (using alpha = 0.05)\n",
    "      alpha = 0.05\n",
    "      if filtered_p <= alpha:\n",
    "          print(\"Result (Filtered): Reject H0. Statistically significant linear relationship found.\")\n",
    "      else:\n",
    "          print(\"Result (Filtered): Fail to reject H0. No statistically significant linear relationship found.\")\n",
    "    except Exception as e:\n",
    "      print(f\"Could not calculate filtered correlation: {e}\")\n",
    "\n",
    "sns.regplot(y=df['coherence'], x=df['time'], label='Context', ax=ax)\n",
    "sns.regplot(x=bwnocontext_incorrect_df['time_to_dig'], y=bwnocontext_incorrect_df['beta_pre_dig'], label='No context', ax=ax)\n",
    "plt.xlabel('Time to Dig (s)', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.ylabel('Coherence', fontsize=20)\n",
    "plt.title('AON-VHP Beta Pre Dig Coherence vs Time to Dig', fontsize=20)\n",
    "plt.legend(fontsize=20) \n",
    "plt.tight_layout()\n",
    "#fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\beta_pre_dig_vs_time_to_dig.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# bwcontext_correct_df=loaded_df[(loaded_df['task']=='BWcontext') & (loaded_df['correct?']=='Correct')]\n",
    "# bwnocontext_correct_df=loaded_df[(loaded_df['task']=='BWnocontext') & (loaded_df['correct?']=='Correct')]\n",
    "# sns.regplot(x=bwcontext_correct_df['time_to_dig'], y=bwcontext_correct_df['gamma_pre_dig'], label='BWcontext',robust=True, order=2)\n",
    "# sns.regplot(x=bwnocontext_correct_df['time_to_dig'], y=bwnocontext_correct_df['gamma_pre_dig'], label='BWnocontext',robust=True, order=2)\n",
    "# plt.xlabel('Time to Dig')\n",
    "# plt.ylabel('Beta Pre Dig')\n",
    "# plt.title('Beta Pre Dig vs Time to Dig for Correct Trials')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# --- Plot SVM regression for BWcontext and BWnocontext separately ---\n",
    "\n",
    "fig2, ax2 = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "# Prepare data for each task\n",
    "for task_name, color in colors.items():\n",
    "  task_df = loaded_df[(loaded_df['task'] == task_name) & (~outlier_condition)]\n",
    "  if task_df.empty:\n",
    "    print(f\"Skipping {task_name}: no data after outlier removal.\")\n",
    "    continue\n",
    "  X_task = task_df['time_to_dig'].values.reshape(-1, 1)\n",
    "  y_task = task_df['beta_pre_dig'].values\n",
    "\n",
    "  # Fit SVM regression\n",
    "  svm_poly_task = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVR(kernel='poly', degree=3, C=1.0, epsilon=0.1)\n",
    "  )\n",
    "  svm_poly_task.fit(X_task, y_task)\n",
    "  x_range_task = np.linspace(X_task.min(), X_task.max(), 200).reshape(-1, 1)\n",
    "  y_pred_task = svm_poly_task.predict(x_range_task)\n",
    "\n",
    "  # Scatter and SVM fit\n",
    "  ax2.scatter(X_task, y_task, color=color, alpha=0.5, label=f'{task_name} data')\n",
    "  ax2.plot(x_range_task, y_pred_task, color=color, linestyle='-', linewidth=2, label=f'{task_name} SVM fit')\n",
    "\n",
    "ax2.set_xlabel('Time to Dig (s)', fontsize=20)\n",
    "ax2.set_ylabel('Coherence', fontsize=20)\n",
    "ax2.set_title('SVM Poly Fit: BWcontext vs BWnocontext', fontsize=20)\n",
    "ax2.legend(fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Get residuals for each group\n",
    "residuals = {}\n",
    "for task_name in colors.keys():\n",
    "  task_df = loaded_df[(loaded_df['task'] == task_name) & (~outlier_condition)]\n",
    "  if task_df.empty:\n",
    "    print(f\"Skipping {task_name} residuals: no data after outlier removal.\")\n",
    "    continue\n",
    "  X_task = task_df['time_to_dig'].values.reshape(-1, 1)\n",
    "  y_task = task_df['beta_pre_dig'].values\n",
    "  svm_poly_task = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVR(kernel='poly', degree=3, C=1.0, epsilon=0.1)\n",
    "  )\n",
    "  svm_poly_task.fit(X_task, y_task)\n",
    "  y_pred_task = svm_poly_task.predict(X_task)\n",
    "  residuals[task_name] = y_task - y_pred_task\n",
    "\n",
    "# t-test on residuals (only if both groups have data)\n",
    "if all(k in residuals and len(residuals[k]) > 0 for k in ['BWcontext', 'BWnocontext']):\n",
    "  t_stat, p_val = ttest_ind(residuals['BWcontext'], residuals['BWnocontext'], equal_var=False)\n",
    "  print(f\"Residuals t-test: t={t_stat:.4f}, p={p_val:.4g}\")\n",
    "  if p_val < 0.05:\n",
    "    print(\"Statistically significant difference in SVM fit residuals between BWcontext and BWnocontext.\")\n",
    "  else:\n",
    "    print(\"No statistically significant difference in SVM fit residuals between BWcontext and BWnocontext.\")\n",
    "else:\n",
    "  print(\"Not enough data for both groups to perform residuals t-test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "print(f\"Number of data points: {len(df)}\")\n",
    "coherence_data = df['coherence']\n",
    "time_data = df['time']\n",
    "# --- Perform the Spearman Rank Correlation Test ---\n",
    "try:\n",
    "    correlation_coefficient_s, p_value_s = spearmanr(coherence_data, time_data)\n",
    "\n",
    "    print(f\"\\nSpearman Rank Correlation Coefficient (rs): {correlation_coefficient_s:.4f}\")\n",
    "    print(f\"P-value: {p_value_s:.4f}\")\n",
    "\n",
    "    # --- Interpretation ---\n",
    "    alpha = 0.05 # Set your significance level\n",
    "    print(f\"\\nSignificance Level (alpha): {alpha}\")\n",
    "\n",
    "    if p_value_s <= alpha:\n",
    "        print(\"Result: Reject the null hypothesis (H0).\")\n",
    "        print(\"Conclusion: There is a statistically significant monotonic relationship between the variables.\")\n",
    "    else:\n",
    "        print(\"Result: Fail to reject the null hypothesis (H0).\")\n",
    "        print(\"Conclusion: There is NOT enough evidence for a statistically significant monotonic relationship.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"Please ensure 'coherence_data' and 'time_data' are populated correctly with numerical lists or arrays of the same length.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lfp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
