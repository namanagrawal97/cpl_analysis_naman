{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Importing packages and the functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import getpass\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import functions\n",
    "import lfp_pre_processing_functions\n",
    "import power_functions\n",
    "import coherence_functions\n",
    "import spectrogram_plotting_functions\n",
    "import plotting_styles\n",
    "import scipy.stats\n",
    "import mne_connectivity\n",
    "import mne\n",
    "importlib.reload(functions) #loads our custom made functions.py file\n",
    "importlib.reload(spectrogram_plotting_functions)\n",
    "importlib.reload(plotting_styles)\n",
    "\n",
    "linestyle = plotting_styles.linestyles\n",
    "colors = plotting_styles.colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Loading the data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code fetches the current 'user' by using getpass. Then it sets the basepath, loads the files and specifies the savepath. Note that the basepath, files and savepath need to be changed depending on where you have kept the files and where you want the results to be stored. In this case, I have set it up to be in a particular folder in my Dropbox account, which is stored locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello CPLab\n",
      "Base path: D:\\Dropbox\\CPLab\n",
      "Save path: D:\\Dropbox\\CPLab\\results\\\n",
      "['D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230529_dk1_nocontext.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230529_dk3_nocontext.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230529_dk5_nocontext.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230529_dk6_nocontext.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230531_dk1_nocontext_day2.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230531_dk3_nocontext_day2.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230531_dk5_nocontext_day2.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230531_dk6_nocontext_day2.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230609_dk1_BW_nocontext_day1.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230609_dk3_BW_nocontext_day1.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230610_dk1_BW_nocontext_day2.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230610_dk3_BW_nocontext_day2.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230615_dk5_BW_context_day1.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230615_dk6_BW_context_day1.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230616_dk5_BW_context_day2.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230616_dk6_BW_context_day2.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230623_dk1_BW_context_day1.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230626_dk1_BW_context_day1.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230626_dk5_BW_nocontext_day1.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230626_dk6_BW_nocontext_day1.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230627_dk1_BW_context_day2.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230627_dk5_BW_nocontext_day2.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230628_dk6_BW_nocontext_day2.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230718_dk1_nocontext_os2.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230718_dk5_nocontext_os2.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230718_dk6_nocontext_os2.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230719_dk1_nocontext_os2_day2_part1.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230719_dk1_nocontext_os2_day2_part2.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230719_dk5_nocontext_os2_day2.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230719_dk6_nocontext_os2_day2.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230807_dk3_BW_context_day1.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230808_dk3_BW_context_day2.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230808_dk5_BW_nocontext_day1_os2.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230808_dk6_BW_nocontext_day1_os2[discard].mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230810_dk5_BW_nocontext_day2_os2.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230817_dk1_BW_context_os2_day1_pt1.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230817_dk1_BW_context_os2_day1_pt2.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230818_dk1_BW_context_os2_day2.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230818_dk3_BW_context_os2_day1.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230821_dk3_BW_context_os2_day2.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230821_dk5_BW_context_day1_os2.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230822_dk1_BW_nocontext_os2_day1.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230823_dk1_BW_nocontext_os2_day2.mat', 'D:\\\\Dropbox\\\\CPLab\\\\all_data_mat_unfiltered\\\\20230823_dk5_BW_context_day2_os2.mat']\n"
     ]
    }
   ],
   "source": [
    "#Fetch the current user\n",
    "user= (getpass.getuser())\n",
    "print(\"Hello\", user)\n",
    "\n",
    "if user == 'CPLab':\n",
    "    base='D:\\\\Dropbox\\\\CPLab'\n",
    "else:\n",
    "    base='C:\\\\Users\\\\{}\\\\Dropbox\\\\CPLab'.format(user)\n",
    "#Set the basepath, savepath and load the data files\n",
    "files = glob.glob(base+'\\\\all_data_mat_unfiltered\\\\*.mat')\n",
    "savepath = base+'\\\\results\\\\'\n",
    "print(\"Base path:\", base)\n",
    "print(\"Save path:\", savepath)\n",
    "print(files)\n",
    "\n",
    "\n",
    "all_bands_dict = {'total':[1,100], 'beta':[12,30], 'gamma':[30,80], 'theta':[4,12]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Waveform and Power Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting LFP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keyboard_dict={'98':'b','119':'w','120':'nc','49':'1','48':'0'} #specifying the map of keyboard annotations to their meanings.\n",
    "all_bands={'total':[1,100],'beta':[12,30], 'gamma':[30,80], 'theta':[4,12]}\n",
    "importlib.reload(lfp_pre_processing_functions) #Reloading the lfp_pre_processing_functions module to ensure we have the latest version\n",
    "#files=[f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230615_dk6_BW_context_day1.mat', f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230626_dk6_BW_nocontext_day1.mat'] #This is just for testing purposes\n",
    "\n",
    "#Initializing a few empty things to store data\n",
    "events_codes_all = {}\n",
    "compiled_data_all_epochs = []\n",
    "compiled_data_list=[]\n",
    "compiled_shuffled_data_list = []\n",
    "baseline_lfp_all = []\n",
    "normalization_comparison_all = []\n",
    "for file in files: #Looping through data files\n",
    "    \n",
    "    ## Get the date, mouse_id and task from the file name\n",
    "    base_name = os.path.basename(file)\n",
    "    base_name, _ = os.path.splitext(base_name)\n",
    "    date, mouse_id, task=lfp_pre_processing_functions.exp_params(base_name) #Using a custom made function [see functions.py]\n",
    "    print(date, mouse_id, task)\n",
    "    if task == 'nocontextday2' or task == 'nocontextos2':\n",
    "        task = 'nocontext'\n",
    "    if task =='nocontext':\n",
    "        continue\n",
    "    f=h5py.File(file, 'r')  ## Open the data file\n",
    "    channels = list(f.keys()) ## Extract channels list from the data file\n",
    "    print(channels)\n",
    "    if not any(\"AON\" in channel or \"vHp\" in channel for channel in channels):\n",
    "        continue\n",
    "    events,reference_electrode=lfp_pre_processing_functions.get_keyboard_and_ref_channels(f,channels)\n",
    "\n",
    "    events_codes=np.array(events['codes'][0]) #saving the keyboard annotations of the events (door open, door close etc.)\n",
    "    events_times=np.array(events['times'][0]) #saving when the events happened\n",
    "    events_codes_all[base_name] = events_codes #saving the codes in a dictionary to be analyzed later for events other than the ones in our keyboard_dict map\n",
    "    \n",
    "    #Generating epochs from events (epochs are basically start of a trial and end of a trial)\n",
    "    epochs=lfp_pre_processing_functions.generate_epochs_with_first_event(events_codes, events_times)\n",
    "\n",
    "    # task Start time\n",
    "    first_event=events_times[0]\n",
    "    #finding global start and end time of all channels, since they start and end recordings at different times\n",
    "    global_start_time, global_end_time=lfp_pre_processing_functions.find_global_start_end_times(f,channels)\n",
    "    \n",
    "    ## Reference electrode finding and padding\n",
    "    reference_time = np.array(reference_electrode['times']).flatten()\n",
    "    reference_value = np.array(reference_electrode['values']).flatten()\n",
    "    padd_ref_data,padded_ref_time=lfp_pre_processing_functions.pad_raw_data_raw_time(reference_value,reference_time,global_start_time,global_end_time,sampling_rate=2000)\n",
    "\n",
    "\n",
    "    for channeli in channels:\n",
    "        if \"AON\" in channeli or  \"vHp\" in channeli :\n",
    "            \n",
    "            channel_id=channeli\n",
    "            # Extracting raw data and time\n",
    "            data_all=f[channeli]\n",
    "            raw_data=np.array(data_all['values']).flatten()\n",
    "            raw_time = np.array(data_all['times']).flatten()\n",
    "            sampling_rate = 2000\n",
    "            print(channel_id)\n",
    "            print(raw_data.shape, raw_time.shape, sampling_rate)\n",
    "            \n",
    "            padded_data,padded_time=lfp_pre_processing_functions.pad_raw_data_raw_time(raw_data,raw_time,global_start_time,global_end_time,sampling_rate)\n",
    "            subtracted_data = padded_data - padd_ref_data\n",
    "            raw_data=subtracted_data\n",
    "            notch_filtered_data = lfp_pre_processing_functions.iir_notch(raw_data, sampling_rate, 60)\n",
    "            \n",
    "            data_before, time, baseline_mean, baseline_std=lfp_pre_processing_functions.baseline_data_normalization(notch_filtered_data, raw_time, first_event, sampling_rate)\n",
    "            first_event_index=np.where(raw_time>first_event)[0][0]\n",
    "\n",
    "            baseline_row=[mouse_id,task,channel_id,np.array(data_before)]\n",
    "            baseline_lfp_all.append(baseline_row)\n",
    "            normalized_data=notch_filtered_data\n",
    "\n",
    "            #Saving non-normalized data and normalized data for plotting\n",
    "            normalization_row=[mouse_id,task,channel_id,[notch_filtered_data[first_event_index:first_event_index+30*sampling_rate]],np.mean(data_before),np.std(data_before),[normalized_data[first_event_index:first_event_index+30*sampling_rate]]]\n",
    "            normalization_comparison_all.append(normalization_row)\n",
    "\n",
    "\n",
    "            for i,epochi in enumerate(epochs):\n",
    "                \n",
    "                compiled_data = pd.DataFrame() # Initializing a dataframe to store the data of a single epoch\n",
    "                compiled_shuffled_data = pd.DataFrame() # Initializing a dataframe to store the shuffled data of a single epoch\n",
    "                door_timestamp = epochi[0][0]\n",
    "                trial_type = epochi[0][1]\n",
    "                dig_type = epochi[1, 1]\n",
    "                dig_timestamp = epochi[1, 0]\n",
    "                print(door_timestamp,trial_type,dig_timestamp,dig_type)\n",
    "                \n",
    "                \n",
    "                data_complete_trial=lfp_pre_processing_functions.extract_complete_trial_data(notch_filtered_data,time,door_timestamp,dig_timestamp,sampling_rate,0.7)\n",
    "                data_trial_before, data_trial_after=lfp_pre_processing_functions.extract_event_data(notch_filtered_data,time,door_timestamp,sampling_rate,0.7)\n",
    "                data_dig_before, data_dig_after=lfp_pre_processing_functions.extract_event_data(notch_filtered_data,time,dig_timestamp,sampling_rate,0.7)\n",
    "                data_door_around=np.append(data_trial_before, data_trial_after)\n",
    "                data_dig_around=np.append(data_dig_before, data_dig_after)\n",
    "                epoch_data = [data_complete_trial, data_trial_before, data_trial_after, data_dig_before, data_dig_after, data_door_around, data_dig_around]\n",
    "                epoch_data = [lfp_pre_processing_functions.zscore_event_data(x, baseline_std) for x in epoch_data]\n",
    "                shuffled_epoch_data = [np.random.permutation(x) for x in epoch_data]  # Shuffle the epoch data\n",
    "                compiled_data = dict(rat=mouse_id, date=date, task=task, channel=channel_id, trial=i, timestamps=[door_timestamp, dig_timestamp],\n",
    "                                     side=keyboard_dict.get(str(int(trial_type)), ''), correct=keyboard_dict.get(str(int(dig_type)), ''), time=time,\n",
    "                                     **dict(zip(['complete_trial', 'pre_door', 'post_door', 'pre_dig', 'post_dig', 'around_door', 'around_dig'], epoch_data)))\n",
    "                compiled_shuffled_data = dict(rat=mouse_id, date=date, task=task, channel=channel_id, trial=i, timestamps=[door_timestamp, dig_timestamp],\n",
    "                                     side=keyboard_dict.get(str(int(trial_type)), ''), correct=keyboard_dict.get(str(int(dig_type)), ''), time=time,\n",
    "                                     **dict(zip(['complete_trial', 'pre_door', 'post_door', 'pre_dig', 'post_dig', 'around_door', 'around_dig'], shuffled_epoch_data)))\n",
    "                compiled_data_list.append(compiled_data)\n",
    "                compiled_shuffled_data_list.append(compiled_shuffled_data)\n",
    "def combine_and_save_data(data_list, name):\n",
    "    compiled_data_all_epochs = []\n",
    "    compiled_data_all_epochs.extend(data_list)\n",
    "    compiled_data_all_epochs = pd.DataFrame(compiled_data_all_epochs)\n",
    "    compiled_data_all_epochs= compiled_data_all_epochs[compiled_data_all_epochs['task']!='nocontext']\n",
    "    compiled_data_all_epochs.to_pickle(savepath+'{}.pkl'.format(name))\n",
    "\n",
    "combine_and_save_data(compiled_data_list, 'compiled_data_all_epochs_truncated')\n",
    "combine_and_save_data(compiled_shuffled_data_list, 'compiled_shuffled_data_all_epochs_truncated')\n",
    "\n",
    "baseline_lfp_all = pd.DataFrame(baseline_lfp_all, columns=['rat', 'task', 'channel', 'data'])\n",
    "baseline_lfp_all.to_pickle(savepath+'baseline_lfp_all.pkl')\n",
    "normalization_comparison_all = pd.DataFrame(normalization_comparison_all, columns=['rat', 'task', 'channel', 'non_normalized_data', 'baseline_mean', 'baseline_std', 'normalized_data'])   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Waveform Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Rat 1-100Hz around door and digging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_data_all_epochs=pd.read_pickle(savepath+'compiled_data_all_epochs.pkl')\n",
    "waveform_data_all = compiled_data_all_epochs.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform_data_all['channel'] = waveform_data_all['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "event_dictionary = {'around_door':'Before and After door open', 'around_dig': 'Before and After Digging'}\n",
    "all_bands_dict = {'total':[1,100], 'beta':[12,30], 'gamma':[30,80], 'theta':[4,12]}\n",
    "rat_list=['dk5']\n",
    "for rat in rat_list:\n",
    "    writer=pd.ExcelWriter(os.path.join(savepath, f'{rat}_waveform_data.xlsx'), engine='xlsxwriter')\n",
    "    \n",
    "    waveform_data = waveform_data_all[waveform_data_all['rat'] == rat]\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(20, 10))\n",
    "    fig.suptitle(f'{rat} LFP (1-100Hz)', fontsize=20)\n",
    "    \n",
    "    subfigs = fig.subfigures(2, 1)\n",
    "    subfigs=subfigs.flatten()\n",
    "    for subfig in subfigs:\n",
    "        subfig.patch.set_edgecolor('black')\n",
    "        subfig.patch.set_linewidth(2)\n",
    "\n",
    "    areas=['AON','vHp']\n",
    "    for outerind, area in enumerate(areas):\n",
    "        subfig=subfigs[outerind]\n",
    "        axs = subfig.subplots(1, 2)\n",
    "        subfig.suptitle(f'{area}', fontsize=16)\n",
    "        waveform_data_area = waveform_data[waveform_data['channel'] == area]\n",
    "        waveform_data_area = waveform_data_area.reset_index(drop=True)\n",
    "\n",
    "        for innerind, col in enumerate(['around_door', 'around_dig']):\n",
    "            data = np.array(waveform_data_area[col].tolist())  # Ensure data is a numpy array\n",
    "            ax = axs[innerind]  # Correct indexing for axs\n",
    "            ax.set_title(f'{event_dictionary[col]}', fontsize=16)            \n",
    "            sheet_dict={}\n",
    "            for task in (['BWcontext', 'BWnocontext']):\n",
    "                task_data = data[waveform_data_area['task'] == task]\n",
    "                \n",
    "                if len(task_data) > 0:\n",
    "                    task_data = np.array([functions.freq_band(row, all_bands_dict['total'][0], all_bands_dict['total'][1], 2000) for row in task_data])\n",
    "                    data_mean = np.mean(task_data, axis=0)\n",
    "                    data_sem = scipy.stats.sem(task_data, axis=0)\n",
    "                    time_axis = np.linspace(-0.7, 0.7, len(data_mean))\n",
    "                    ax.plot(time_axis, data_mean, color=plotting_styles.colors[task])\n",
    "                    ax.fill_between(time_axis, data_mean - data_sem, data_mean + data_sem, alpha=0.2, color=plotting_styles.colors[task])\n",
    "                    sheet_dict[f'{task}_mean'] = data_mean\n",
    "                    sheet_dict[f'{task}_sem'] = data_sem\n",
    "            sheet_dict['time'] = time_axis\n",
    "            sheet_df=pd.DataFrame(sheet_dict)\n",
    "            sheet_df.to_excel(writer, sheet_name=f'{area}_{col}', index=False)\n",
    "            ax.vlines(0, ax.get_ylim()[0], ax.get_ylim()[1], color='k', linestyle='--')\n",
    "            ax.set_xlabel('Time (s)', fontsize=14)\n",
    "            ax.set_ylabel('Amplitude (uV)', fontsize=14)\n",
    "            ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "            #ax.tick_params(axis='both', which='minor', labelsize=10)\n",
    "    #writer.close()\n",
    "    #fig.savefig(os.path.join(savepath,f' LFP_total_waveform_{rat}'), dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All rats alls bands around door and digging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "compiled_data_all_epochs = pd.read_pickle(savepath+'compiled_data_all_epochs.pkl')\n",
    "rat_list=list(np.unique(compiled_data_all_epochs['rat']))\n",
    "window = [-2, 2]  # Set the window for the waveform\n",
    "\n",
    "#band = 'total'  # Insert the band of interest\n",
    "tasks = ['BWcontext', 'BWnocontext']\n",
    "areas=['AON','vHp']\n",
    "compiled_data_all_epochs['around_door'] = compiled_data_all_epochs['pre_door'].apply(lambda x: x.tolist()) + compiled_data_all_epochs['post_door'].apply(lambda x: x.tolist())\n",
    "compiled_data_all_epochs['around_dig'] = compiled_data_all_epochs['pre_dig'].apply(lambda x: x.tolist()) + compiled_data_all_epochs['post_dig'].apply(lambda x: x.tolist())\n",
    "print(np.array(compiled_data_all_epochs['around_door'][0]).shape, np.array(compiled_data_all_epochs['around_dig'][0]).shape)\n",
    "all_bands_dict = {'total':[1,100], 'beta':[12,30], 'gamma':[30,80], 'theta':[4,12]}\n",
    "\n",
    "for rati in rat_list:\n",
    "    rat_dict = {}\n",
    "    rat_data = compiled_data_all_epochs[compiled_data_all_epochs['rat'] == rati]\n",
    "    rat_data['channel']=rat_data['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "    rat_data = rat_data.reset_index(drop=True)\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(10, 10))    \n",
    "    subfigs = fig.subfigures(2, 1)\n",
    "    subfigs=subfigs.flatten()\n",
    "    subfigs[1].set_facecolor('0.85')\n",
    "    fig.suptitle(f'{rati}')\n",
    "    \n",
    "    for outerind, area in enumerate(areas):\n",
    "        subfig=subfigs[outerind]\n",
    "        axs = subfig.subplots(4, 2)\n",
    "        \n",
    "        rat_data_area = rat_data[rat_data['channel'] == area]\n",
    "        rat_data_area = rat_data_area.reset_index(drop=True)   \n",
    "    \n",
    "        for i, band in enumerate(all_bands_dict.keys()):\n",
    "            rat_data_band=rat_data_area.__deepcopy__()\n",
    "            for col in (['around_door', 'around_dig']):\n",
    "                rat_data_band[col] = rat_data_area[col].apply(lambda x: functions.freq_band(x, all_bands_dict[band][0], all_bands_dict[band][1], 2000))\n",
    "\n",
    "            rat_data_band_grouped = rat_data_band.groupby(['task', 'channel'])\n",
    "            for (task, channel), group in rat_data_band_grouped:\n",
    "                group=group.reset_index(drop=True)\n",
    "                print(group.shape)\n",
    "                #group['around_dig']=np.concatenate([group['pre_dig'], group['post_dig']], axis=1)\n",
    "                for j, col in enumerate(['around_door', 'around_dig']):\n",
    "                    data = np.array(group[col])\n",
    "                    data_mean = np.mean(data, axis=0)\n",
    "                    data_sem = scipy.stats.sem(data, axis=0)\n",
    "                    time_axis = np.linspace(-0.7, 0.7, len(data_mean))\n",
    "                    ax = axs[i, j]\n",
    "                    ax.set_title(f'{band} {channel} {col}')\n",
    "                    ax.plot(time_axis, data_mean, color=plotting_styles.colors[task])\n",
    "                    ax.fill_between(time_axis, data_mean - data_sem, data_mean + data_sem, alpha=0.2, color=plotting_styles.colors[task])\n",
    "                    ax.vlines(0, ax.get_ylim()[0], ax.get_ylim()[1], color='k', linestyle='--')\n",
    "    #fig.savefig(os.path.join(savepath,f' LFP_waveform{rati}'), dpi=300)\n",
    "    plt.show()\n",
    "    #plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaged across rats single band "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform_data = compiled_data_all_epochs.copy()\n",
    "waveform_data['channel'] = waveform_data['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "event_dictionary = {'around_door':'Before and After door open', 'around_dig': 'Before and After Digging'}\n",
    "fig = plt.figure(constrained_layout=True, figsize=(20, 10))\n",
    "fig.suptitle(f'raw LFP averaged across rats', fontsize=20)\n",
    "\n",
    "subfigs = fig.subfigures(2, 1)\n",
    "subfigs=subfigs.flatten()\n",
    "for subfig in subfigs:\n",
    "    subfig.patch.set_edgecolor('black')\n",
    "    subfig.patch.set_linewidth(0.5)\n",
    "areas=['AON','vHp']\n",
    "for outerind, area in enumerate(areas):\n",
    "    subfig=subfigs[outerind]\n",
    "    axs = subfig.subplots(1, 2)\n",
    "    subfig.suptitle(f'{area}', fontsize= 16) \n",
    "    waveform_data_area = waveform_data[waveform_data['channel'] == area]\n",
    "    waveform_data_area = waveform_data_area.reset_index(drop=True)\n",
    "\n",
    "    for innerind, col in enumerate(['around_door', 'around_dig']):\n",
    "        data = np.array(waveform_data_area[col].tolist())  # Ensure data is a numpy array\n",
    "        ax = axs[innerind]  # Correct indexing for axs\n",
    "        ax.set_title(f'{event_dictionary[col]}', fontsize=14)\n",
    "        for task in (['BWcontext', 'BWnocontext']):\n",
    "            task_data = data[waveform_data_area['task'] == task]\n",
    "            if len(task_data) > 0:\n",
    "            \n",
    "                data_mean = np.mean(task_data, axis=0)\n",
    "                data_sem = scipy.stats.sem(task_data, axis=0)\n",
    "                time_axis = np.linspace(-2, 2, len(data_mean))\n",
    "                \n",
    "                ax.plot(time_axis, data_mean, color=plotting_styles.colors[task])\n",
    "                ax.fill_between(time_axis, data_mean - data_sem, data_mean + data_sem, alpha=0.2, color=plotting_styles.colors[task])\n",
    "        ax.vlines(0, ax.get_ylim()[0], ax.get_ylim()[1], color='k', linestyle='--')\n",
    "        ax.set_xlabel('Time (s)', fontsize=14)\n",
    "        ax.set_ylabel('Amplitude (uV)', fontsize=14)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "        ax.tick_params(axis='both', which='minor', labelsize=10)\n",
    "#fig.savefig(os.path.join(savepath,f' LFP_raw_waveform_averaged'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaged across rats all bands (To be Deleted later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform_data = compiled_data_all_epochs.copy()\n",
    "waveform_data['channel'] = waveform_data['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "waveform_data = waveform_data.reset_index(drop=True)\n",
    "fig = plt.figure(constrained_layout=True, figsize=(20, 10))\n",
    "subfigs = fig.subfigures(2, 1)\n",
    "subfigs=subfigs.flatten()\n",
    "subfigs[1].set_facecolor('0.85')\n",
    "fig.suptitle(f'Waveform')\n",
    "\n",
    "for i, band in enumerate(all_bands_dict.keys()):\n",
    "    print(band)\n",
    "\n",
    "waveform_data_grouped = waveform_data.groupby(['task', 'channel'])\n",
    "for outerind, area in enumerate(areas):\n",
    "    subfig=subfigs[outerind]\n",
    "    axs = subfig.subplots(4, 2)\n",
    "    waveform_data_area = waveform_data[waveform_data['channel'] == area]\n",
    "    waveform_data_area = waveform_data_area.reset_index(drop=True)\n",
    "    \n",
    "    for i, band in enumerate(all_bands_dict.keys()):\n",
    "        for col in (['around_door', 'around_dig']):\n",
    "            waveform_data_area[col+'_'+band] = waveform_data_area[col].apply(lambda x: functions.freq_band(x, all_bands_dict[band][0], all_bands_dict[band][1], 2000))\n",
    "\n",
    "        data = waveform_data_area[[f'around_door_{band}', f'around_dig_{band}']]\n",
    "        data_mean = data.groupby(waveform_data_area['task']).mean() \n",
    "        data_sem = data.groupby(waveform_data_area['task']).sem()\n",
    "        time_axis = np.linspace(-2, 2, len(data_mean.columns))\n",
    "        for j, task in enumerate(tasks):\n",
    "            ax = axs[i, j]\n",
    "            ax.set_title(f'{band} {task}')\n",
    "            ax.plot(time_axis, data_mean.loc[task], color=plotting_styles.colors[task])\n",
    "            ax.fill_between(time_axis, data_mean.loc[task] - data_sem.loc[task], data_mean.loc[task] + data_sem.loc[task], alpha=0.2, color=plotting_styles.colors[task])\n",
    "            ax.vlines(0, ax.get_ylim()[0], ax.get_ylim()[1], color='k', linestyle='--')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Power Spectra Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Baseline Power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotting_styles\n",
    "from mne.time_frequency import psd_array_multitaper\n",
    "importlib.reload(plotting_styles)\n",
    "importlib.reload(power_functions)\n",
    "linestyle = plotting_styles.linestyles\n",
    "colors = plotting_styles.colors\n",
    "baseline_lfp_all = pd.read_pickle(savepath+'baseline_lfp_all.pkl')\n",
    "df= baseline_lfp_all.__deepcopy__()\n",
    "df['channel']=df['channel'].apply(lambda x:'AON' if 'AON' in x else 'vHp')\n",
    "channel_experiment_group=df.groupby(['task','channel'])\n",
    "channel_dict = {'BWcontext_AON': 'context AON', 'BWcontext_vHp': 'context vHp',\n",
    "                'BWnocontext_AON': 'No context AON', 'BWnocontext_vHp': 'No context vHp'}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "mean_dict={}\n",
    "for channel, data in channel_experiment_group:\n",
    "    print(channel)\n",
    "    data_array=np.vstack(data['data'].to_numpy())\n",
    "    print(data_array.shape)\n",
    "    data_array_welch = np.array([power_functions.apply_welch_transform(row) for row in data_array]) # Applying Welch's method to each row of data_array\n",
    "    print(data_array_welch.shape)\n",
    "    freqs = np.linspace(0,1000,num=int(data_array_welch.shape[1]))  # Assuming the frequency range is 0-1000 Hz\n",
    "    print(freqs.shape)\n",
    "\n",
    "    data_array_welch_mean = np.mean(data_array_welch, axis=0)\n",
    "    data_array_welch_std = np.std(data_array_welch, axis=0)\n",
    "    print(data_array_welch_mean.shape, data_array_welch_std.shape)\n",
    "    mean_dict[channel[0] + '_' + channel[1] + '_mean'] = data_array_welch_mean\n",
    "    mean_dict[channel[0] + '_' + channel[1] + '_std'] = data_array_welch_std\n",
    "    \n",
    "    ax.plot(freqs,data_array_welch_mean, linestyle=linestyle[channel[1]], color=colors[channel[0]], label=f'{channel[0]} {channel[1]}')\n",
    "    ax.fill_between(freqs,data_array_welch_mean-data_array_welch_std,data_array_welch_mean+data_array_welch_std, alpha=0.1, color=colors[channel[0]])\n",
    "    #ax.set_yscale('log')\n",
    "    ax.set_xlim(0,100)\n",
    "    ax.legend(loc='upper right', fontsize=20)\n",
    "    ax.set_title('Baseline Power Spectral Density', fontsize=20)\n",
    "    ax.set_xlabel('Frequency (Hz)', fontsize=20)\n",
    "    ax.set_ylabel('Power (V^2/Hz)', fontsize=20)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "#    ax.set_yscale('log')\n",
    "mean_dict['frequency']=freqs\n",
    "mean_df=pd.DataFrame(mean_dict)\n",
    "#mean_df.to_csv(savepath+'baseline_power_truncated.csv')\n",
    "#plt.savefig(savepath+'baseline_power_truncated.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "# Calculate multitaper PSD for each group and plot\n",
    "fig_mt, ax_mt = plt.subplots(figsize=(15, 10))\n",
    "mt_mean_dict = {}\n",
    "for channel, data in channel_experiment_group:\n",
    "    data_array = np.vstack(data['data'].to_numpy())\n",
    "    # Multitaper PSD: average across trials\n",
    "    psds = []\n",
    "    for row in data_array:\n",
    "        psd, freqs_mt = psd_array_multitaper(row, sfreq=2000,bandwidth=2, fmin=0, fmax=1000, adaptive=True, normalization='full', verbose=0)\n",
    "        psds.append(psd)\n",
    "    psds = np.array(psds)\n",
    "    psd_mean = psds.mean(axis=0)\n",
    "    psd_std = psds.std(axis=0)\n",
    "    mt_mean_dict[channel[0] + '_' + channel[1] + '_mean'] = psd_mean\n",
    "    mt_mean_dict[channel[0] + '_' + channel[1] + '_std'] = psd_std\n",
    "    ax_mt.plot(freqs_mt, psd_mean, linestyle=linestyle[channel[1]], color=colors[channel[0]], label=f'{channel[0]}_{channel[1]}')\n",
    "    ax_mt.fill_between(freqs_mt, psd_mean-psd_std, psd_mean+psd_std, alpha=0.1, color=colors[channel[0]])\n",
    "ax_mt.set_xlim(0, 100)\n",
    "handles, labels = ax_mt.get_legend_handles_labels()\n",
    "ax_mt.legend(handles, [channel_dict[l] for l in labels], loc='upper right', fontsize=20)\n",
    "ax_mt.set_title('Baseline Power Spectral Density (Multitaper)', fontsize=20)\n",
    "ax_mt.set_xlabel('Frequency (Hz)', fontsize=20)\n",
    "ax_mt.set_ylabel('Power (V^2/Hz)', fontsize=20)\n",
    "ax_mt.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "mt_mean_dict['frequency'] = freqs_mt\n",
    "mt_mean_df = pd.DataFrame(mt_mean_dict)\n",
    "mt_mean_df.to_csv(savepath+'baseline_psd_multitaper.csv')\n",
    "plt.savefig(savepath+'baseline_psd_multitaper.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close(fig_mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Prepare data for ANOVA: for each frequency, compare power between tasks\n",
    "# We'll do this for both AON and vHp channels\n",
    "\n",
    "results = {'frequency': [], 'AON_F': [], 'AON_p': [], 'vHp_F': [], 'vHp_p': []}\n",
    "tasks = ['BWcontext', 'BWnocontext']\n",
    "def make_welch_data_dfs(data, task, channel):\n",
    "    data_task_channel = data[(data['task'] == task) & (data['channel'] == channel)]\n",
    "    data_array = np.vstack(data_task_channel['data'].to_numpy())\n",
    "    data_array_welch = np.array([power_functions.apply_welch_transform(row) for row in data_array])  # Applying Welch's method to each row of data_array\n",
    "    return data_array_welch\n",
    "\n",
    "aon_context_vals= make_welch_data_dfs(df, 'BWcontext', 'AON')\n",
    "aon_nocontext_vals= make_welch_data_dfs(df, 'BWnocontext', 'AON')\n",
    "vHp_context_vals= make_welch_data_dfs(df, 'BWcontext', 'vHp')\n",
    "vHp_nocontext_vals= make_welch_data_dfs(df, 'BWnocontext', 'vHp')\n",
    "for freq in range(aon_context_vals.shape[1]):\n",
    "    aon_f, aon_p = f_oneway(aon_context_vals[:, freq], aon_nocontext_vals[:, freq])\n",
    "    vHp_f, vHp_p = f_oneway(vHp_context_vals[:, freq], vHp_nocontext_vals[:, freq])\n",
    "    \n",
    "    results['frequency'].append(freq)\n",
    "    results['AON_F'].append(aon_f)\n",
    "    results['AON_p'].append(aon_p)\n",
    "    results['vHp_F'].append(vHp_f)\n",
    "    results['vHp_p'].append(vHp_p)\n",
    "    # Convert results to DataFrame and filter for frequency 1 to 100\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df[(results_df['frequency'] >= 1) & (results_df['frequency'] <= 100)]\n",
    "results_df.to_csv(savepath + 'anova_psd_per_frequency_1_100.csv', index=False)\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# For each frequency, extract power for each task and channel\n",
    "for i, freq in enumerate(mean_df['frequency']):\n",
    "    # For ANOVA, we need the raw values, not the means, so we go back to the original data\n",
    "    # Get all AON/vHp power values at this frequency for each task\n",
    "    aon_mask = (data['channel'] == 'AON')\n",
    "    vhp_mask = (data['channel'] == 'vHp')\n",
    "    context_mask = (data['task'] == 'BWcontext')\n",
    "    nocontext_mask = (data['task'] == 'BWnocontext')\n",
    "\n",
    "    aon_context_vals = data_array_welch[aon_mask & context_mask, i]\n",
    "    aon_nocontext_vals = data_array_welch[aon_mask & nocontext_mask, i]\n",
    "    vhp_context_vals = data_array_welch[vhp_mask & context_mask, i]\n",
    "    vhp_nocontext_vals = data_array_welch[vhp_mask & nocontext_mask, i]\n",
    "\n",
    "    # ANOVA for AON\n",
    "    if len(aon_context_vals) > 1 and len(aon_nocontext_vals) > 1:\n",
    "        F_aon, p_aon = f_oneway(aon_context_vals, aon_nocontext_vals)\n",
    "    else:\n",
    "        F_aon, p_aon = float('nan'), float('nan')\n",
    "\n",
    "    # ANOVA for vHp\n",
    "    if len(vhp_context_vals) > 1 and len(vhp_nocontext_vals) > 1:\n",
    "        F_vhp, p_vhp = f_oneway(vhp_context_vals, vhp_nocontext_vals)\n",
    "    else:\n",
    "        F_vhp, p_vhp = float('nan'), float('nan')\n",
    "\n",
    "    results['frequency'].append(freq)\n",
    "    results['AON_F'].append(F_aon)\n",
    "    results['AON_p'].append(p_aon)\n",
    "    results['vHp_F'].append(F_vhp)\n",
    "    results['vHp_p'].append(p_vhp)\n",
    "\n",
    "anova_df = pd.DataFrame(results)\n",
    "anova_df.to_csv(savepath + 'anova_psd_per_frequency.csv', index=False)\n",
    "print(anova_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaselinePower Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.time_frequency import psd_array_multitaper\n",
    "\n",
    "baseline_lfp_all = pd.read_pickle(savepath+'baseline_lfp_all.pkl') #Loading the baseline LFP data\n",
    "df= baseline_lfp_all.__deepcopy__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "importlib.reload(plotting_styles)\n",
    "importlib.reload(power_functions)\n",
    "linestyles = plotting_styles.linestyles\n",
    "colors = plotting_styles.colors\n",
    "brain_areas = ['AON','vHp']\n",
    "\n",
    "\n",
    "number_per_segment = 2000\n",
    "tukey_window = scipy.signal.get_window(('tukey', 0.2), number_per_segment)    \n",
    "all_bands_dict = {'total':[1,100], 'beta':[12,30], 'gamma':[30,80], 'theta':[4,12]}\n",
    "task_dict = {'BWcontext': 'Context', 'BWnocontext': 'No Context'}\n",
    "df['data']=df['data'].apply(lambda x:power_functions.apply_welch_transform(x))\n",
    "\n",
    "for band_name, band_values in all_bands_dict.items():\n",
    "    df[band_name+'_power']=df['data'].apply(lambda x:power_functions.get_band_power(x, band_values[0], band_values[1]))\n",
    "\n",
    "writer=pd.ExcelWriter(savepath+'baseline_power_per_band_truncated.xlsx')\n",
    "fig, axs = plt.subplots(1,2, figsize=(15, 10), sharey=True)\n",
    "axs=axs.flatten()\n",
    "for i, area in enumerate(brain_areas):\n",
    "    data = df[df['channel'].str.contains(area)]\n",
    "    data_melted = data.melt(id_vars=['rat','task','channel'], value_vars=['total_power','beta_power','gamma_power','theta_power'], var_name='band', value_name='power')\n",
    "    sns.boxplot(data=data_melted, x='band', y='power', hue='task', hue_order=['BWcontext','BWnocontext'], palette=colors, showfliers=False, ax=axs[i])\n",
    "    sns.stripplot(data=data_melted, x='band', y='power', hue='task', hue_order=['BWcontext','BWnocontext'], palette=colors, dodge=True, alpha=0.5, jitter=0.2, ax=axs[i], linewidth=1, legend=False )\n",
    "#    axs[i].set_yscale('log')\n",
    "    axs[i].set_title(f'Baseline {area} Power per Band', fontsize=20)\n",
    "    axs[i].set_xlabel('Band', fontsize=20)\n",
    "    axs[i].set_ylabel('Power V^2', fontsize=20)\n",
    "    axs[i].legend(loc='upper right', fontsize=15)\n",
    "    axs[i].set_xticks(([0,1,2,3]),list(all_bands_dict.keys()))\n",
    "    axs[i].tick_params(axis='both', which='major', labelsize=15)\n",
    "    data_melted.to_excel(writer, sheet_name=area)\n",
    "writer.close()\n",
    "plt.savefig(savepath+'baseline_power_per_band_truncated.png', format='png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "baseline_lfp_all = pd.read_pickle(savepath+'baseline_lfp_all.pkl') #Loading the baseline LFP data\n",
    "df= baseline_lfp_all.__deepcopy__()\n",
    "\n",
    "# Calculate multitaper PSD and band power for each row\n",
    "df['data_mt'] = df['data'].apply(lambda x: psd_array_multitaper(x, sfreq=2000, fmin=0, fmax=1000, adaptive=True,bandwidth=2, normalization='full', verbose=0, max_iter=500)[0])\n",
    "\n",
    "for band_name, band_values in all_bands_dict.items():\n",
    "    # df[band_name + '_power_mt'] = df['data_mt'].apply(lambda x: psd_array_multitaper(x, sfreq=2000, fmin=band_values[0], fmax=band_values[1], adaptive=True,bandwidth=2, normalization='full', verbose=0, max_iter=500,faverage=True)[0])\n",
    "\n",
    "    df[band_name + '_power_mt'] = df['data_mt'].apply(lambda x: power_functions.get_band_power(x, band_values[0], band_values[1]))\n",
    "    # Log-normalize multitaper band power, handling log(0) by adding a small epsilon\n",
    "    epsilon = 1e-12\n",
    "    # df[band_name + '_power_mt'] = df[band_name + '_power_mt'].apply(lambda x: np.log10(x + epsilon))\n",
    "    # Plot multitaper band power\n",
    "writer_mt = pd.ExcelWriter(savepath + 'baseline_power_per_band_multitaper_truncated.xlsx')\n",
    "\n",
    "fig_mt, axs_mt = plt.subplots(1, 2, figsize=(15, 10), sharey=True)\n",
    "axs_mt = axs_mt.flatten()\n",
    "for i, area in enumerate(brain_areas):\n",
    "    data_mt = df[df['channel'].str.contains(area)]\n",
    "    data_melted_mt = data_mt.melt(\n",
    "        id_vars=['rat', 'task', 'channel'],\n",
    "        value_vars=['total_power_mt', 'beta_power_mt', 'gamma_power_mt', 'theta_power_mt'],\n",
    "        var_name='band', value_name='power'\n",
    "    )\n",
    "    # Plot log-normalized multitaper band power\n",
    "    sns.barplot(\n",
    "        data=data_melted_mt, x='band', y='power', hue='task',\n",
    "        hue_order=['BWcontext', 'BWnocontext'], palette=colors, ax=axs_mt[i]\n",
    "    )\n",
    "    sns.stripplot(\n",
    "        data=data_melted_mt, x='band', y='power', hue='task',\n",
    "        hue_order=['BWcontext', 'BWnocontext'], palette=colors, dodge=True, alpha=0.5, jitter=0.2,\n",
    "        ax=axs_mt[i], linewidth=1, legend=False\n",
    "    )\n",
    "    axs_mt[i].set_title(f'Baseline {area} Power per band', fontsize=20)\n",
    "    axs_mt[i].set_xlabel('Band', fontsize=20)\n",
    "    axs_mt[i].set_ylabel('Power (V^2)', fontsize=20)\n",
    "    handles, labels = axs_mt[i].get_legend_handles_labels()\n",
    "    axs_mt[i].legend(handles, [task_dict[l] for l in labels], loc='upper right', fontsize=15)\n",
    "    #axs_mt[i].legend(loc='upper right', fontsize=15)\n",
    "    axs_mt[i].set_xticks([0, 1, 2, 3], list(all_bands_dict.keys()))\n",
    "    axs_mt[i].tick_params(axis='both', which='major', labelsize=15)\n",
    "    data_melted_mt.to_excel(writer_mt, sheet_name=area)\n",
    "writer_mt.close()\n",
    "plt.savefig(savepath + 'baseline_power_per_band_multitaper.png', format='png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will plot the power spectra for each rat and the mean power spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will plot the power spectra for the complete trial # [NOT USED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(20, 10), sharex=True, sharey=True)\n",
    "axs=axs.flatten()\n",
    "fig.suptitle('Power Spectral Density')\n",
    "linestyles = {'AON': '-', 'vHp': '--'}\n",
    "\n",
    "for i,rati in enumerate(rat_list):\n",
    "    rat_data=power_df[power_df['rat']==rati]\n",
    "    rat_data=rat_data.reset_index(drop=True)\n",
    "    rat_data_grouped=rat_data.groupby(['task','channel'])\n",
    "    for (task, channel),group in rat_data_grouped:\n",
    "        print(task, channel)\n",
    "        group=group.reset_index(drop=True)\n",
    "        col='complete_trial'\n",
    "        data = np.array(group[col])\n",
    "        data_mean = np.mean(data, axis=0)\n",
    "        data_sem = scipy.stats.sem(data, axis=0)\n",
    "        freq = np.linspace(0, 1000, len(data_mean))        \n",
    "        ax = axs[i]\n",
    "        ax.set_title(f'{rati}')\n",
    "        ax.plot(freq, data_mean, color=colors[task], linestyle=linestyles[channel])\n",
    "        ax.fill_between(freq, data_mean - data_sem, data_mean + data_sem, alpha=0.2, color=colors[task])\n",
    "        ax.set_xlim(0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 Average Power Spectra across all rats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 Event Power Spectra individual Rats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Events PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.time_frequency import psd_array_multitaper\n",
    "\n",
    "importlib.reload(power_functions)\n",
    "compiled_data_all_epochs = pd.read_pickle(savepath+'compiled_data_all_epochs_truncated.pkl')\n",
    "power_df=compiled_data_all_epochs.__deepcopy__()\n",
    "# number_per_segment = 700\n",
    "# tukey_window = scipy.signal.get_window(('tukey', 0.1), number_per_segment)\n",
    "columns= ['complete_trial','pre_door', 'post_door', 'pre_dig', 'post_dig', 'around_door', 'around_dig']\n",
    "\n",
    "power_df.loc[:,columns]=power_df.loc[:,columns].applymap(lambda x:power_functions.apply_welch_transform(x))\n",
    "events_dict={'pre_door':' Pre Door','post_door':'Post Door','pre_dig':'Pre Dig','post_dig':'Post Dig'}\n",
    "fig, axs=plt.subplots(1,4, figsize=(40,10), sharex=True, sharey=True)\n",
    "axs=axs.flatten()\n",
    "for ax in axs:\n",
    "    ax.tick_params(axis='y', which='both', labelleft=True)\n",
    "writer=pd.ExcelWriter(savepath+'events_power_spectral_density_truncated.xlsx')\n",
    "for i, event in enumerate(events_dict.keys()):\n",
    "\n",
    "    data = power_df[['rat','task','channel',event]]\n",
    "    data['channel']=data['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "    data_groups=data.groupby(['task','channel'])\n",
    "    mean_data_dict={}\n",
    "    for (task, channel), group in data_groups:\n",
    "        group=group.reset_index(drop=True)\n",
    "        data = np.array(group[event])\n",
    "        data_mean = np.mean(data, axis=0)\n",
    "        data_sem = scipy.stats.sem(data, axis=0)\n",
    "        mean_data_dict[task+'_'+channel+'_mean']=data_mean\n",
    "        mean_data_dict[task+'_'+channel+'_sem']=data_sem\n",
    "        freq = np.linspace(0, 1000, len(data_mean))\n",
    "        ax = axs[i]\n",
    "        ax.set_title(f'{events_dict[event]}', fontsize=20)\n",
    "        ax.plot(freq, data_mean, color=plotting_styles.colors[task], linestyle=plotting_styles.linestyles[channel])\n",
    "        ax.fill_between(freq, data_mean - data_sem, data_mean + data_sem, alpha=0.2, color=plotting_styles.colors[task])\n",
    "        ax.set_xlim(0, 100)\n",
    "        #ax.set_yscale('log')\n",
    "        ax.set_xlabel('Frequency (Hz)', fontsize=20)\n",
    "        if i == 0:\n",
    "            ax.set_ylabel('Power (V^2/Hz)', fontsize=25)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "    mean_df=pd.DataFrame(mean_data_dict)\n",
    "    mean_df.to_excel(writer, sheet_name=event)\n",
    "writer.close()\n",
    "fig.savefig(savepath+'events_power_spectral_density_truncated.png', format='png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "importlib.reload(power_functions)\n",
    "compiled_data_all_epochs = pd.read_pickle(savepath+'compiled_data_all_epochs_truncated.pkl')\n",
    "power_df=compiled_data_all_epochs.__deepcopy__()\n",
    "# number_per_segment = 700\n",
    "# tukey_window = scipy.signal.get_window(('tukey', 0.1), number_per_segment)\n",
    "columns= ['pre_door', 'post_door', 'pre_dig', 'post_dig', 'around_door', 'around_dig']\n",
    "# Apply multitaper PSD to each event column\n",
    "def multitaper_transform(x):\n",
    "    # x is a 1D array or list of values\n",
    "    psd, _ = psd_array_multitaper(x, sfreq=2000, fmin=0, fmax=1000, adaptive=True, bandwidth=6, normalization='full', verbose=0, max_iter=1000)\n",
    "    return psd\n",
    "\n",
    "power_df.loc[:, columns] = power_df.loc[:, columns].applymap(multitaper_transform)\n",
    "\n",
    "events_dict = {'pre_door': ' Pre Door', 'post_door': 'Post Door', 'pre_dig': 'Pre Dig', 'post_dig': 'Post Dig'}\n",
    "fig, axs = plt.subplots(1, 4, figsize=(40, 10), sharex=True, sharey=True)\n",
    "axs = axs.flatten()\n",
    "for ax in axs:\n",
    "    ax.tick_params(axis='y', which='both', labelleft=True)\n",
    "writer = pd.ExcelWriter(savepath + 'events_power_spectral_density_multitaper_truncated.xlsx')\n",
    "for i, event in enumerate(events_dict.keys()):\n",
    "    data = power_df[['rat', 'task', 'channel', event]]\n",
    "    data['channel'] = data['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "    data_groups = data.groupby(['task', 'channel'])\n",
    "    mean_data_dict = {}\n",
    "    for (task, channel), group in data_groups:\n",
    "        group = group.reset_index(drop=True)\n",
    "        data_arr = np.array(group[event])\n",
    "        data_mean = np.mean(data_arr, axis=0)\n",
    "        data_sem = scipy.stats.sem(data_arr, axis=0)\n",
    "        mean_data_dict[task + '_' + channel + '_mean'] = data_mean\n",
    "        mean_data_dict[task + '_' + channel + '_sem'] = data_sem\n",
    "        freq = np.linspace(0, 1000, len(data_mean))\n",
    "        ax = axs[i]\n",
    "        ax.set_title(f'{events_dict[event]}', fontsize=20)\n",
    "        ax.plot(freq, data_mean, color=plotting_styles.colors[task], linestyle=plotting_styles.linestyles[channel], label=f'{task_dict[task]} {channel}')\n",
    "        ax.fill_between(freq, data_mean - data_sem, data_mean + data_sem, alpha=0.2, color=plotting_styles.colors[task])\n",
    "        ax.set_xlim(0, 100)\n",
    "        # ax.set_yscale('log')\n",
    "        ax.set_xlabel('Frequency (Hz)', fontsize=20)\n",
    "        if i == 0:\n",
    "            ax.set_ylabel('Power (V^2/Hz)', fontsize=25)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "    mean_df = pd.DataFrame(mean_data_dict)\n",
    "    mean_df.to_excel(writer, sheet_name=event)\n",
    "writer.close()\n",
    "fig.savefig(savepath + 'events_power_spectral_density_multitaper_truncated.png', format='png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Power for 1s around digging only [NOT USED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_list=['around_dig','around_door']\n",
    "fig, axs=plt.subplots(2,2, figsize=(20,10), sharex=True, sharey=True)\n",
    "axs=axs.flatten()\n",
    "writer=pd.ExcelWriter(savepath+'events_power_spectral_density.xlsx')\n",
    "for i, event in enumerate(events_list):\n",
    "\n",
    "    data = power_df[['rat','task','channel',event]]\n",
    "    data['channel']=data['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "    data_groups=data.groupby(['task','channel'])\n",
    "    mean_data_dict={}\n",
    "    for (task, channel), group in data_groups:\n",
    "        group=group.reset_index(drop=True)\n",
    "        data = np.array(group[event])\n",
    "        data_mean = np.mean(data, axis=0)\n",
    "        data_sem = scipy.stats.sem(data, axis=0)\n",
    "        mean_data_dict[task+'_'+channel+'_mean']=data_mean\n",
    "        mean_data_dict[task+'_'+channel+'_sem']=data_sem\n",
    "        freq = np.linspace(0, 1000, len(data_mean))\n",
    "        ax = axs[i]\n",
    "        ax.set_title(f'{event}')\n",
    "        ax.plot(freq, data_mean, color=plotting_styles.colors[task], linestyle=plotting_styles.linestyles[channel])\n",
    "        ax.fill_between(freq, data_mean - data_sem, data_mean + data_sem, alpha=0.2, color=plotting_styles.colors[task])\n",
    "        ax.set_xlim(0, 100)\n",
    "        ax.set_xlabel('Frequency (Hz)')\n",
    "        ax.set_ylabel('Power uV^2/Hz')\n",
    "    mean_df=pd.DataFrame(mean_data_dict)\n",
    "    #mean_df.to_excel(writer, sheet_name=event)\n",
    "#writer.close()\n",
    "#plt.savefig(savepath+'events_power_spectral_density.png')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Power Boxplots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per Trial [NOT USED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "compiled_data_all_epochs=pd.read_pickle(savepath+'compiled_data_all_epochs_truncated.pkl')\n",
    "boxplot_df=compiled_data_all_epochs.__deepcopy__()\n",
    "\n",
    "boxplot_df.loc[:,['pre_door','post_door','pre_dig','post_dig']]=boxplot_df.loc[:,['pre_door','post_door','pre_dig','post_dig']].applymap(lambda x: power_functions.apply_welch_transform(x))\n",
    "new_boxplot_df=boxplot_df[['rat', 'task', 'date', 'channel','trial']].copy()\n",
    "bands_dict = {'beta': [12, 30], 'gamma': [30, 80], 'theta': [4, 12], 'total': [1, 100]}\n",
    "for col in ['pre_door','post_door','pre_dig','post_dig']:\n",
    "    for band, (band_start, band_end) in bands_dict.items():\n",
    "        new_boxplot_df[band + '_' + col] = boxplot_df[col].apply(lambda x: power_functions.get_band_power(x, band_start, band_end))\n",
    "\n",
    "new_boxplot_df['unique_id'] = new_boxplot_df['rat'] + '_' + new_boxplot_df['task']+ '_' + new_boxplot_df['date']\n",
    "\n",
    "all_boxplot_df=new_boxplot_df.__deepcopy__()\n",
    "\n",
    "all_boxplot_df['channel'] = all_boxplot_df['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "aon_channels=all_boxplot_df[all_boxplot_df['channel']=='AON']\n",
    "vhp_channels=all_boxplot_df[all_boxplot_df['channel']=='vHp']\n",
    "\n",
    "area_list= ['AON', 'vHp']\n",
    "for area in area_list:\n",
    "    area_channels = all_boxplot_df[all_boxplot_df['channel'] == area]\n",
    "    writer=pd.ExcelWriter(savepath+'events_power_per_band_{}_truncated.xlsx'.format(area), engine='xlsxwriter')\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(40, 10), sharex=True, sharey=True)\n",
    "\n",
    "    axs=axs.flatten()\n",
    "    for ax in axs:\n",
    "        ax.tick_params(axis='y', which='both', labelleft=True)\n",
    "    events_dict={'pre_door':'Pre Door', 'post_door':'Post Door', 'pre_dig':'Pre Dig', 'post_dig':'Post Dig'}\n",
    "    for i, event in enumerate(events_dict.keys()):\n",
    "        area_df=area_channels.__deepcopy__()\n",
    "        event_cols = [col for col in area_df.columns if event in col]\n",
    "        print(event_cols)\n",
    "        event_df = area_df[['rat', 'task', 'channel','trial', *event_cols]]\n",
    "        event_df_melted = pd.melt(event_df, id_vars=['rat', 'task', 'channel','trial'], var_name='band', value_name='power')\n",
    "        event_df_melted['band'] = event_df_melted['band'].apply(lambda x: x.split('_')[0])\n",
    "        ax=axs[i]\n",
    "        #sns.boxplot(data=event_df_melted, x='band', y='power', hue='task', hue_order=['BWcontext','BWnocontext'], palette=colors, showfliers=False, ax=axs[i])\n",
    "        #sns.stripplot(data=event_df_melted, x='band', y='power', hue='task', hue_order=['BWcontext','BWnocontext'], palette=colors, dodge=True, alpha=0.5, jitter=0.2, ax=axs[i], linewidth=1, legend=False )\n",
    "        sns.violinplot(x='band',y='power',hue='task',hue_order=['BWcontext','BWnocontext'],data=event_df_melted, ax=ax)\n",
    "        #sns.stripplot(x='band',y='power',hue='task',hue_order=['BWcontext','BWnocontext'],data=event_df,dodge=True,edgecolor='black',linewidth=1,jitter=True, legend=False, ax=ax, color=\".3\", size=2)\n",
    "\n",
    "        ax.set_title(f'{events_dict[event]} {area}', fontsize=20)\n",
    "        ax.set_xlabel('Band', fontsize=20)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        #axs[i].set_yscale('log')\n",
    "        #axs[i].set_ylim(1e-3, 1e3)\n",
    "        if i == 0:\n",
    "            axs[i].set_ylabel('Power (V^2)', fontsize=25)\n",
    "        event_df_melted.to_excel(writer, sheet_name=event)\n",
    "    writer.close()\n",
    "    plt.savefig(savepath+'events_power_per_band_{}_truncated.png'.format(area), format='png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per Trial Multitaper [NOT USED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from mne.time_frequency import psd_array_multitaper\n",
    "\n",
    "compiled_data_all_epochs = pd.read_pickle(savepath + 'compiled_data_all_epochs_truncated.pkl')\n",
    "boxplot_df = compiled_data_all_epochs.__deepcopy__()\n",
    "\n",
    "event_cols = ['pre_door', 'post_door', 'pre_dig', 'post_dig']\n",
    "bands_dict = {'beta': [12, 30], 'gamma': [30, 80], 'theta': [4, 12], 'total': [1, 100]}\n",
    "sfreq = 2000\n",
    "epsilon = 1e-12  # for log-normalization\n",
    "\n",
    "# Apply multitaper PSD to each event column\n",
    "for col in event_cols:\n",
    "    boxplot_df[col] = boxplot_df[col].apply(lambda x: psd_array_multitaper(x, sfreq=sfreq, fmin=0, fmax=1000, adaptive=True, normalization='full', verbose=0, max_iter=500, bandwidth=4)[0])\n",
    "\n",
    "# Calculate band power from multitaper PSD and log-normalize\n",
    "new_boxplot_df = boxplot_df[['rat', 'task', 'date', 'channel', 'trial']].copy()\n",
    "for col in event_cols:\n",
    "    for band, (band_start, band_end) in bands_dict.items():\n",
    "        new_boxplot_df[f'{band}_{col}_mt'] = boxplot_df[col].apply(lambda x: np.log10(power_functions.get_band_power(x, band_start, band_end) + epsilon))\n",
    "\n",
    "new_boxplot_df['unique_id'] = new_boxplot_df['rat'] + '_' + new_boxplot_df['task'] + '_' + new_boxplot_df['date']\n",
    "all_boxplot_df = new_boxplot_df.__deepcopy__()\n",
    "all_boxplot_df['channel'] = all_boxplot_df['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "all_boxplot_df.to_excel(savepath + 'power_per_trial_mt.xlsx', index=False)\n",
    "area_list = ['AON', 'vHp']\n",
    "for area in area_list:\n",
    "    area_channels = all_boxplot_df[all_boxplot_df['channel'] == area]\n",
    "    writer = pd.ExcelWriter(savepath + f'events_power_per_band_multitaper_log10_{area}_truncated.xlsx', engine='xlsxwriter')\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(40, 10), sharex=True, sharey=True)\n",
    "    axs = axs.flatten()\n",
    "    for ax in axs:\n",
    "        ax.tick_params(axis='y', which='both', labelleft=True)\n",
    "    events_dict = {'pre_door': 'Pre Door', 'post_door': 'Post Door', 'pre_dig': 'Pre Dig', 'post_dig': 'Post Dig'}\n",
    "    for i, event in enumerate(events_dict.keys()):\n",
    "        event_cols_mt = [f'{band}_{event}_mt' for band in bands_dict.keys()]\n",
    "        event_df = area_channels[['rat', 'task', 'channel', 'trial', *event_cols_mt]]\n",
    "        event_df_melted = pd.melt(event_df, id_vars=['rat', 'task', 'channel', 'trial'], var_name='band', value_name='power')\n",
    "        event_df_melted['band'] = event_df_melted['band'].apply(lambda x: x.split('_')[0])\n",
    "        ax = axs[i]\n",
    "        sns.violinplot(x='band', y='power', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=event_df_melted, ax=ax)\n",
    "        ax.set_title(f'{events_dict[event]} {area} (Multitaper, log10)', fontsize=20)\n",
    "        ax.set_xlabel('Band', fontsize=20)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        if i == 0:\n",
    "            ax.set_ylabel('log10 Power (V^2)', fontsize=25)\n",
    "        event_df_melted.to_excel(writer, sheet_name=event)\n",
    "    writer.close()\n",
    "    plt.savefig(savepath + f'events_power_per_band_multitaper_log10_{area}_truncated.png', format='png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean across all trials Welch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(power_functions)\n",
    "compiled_data_all_epochs=pd.read_pickle(savepath+'compiled_data_all_epochs_truncated.pkl')\n",
    "boxplot_df=compiled_data_all_epochs.__deepcopy__()\n",
    "event_list=['pre_door','post_door','pre_dig','post_dig']\n",
    "boxplot_df.loc[:,event_list]=boxplot_df.loc[:,event_list].applymap(lambda x: power_functions.apply_welch_transform(x))\n",
    "new_boxplot_df=power_functions.get_all_band_power_from_welchdf(boxplot_df, event_list)\n",
    "new_boxplot_df['channel'] = new_boxplot_df['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "new_boxplot_df['unique_id'] = new_boxplot_df['rat'] + '_' + new_boxplot_df['task']+ '_' + new_boxplot_df['date']\n",
    "print(new_boxplot_df.columns)\n",
    "# Group by unique_id and channel, then take the mean across rows for columns containing 'pre' or 'post'\n",
    "pre_post_cols = [col for col in new_boxplot_df.columns if ('pre' in col or 'post' in col)]\n",
    "mean_data_list = []\n",
    "\n",
    "mean_boxplot_df=new_boxplot_df.__deepcopy__()\n",
    "unique_id_list=list(np.unique(mean_boxplot_df['unique_id']))\n",
    "mean_data_list=[]\n",
    "\n",
    "for unique_id in unique_id_list:\n",
    "    unique_id_df=mean_boxplot_df[mean_boxplot_df['unique_id']==unique_id]\n",
    "    unique_id_df_grouped=unique_id_df.groupby(['channel'])\n",
    "    for channel, group in unique_id_df_grouped:\n",
    "        print(channel)\n",
    "        group=group.reset_index(drop=True)\n",
    "        columns = [col for col in group.columns if 'pre' in col or 'post' in col]\n",
    "        print(columns)\n",
    "        rat_id=group['rat'].iloc[0]\n",
    "        task_id=group['task'].iloc[0]\n",
    "        date_id=group['date'].iloc[0]\n",
    "        channel_id=group['channel'].iloc[0]\n",
    "        mean_data_dict={}\n",
    "        for col in columns:\n",
    "            data=np.array(group[col])\n",
    "            data_mean=np.mean(data,axis=0)\n",
    "            data_sem=scipy.stats.sem(data,axis=0)\n",
    "            mean_data_dict[col+'_mean']=data_mean\n",
    "            mean_data_dict[col+'_sem']=data_sem\n",
    "        mean_data_dict['rat']=rat_id\n",
    "        mean_data_dict['task']=task_id\n",
    "        mean_data_dict['date']=date_id\n",
    "        mean_data_dict['channel']=channel_id\n",
    "        mean_data_list.append(mean_data_dict)\n",
    "mean_df=pd.DataFrame(mean_data_list)\n",
    "mean_df.drop(columns=['date'], inplace=True)\n",
    "\n",
    "\n",
    "mean_df_melted=pd.melt(mean_df, id_vars=['rat','task','channel'], var_name='band', value_name='power')\n",
    "mean_df_melted['band name']=mean_df_melted['band'].apply(lambda x: x.split('_')[0])\n",
    "mean_df_melted['event']=mean_df_melted['band'].apply(lambda x: x.split('_')[1:3])\n",
    "mean_df_melted['event']=mean_df_melted['event'].apply(lambda x: '_'.join(x))\n",
    "mean_df_melted['type']=mean_df_melted['band'].apply(lambda x: x.split('_')[-1])\n",
    "cols = list(mean_df_melted.columns)\n",
    "cols.append(cols.pop(cols.index('power')))\n",
    "mean_df_melted = mean_df_melted[cols]\n",
    "mean_df_melted.drop(columns=['band'], inplace=True)\n",
    "mean_df_melted_grouped=mean_df_melted.groupby(['event'])\n",
    "writer=pd.ExcelWriter(savepath+'mean_across_trials_power_truncated.xlsx')\n",
    "for event, group in mean_df_melted_grouped:\n",
    "    print(event)\n",
    "    group=group.reset_index(drop=True)\n",
    "    group.to_excel(writer, sheet_name=event[0])\n",
    "writer.close()\n",
    "arealist=['AON','vHp']\n",
    "for area in arealist:\n",
    "    fig,axs=plt.subplots(1,4,figsize=(40,10), sharey=True)\n",
    "    axs=axs.flatten()\n",
    "    for ax in axs:\n",
    "        ax.tick_params(axis='y', which='both', labelleft=True)\n",
    "    events_dict={'pre_door':'Pre Door', 'post_door':'Post Door', 'pre_dig':'Pre Dig', 'post_dig':'Post Dig'}\n",
    "    for i,event in enumerate(events_dict.keys()):\n",
    "        ax=axs[i]\n",
    "        ## Plotting AON mean power\n",
    "        plotting_df=mean_df_melted[(mean_df_melted['channel'].str.contains(area)) & (mean_df_melted['type']=='mean') & (mean_df_melted['event']==event)] \n",
    "        # Remove outliers using the IQR method for each band name\n",
    "        def remove_outliers_iqr(df, value_col='power', group_col='band name'):\n",
    "            def iqr_filter(group):\n",
    "                q1 = group[value_col].quantile(0.25)\n",
    "                q3 = group[value_col].quantile(0.75)\n",
    "                iqr = q3 - q1\n",
    "                lower = q1 - 1.5 * iqr\n",
    "                upper = q3 + 1.5 * iqr\n",
    "                return group[(group[value_col] >= lower) & (group[value_col] <= upper)]\n",
    "            return df.groupby(group_col, group_keys=False).apply(iqr_filter)\n",
    "\n",
    "        plotting_df = remove_outliers_iqr(plotting_df)\n",
    "        sns.boxplot(x='band name',y='power',hue='task',data=plotting_df,ax=ax, showfliers=False)\n",
    "        sns.stripplot(x='band name',y='power',hue='task',data=plotting_df,dodge=True,edgecolor='black',linewidth=1,jitter=True, legend=False, ax=ax, color=\".3\", size=2)\n",
    "        #ax.set_yscale('log')\n",
    "        ax.set_xlabel('Band', fontsize=20)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        ax.legend(loc='upper left', fontsize=15)\n",
    "        if i == 0:\n",
    "            ax.set_ylabel('Power (V^2)', fontsize=25)\n",
    "        ax.set_title(f'{area} power {events_dict[event]}', fontsize=20)\n",
    "    fig.savefig(savepath+f'mean_power_across_trials_{area}_truncated.png', format='png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "#mean_df=pd.DataFrame(mean_data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multitaper Mean across trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.time_frequency import psd_array_multitaper\n",
    "\n",
    "\n",
    "importlib.reload(power_functions)\n",
    "compiled_data_all_epochs = pd.read_pickle(savepath+'compiled_data_all_epochs_truncated.pkl')\n",
    "power_df=compiled_data_all_epochs.__deepcopy__()\n",
    "# number_per_segment = 700\n",
    "# tukey_window = scipy.signal.get_window(('tukey', 0.1), number_per_segment)\n",
    "columns= ['pre_door', 'post_door', 'pre_dig', 'post_dig', 'around_door', 'around_dig']\n",
    "# Apply multitaper PSD to each event column\n",
    "def multitaper_transform(x):\n",
    "    # x is a 1D array or list of values\n",
    "    psd, _ = psd_array_multitaper(x, sfreq=2000, fmin=0, fmax=1000, adaptive=True, bandwidth=6, normalization='full', verbose=0, max_iter=1000)\n",
    "    return psd\n",
    "\n",
    "power_df.loc[:, columns] = power_df.loc[:, columns].applymap(multitaper_transform)\n",
    "\n",
    "new_boxplot_df = power_functions.get_all_band_power_from_welchdf(power_df, event_list)\n",
    "new_boxplot_df['channel'] = new_boxplot_df['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "new_boxplot_df['unique_id'] = new_boxplot_df['rat'] + '_' + new_boxplot_df['task'] + '_' + new_boxplot_df['date']\n",
    "print(new_boxplot_df.columns)\n",
    "\n",
    "pre_post_cols = [col for col in new_boxplot_df.columns if ('pre' in col or 'post' in col)]\n",
    "mean_data_list = []\n",
    "\n",
    "mean_boxplot_df = new_boxplot_df.__deepcopy__()\n",
    "unique_id_list = list(np.unique(mean_boxplot_df['unique_id']))\n",
    "mean_data_list = []\n",
    "\n",
    "for unique_id in unique_id_list:\n",
    "    unique_id_df = mean_boxplot_df[mean_boxplot_df['unique_id'] == unique_id]\n",
    "    unique_id_df_grouped = unique_id_df.groupby(['channel'])\n",
    "    for channel, group in unique_id_df_grouped:\n",
    "        print(channel)\n",
    "        group = group.reset_index(drop=True)\n",
    "        columns = [col for col in group.columns if 'pre' in col or 'post' in col]\n",
    "        print(columns)\n",
    "        rat_id = group['rat'].iloc[0]\n",
    "        task_id = group['task'].iloc[0]\n",
    "        date_id = group['date'].iloc[0]\n",
    "        channel_id = group['channel'].iloc[0]\n",
    "        mean_data_dict = {}\n",
    "        for col in columns:\n",
    "            data = np.array(group[col])\n",
    "            data_mean = np.mean(data, axis=0)\n",
    "            data_sem = scipy.stats.sem(data, axis=0)\n",
    "            mean_data_dict[col + '_mean'] = data_mean\n",
    "            mean_data_dict[col + '_sem'] = data_sem\n",
    "        mean_data_dict['rat'] = rat_id\n",
    "        mean_data_dict['task'] = task_id\n",
    "        mean_data_dict['date'] = date_id\n",
    "        mean_data_dict['channel'] = channel_id\n",
    "        mean_data_list.append(mean_data_dict)\n",
    "mean_df = pd.DataFrame(mean_data_list)\n",
    "mean_df.drop(columns=['date'], inplace=True)\n",
    "\n",
    "mean_df_melted = pd.melt(mean_df, id_vars=['rat', 'task', 'channel'], var_name='band', value_name='power')\n",
    "mean_df_melted['band name'] = mean_df_melted['band'].apply(lambda x: x.split('_')[0])\n",
    "mean_df_melted['event'] = mean_df_melted['band'].apply(lambda x: x.split('_')[1:3])\n",
    "mean_df_melted['event'] = mean_df_melted['event'].apply(lambda x: '_'.join(x))\n",
    "mean_df_melted['type'] = mean_df_melted['band'].apply(lambda x: x.split('_')[-1])\n",
    "cols = list(mean_df_melted.columns)\n",
    "cols.append(cols.pop(cols.index('power')))\n",
    "mean_df_melted = mean_df_melted[cols]\n",
    "mean_df_melted.drop(columns=['band'], inplace=True)\n",
    "mean_df_melted=mean_df_melted[mean_df_melted['band name']!= 'total'] #Remove total band if it exists\n",
    "mean_df_melted_grouped = mean_df_melted.groupby(['event'])\n",
    "writer_mt = pd.ExcelWriter(savepath + 'events_power_perband_multitaper_truncated.xlsx')\n",
    "for event, group in mean_df_melted_grouped:\n",
    "    print(event)\n",
    "    group = group.reset_index(drop=True)\n",
    "    group.to_excel(writer_mt, sheet_name=event[0])\n",
    "writer_mt.close()\n",
    "arealist = ['AON', 'vHp']\n",
    "for area in arealist:\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(40, 10), sharey=True)\n",
    "    axs = axs.flatten()\n",
    "    for ax in axs:\n",
    "        ax.tick_params(axis='y', which='both', labelleft=True)\n",
    "    events_dict = {'pre_door': 'Pre Door', 'post_door': 'Post Door', 'pre_dig': 'Pre Dig', 'post_dig': 'Post Dig'}\n",
    "    for i, event in enumerate(events_dict.keys()):\n",
    "        ax = axs[i]\n",
    "        plotting_df = mean_df_melted[\n",
    "            (mean_df_melted['channel'].str.contains(area)) &\n",
    "            (mean_df_melted['type'] == 'mean') &\n",
    "            (mean_df_melted['event'] == event)\n",
    "        ]\n",
    "        def remove_outliers_iqr(df, value_col='power', group_col='band name'):\n",
    "            def iqr_filter(group):\n",
    "                q1 = group[value_col].quantile(0.25)\n",
    "                q3 = group[value_col].quantile(0.75)\n",
    "                iqr = q3 - q1\n",
    "                lower = q1 - 1.5 * iqr\n",
    "                upper = q3 + 1.5 * iqr\n",
    "                return group[(group[value_col] >= lower) & (group[value_col] <= upper)]\n",
    "            return df.groupby(group_col, group_keys=False).apply(iqr_filter)\n",
    "        plotting_df = remove_outliers_iqr(plotting_df)\n",
    "        sns.barplot(x='band name', y='power', hue='task', data=plotting_df, ax=ax)\n",
    "        sns.stripplot(x='band name', y='power', hue='task', data=plotting_df, dodge=True, palette=colors, jitter=True, legend=False, ax=ax, linewidth=1, alpha=0.5)\n",
    "        ax.set_xlabel('Band', fontsize=20)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        ax.legend(handles, [task_dict[l] for l in labels], loc='upper right', fontsize=15)\n",
    "        #ax.legend(loc='upper left', fontsize=15)\n",
    "        if i == 0:\n",
    "            ax.set_ylabel('Power (V^2)', fontsize=25)\n",
    "        else:\n",
    "            ax.set_ylabel('')\n",
    "        ax.set_title(f'{area} power {events_dict[event]}', fontsize=20)\n",
    "    fig.savefig(savepath + f'events_power_perband_{area}_multitaper.png', format='png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a mean across channels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_boxplot_df=boxplot_df.__deepcopy__()\n",
    "unique_id_list=list(np.unique(mean_boxplot_df['unique_id']))\n",
    "mean_data_list=[]\n",
    "sem_data_list=[]\n",
    "mean_boxplot_df['channel']=mean_boxplot_df['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "mean_boxplot_df_grouped=mean_boxplot_df.groupby(['task', 'channel', 'trial'])\n",
    "for (task, channel, trial), group in mean_boxplot_df_grouped:\n",
    "    print(task, channel, trial)\n",
    "    group=group.reset_index(drop=True)\n",
    "    columns=group.columns[-21:-1]\n",
    "    data_array=np.array(group[columns])\n",
    "    data_mean=np.mean(data_array, axis=0)\n",
    "    data_sem=scipy.stats.sem(data_array, axis=0)\n",
    "    print(data_mean)\n",
    "    print(data_sem)\n",
    "    mean_data_dict = {col: data_mean[idx] for idx, col in enumerate(columns)}\n",
    "    sem_data_dict = {col: data_sem[idx] for idx, col in enumerate(columns)}\n",
    "    mean_data_dict['task'] = task\n",
    "    mean_data_dict['channel'] = channel\n",
    "    mean_data_dict['trial'] = trial\n",
    "    sem_data_dict['task'] = task\n",
    "    sem_data_dict['channel'] = channel\n",
    "    sem_data_dict['trial'] = trial\n",
    "    mean_data_list.append(mean_data_dict)\n",
    "    sem_data_list.append(sem_data_dict)\n",
    "\n",
    "mean_df = pd.DataFrame(mean_data_list)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for task in ['BWcontext', 'BWnocontext']:\n",
    "    task_data = mean_df[(mean_df['task'] == task) & (mean_df['channel'] == 'AON')]\n",
    "    ax.plot(task_data['trial'], task_data['total_complete_trial'], label=task, marker='o')\n",
    "ax.set_xlabel('Trial Number')\n",
    "ax.set_ylabel('AON Power in total complete trial')\n",
    "ax.set_title('AON Power in total complete trial across Trials')\n",
    "ax.set_xticks(np.arange(0, 20, 1))\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean in groups of 5 trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_df_grouped=boxplot_df.groupby(['unique_id'])\n",
    "mean_data_list=[]\n",
    "for unique_id, group in boxplot_df_grouped:\n",
    "    print(unique_id)\n",
    "    num_of_trials=len(group['trial'].unique())\n",
    "    print(num_of_trials)\n",
    "    group=group.reset_index(drop=True)\n",
    "    print()\n",
    "    for channel in group['channel'].unique():\n",
    "        i=0\n",
    "        group_channel=group[group['channel']==channel]\n",
    "        group_channel=group_channel.reset_index(drop=True)\n",
    "        \n",
    "        while i < 16:\n",
    "            print(i)\n",
    "            group_trial = group_channel[(group_channel['trial'] >= i) & (group_channel['trial'] < i + 4)]\n",
    "            group_trial_data_array = np.array(group_trial.loc[:, 'beta_pre_door':'total_around_dig'])\n",
    "            data_mean= group_trial_data_array.mean(axis=0)\n",
    "            row = {**group_channel.iloc[0][['rat', 'task', 'channel', 'unique_id']].to_dict(),\n",
    "                   **{'trial': f'{i}-{i + 4}'},\n",
    "                   **dict(zip(group_trial.loc[:, 'beta_pre_door':'total_around_dig'].columns, data_mean))}\n",
    "            mean_data_list.append(row)\n",
    "\n",
    "            i=i+4\n",
    "mean_df = pd.DataFrame(mean_data_list)\n",
    "mean_df_melted=pd.melt(mean_df, id_vars=['rat','task','channel','trial', 'unique_id'], var_name='band_event', value_name='power')\n",
    "mean_df_melted['band name']=mean_df_melted['band_event'].apply(lambda x: x.split('_')[0])\n",
    "mean_df_melted['event']=mean_df_melted['band_event'].apply(lambda x: x.split('_')[1:3])\n",
    "mean_df_melted['event']=mean_df_melted['event'].apply(lambda x: '_'.join(x))\n",
    "mean_df_melted_grouped=mean_df_melted.groupby(['event'])\n",
    "writer=pd.ExcelWriter(savepath+'power_boxplot_average_per_4_trials.xlsx')\n",
    "for event, group in mean_df_melted_grouped:\n",
    "    print(event)\n",
    "    group=group.reset_index(drop=True)\n",
    "    group.drop(columns=['band_event','event'], inplace=True)\n",
    "    group.to_excel(writer, sheet_name=event[0])\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Power Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import spectrogram\n",
    "\n",
    "power_spec_df = compiled_data_all_epochs.__deepcopy__()\n",
    "print(power_spec_df.iloc[0,-2].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_spec_df.iloc[:, -2:] = power_spec_df.iloc[:, -2:].applymap(lambda x: spectrogram(x, fs=2000, nperseg=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(power_spec_df.iloc[0,-2][1])\n",
    "for col in ['around_door','around_dig']:\n",
    "\n",
    "    power_spec_df[col+'_f'] = power_spec_df[col].apply(lambda x: x[0])\n",
    "    power_spec_df[col+'_t'] = power_spec_df[col].apply(lambda x: x[1])\n",
    "    power_spec_df[col+'_sxx'] = power_spec_df[col].apply(lambda x: x[2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_spec_df['channel'] = power_spec_df['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "power_spec_df_grouped = power_spec_df.groupby(['task', 'channel'])\n",
    "for (task, channel), group in power_spec_df_grouped:\n",
    "    group = group.reset_index(drop=True)\n",
    "    for col in ['around_door', 'around_dig']:\n",
    "        data = np.array(group[col + '_sxx'])\n",
    "        data_mean = np.mean(data, axis=0)\n",
    "        print(data_mean.shape)\n",
    "        freq = group[col + '_f'].iloc[0]\n",
    "        time = group[col + '_t'].iloc[0]\n",
    "        time_adjusted=np.linspace(-2,2,len(time))\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(20, 10), constrained_layout=True)\n",
    "        im = ax.pcolormesh(time_adjusted, freq, data_mean, shading='gouraud', vmin=0, vmax=0.5)\n",
    "        fig.colorbar(im, ax=ax)\n",
    "        ax.set_title(f'{task} {channel} {col}')\n",
    "        ax.set_ylim(0, 100)\n",
    "        # ax.set_xticks(np.arange(-2, 3, 1))  # Set x-ticks from -2 to 2 seconds\n",
    "        # ax.set_xticklabels(np.arange(-2, 3, 1))  # Set x-tick labels from -2 to 2 seconds\n",
    "        ax.vlines(0, 0, 100, color='red', linestyle='--')\n",
    "        ax.set_xlabel('Time (s)', fontsize=20)\n",
    "\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        ax.set_ylabel('Frequency (Hz)', fontsize=20)\n",
    "        i = i + 1\n",
    "        fig.savefig(savepath + f'power_mean_spectrogram_{task}_{channel}_{col}.png', dpi=300)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Coherence Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Coherence functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating static data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "importlib.reload(coherence_functions)\n",
    "# --- Basic Parameters ---\n",
    "sfreq = 1000  # Sampling frequency in Hz\n",
    "n_epochs = 20  # Number of trials\n",
    "n_times = 2000  # Number of time points per trial (2 seconds of data)\n",
    "times = np.arange(n_times) / sfreq  # Time vector for one epoch\n",
    "n_signals = 3  # We'll create 3 channels\n",
    "\n",
    "# We will test connectivity in the beta band\n",
    "freq_of_interest = 20.0  # 20 Hz\n",
    "# --- Generate Data for Static Connectivity ---\n",
    "\n",
    "# Initialize data array: (n_epochs, n_signals, n_times)\n",
    "static_data = np.random.randn(n_epochs, n_signals, n_times) * 0.1  # Add background noise\n",
    "\n",
    "# Create the shared 20 Hz sine wave component\n",
    "shared_signal = np.sin(2 * np.pi * freq_of_interest * times)\n",
    "\n",
    "# Add the shared signal to the first two channels for all epochs\n",
    "static_data[:, 0, :] += shared_signal\n",
    "static_data[:, 1, :] += shared_signal\n",
    "\n",
    "print(\"Shape of static_data:\", static_data.shape)\n",
    "ch_names=['AON', 'vHp', 'PFC']  # Example channel names\n",
    "info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types='eeg')\n",
    "static_data_mne=mne.EpochsArray(static_data, info)\n",
    "print(static_data_mne)\n",
    "# Plot the static_data for each channel in the first epoch\n",
    "fig, axs = plt.subplots(n_signals, 1, figsize=(12, 6), sharex=True)\n",
    "for i, ch in enumerate(ch_names):\n",
    "    axs[i].plot(times, static_data[0, i, :], label=f'Channel: {ch}')\n",
    "    axs[i].set_ylabel('Amplitude')\n",
    "    axs[i].legend(loc='upper right')\n",
    "axs[-1].set_xlabel('Time (s)')\n",
    "plt.suptitle('Static Data Example (Epoch 0)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "coherence_band_sce = coherence_functions.convert_epoch_to_coherence(static_data_mne)\n",
    "print(coherence_band_sce)\n",
    "coherence_band_time=coherence_functions.convert_epoch_to_coherence_time(static_data_mne)\n",
    "print(coherence_band_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncating LFP data and loading it into MNE arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import mne_connectivity\n",
    "#files=[f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230616_dk6_BW_context_day2.mat']\n",
    "event_data_df=[]\n",
    "con_data_df=[]\n",
    "\n",
    "con_data_df_shuffled=[]\n",
    "shuffled_event_data_df=[]\n",
    "events_codes_all = {}\n",
    "random_baseline_data=[]\n",
    "baseline_lfp_all=[]\n",
    "time_window = 0.4\n",
    "fs= 2000\n",
    "for file_num,file in enumerate(files):\n",
    "    #if 'dk1' in file:\n",
    "        \n",
    "        #print(file)\n",
    "        base_name = os.path.basename(file)\n",
    "        base_name, _ = os.path.splitext(base_name)\n",
    "\n",
    "        date, rat_id, task = lfp_pre_processing_functions.exp_params(base_name)\n",
    "        print(date, rat_id, task)\n",
    "        if task == 'nocontextday2' or task == 'nocontextos2':\n",
    "            task = 'nocontext'\n",
    "        if task =='nocontext':\n",
    "            continue\n",
    "        if rat_id=='dk3':\n",
    "            continue\n",
    "        f = h5py.File(file, 'r')\n",
    "        channels = list(f.keys())\n",
    "        #print(channels)\n",
    "         \n",
    "        if not any(\"AON\" in channel or \"vHp\" in channel for channel in channels):\n",
    "            print(\"No AON or vHp channels in this file\")\n",
    "            continue\n",
    "\n",
    "        events,reference_electrode=lfp_pre_processing_functions.get_keyboard_and_ref_channels(f,channels)\n",
    "\n",
    "    #finding global start and end time of all channels, since they start and end recordings at different times\n",
    "        global_start_time, global_end_time=lfp_pre_processing_functions.find_global_start_end_times(f,channels)\n",
    "        \n",
    "        ## Reference electrode finding and padding\n",
    "        reference_time = np.array(reference_electrode['times']).flatten()\n",
    "        reference_value = np.array(reference_electrode['values']).flatten()\n",
    "        padd_ref_data,padded_ref_time=lfp_pre_processing_functions.pad_raw_data_raw_time(reference_value,reference_time,global_start_time,global_end_time,sampling_rate=2000)\n",
    "\n",
    "        events_codes = np.array(events['codes'][0])\n",
    "        events_times = np.array(events['times'][0])\n",
    "        events_codes_all[base_name] = events_codes\n",
    "        epochs = lfp_pre_processing_functions.generate_epochs_with_first_event(events_codes, events_times)\n",
    "        #epochs = functions.generate_specific_num_of_epochs_with_first_event(events_codes, events_times,5)\n",
    "        aon_lfp_channels=[x for x in channels if 'AON' in x ]\n",
    "        vHp_lfp_channels=[x for x in channels if 'vHp' in x ]\n",
    "        all_channels=np.concatenate((aon_lfp_channels,vHp_lfp_channels))\n",
    "        #print(all_channels)\n",
    "        \n",
    "        mne_baseline_data=np.zeros((1,len(all_channels),4000))\n",
    "        mne_epoch_door_before=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_door_after=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_dig_before=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_dig_after=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_around_door=np.zeros((len(epochs),len(all_channels),int(time_window*fs)*2))\n",
    "        mne_epoch_around_dig=np.zeros((len(epochs),len(all_channels),int(time_window*fs)*2))\n",
    "        \n",
    "        mne_baseline_data_shuffled=np.zeros((1,len(all_channels),4000))\n",
    "        mne_epoch_door_before_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_door_after_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_dig_before_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_dig_after_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_around_door_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs*2)))\n",
    "        mne_epoch_around_dig_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs*2)))\n",
    "\n",
    "        print(f'File {rat_id} {task} {date} has {len(epochs)} epochs and {len(all_channels)} channels')\n",
    "\n",
    "\n",
    "        first_event = events_times[0]\n",
    "        \n",
    "        for channel_num,channeli in enumerate(all_channels):\n",
    "            if \"AON\" in channeli or \"vHp\" in channeli:\n",
    "                channel_id = channeli\n",
    "                data_all = f[channeli]\n",
    "                raw_data = np.array(data_all['values']).flatten()\n",
    "                raw_time = np.array(data_all['times']).flatten()\n",
    "                sampling_rate = int(1 / data_all['interval'][0][0])\n",
    "                #print(raw_data.shape, raw_time.shape, sampling_rate)\n",
    "                padded_data,padded_time=lfp_pre_processing_functions.pad_raw_data_raw_time(raw_data,raw_time,global_start_time,global_end_time,sampling_rate)\n",
    "                subtracted_data = padded_data - padd_ref_data\n",
    "                raw_data=subtracted_data\n",
    "                notch_filtered_data = lfp_pre_processing_functions.iir_notch(raw_data, sampling_rate, 60)\n",
    "\n",
    "                data_before, time, baseline_mean, baseline_std=lfp_pre_processing_functions.baseline_data_normalization(notch_filtered_data, raw_time, first_event, sampling_rate)\n",
    "                first_event_index=np.where(raw_time>first_event)[0][0]\n",
    "\n",
    "                mne_baseline_data[0,channel_num,:]=list(data_before)\n",
    "                mne_baseline_data_shuffled[0,channel_num,:]=list(np.random.permutation(data_before))\n",
    "                total = notch_filtered_data\n",
    "\n",
    "                \n",
    "                for i, epochi in enumerate(epochs):\n",
    "                    door_timestamp = epochi[0][0]\n",
    "                    trial_type = epochi[0][1]\n",
    "                    dig_type = epochi[1, 1]\n",
    "                    #print(dig_type)\n",
    "                    dig_timestamp = epochi[1, 0]\n",
    "                    #print(door_timestamp, trial_type, dig_timestamp, dig_type)\n",
    "                    data_trial_before, data_trial_after=lfp_pre_processing_functions.extract_event_data(notch_filtered_data,time,door_timestamp,sampling_rate,0.7)\n",
    "                    data_dig_before, data_dig_after=lfp_pre_processing_functions.extract_event_data(notch_filtered_data,time,dig_timestamp,sampling_rate,0.7)\n",
    "                    data_around_door=np.concatenate((data_trial_before, data_trial_after))\n",
    "                    data_around_dig=np.concatenate((data_dig_before, data_dig_after))\n",
    "\n",
    "                    epoch_data = [data_trial_before, data_trial_after, data_dig_before, data_dig_after, data_around_door, data_around_dig]\n",
    "                    event_data_list = [lfp_pre_processing_functions.zscore_event_data(x, baseline_std) for x in epoch_data]\n",
    "\n",
    "                    mne_epoch_door_before[i,channel_num,:]=list(event_data_list[0][-int(time_window*fs):])\n",
    "                    mne_epoch_door_after[i,channel_num,:]=list(event_data_list[1][:int(time_window*fs)])\n",
    "                    mne_epoch_dig_before[i,channel_num,:]=list(event_data_list[2][-int(time_window*fs):])\n",
    "                    mne_epoch_dig_after[i,channel_num,:]=list(event_data_list[3][:int(time_window*fs)])\n",
    "                    mid_point = int(len(event_data_list[4]) / 2)\n",
    "                    mne_epoch_around_door[i,channel_num,:]=list(event_data_list[4][mid_point-int(time_window*fs):mid_point+int(time_window*fs)])\n",
    "                    mne_epoch_around_dig[i,channel_num,:]=list(event_data_list[5][mid_point-int(time_window*fs):mid_point+int(time_window*fs)])\n",
    "\n",
    "                    mne_epoch_door_before_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[0][-int(time_window*fs):]))\n",
    "                    mne_epoch_door_after_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[1][:int(time_window*fs)]))\n",
    "                    mne_epoch_dig_before_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[2][-int(time_window*fs):]))\n",
    "                    mne_epoch_dig_after_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[3][:int(time_window*fs)]))\n",
    "                    mne_epoch_around_door_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[4][mid_point-int(time_window*fs):mid_point+int(time_window*fs)]))\n",
    "                    mne_epoch_around_dig_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[5][mid_point-int(time_window*fs):mid_point+int(time_window*fs)]))\n",
    "\n",
    "        if len(all_channels)>0:\n",
    "            fs=2000\n",
    "            freqs = np.arange(1,100)\n",
    "            n_cycles = freqs/3\n",
    "            info = mne.create_info(ch_names=list(all_channels), sfreq=fs, ch_types='eeg')\n",
    "            mne_baseline = mne.EpochsArray(mne_baseline_data, info)\n",
    "            mne_epoch_door_before = mne.EpochsArray(mne_epoch_door_before, info)\n",
    "            mne_epoch_door_after= mne.EpochsArray(mne_epoch_door_after, info)\n",
    "            mne_epoch_dig_before = mne.EpochsArray(mne_epoch_dig_before, info)\n",
    "            mne_epoch_dig_after = mne.EpochsArray(mne_epoch_dig_after, info)\n",
    "            mne_epoch_around_door = mne.EpochsArray(mne_epoch_around_door, info)\n",
    "            mne_epoch_around_dig = mne.EpochsArray(mne_epoch_around_dig, info)\n",
    "            \n",
    "            row_list=[file_num,rat_id,task,mne_baseline,mne_epoch_door_before,mne_epoch_door_after,mne_epoch_dig_before,mne_epoch_dig_after,mne_epoch_around_door,mne_epoch_around_dig]\n",
    "            \n",
    "            mne_baseline_shuffled = mne.EpochsArray(mne_baseline_data_shuffled, info)\n",
    "            mne_epoch_door_before_shuffled = mne.EpochsArray(mne_epoch_door_before_shuffled, info)\n",
    "            mne_epoch_door_after_shuffled = mne.EpochsArray(mne_epoch_door_after_shuffled, info)\n",
    "            mne_epoch_dig_before_shuffled = mne.EpochsArray(mne_epoch_dig_before_shuffled, info)\n",
    "            mne_epoch_dig_after_shuffled = mne.EpochsArray(mne_epoch_dig_after_shuffled, info)\n",
    "            mne_epoch_around_door_shuffled = mne.EpochsArray(mne_epoch_around_door_shuffled, info)\n",
    "            mne_epoch_around_dig_shuffled = mne.EpochsArray(mne_epoch_around_dig_shuffled, info)\n",
    "            row_list_shuffled=[file_num,rat_id,task,mne_baseline_shuffled,mne_epoch_door_before_shuffled,mne_epoch_door_after_shuffled,mne_epoch_dig_before_shuffled,mne_epoch_dig_after_shuffled,mne_epoch_around_door_shuffled,mne_epoch_around_dig_shuffled]\n",
    "            shuffled_event_data_df.append(row_list_shuffled)\n",
    "\n",
    "            con_data_df.append(row_list)\n",
    "            con_data_df_shuffled.append(row_list_shuffled)\n",
    "\n",
    "\n",
    "con_data_df=pd.DataFrame(con_data_df, columns=['experiment','rat_id','task','mne_baseline','mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after','mne_epoch_around_door','mne_epoch_around_dig'])\n",
    "#con_data_df.to_pickle(savepath+f'mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "con_data_df_shuffled=pd.DataFrame(con_data_df_shuffled, columns=['experiment','rat_id','task','mne_baseline','mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after','mne_epoch_around_door','mne_epoch_around_dig'])\n",
    "#con_data_df_shuffled.to_pickle(savepath+f'mne_epochs_array_df_shuffled_truncated_{int(time_window*fs)}.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.append(event_data_list[2],event_data_list[3]))\n",
    "print(event_data_list[2])\n",
    "print(event_data_list[3])\n",
    "print((np.append(event_data_list[2],event_data_list[3]))[3000:5000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Baseline Coherence Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(coherence_functions)\n",
    "con_data_df=pd.read_pickle(savepath+'mne_epochs_array_df_truncated.pkl')\n",
    "baseline_df=con_data_df.__deepcopy__()\n",
    "baseline_df=baseline_df[baseline_df['rat_id']!='dk3']\n",
    "baseline_df['mne_baseline']=baseline_df['mne_baseline'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_density(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax= plt.subplots(1, 1, figsize=(10, 5), sharex=True, sharey=True)\n",
    "writer=pd.ExcelWriter(savepath+'baseline_coherence_density.xlsx')\n",
    "baseline_df_grouped=baseline_df.groupby(['task'])\n",
    "task_dict={'BWcontext':'Context','BWnocontext':'No Context'}\n",
    "baseline_dict={}\n",
    "for (task, group) in baseline_df_grouped:\n",
    "    print(task[0])\n",
    "    group=group.reset_index(drop=True)\n",
    "    data = np.array(group['mne_baseline'].tolist())\n",
    "    data_mean = np.mean(data, axis=0)\n",
    "    data_sem = scipy.stats.sem(data, axis=0)\n",
    "    freq = np.linspace(0, 100, len(data_mean))\n",
    "    ax.plot(freq, data_mean, label=task_dict[task[0]])\n",
    "    ax.fill_between(freq, data_mean - data_sem, data_mean + data_sem, alpha=0.2)\n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.set_title(f'Baseline AON-vHp Coherence Density')\n",
    "    ax.set_xlabel('Frequency (Hz)')\n",
    "    ax.set_ylabel('Coherence (Z-transformed)')\n",
    "    ax.legend()\n",
    "    baseline_dict[f'{task[0]}_mean'] = data_mean\n",
    "    baseline_dict[f'{task[0]}_sem'] = data_sem\n",
    "baseline_dict['frequency'] = freq\n",
    "mean_df = pd.DataFrame(baseline_dict)\n",
    "mean_df.to_excel(writer, sheet_name='mean_coherence_density')\n",
    "writer.close()\n",
    "\n",
    "fig.savefig(savepath+'baseline_coherence_density.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Baseline Coherence Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(coherence_functions)\n",
    "con_data_df=pd.read_pickle(savepath+'mne_epochs_array_df_truncated.pkl')\n",
    "baseline_df=con_data_df.__deepcopy__()\n",
    "baseline_df=baseline_df[baseline_df['rat_id']!='dk3']\n",
    "task_dict={'BWcontext':'Context','BWnocontext':'No Context'}\n",
    "bands_dict = {'beta': [12, 30], 'gamma': [30, 80],'theta':[4,12], 'total': [1, 100]}\n",
    "for col in ['mne_baseline']:\n",
    "    print(col)\n",
    "    for band, (band_start, band_end) in bands_dict.items():\n",
    "        baseline_df[band + '_' + col] = baseline_df[col].apply(lambda x: coherence_functions.convert_epoch_to_coherence_baseline(x, band_start=band_start, band_end=band_end))\n",
    "baseline_df.drop(columns=['mne_baseline', 'mne_epoch_door_before', 'mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after','mne_epoch_around_door','mne_epoch_around_dig'], inplace=True)\n",
    "baseline_df_melted=pd.melt(baseline_df, id_vars=['experiment','rat_id','task'], var_name='band', value_name='coherence')\n",
    "baseline_df_melted['band']=baseline_df_melted['band'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "\n",
    "####Plotting coherence per band\n",
    "fig, axs= plt.subplots(1, 1, figsize=(20, 10), sharex=True, sharey=True)\n",
    "sns.barplot(x='band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=baseline_df_melted, legend=True, ax=axs)\n",
    "sns.stripplot(x='band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=baseline_df_melted, dodge=True, edgecolor='black', linewidth=1, jitter=True, legend=False, ax=axs)\n",
    "axs.set_title('Baseline Coherence per band', fontsize=20)\n",
    "axs.set_ylabel('Coherence (Z-transformed)', fontsize=20)\n",
    "axs.set_xlabel('')\n",
    "axs.tick_params(axis='both', which='major', labelsize=20)\n",
    "handles, labels = axs.get_legend_handles_labels()\n",
    "axs.legend(handles, [task_dict[l] for l in labels], loc='upper left', fontsize=15)\n",
    "fig.savefig(savepath+'baseline_coherence_per_band_truncated.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "###Writing the baseline coherence per band to excel\n",
    "writer=pd.ExcelWriter(savepath+'baseline_coherence_per_band_truncated.xlsx')\n",
    "baseline_df_melted.to_excel(writer)\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Coherence around events [door before, door after, dig before, dig after]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating AON-vHp connectivity Spectrogram from Epochs Array and Saving if as a pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df=pd.read_pickle(savepath+'mne_epochs_array_df_truncated.pkl')\n",
    "event_list=['mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after']\n",
    "print(event_list)\n",
    "BWcontext_data=con_data_df[(con_data_df['task']=='BWcontext')]\n",
    "BWnocontext_data=con_data_df[(con_data_df['task']=='BWnocontext')]\n",
    "task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "fs=2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rat_list=np.unique(con_data_df['rat_id'])\n",
    "print(rat_list)\n",
    "all_con_data=[]\n",
    "all_con_data_mean=[]\n",
    "for task_num,task_name in enumerate(task_data_dict.keys()):\n",
    "        task_data=task_data_dict[task_name]\n",
    "    #print(task_name)\n",
    "    # for rat_num, rat_name in enumerate(rat_list):\n",
    "    #     rat_task_data=task_data[task_data['rat_id']==rat_name]\n",
    "        row=[task_name]\n",
    "    #     #print(row)\n",
    "        row_2=[task_name]\n",
    "        for event in event_list:\n",
    "            #print(event)\n",
    "            event_epoch_list=task_data[event]\n",
    "            aon_vHp_con=[]\n",
    "            for event_epoch in event_epoch_list:\n",
    "                    #print(row,event, event_epoch) \n",
    "                    fmin=1\n",
    "                    fmax=100\n",
    "                    freqs = np.arange(fmin,fmax)\n",
    "                    n_cycles = freqs/3\n",
    "                           \n",
    "                    con = mne_connectivity.spectral_connectivity_epochs(event_epoch, method='coh', sfreq=int(fs),\n",
    "                                                         mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                                         cwt_n_cycles=n_cycles, verbose=False, fmin=1, fmax=100, faverage=False)\n",
    "                    coh = con.get_data(output='dense')\n",
    "                    indices = con.names\n",
    "                    \n",
    "\n",
    "                    for i in range(coh.shape[0]):\n",
    "                        for j in range(coh.shape[1]):\n",
    "                            if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                                coherence= coh[i,j,:,:]\n",
    "                                coherence=np.arctanh(coherence) #apply Fisher's z-transformation\n",
    "                                aon_vHp_con.append(coherence)\n",
    "            row.append(np.mean(aon_vHp_con, axis=0))\n",
    "            row_2.append(np.mean(aon_vHp_con))\n",
    "        all_con_data.append(row)                    \n",
    "        all_con_data_mean.append(row_2)\n",
    "# Convert all_con_data to a DataFrame for easier manipulation\n",
    "all_con_data_df = pd.DataFrame(all_con_data, columns=['task'] + event_list)\n",
    "all_con_data_df.to_pickle(savepath+'coherence_spectrogram_around_events.pkl')\n",
    "fs=2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_con_data_df=pd.read_pickle(savepath+'coherence_spectrogram_around_events.pkl')\n",
    "event_list=['mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after']\n",
    "\n",
    "times=np.arange(0, 2, 1/fs)\n",
    "fig, axs=plt.subplots(2,4, figsize=(20,10), sharey=True)\n",
    "vmin = all_con_data_df[event_list].applymap(np.min).min().min()\n",
    "vmax = all_con_data_df[event_list].applymap(np.max).max().max()\n",
    "event_names=['Before Door','After Door','Before Dig','After Dig']\n",
    "for i, event in enumerate(event_list):\n",
    "    axs[0,i].imshow(all_con_data_df[event][0], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[0,i].set_xlabel('Time (s)')\n",
    "    axs[0,i].set_ylabel('Frequency (Hz)')\n",
    "    axs[0,i].set_title(event_names[i])\n",
    "\n",
    "    axs[1,i].imshow(all_con_data_df[event][1], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[1,i].set_xlabel('Time (s)')\n",
    "    axs[1,i].set_ylabel('Frequency (Hz)')\n",
    "    axs[1,i].set_title(event_names[i])\n",
    "    axs[0,0].text(-0.3, 0.5, 'BW Context', transform=axs[0,0].transAxes, fontsize=14, verticalalignment='center', rotation=90)\n",
    "    axs[1,0].text(-0.3, 0.5, 'BW No Context', transform=axs[1,0].transAxes, fontsize=14, verticalalignment='center', rotation=90)\n",
    "    # Add a colorbar\n",
    "cbar = fig.colorbar(axs[0,0].images[0], ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "cbar.set_label('Coherence')\n",
    "#fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\aon_vhp_coherence_event_spectrogram.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Coherence around events [around door and around dig]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating AON-vHp connectivity around door and dig and saving it in a pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df_clean=pd.read_pickle(savepath+'mne_epochs_array_df_truncated.pkl')\n",
    "\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "\n",
    "print(event_list)\n",
    "BWcontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWcontext')]\n",
    "BWnocontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWnocontext')]\n",
    "task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "\n",
    "\n",
    "all_con_data=[]\n",
    "all_con_data_mean=[]\n",
    "for task_num,task_name in enumerate(task_data_dict.keys()):\n",
    "        task_data=task_data_dict[task_name]\n",
    "        row=[task_name]\n",
    "         #print(row)\n",
    "        row_2=[task_name]\n",
    "        for event in event_list:\n",
    "            #print(event)\n",
    "            event_epoch_list=task_data[event]\n",
    "            aon_vHp_con=[]\n",
    "            for event_epoch in event_epoch_list:\n",
    "                    #print(row,event, event_epoch) \n",
    "                    if event_epoch.events.shape[0] <5:\n",
    "                        print(f\"Skipping {event} for {task_name} due to insufficient events\")\n",
    "                        continue\n",
    "                    fmin=1\n",
    "                    fmax=100\n",
    "                    fs=2000\n",
    "                    freqs = np.arange(fmin,fmax)\n",
    "                    n_cycles = freqs/3\n",
    "                    # con= mne_connectivity.spectral_connectivity_time(event_epoch, method='coh', sfreq=int(fs), average=False,\n",
    "                    #                                      mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                    #                                      n_cycles=n_cycles, verbose=False, fmin=1, fmax=100, faverage=False)\n",
    "                    # coh = con.get_data(output='dense')\n",
    "                    # indices = con.names\n",
    "                    # print(coh.shape, indices)a\n",
    "                    # for i in range(coh.shape[0]):\n",
    "                    #     for j in range(coh.shape[1]):\n",
    "                    #         if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                    #             coherence= coh[i,j,:]\n",
    "                    #             coherence=np.arctanh(coherence)\n",
    "                    #             aon_vHp_con.append(coherence)\n",
    "\n",
    "                    con = mne_connectivity.spectral_connectivity_epochs(event_epoch, method='coh', sfreq=int(fs),\n",
    "                                                         mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                                         cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False)\n",
    "                    coh = con.get_data(output='dense')\n",
    "                    indices = con.names\n",
    "                    \n",
    "\n",
    "                    for i in range(coh.shape[0]):\n",
    "                        for j in range(coh.shape[1]):\n",
    "                            if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                                coherence= coh[i,j,:,:]\n",
    "                                coherence=np.arctanh(coherence)\n",
    "                                aon_vHp_con.append(coherence)\n",
    "            row.append(np.mean(aon_vHp_con, axis=0))\n",
    "            row_2.append(np.mean(aon_vHp_con))\n",
    "        all_con_data.append(row)                    \n",
    "        all_con_data_mean.append(row_2)\n",
    "# Convert all_con_data to a DataFrame for easier manipulation\n",
    "all_con_data_df = pd.DataFrame(all_con_data, columns=['task'] + event_list)\n",
    "all_con_data_df.to_pickle(savepath+'coherence_spectrogram_around_door_dig_truncated.pkl')\n",
    "fs=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_con_data_df=pd.read_pickle(savepath+'coherence_spectrogram_around_door_dig_truncated.pkl')\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "fs=2000\n",
    "times=np.arange(-2, 2, 1/fs)\n",
    "fig, axs=plt.subplots(2,2, figsize=(20,10), sharey=True)\n",
    "vmin = all_con_data_df[event_list].applymap(np.min).min().min()\n",
    "vmax = all_con_data_df[event_list].applymap(np.max).max().max()\n",
    "event_names=['Around Door','Around Dig']\n",
    "for i, event in enumerate(event_list):\n",
    "    axs[0,i].imshow(all_con_data_df[event][0], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[0,i].set_xlabel('')\n",
    "\n",
    "    axs[0,i].set_ylabel('Frequency (Hz)', fontsize=20)\n",
    "    axs[0,i].set_title(event_names[i], fontsize=20)\n",
    "    axs[0,i].vlines(0, 0, 100, color='k', linestyle='--')\n",
    "    axs[0,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    axs[1,i].imshow(all_con_data_df[event][1], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[1,i].set_xlabel('Time (s)', fontsize=20)\n",
    "    axs[1,i].set_ylabel('Frequency (Hz)', fontsize=20)\n",
    "    axs[1,i].set_title(event_names[i], fontsize=20)\n",
    "    axs[1,i].vlines(0, 0, 100, color='k', linestyle='--')\n",
    "\n",
    "    axs[0,0].text(-0.2, 0.5, 'Context', transform=axs[0,0].transAxes, fontsize=18, verticalalignment='center', rotation=90)\n",
    "    axs[1,0].text(-0.2, 0.5, 'No Context', transform=axs[1,0].transAxes, fontsize=18, verticalalignment='center', rotation=90)\n",
    "    axs[0,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    axs[1,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    axs[0,i].set_xticks(np.arange(-2, 3, 1))  # Set x-ticks from -2 to 2 seconds\n",
    "    axs[0,i].set_xticklabels(np.arange(-2, 3, 1))  # Set x-tick labels from -2 to 2 seconds\n",
    "    axs[1,i].set_xticks(np.arange(-2, 3, 1))  # Set x-ticks from -2 to 2 seconds\n",
    "    axs[1,i].set_xticklabels(np.arange(-2, 3, 1))  # Set x-tick labels from -2 to 2 seconds\n",
    "\n",
    "    # Add a colorbar\n",
    "cbar = fig.colorbar(axs[0,0].images[0], ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "cbar.set_label('Coherence', loc='center', fontsize=20, labelpad=10)\n",
    "cbar.ax.tick_params(labelsize=20)  # Set colorbar tick label size\n",
    "\n",
    "fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\aon_vhp_coherence_event_spectrogram.png',format='png', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Coherogram for each experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df_clean=pd.read_pickle(savepath+'mne_epochs_array_df_truncated.pkl')\n",
    "\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "\n",
    "print(event_list)\n",
    "\n",
    "test_list = [con_data_df_clean.iloc[0]]\n",
    "mean_con_data=pd.DataFrame()\n",
    "def epoch_coherogram(epoch, fmin=1, fmax=100, fs=2000):\n",
    "        print(epoch.events.shape)\n",
    "    # if epoch.events.shape[0] < 5:\n",
    "    #     print(\"Not enough events in the epoch\")\n",
    "    #     return None\n",
    "    # else:\n",
    "        freqs = np.arange(fmin, fmax)\n",
    "        n_cycles = freqs / 3\n",
    "        con = mne_connectivity.spectral_connectivity_epochs(epoch, method='coh', sfreq=int(fs),\n",
    "                                                            mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                                            cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False)\n",
    "        coh = con.get_data(output='dense')\n",
    "        indices = con.names\n",
    "        aon_vHp_con = []\n",
    "        for i in range(coh.shape[0]):\n",
    "            for j in range(coh.shape[1]):\n",
    "                if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                    coherence= coh[i,j,:,:]\n",
    "                    coherence=np.arctanh(coherence)\n",
    "                    aon_vHp_con.append(coherence)\n",
    "        \n",
    "        mean_con = np.mean(aon_vHp_con, axis=0)\n",
    "        return mean_con\n",
    "mean_con_data['around_dig_mean_con'] = con_data_df_clean['mne_epoch_around_dig'].apply(epoch_coherogram)\n",
    "mean_con_data['around_door_mean_con'] = con_data_df_clean['mne_epoch_around_door'].apply(epoch_coherogram)\n",
    "\n",
    "mean_con_data['experiment'] = con_data_df_clean['experiment']\n",
    "mean_con_data['task'] = con_data_df_clean['task']\n",
    "mean_con_data['rat_id'] = con_data_df_clean['rat_id']\n",
    "mean_con_data.dropna(inplace=True)\n",
    "mean_con_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin = mean_con_data['around_dig_mean_con'].apply(np.min).min()\n",
    "vmax = mean_con_data['around_dig_mean_con'].apply(np.max).max()\n",
    "\n",
    "BWcontext_data=mean_con_data[(mean_con_data['task']=='BWcontext')]\n",
    "BWnocontext_data=mean_con_data[(mean_con_data['task']=='BWnocontext')]\n",
    "task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for group_name, group_df in task_data_dict.items():\n",
    "    fig, axs = plt.subplots(group_df.shape[0] // 5 + int(group_df.shape[0] % 5 != 0), 5, figsize=(25, 10), sharex=True, sharey=True)\n",
    "    axs = axs.flatten()\n",
    "    for i, (idx, row) in enumerate(group_df.iterrows()):\n",
    "        data = np.array(row['around_dig_mean_con'])\n",
    "        ax = axs[i]\n",
    "        im = ax.imshow(data, extent=[-0.7, 0.7, 1, 100], aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        ax.set_title(f\"{row['rat_id']} {row['experiment']}\")\n",
    "        ax.axvline(0, color='k', linestyle='--', linewidth=2)\n",
    "        ax.axhline(12, color='green', linestyle='--')\n",
    "        ax.axhline(30, color='green', linestyle='--')\n",
    "    for j in range(i + 1, len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "    fig.suptitle(f\"{group_name} AON-vHp Coherence Around Dig\", fontsize=16)\n",
    "    fig.colorbar(im, ax=axs, orientation='vertical', fraction=0.02)\n",
    "    #fig.savefig(savepath + f'coherence_around_dig_{group_name}.png', dpi=300, bbox_inches='tight')\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Coherence Boxplots "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Aon-vHp connectivity per band and storing it in pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "importlib.reload(coherence_functions)\n",
    "con_data_df_clean=pd.read_pickle(savepath+f'mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "rat_list=['dk1', 'dk5', 'dk6']\n",
    "single_baseline_epoch=con_data_df_clean['mne_epoch_door_before'].iloc[0]\n",
    "theta_band=[4,8]\n",
    "\n",
    "theta_coherence=coherence_functions.convert_epoch_to_coherence_time(single_baseline_epoch)\n",
    "print(theta_coherence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(coherence_functions)\n",
    "time_window = 0.4\n",
    "fs=2000\n",
    "\n",
    "con_data_df_clean=pd.read_pickle(savepath+f'mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "con_data_df_clean['coherence_door_before']=con_data_df_clean['mne_epoch_door_before'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_mt(x))\n",
    "con_data_df_clean['coherence_door_after']=con_data_df_clean['mne_epoch_door_after'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_mt(x))\n",
    "con_data_df_clean['coherence_dig_before']=con_data_df_clean['mne_epoch_dig_before'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_mt(x))\n",
    "con_data_df_clean['coherence_dig_after']=con_data_df_clean['mne_epoch_dig_after'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_mt(x))\n",
    "con_data_df_clean.drop(columns=['mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after'], inplace=True)\n",
    "con_data_df_clean.to_pickle(savepath+f'coherence_boxplot_mt_{int(fs*time_window)}.pkl')\n",
    "\n",
    "con_data_df_clean=pd.read_pickle(savepath+f'mne_epochs_array_df_shuffled_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "con_data_df_clean['coherence_door_before']=con_data_df_clean['mne_epoch_door_before'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_mt(x))\n",
    "con_data_df_clean['coherence_door_after']=con_data_df_clean['mne_epoch_door_after'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_mt(x))\n",
    "con_data_df_clean['coherence_dig_before']=con_data_df_clean['mne_epoch_dig_before'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_mt(x))\n",
    "con_data_df_clean['coherence_dig_after']=con_data_df_clean['mne_epoch_dig_after'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_mt(x))\n",
    "con_data_df_clean.drop(columns=['mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after'], inplace=True)\n",
    "con_data_df_clean.to_pickle(savepath+f'coherence_boxplot_mt_shuffled_{int(fs*time_window)}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 0.4\n",
    "fs=2000\n",
    "con_data_df_clean=pd.read_pickle(savepath+f'coherence_boxplot_mt_{int(fs*time_window)}.pkl')\n",
    "fig, axs = plt.subplots(1, 4, figsize=(40, 10), sharey=True)\n",
    "axs = axs.flatten()\n",
    "for ax in axs:\n",
    "    ax.tick_params(axis='y', which='both', labelleft=True)\n",
    "writer=pd.ExcelWriter(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\coherence_per_band_{int(time_window*fs)}.xlsx')\n",
    "events_dict={'coherence_door_before':'Pre Door', 'coherence_door_after': 'Post Door', 'coherence_dig_before':'Pre Dig', 'coherence_dig_after':'Post Dig'}\n",
    "for i, event in enumerate(events_dict.keys()):\n",
    "    event_data = con_data_df_clean[event]\n",
    "    event_data_df = pd.DataFrame(event_data.tolist())\n",
    "    event_data_df.reset_index(drop=True, inplace=True)\n",
    "    event_data_df['rat_id'] = con_data_df_clean['rat_id'].reset_index(drop=True)\n",
    "    event_data_df['task'] = con_data_df_clean['task'].reset_index(drop=True)\n",
    "    event_data_df_melted = pd.melt(event_data_df, id_vars=['rat_id', 'task'], value_vars=['total', 'theta', 'beta', 'gamma'], var_name='band', value_name='coherence')\n",
    "    sns.barplot(x='band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=event_data_df_melted, legend=True, ax=axs[i])\n",
    "    sns.stripplot(x='band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=event_data_df_melted, dodge=True, edgecolor='black', linewidth=1, jitter=True, legend=False, ax=axs[i])\n",
    "    #axs[i].set_xticklabels(['Total', 'Theta', 'Beta', 'Gamma'])\n",
    "    axs[i].legend(title='Task', fontsize=20, title_fontsize=20, loc='upper right')\n",
    "    \n",
    "    axs[i].set_title(f'{events_dict[event]}', fontsize=20)\n",
    "    if i == 0:\n",
    "        axs[i].set_ylabel('Coherence (Z-transformed)', fontsize=20)\n",
    "    else:\n",
    "        axs[i].set_ylabel('')\n",
    "    axs[i].set_xlabel('')\n",
    "    axs[i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    event_data_df_melted.to_excel(writer, sheet_name=event)\n",
    "writer.close()\n",
    "fig.savefig(savepath+f'coherence_per_band_{int(time_window*fs)}.png', format='png',dpi=300, bbox_inches='tight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\"\"\"Shuffled coherence boxplot per band\"\"\"\n",
    "\n",
    "time_window = 0.4\n",
    "fs=2000\n",
    "con_data_df_clean=pd.read_pickle(savepath+f'coherence_boxplot_mt_shuffled_{int(fs*time_window)}.pkl')\n",
    "fig, axs = plt.subplots(1, 4, figsize=(40, 10), sharey=True)\n",
    "axs = axs.flatten()\n",
    "for ax in axs:\n",
    "    ax.tick_params(axis='y', which='both', labelleft=True)\n",
    "writer=pd.ExcelWriter(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\coherence_per_band_shuffled_{int(time_window*fs)}.xlsx')\n",
    "events_dict={'coherence_door_before':'Pre Door', 'coherence_door_after': 'Post Door', 'coherence_dig_before':'Pre Dig', 'coherence_dig_after':'Post Dig'}\n",
    "for i, event in enumerate(events_dict.keys()):\n",
    "    event_data = con_data_df_clean[event]\n",
    "    event_data_df = pd.DataFrame(event_data.tolist())\n",
    "    event_data_df.reset_index(drop=True, inplace=True)\n",
    "    event_data_df['rat_id'] = con_data_df_clean['rat_id'].reset_index(drop=True)\n",
    "    event_data_df['task'] = con_data_df_clean['task'].reset_index(drop=True)\n",
    "    event_data_df_melted = pd.melt(event_data_df, id_vars=['rat_id', 'task'], value_vars=['total', 'theta', 'beta', 'gamma'], var_name='band', value_name='coherence')\n",
    "    sns.barplot(x='band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=event_data_df_melted, legend=True, ax=axs[i])\n",
    "    sns.stripplot(x='band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=event_data_df_melted, dodge=True, edgecolor='black', linewidth=1, jitter=True, legend=False, ax=axs[i])\n",
    "    #axs[i].set_xticklabels(['Total', 'Theta', 'Beta', 'Gamma'])\n",
    "    axs[i].legend(title='Task', fontsize=20, title_fontsize=20, loc='upper right')\n",
    "    \n",
    "    axs[i].set_title(f'{events_dict[event]}', fontsize=20)\n",
    "    if i == 0:\n",
    "        axs[i].set_ylabel('Coherence (Z-transformed)', fontsize=20)\n",
    "    else:\n",
    "        axs[i].set_ylabel('')\n",
    "    axs[i].set_xlabel('')\n",
    "    axs[i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    event_data_df_melted.to_excel(writer, sheet_name=event)\n",
    "writer.close()\n",
    "fig.savefig(savepath+f'coherence_per_band_shuffled_{int(time_window*fs)}.png', format='png',dpi=300, bbox_inches='tight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.anova import AnovaRM\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "\n",
    "con_data_df_clean=pd.read_pickle(savepath+f'coherence_boxplot_mt_shuffled_{int(fs*time_window)}.pkl')\n",
    "fig, axs = plt.subplots(1, 4, figsize=(40, 10), sharey=True)\n",
    "axs = axs.flatten()\n",
    "for ax in axs:\n",
    "    ax.tick_params(axis='y', which='both', labelleft=True)\n",
    "writer=pd.ExcelWriter(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\coherence_boxplot_mt_shuffled_{int(fs*time_window)}.xlsx')\n",
    "events_dict={'coherence_door_before':'Pre Door', 'coherence_door_after': 'Post Door', 'coherence_dig_before':'Pre Dig', 'coherence_dig_after':'Post Dig'}\n",
    "for i, event in enumerate(events_dict.keys()):\n",
    "    event_data = con_data_df_clean[event]\n",
    "    event_data_df = pd.DataFrame(event_data.tolist())\n",
    "    event_data_df.reset_index(drop=True, inplace=True)\n",
    "    event_data_df['rat_id'] = con_data_df_clean['rat_id'].reset_index(drop=True)\n",
    "    event_data_df['task'] = con_data_df_clean['task'].reset_index(drop=True)\n",
    "    event_data_df_melted = pd.melt(event_data_df, id_vars=['rat_id', 'task'], value_vars=['total', 'theta', 'beta', 'gamma'], var_name='band', value_name='coherence')\n",
    "    sns.barplot(x='band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=event_data_df_melted, legend=True, ax=axs[i])\n",
    "    sns.stripplot(x='band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=event_data_df_melted, dodge=True, edgecolor='black', linewidth=1, jitter=True, legend=False, ax=axs[i])\n",
    "    #axs[i].set_xticklabels(['Total', 'Theta', 'Beta', 'Gamma'])\n",
    "    axs[i].legend(title='Task', fontsize=20, title_fontsize=20, loc='upper right')\n",
    "    \n",
    "    axs[i].set_title(f'{events_dict[event]}', fontsize=20)\n",
    "    if i == 0:\n",
    "        axs[i].set_ylabel('Coherence (A.U.)', fontsize=20)\n",
    "    else:\n",
    "        axs[i].set_ylabel('')\n",
    "    axs[i].set_xlabel('')\n",
    "    axs[i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    event_data_df_melted.to_excel(writer, sheet_name=event)\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    # Prepare data for repeated measures ANOVA\n",
    "    # Each rat_id is a subject, band is within-subject, task is between-subject\n",
    "    anova_results = {}\n",
    "    posthoc_results = {}\n",
    "\n",
    "    # Only keep rats that have both tasks for proper repeated measures\n",
    "    rats_with_both = event_data_df_melted.groupby('rat_id')['task'].nunique()\n",
    "    rats_with_both = rats_with_both[rats_with_both == 2].index.tolist()\n",
    "    filtered_df = event_data_df_melted[event_data_df_melted['rat_id'].isin(rats_with_both)]\n",
    "\n",
    "    # Pivot to wide format for repeated measures ANOVA\n",
    "    for band in ['total', 'theta', 'beta', 'gamma']:\n",
    "        band_df = filtered_df[filtered_df['band'] == band]\n",
    "        # ANOVA: repeated measures on band, between on task\n",
    "        # For each rat, we need both tasks\n",
    "        # We'll use a mixed-effects model for repeated measures\n",
    "        model = ols('coherence ~ C(task)', data=band_df).fit()\n",
    "        aov_table = sm.stats.anova_lm(model, typ=2)\n",
    "        anova_results[band] = aov_table\n",
    "\n",
    "        # Posthoc: LSD (least significant difference) test\n",
    "        mc = MultiComparison(band_df['coherence'], band_df['task'])\n",
    "        posthoc = mc.tukeyhsd()  # Tukey is more conservative, but LSD is not directly available in statsmodels\n",
    "        posthoc_results[band] = posthoc.summary()\n",
    "        print(f\"ANOVA results for {band} band in {events_dict[event]}\")\n",
    "        print(aov_table)\n",
    "        print(f\"Posthoc (Tukey HSD) results for {band} band:\")\n",
    "        print(posthoc.summary())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting AON-vHp connectivity separated by Bands ## [NOT USED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df_clean=pd.read_pickle(savepath+'coherence_boxplot_per_event_per_band_single_value.pkl')\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20, 10))\n",
    "axs = axs.flatten()\n",
    "writer = pd.ExcelWriter(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\coherence_band_per_event.xlsx')\n",
    "\n",
    "bands = ['total', 'theta', 'beta', 'gamma']\n",
    "events = ['coherence_door_before', 'coherence_door_after', 'coherence_dig_before', 'coherence_dig_after']\n",
    "\n",
    "for i, band in enumerate(bands):\n",
    "    band_data = []\n",
    "    for event in events:\n",
    "        event_data = con_data_df_clean[event]\n",
    "        event_data_df = pd.DataFrame(event_data.tolist())\n",
    "        event_data_df.reset_index(drop=True, inplace=True)\n",
    "        event_data_df['rat_id'] = con_data_df_clean['rat_id'].reset_index(drop=True)\n",
    "        event_data_df['task'] = con_data_df_clean['task'].reset_index(drop=True)\n",
    "        event_data_df['event'] = event\n",
    "        event_data_df['band'] = band\n",
    "        event_data_df['coherence'] = event_data_df[band]\n",
    "        band_data.append(event_data_df[['rat_id', 'task', 'event', 'band', 'coherence']])\n",
    "    \n",
    "    band_data_df = pd.concat(band_data, ignore_index=True)\n",
    "    sns.boxplot(x='event', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=band_data_df, showfliers=False, legend=False, ax=axs[i])\n",
    "    sns.stripplot(x='event', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=band_data_df, dodge=True, edgecolor='black', linewidth=1, jitter=True, legend=False, ax=axs[i])\n",
    "    axs[i].set_xticklabels(['Door Before', 'Door After', 'Dig Before', 'Dig After'], rotation=0)\n",
    "    axs[i].set_title(band.capitalize())\n",
    "    axs[i].set_ylabel('Coherence')\n",
    "    axs[i].set_xlabel('')\n",
    "    band_data_df.to_excel(writer, sheet_name=band)\n",
    "\n",
    "writer.close()\n",
    "\n",
    "# Create custom legend handles and labels\n",
    "from matplotlib.lines import Line2D\n",
    "colors = {'BWnocontext': '#ff7f0e', 'BWcontext': '#1f77b4'}\n",
    "\n",
    "handles = [\n",
    "    Line2D([0], [0], color=colors['BWcontext'], marker='o', linestyle='', markersize=10, label='BWcontext'),\n",
    "    Line2D([0], [0], color=colors['BWnocontext'], marker='o', linestyle='', markersize=10, label='BWnocontext')\n",
    "]\n",
    "\n",
    "# Add the custom legend to the figure\n",
    "fig.legend(handles=handles, loc='upper right', bbox_to_anchor=(1.1, 1), title='Task')\n",
    "fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\coherence_band_per_event.png', dpi=600)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same boxplot as above but for a single band ## [NOT USED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df_clean=pd.read_pickle(savepath+'coherence_boxplot_per_event_per_band_single_value.pkl')\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "bands = ['beta']\n",
    "events = ['coherence_door_before', 'coherence_door_after', 'coherence_dig_before', 'coherence_dig_after']\n",
    "\n",
    "for i, band in enumerate(bands):\n",
    "    band_data = []\n",
    "    for event in events:\n",
    "        event_data = con_data_df_clean[event]\n",
    "        event_data_df = pd.DataFrame(event_data.tolist())\n",
    "        event_data_df.reset_index(drop=True, inplace=True)\n",
    "        event_data_df['rat_id'] = con_data_df_clean['rat_id'].reset_index(drop=True)\n",
    "        event_data_df['task'] = con_data_df_clean['task'].reset_index(drop=True)\n",
    "        event_data_df['event'] = event\n",
    "        event_data_df['band'] = band\n",
    "        event_data_df['coherence'] = event_data_df[band]\n",
    "        band_data.append(event_data_df[['rat_id', 'task', 'event', 'band', 'coherence']])\n",
    "    \n",
    "    band_data_df = pd.concat(band_data, ignore_index=True)\n",
    "    sns.boxplot(x='event', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=band_data_df, showfliers=False, legend=False, ax=axs)\n",
    "    sns.stripplot(x='event', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=band_data_df, dodge=True, edgecolor='black', linewidth=1, jitter=True, legend=False, ax=axs)\n",
    "    axs.set_xticklabels(['Pre Door', 'Post Door', 'Pre Dig', 'Post Dig'], rotation=0)\n",
    "    axs.set_title(band.capitalize()+' Band Coherence between AON and vHp', fontsize=20)\n",
    "    \n",
    "    axs.set_ylabel('Coherence', fontsize=20)\n",
    "    axs.set_xlabel('Behavior Events', fontsize=20)\n",
    "    axs.tick_params(axis='both', which='major', labelsize=20)\n",
    "    axs.tick_params(axis='both', which='minor', labelsize=20)\n",
    "    #axs.legend(title='', fontsize=20, loc='upper right' )\n",
    "# # Create custom legend handles and labels\n",
    "from matplotlib.lines import Line2D\n",
    "colors = {'BWnocontext': '#ff7f0e', 'BWcontext': '#1f77b4'}\n",
    "\n",
    "handles = [\n",
    "    Line2D([0], [0], color=colors['BWcontext'], marker='o', linestyle='', markersize=10, label='Context'),\n",
    "    Line2D([0], [0], color=colors['BWnocontext'], marker='o', linestyle='', markersize=10, label='No Context')\n",
    "]\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add the custom legend to the figure\n",
    "fig.legend(handles=handles, loc='upper right', bbox_to_anchor=(0.4, 0.95), title='', fontsize=20, ncol=1)\n",
    "fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\coherence_beta_band_per_event.png', dpi=600, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase Based Connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating phase coherograms for each experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df_clean=pd.read_pickle(savepath+'mne_epochs_array_df_shuffled_truncated_1400.pkl')\n",
    "\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "\n",
    "print(event_list)\n",
    "\n",
    "test_list = [con_data_df_clean.iloc[0]]\n",
    "mean_con_data=pd.DataFrame()\n",
    "def epoch_coherogram(epoch, fmin=1, fmax=100, fs=2000):\n",
    "    print(epoch.events.shape)\n",
    "    if epoch.events.shape[0] < 5:\n",
    "        print(\"Not enough events in the epoch\")\n",
    "        return None\n",
    "    else:\n",
    "        freqs = np.arange(fmin, fmax)\n",
    "        n_cycles = freqs / 3\n",
    "        con = mne_connectivity.spectral_connectivity_epochs(epoch, method='pli', sfreq=int(fs),\n",
    "                                                            mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                                            cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False)\n",
    "        coh = con.get_data(output='dense')\n",
    "        indices = con.names\n",
    "        aon_vHp_con = []\n",
    "        for i in range(coh.shape[0]):\n",
    "            for j in range(coh.shape[1]):\n",
    "                if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                    coherence= coh[i,j,:,:]\n",
    "                    #coherence=np.arctanh(coherence)\n",
    "                    aon_vHp_con.append(coherence)\n",
    "        \n",
    "        mean_con = np.mean(aon_vHp_con, axis=0)\n",
    "        return mean_con\n",
    "test_pli = epoch_coherogram(test_list[0]['mne_epoch_around_door'])\n",
    "plt.imshow(test_pli, extent=[-0.7, 0.7, 1, 100], aspect='auto', origin='lower', cmap='jet')\n",
    "plt.colorbar()\n",
    "\n",
    "mean_con_data['around_dig_mean_con'] = con_data_df_clean['mne_epoch_around_dig'].apply(epoch_coherogram)\n",
    "mean_con_data['around_door_mean_con'] = con_data_df_clean['mne_epoch_around_door'].apply(epoch_coherogram)\n",
    "\n",
    "mean_con_data['experiment'] = con_data_df_clean['experiment']\n",
    "mean_con_data['task'] = con_data_df_clean['task']\n",
    "mean_con_data['rat_id'] = con_data_df_clean['rat_id']\n",
    "mean_con_data.dropna(inplace=True)\n",
    "mean_con_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin = mean_con_data['around_dig_mean_con'].apply(np.min).min()\n",
    "vmax = mean_con_data['around_dig_mean_con'].apply(np.max).max()\n",
    "\n",
    "BWcontext_data=mean_con_data[(mean_con_data['task']=='BWcontext')]\n",
    "BWnocontext_data=mean_con_data[(mean_con_data['task']=='BWnocontext')]\n",
    "task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for group_name, group_df in task_data_dict.items():\n",
    "    fig, axs = plt.subplots(group_df.shape[0] // 5 + int(group_df.shape[0] % 5 != 0), 5, figsize=(25, 10), sharex=True, sharey=True)\n",
    "    axs = axs.flatten()\n",
    "    for i, (idx, row) in enumerate(group_df.iterrows()):\n",
    "        data = np.array(row['around_dig_mean_con'])\n",
    "        ax = axs[i]\n",
    "        im = ax.imshow(data, extent=[-0.7, 0.7, 1, 100], aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        ax.set_title(f\"{row['rat_id']} {row['experiment']}\")\n",
    "        ax.axvline(0, color='k', linestyle='--', linewidth=2)\n",
    "        ax.axhline(12, color='green', linestyle='--')\n",
    "        ax.axhline(30, color='green', linestyle='--')\n",
    "    for j in range(i + 1, len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "    fig.suptitle(f\"{group_name} AON-vHp Phase Lock Value Around Dig\", fontsize=16)\n",
    "    fig.colorbar(im, ax=axs, orientation='vertical', fraction=0.02)\n",
    "    fig.savefig(savepath + f'shuffled_pli_around_dig_{group_name}.png', dpi=300, bbox_inches='tight')\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average PLI around door and dig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df_clean=pd.read_pickle(savepath+'mne_epochs_array_df_truncated_1400.pkl')\n",
    "\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "\n",
    "print(event_list)\n",
    "BWcontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWcontext')]\n",
    "BWnocontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWnocontext')]\n",
    "task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "\n",
    "\n",
    "all_con_data=[]\n",
    "all_con_data_mean=[]\n",
    "for task_num,task_name in enumerate(task_data_dict.keys()):\n",
    "        task_data=task_data_dict[task_name]\n",
    "        row=[task_name]\n",
    "         #print(row)\n",
    "        row_2=[task_name]\n",
    "        for event in event_list:\n",
    "            #print(event)\n",
    "            event_epoch_list=task_data[event]\n",
    "            aon_vHp_con=[]\n",
    "            for event_epoch in event_epoch_list:\n",
    "                    #print(row,event, event_epoch) \n",
    "                    if event_epoch.events.shape[0] <5:\n",
    "                        print(f\"Skipping {event} for {task_name} due to insufficient events\")\n",
    "                        continue\n",
    "                    fmin=1\n",
    "                    fmax=100\n",
    "                    fs=2000\n",
    "                    freqs = np.arange(fmin,fmax)\n",
    "                    n_cycles = freqs/3\n",
    "                    # con= mne_connectivity.spectral_connectivity_time(event_epoch, method='coh', sfreq=int(fs), average=False,\n",
    "                    #                                      mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                    #                                      n_cycles=n_cycles, verbose=False, fmin=1, fmax=100, faverage=False)\n",
    "                    # coh = con.get_data(output='dense')\n",
    "                    # indices = con.names\n",
    "                    # print(coh.shape, indices)a\n",
    "                    # for i in range(coh.shape[0]):\n",
    "                    #     for j in range(coh.shape[1]):\n",
    "                    #         if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                    #             coherence= coh[i,j,:]\n",
    "                    #             coherence=np.arctanh(coherence)\n",
    "                    #             aon_vHp_con.append(coherence)\n",
    "\n",
    "                    con = mne_connectivity.spectral_connectivity_epochs(event_epoch, method='pli', sfreq=int(fs),\n",
    "                                                         mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                                         cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False)\n",
    "                    coh = con.get_data(output='dense')\n",
    "                    indices = con.names\n",
    "                    \n",
    "\n",
    "                    for i in range(coh.shape[0]):\n",
    "                        for j in range(coh.shape[1]):\n",
    "                            if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                                coherence= coh[i,j,:,:]\n",
    "                                #coherence=np.arctanh(coherence)\n",
    "                                aon_vHp_con.append(coherence)\n",
    "            row.append(np.mean(aon_vHp_con, axis=0))\n",
    "            row_2.append(np.mean(aon_vHp_con))\n",
    "        all_con_data.append(row)                    \n",
    "        all_con_data_mean.append(row_2)\n",
    "# Convert all_con_data to a DataFrame for easier manipulation\n",
    "all_con_data_df = pd.DataFrame(all_con_data, columns=['task'] + event_list)\n",
    "all_con_data_df.to_pickle(savepath+'pli_coherogram_around_door_dig_truncated.pkl')\n",
    "fs=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_con_data_df=pd.read_pickle(savepath+'pli_coherogram_around_door_dig_truncated.pkl')\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "fs=2000\n",
    "times=np.arange(-0.7, 0.7, 1/fs)\n",
    "fig, axs=plt.subplots(2,2, figsize=(20,10), sharey=True)\n",
    "fig.suptitle('AON-vHp Phase Lag Index Around Door and Dig', fontsize=20)\n",
    "vmin = all_con_data_df[event_list].applymap(np.min).min().min()\n",
    "vmax = all_con_data_df[event_list].applymap(np.max).max().max()\n",
    "event_names=['Around Door','Around Dig']\n",
    "for i, event in enumerate(event_list):\n",
    "    axs[0,i].imshow(all_con_data_df[event][0], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=0, vmax=1)\n",
    "    axs[0,i].set_xlabel('')\n",
    "    axs[0,i].set_ylabel('Frequency (Hz)', fontsize=20)\n",
    "    axs[0,i].set_title(event_names[i], fontsize=20)\n",
    "    axs[0,i].vlines(0, 0, 100, color='k', linestyle='--')\n",
    "    axs[0,i].hlines(12, times[0], times[-1], color='green', linestyle='--')\n",
    "    axs[0,i].hlines(30, times[0], times[-1], color='green', linestyle='--')\n",
    "    \n",
    "    axs[1,i].imshow(all_con_data_df[event][1], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=0, vmax=1)\n",
    "    axs[1,i].set_xlabel('Time (s)', fontsize=20)\n",
    "    axs[1,i].set_ylabel('Frequency (Hz)', fontsize=20)\n",
    "    axs[1,i].set_title(event_names[i], fontsize=20)\n",
    "    axs[1,i].vlines(0, 0, 100, color='k', linestyle='--')\n",
    "    axs[1,i].hlines(12, times[0], times[-1], color='green', linestyle='--')\n",
    "    axs[1,i].hlines(30, times[0], times[-1], color='green', linestyle='--')\n",
    "    \n",
    "    axs[0,0].text(-0.2, 0.5, 'Context', transform=axs[0,0].transAxes, fontsize=18, verticalalignment='center', rotation=90)\n",
    "    axs[1,0].text(-0.2, 0.5, 'No Context', transform=axs[1,0].transAxes, fontsize=18, verticalalignment='center', rotation=90)\n",
    "    axs[0,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    axs[1,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    # axs[0,i].set_xticks(np.arange(-2, 3, 1))  # Set x-ticks from -2 to 2 seconds\n",
    "    # axs[0,i].set_xticklabels(np.arange(-2, 3, 1))  # Set x-tick labels from -2 to 2 seconds\n",
    "    # axs[1,i].set_xticks(np.arange(-2, 3, 1))  # Set x-ticks from -2 to 2 seconds\n",
    "    # axs[1,i].set_xticklabels(np.arange(-2, 3, 1))  # Set x-tick labels from -2 to 2 seconds\n",
    "\n",
    "    # Add a colorbar\n",
    "cbar = fig.colorbar(axs[0,0].images[0], ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "cbar.set_label('PLI', loc='center', fontsize=20, labelpad=10)\n",
    "cbar.ax.tick_params(labelsize=20)  # Set colorbar tick label size\n",
    "\n",
    "fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\aon_vhp_pli_coherogram.png',format='png', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase Slope Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df_clean=pd.read_pickle(savepath+'mne_epochs_array_df_truncated.pkl')\n",
    "\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "\n",
    "print(event_list)\n",
    "\n",
    "test_list = [con_data_df_clean.iloc[0]]\n",
    "mean_con_data=pd.DataFrame()\n",
    "#epoch_psi(epoch, fmin=1, fmax=100, fs=2000):\n",
    "def epoch_psi(epoch, fmin=12, fmax=30, fs=2000):\n",
    "    print(epoch.events.shape)\n",
    "    aon_indices = [i for i, ch in enumerate(epoch.ch_names) if 'AON' in ch]\n",
    "    vHp_indices = [i for i, ch in enumerate(epoch.ch_names) if 'vHp' in ch]\n",
    "    indices = mne_connectivity.seed_target_indices(aon_indices, vHp_indices)\n",
    "    fmin = 12\n",
    "    fmax = 30\n",
    "    fs = 2000\n",
    "    if epoch.events.shape[0] < 5:\n",
    "        print(\"Not enough events in the epoch\")\n",
    "        # Return empty arrays or np.nan to avoid TypeError\n",
    "        return [], []\n",
    "    else:\n",
    "        freqs = np.arange(fmin, fmax)\n",
    "        n_cycles = freqs / 3\n",
    "        con = mne_connectivity.phase_slope_index(\n",
    "            epoch, indices=indices, sfreq=int(fs),\n",
    "            mode='cwt_morlet', cwt_freqs=freqs,\n",
    "            cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax\n",
    "        )\n",
    "        coh = con.get_data()\n",
    "        print(coh.shape)\n",
    "        indices = con.names\n",
    "\n",
    "        mean_con = np.mean(coh, axis=0)\n",
    "        mean_con = list(mean_con[0, :])\n",
    "        all_cons = np.array([coh[i, 0, :] for i in range(coh.shape[0])])\n",
    "        return mean_con, all_cons\n",
    "epoch = test_list[0]['mne_epoch_around_door']\n",
    "mean_con, all_cons = epoch_psi(epoch)\n",
    "\n",
    "# Generate time axis matching the length of mean_con (assuming -2 to 2 seconds)\n",
    "times = np.linspace(-0.7, 0.7, len(mean_con))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(times, mean_con, label='AON-vHp PSI')\n",
    "\n",
    "\n",
    "psi_data_df = pd.DataFrame()\n",
    "psi_data_df['around_dig_mean_con'], psi_data_df['around_dig_all_cons'] = zip(*con_data_df_clean['mne_epoch_around_dig'].apply(epoch_psi))\n",
    "psi_data_df['around_door_mean_con'], psi_data_df['around_door_all_cons'] = zip(*con_data_df_clean['mne_epoch_around_door'].apply(epoch_psi))\n",
    "psi_data_df['experiment'] = con_data_df_clean['experiment']\n",
    "psi_data_df['task'] = con_data_df_clean['task']\n",
    "psi_data_df['rat_id'] = con_data_df_clean['rat_id']\n",
    "psi_data_df.dropna(inplace=True)\n",
    "psi_data_df = psi_data_df[psi_data_df['around_dig_mean_con'].apply(lambda x: len(x) > 0) & psi_data_df['around_door_mean_con'].apply(lambda x: len(x) > 0)]\n",
    "psi_data_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 10), sharex=True, sharey=True)\n",
    "BWcontext_data = psi_data_df[(psi_data_df['task'] == 'BWcontext')]\n",
    "BWnocontext_data = psi_data_df[(psi_data_df['task'] == 'BWnocontext')]\n",
    "\n",
    "#BWcontext_data_mean_con = np.mean(np.array(BWcontext_data['around_dig_mean_con']), axis=0)\n",
    "\n",
    "#ax.plot(times, psi_data_df['around_dig_mean_con'], label='AON-vHp PSI Around Dig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Stack the lists vertically and compute the mean across axis 0\n",
    "bwcontext_stacked = np.vstack(BWcontext_data['around_dig_mean_con'].values)\n",
    "bwcontext_mean = np.mean(bwcontext_stacked, axis=0)\n",
    "\n",
    "bwnocontext_stacked = np.vstack(BWnocontext_data['around_dig_mean_con'].values)\n",
    "bwnocontext_mean = np.mean(bwnocontext_stacked, axis=0)\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(20, 10))\n",
    "ax.plot(times, bwnocontext_mean, label=' BW No Context', color='orange')\n",
    "ax.plot(times, bwcontext_mean, label=' BW Context', color='blue')\n",
    "ax.set_title('AON-vHp PSI Beta Band Around Dig', fontsize=20)\n",
    "plt.axhline(0, color='k', linestyle='--')\n",
    "plt.axvline(0, color='k', linestyle='-', linewidth=2)\n",
    "ax.set_xlabel('Time (s)', fontsize=20)\n",
    "ax.set_ylabel('Phase Slope Index', fontsize=20)\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "#ax.legend(handles, [task_dict[l] for l in labels], loc='upper left', fontsize=15)\n",
    "fig.savefig(savepath + 'aon_vhp_psi_around_dig.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=test_list[0]['mne_epoch_around_door']\n",
    "epoch.ch_names\n",
    "\n",
    "print(aon_indices, vHp_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_con_data['around_dig_mean_con'] = con_data_df_clean['mne_epoch_around_dig'].apply(epoch_coherogram)\n",
    "mean_con_data['around_door_mean_con'] = con_data_df_clean['mne_epoch_around_door'].apply(epoch_coherogram)\n",
    "\n",
    "mean_con_data['experiment'] = con_data_df_clean['experiment']\n",
    "mean_con_data['task'] = con_data_df_clean['task']\n",
    "mean_con_data['rat_id'] = con_data_df_clean['rat_id']\n",
    "mean_con_data.dropna(inplace=True)\n",
    "mean_con_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference in coherence between BWContext and BWnOContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_con_data_df_net=all_con_data_df.__deepcopy__()\n",
    "all_con_data_df_net.set_index('task', inplace=True)\n",
    "all_con_data_df_net.loc['difference'] = all_con_data_df_net.loc['BWcontext'] - all_con_data_df_net.loc['BWnocontext']\n",
    "all_con_data_df_net.reset_index(inplace=True)\n",
    "\n",
    "fs=2000\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "times=np.arange(-2, 2, 1/fs)\n",
    "fig, axs=plt.subplots(1,2, figsize=(20,10), sharey=True)\n",
    "fig.suptitle('Difference in Coherence between BW Context and BW No Context')\n",
    "axs=axs.flatten()\n",
    "vmin = all_con_data_df_net[event_list].applymap(np.min).min().min()\n",
    "vmax = all_con_data_df_net[event_list].applymap(np.max).max().max()\n",
    "event_names=['Around Door','Around Dig']\n",
    "for i, event in enumerate(event_list):\n",
    "    axs[i].imshow(all_con_data_df_net[event][2], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[i].set_xlabel('Time (s)')\n",
    "    axs[i].set_ylabel('Frequency (Hz)')\n",
    "    axs[i].set_title(event_names[i])\n",
    "    axs[i].vlines(0, 0, 100, color='k', linestyle='--')\n",
    "\n",
    "cbar = fig.colorbar(axs[0].images[0], ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "cbar.set_label('Coherence')\n",
    "#fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\aon_vhp_coherence_event_spectrogram.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase Difference manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df_clean=pd.read_pickle(savepath+'mne_epochs_array_df_truncated_1400.pkl')\n",
    "\n",
    "\n",
    "for row in con_data_df_clean.itertuples(index=False):\n",
    "    experiment = row.experiment\n",
    "    rat_id = row.rat_id\n",
    "    task = row.task\n",
    "    mne_epoch = row.mne_epoch_door_before\n",
    "    data_around_dig = row.mne_epoch_around_dig\n",
    "    data_before_dig = row.mne_epoch_dig_before\n",
    "    data_after_dig = row.mne_epoch_dig_after\n",
    "    data_before_door = row.mne_epoch_door_before\n",
    "    data_after_door = row.mne_epoch_door_after\n",
    "\n",
    "    event_of_interest = data_before_dig ### CHANGE THIS TO THE DESIRED EVENT\n",
    "\n",
    "    print(f'Processing Rat: {rat_id}, Experiment: {experiment}, Task: {task}')\n",
    "    print(event_of_interest.get_data().shape)  # Should be (n_epochs, n_channels,n_times)\n",
    "    single_data = event_of_interest.get_data()[0, 0, :]  # Get data for the first channel\n",
    "    print(single_data.shape)  # Should be (n_times,)\n",
    "    fs=2000\n",
    "    l_freq =12\n",
    "    h_freq = 30\n",
    "    iir_filter = mne.filter.create_filter(single_data, sfreq=fs,l_freq=l_freq, h_freq=h_freq, method='iir', verbose=False)\n",
    "    event_of_interest.filter(l_freq=l_freq, h_freq=h_freq, method='iir', iir_params=iir_filter, verbose=False)\n",
    "    event_of_interest.apply_hilbert(envelope=False, n_jobs=1, verbose=False)\n",
    "\n",
    "    aon_indices = [i for i, ch in enumerate(event_of_interest.ch_names) if 'AON' in ch]\n",
    "    vhp_indices = [i for i, ch in enumerate(event_of_interest.ch_names) if 'vHp' in ch]\n",
    "    aon_channels = [event_of_interest.ch_names[i] for i in aon_indices]\n",
    "    vhp_channels = [event_of_interest.ch_names[i] for i in vhp_indices]\n",
    "    print(aon_indices, vhp_indices, aon_channels, vhp_channels)\n",
    "    aon_vhp_pairs = [(aon_ch, vhp_ch) for aon_ch in aon_channels for vhp_ch in vhp_channels]\n",
    "    print(aon_vhp_pairs)\n",
    "    num_of_cols = event_of_interest.get_data().shape[0]\n",
    "    num_of_rows = len(aon_vhp_pairs)\n",
    "\n",
    "    fig, axs = plt.subplots(num_of_rows, num_of_cols, subplot_kw={'projection': 'polar'},figsize=(40, 10))\n",
    "    fig.suptitle(f'AON-vHp Phase Difference Around Dig for Rat: {rat_id}, Experiment: {experiment}, Task: {task}', fontsize=20)\n",
    "    for i, (aon_ch, vhp_ch) in enumerate(aon_vhp_pairs):\n",
    "        for j in range(num_of_cols):\n",
    "            ax = axs[i, j]\n",
    "            ax.set_xticklabels([])          # remove theta labels\n",
    "            ax.set_yticklabels([])          # remove radial labels\n",
    "            # or hide ticks entirely:\n",
    "            #ax.xaxis.set_ticks([])\n",
    "            ax.yaxis.set_ticks([])\n",
    "\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(f'{aon_ch} - {vhp_ch}', fontsize=10)\n",
    "            aon_index = aon_indices[aon_channels.index(aon_ch)]\n",
    "            vhp_index = vhp_indices[vhp_channels.index(vhp_ch)]\n",
    "            aon_epoch_data = np.angle(event_of_interest.get_data()[j,aon_index, :])\n",
    "            vhp_epoch_data = np.angle(event_of_interest.get_data()[j,vhp_index, :])\n",
    "            #print(aon_index, vhp_index, aon_epoch_data.shape, vhp_epoch_data.shape)\n",
    "            phase_diff = aon_epoch_data - vhp_epoch_data\n",
    "            ispc = np.abs(np.mean(np.exp(1j * phase_diff)))\n",
    "            pli = abs(np.mean(np.sign(np.imag(np.exp(1j * phase_diff)))))\n",
    "            phase_diff_wrapped = np.mod(phase_diff, 2 * np.pi)\n",
    "            ax.hist(phase_diff_wrapped, bins=50, color='red', alpha=0.7, density=True)\n",
    "            if i== 0:\n",
    "                ax.set_title(f'trial {j}\\nispc:{ispc:.2f} pli:{pli:.2f}', fontsize=10)\n",
    "            else:\n",
    "                ax.set_title(f'ispc:{ispc:.2f} pli:{pli:.2f}', fontsize=10)\n",
    "    fig.savefig(savepath + f'{task}_{rat_id}_{experiment}_phase_difference_aon_vhp_before_dig.png', dpi=100, bbox_inches='tight')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GC measures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df=pd.read_pickle(savepath+'mne_epochs_array_df.pkl')\n",
    "#con_data_df=pd.DataFrame(con_data_df, columns=['rat_id','task','mne_baseline','mne_epoch_door_before','mne_epoch_door_after',\n",
    "                                               #'mne_epoch_dig_before','mne_epoch_dig_after', 'mne_epoch_around_door', 'mne_epoch_around_dig'])\n",
    "con_data_df_clean=con_data_df[con_data_df['rat_id']!='dk3']\n",
    "con_data_df_clean=con_data_df_clean[con_data_df_clean['task']!='nocontext']\n",
    "\n",
    "\n",
    "def calculate_net_gc(mne_data):\n",
    "        aon_signals=[\n",
    "        idx\n",
    "        for idx, ch_info in enumerate(mne_data.info[\"chs\"])\n",
    "        if \"AON\" in ch_info[\"ch_name\"]\n",
    "        ]\n",
    "        print(aon_signals)\n",
    "        vhp_signals=[\n",
    "            idx\n",
    "            for idx, ch_info in enumerate(mne_data.info[\"chs\"])\n",
    "            if \"vHp\" in ch_info[\"ch_name\"]\n",
    "        ]\n",
    "        print(vhp_signals)\n",
    "\n",
    "        indices_aon_vhp = (np.array([aon_signals]), np.array([vhp_signals]))\n",
    "        indices_vhp_aon = (np.array([vhp_signals]), np.array([aon_signals]))\n",
    "\n",
    "        gc_ab = mne_connectivity.spectral_connectivity_epochs(\n",
    "        mne_data,\n",
    "        method=[\"gc\"],\n",
    "        indices=indices_aon_vhp,\n",
    "        fmin=2.5,\n",
    "        fmax=100,\n",
    "        rank=None,\n",
    "        gc_n_lags=20,\n",
    "        )\n",
    "        freqs = gc_ab.freqs\n",
    "\n",
    "        gc_ba = mne_connectivity.spectral_connectivity_epochs(\n",
    "            mne_data,\n",
    "            method=[\"gc\"],\n",
    "            indices=indices_vhp_aon,\n",
    "            fmin=2.5,\n",
    "            fmax=100,\n",
    "            rank=None,\n",
    "            gc_n_lags=20,\n",
    "        )\n",
    "        freqs = gc_ba.freqs\n",
    "\n",
    "        net_gc = gc_ab.get_data() - gc_ba.get_data()\n",
    "        return net_gc[0], freqs\n",
    "\n",
    "gc_data_df=pd.DataFrame()\n",
    "gc_data_df['rat_id']=con_data_df_clean['rat_id']\n",
    "gc_data_df['task']=con_data_df_clean['task']\n",
    "gc_data_df['door_before']=con_data_df_clean['mne_epoch_door_before'].apply(lambda x: calculate_net_gc(x)[0])\n",
    "gc_data_df['door_after']=con_data_df_clean['mne_epoch_door_after'].apply(lambda x: calculate_net_gc(x)[0])\n",
    "gc_data_df['dig_before']=con_data_df_clean['mne_epoch_dig_before'].apply(lambda x: calculate_net_gc(x)[0])\n",
    "gc_data_df['dig_after']=con_data_df_clean['mne_epoch_dig_after'].apply(lambda x: calculate_net_gc(x)[0])\n",
    "gc_data_df['around_door']=con_data_df_clean['mne_epoch_around_door'].apply(lambda x: calculate_net_gc(x)[0])\n",
    "gc_data_df['around_dig']=con_data_df_clean['mne_epoch_around_dig'].apply(lambda x: calculate_net_gc(x)[0])\n",
    "gc_data_df['around_door_truncated']=con_data_df_clean['mne_epoch_around_door_truncated'].apply(lambda x: calculate_net_gc(x)[0])\n",
    "gc_data_df['around_dig_truncated']=con_data_df_clean['mne_epoch_around_dig_truncated'].apply(lambda x: calculate_net_gc(x)[0])\n",
    "\n",
    "gc_data_df['freqs']=con_data_df_clean['mne_epoch_dig_after'].apply(lambda x: calculate_net_gc(x)[1])\n",
    "gc_data_df['freqs_door']=con_data_df_clean['mne_epoch_door_after'].apply(lambda x: calculate_net_gc(x)[1])\n",
    "gc_data_df['freqs_trunc']=con_data_df_clean['mne_epoch_around_door_truncated'].apply(lambda x: calculate_net_gc(x)[1])\n",
    "\n",
    "gc_data_df=pd.DataFrame(gc_data_df, columns=['rat_id','task','door_before','door_after','dig_before','dig_after','around_door','around_dig','around_door_truncated','around_dig_truncated', 'freqs', 'freqs_door','freqs_trunc'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "gc_data_df_bwcontext=gc_data_df[gc_data_df['task']=='BWcontext']\n",
    "gc_data_df_bwnocontext=gc_data_df[gc_data_df['task']=='BWnocontext']\n",
    "writer=pd.ExcelWriter(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\GC_BWcontext_vs_BWnocontext_AON_vHp.xlsx')\n",
    "\n",
    "fig,axs=plt.subplots(4,2, sharex=True, sharey=True, figsize=(20,10))\n",
    "axs=axs.flatten()\n",
    "fig.suptitle('BW Context vs  BW No Context AON-vHp Granger Causality')\n",
    "events_dict={'door_before': 'Door Before','door_after': 'Door After','dig_before': 'Dig Before','dig_after': 'Dig After','around_door': 'Around Door','around_dig': 'Around Dig','around_door_truncated': 'Around Door Truncated','around_dig_truncated': 'Around Dig Truncated'}\n",
    "for i,event in enumerate(events_dict.keys()):\n",
    "    ax=axs[i]\n",
    "    bwcontext_mean=np.mean(gc_data_df_bwcontext[event], axis=0)\n",
    "    bwnocontext_mean=np.mean(gc_data_df_bwnocontext[event], axis=0)\n",
    "    bwcontext_sem=scipy.stats.sem(gc_data_df_bwcontext[event], axis=0)\n",
    "    bwnocontext_sem=scipy.stats.sem(gc_data_df_bwnocontext[event], axis=0)\n",
    "    \n",
    "    freqs=np.linspace(2.5,100,len(bwcontext_mean))\n",
    "    \n",
    "    mean_dict={'frequency':freqs,'bwcontext':bwcontext_mean,'bwnocontext':bwnocontext_mean,'bwcontext_sem':bwcontext_sem,'bwnocontext_sem':bwnocontext_sem}\n",
    "    mean_df=pd.DataFrame(mean_dict)\n",
    "    mean_df.to_excel(writer, sheet_name=event)\n",
    "\n",
    "    ax.plot((freqs[0], freqs[-1]), (0, 0), linewidth=2, linestyle=\"--\", color=\"k\")\n",
    "    ax.axvspan(4,12, alpha=0.2, color='red', label='Theta Range')\n",
    "    ax.axvspan(12,30, alpha=0.2, color='green', label='Beta Range')\n",
    "    ax.axvspan(30,80, alpha=0.2, color='grey', label='Gamma Range')\n",
    "    ax.plot(freqs, bwcontext_mean, linewidth=2, label='BWcontext mean')\n",
    "    ax.fill_between(freqs, bwcontext_mean - bwcontext_sem, bwcontext_mean + bwcontext_sem, alpha=0.2, label='BWcontext SEM')\n",
    "    ax.plot(freqs, gc_data_df_bwnocontext[event].mean(), linewidth=2, label='BW no context mean')\n",
    "    ax.fill_between(freqs, bwnocontext_mean - bwnocontext_sem, bwnocontext_mean + bwnocontext_sem, alpha=0.2, label='BW no context SEM')    \n",
    "    ax.set_title(f\"{events_dict[event]}\", fontsize=8)\n",
    "    #ax.legend()\n",
    "writer.close()\n",
    "fig.tight_layout()\n",
    "fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\GC_BWcontext_vs_BWnocontext_AON_vHp.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "#%matplotlib qt\n",
    "bands_dict={'total':[2.5,100],'theta': [4,12],'beta':[12,30],'gamma':[30,80], 'beta+theta':[4,30]}\n",
    "\n",
    "def calculate_gc_per_band(gc_array,freqs_array, bands_dict):\n",
    "    freqs_array = np.array(freqs_array)  # Convert freqs_array to numpy array\n",
    "    print(len(gc_array))\n",
    "    gc_bands_dict={}\n",
    "    for band in bands_dict.keys():\n",
    "        band_indices=np.where((freqs_array>=bands_dict[band][0]) & (freqs_array<=bands_dict[band][1]))\n",
    "        gc_band=gc_array[band_indices]\n",
    "\n",
    "        gc_bands_dict[band]=(np.sum(gc_band)*0.5)/len(gc_band)\n",
    "\n",
    "    return gc_bands_dict\n",
    "\n",
    "test_gc=gc_data_df_bwcontext['door_before'].iloc[0]\n",
    "test_freqs=gc_data_df_bwcontext['freqs'].iloc[0]\n",
    "test_gc_band=calculate_gc_per_band(test_gc,test_freqs, bands_dict)\n",
    "\n",
    "gc_cols = ['door_before', 'door_after', 'dig_before', 'dig_after','around_door','around_dig', 'around_door_truncated', 'around_dig_truncated']\n",
    "gc_data_df_bands = []\n",
    "\n",
    "for index, row in gc_data_df.iterrows():\n",
    "    rat_id = row['rat_id']\n",
    "    task = row['task']\n",
    "    for gc_col in gc_cols:\n",
    "        if gc_col=='around_door_truncated' or gc_col=='around_dig_truncated':\n",
    "            freqs = row['freqs_trunc']\n",
    "        elif gc_col=='around_door' or gc_col=='around_dig':\n",
    "            freqs = row['freqs_door']\n",
    "        else:\n",
    "            freqs = row['freqs']\n",
    "        gc_values = calculate_gc_per_band(row[gc_col], freqs, bands_dict)\n",
    "        for band, gc_value in gc_values.items():\n",
    "            gc_data_df_bands.append({\n",
    "                'rat_id': rat_id,\n",
    "                'task': task,\n",
    "                'event': gc_col,\n",
    "                'band': band,\n",
    "                'gcvalue': gc_value\n",
    "            })\n",
    "\n",
    "gc_data_df_bands = pd.DataFrame(gc_data_df_bands)\n",
    "gc_data_df_bands=gc_data_df_bands[gc_data_df_bands['task']!='nocontext']\n",
    "print(gc_data_df_bands)\n",
    "writer=pd.ExcelWriter(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\GC_net_AoN_vHp.xlsx')\n",
    "fig, axs=plt.subplots(4,2, sharex=False, sharey=True, figsize=(15,10))\n",
    "axs=axs.flatten()\n",
    "fig.suptitle('Average Net AON -> vHp granger causality per band')\n",
    "for i, event in enumerate(gc_cols):\n",
    "    print(i, event)\n",
    "    ax=axs[i]\n",
    "    gc_event=gc_data_df_bands[gc_data_df_bands['event']==event]\n",
    "    gc_event.to_excel(writer, sheet_name=event)\n",
    "    ax.axhline(0, color='black', lw=1)\n",
    "    sns.boxplot(x='band',y='gcvalue',hue='task',hue_order=['BWcontext','BWnocontext'],data=gc_event,showfliers=False, ax=ax)\n",
    "    sns.stripplot(x='band',y='gcvalue',hue='task',hue_order=['BWcontext','BWnocontext'],data=gc_event,dodge=True,edgecolor='black',linewidth=1,jitter=True, legend=False, ax=ax)\n",
    "    ax.set_title(f\"{event}\", fontsize=10)\n",
    "    ax.set_xlabel('')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\GC_net_AoN_vHp.png')\n",
    "plt.show()\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making GC Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n",
      "[2, 3]\n",
      "(array([[0, 1]]), array([[2, 3]]))\n",
      "(array([0, 0, 1, 1]), array([2, 3, 2, 3]))\n",
      "Connectivity computation...\n",
      "    computing connectivity for 1 connections\n",
      "    using t=0.000s..1.399s for estimation (2800 points)\n",
      "    frequencies: 2.5Hz..99.5Hz (98 points)\n",
      "Estimated data ranks:\n",
      "    connection 1 - seeds (2); targets (2)\n",
      "    using CWT with Morlet wavelets to estimate spectra\n",
      "    the following metrics will be computed: GC\n",
      "    computing cross-spectral density for epochs 1..16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2283395382.py:39: RuntimeWarning: There were no Annotations stored in <EpochsArray | 20 events (all good), 0 – 1.399 s (baseline off), ~1.7 MB, data loaded,\n",
      " '1': 20>, so metadata was not modified.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2283395382.py:39: RuntimeWarning: fmin=2.500 Hz corresponds to 3.500 < 5 cycles based on the epoch length 1.400 sec, need at least 2.000 sec epochs or fmin=3.571. Spectrum estimate will be unreliable.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of  16 | elapsed:    1.2s remaining:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  16 | elapsed:    1.6s remaining:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  16 | elapsed:    1.9s remaining:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    computing cross-spectral density for epochs 17..20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed:    2.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   4 | elapsed:    0.4s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing GC for connection 1 of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| frequency blocks : 7/7 [01:54<00:00,   16.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Connectivity computation done]\n",
      "(1, 98, 2800) ['LFP3_AON', 'LFP4_AON', 'LFP1_vHp', 'LFP2_vHp']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x15a004459d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Test Case\n",
    "con_data_df_clean=pd.read_pickle(savepath+'mne_epochs_array_df_truncated_1400.pkl')\n",
    "\n",
    "test_epoch = con_data_df_clean['mne_epoch_around_dig'].iloc[0]\n",
    "fmin=2.5\n",
    "fmax=100\n",
    "fs=2000\n",
    "freqs = np.arange(fmin,fmax)\n",
    "n_cycles = freqs/3\n",
    "\n",
    "###Specifying the Indices for AON and vHp channels\n",
    "aon_signals=[\n",
    "idx\n",
    "for idx, ch_info in enumerate(test_epoch.info[\"chs\"])\n",
    "if \"AON\" in ch_info[\"ch_name\"]\n",
    "]\n",
    "print(aon_signals)\n",
    "vhp_signals=[\n",
    "    idx\n",
    "    for idx, ch_info in enumerate(test_epoch.info[\"chs\"])\n",
    "    if \"vHp\" in ch_info[\"ch_name\"]\n",
    "]\n",
    "print(vhp_signals)\n",
    "\n",
    "indices_aon_vhp = (np.array([aon_signals]), np.array([vhp_signals]))\n",
    "print(indices_aon_vhp)\n",
    "import itertools\n",
    "\n",
    "indices_pairs = list(itertools.product(aon_signals, vhp_signals))\n",
    "indices = (\n",
    "    np.array([pair[0] for pair in indices_pairs]),\n",
    "    np.array([pair[1] for pair in indices_pairs])\n",
    ")\n",
    "print(indices)\n",
    "# indices = [([aon], [vhp]) for aon in aon_signals for vhp in vhp_signals]\n",
    "# print(indices)\n",
    "\n",
    "\n",
    "con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n",
    "                                        mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                        cwt_n_cycles=n_cycles, verbose=True, fmin=fmin, fmax=fmax, faverage=False, gc_n_lags=40, n_jobs=-1)\n",
    "coh = con.get_data()\n",
    "indices = con.names\n",
    "aon_vHp_con = []\n",
    "print(coh.shape, indices)\n",
    "\n",
    "plt.imshow(coh[0, :, :], extent=[-0.7, 0.7, 1, 100], aspect='auto', origin='lower', cmap='jet')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 8, Task: BWnocontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: There were no Annotations stored in <EpochsArray | 20 events (all good), 0 – 1.399 s (baseline off), ~1.7 MB, data loaded,\n",
      " '1': 20>, so metadata was not modified.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: fmin=2.500 Hz corresponds to 3.500 < 5 cycles based on the epoch length 1.400 sec, need at least 2.000 sec epochs or fmin=3.571. Spectrum estimate will be unreliable.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 10, Task: BWnocontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: There were no Annotations stored in <EpochsArray | 20 events (all good), 0 – 1.399 s (baseline off), ~1.7 MB, data loaded,\n",
      " '1': 20>, so metadata was not modified.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: fmin=2.500 Hz corresponds to 3.500 < 5 cycles based on the epoch length 1.400 sec, need at least 2.000 sec epochs or fmin=3.571. Spectrum estimate will be unreliable.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk5, Experiment: 12, Task: BWcontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: There were no Annotations stored in <EpochsArray | 20 events (all good), 0 – 1.399 s (baseline off), ~1.7 MB, data loaded,\n",
      " '1': 20>, so metadata was not modified.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: fmin=2.500 Hz corresponds to 3.500 < 5 cycles based on the epoch length 1.400 sec, need at least 2.000 sec epochs or fmin=3.571. Spectrum estimate will be unreliable.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk6, Experiment: 13, Task: BWcontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: There were no Annotations stored in <EpochsArray | 20 events (all good), 0 – 1.399 s (baseline off), ~1.7 MB, data loaded,\n",
      " '1': 20>, so metadata was not modified.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: fmin=2.500 Hz corresponds to 3.500 < 5 cycles based on the epoch length 1.400 sec, need at least 2.000 sec epochs or fmin=3.571. Spectrum estimate will be unreliable.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk5, Experiment: 14, Task: BWcontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: There were no Annotations stored in <EpochsArray | 20 events (all good), 0 – 1.399 s (baseline off), ~1.7 MB, data loaded,\n",
      " '1': 20>, so metadata was not modified.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: fmin=2.500 Hz corresponds to 3.500 < 5 cycles based on the epoch length 1.400 sec, need at least 2.000 sec epochs or fmin=3.571. Spectrum estimate will be unreliable.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk6, Experiment: 15, Task: BWcontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: There were no Annotations stored in <EpochsArray | 20 events (all good), 0 – 1.399 s (baseline off), ~1.7 MB, data loaded,\n",
      " '1': 20>, so metadata was not modified.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: fmin=2.500 Hz corresponds to 3.500 < 5 cycles based on the epoch length 1.400 sec, need at least 2.000 sec epochs or fmin=3.571. Spectrum estimate will be unreliable.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 16, Task: BWcontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: There were no Annotations stored in <EpochsArray | 20 events (all good), 0 – 1.399 s (baseline off), ~1.7 MB, data loaded,\n",
      " '1': 20>, so metadata was not modified.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: fmin=2.500 Hz corresponds to 3.500 < 5 cycles based on the epoch length 1.400 sec, need at least 2.000 sec epochs or fmin=3.571. Spectrum estimate will be unreliable.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 17, Task: BWcontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: There were no Annotations stored in <EpochsArray | 20 events (all good), 0 – 1.399 s (baseline off), ~1.7 MB, data loaded,\n",
      " '1': 20>, so metadata was not modified.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: fmin=2.500 Hz corresponds to 3.500 < 5 cycles based on the epoch length 1.400 sec, need at least 2.000 sec epochs or fmin=3.571. Spectrum estimate will be unreliable.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk5, Experiment: 18, Task: BWnocontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: There were no Annotations stored in <EpochsArray | 20 events (all good), 0 – 1.399 s (baseline off), ~1.7 MB, data loaded,\n",
      " '1': 20>, so metadata was not modified.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: fmin=2.500 Hz corresponds to 3.500 < 5 cycles based on the epoch length 1.400 sec, need at least 2.000 sec epochs or fmin=3.571. Spectrum estimate will be unreliable.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk6, Experiment: 19, Task: BWnocontext\n",
      "(array([[0]]), array([[1, 2]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: There were no Annotations stored in <EpochsArray | 20 events (all good), 0 – 1.399 s (baseline off), ~1.7 MB, data loaded,\n",
      " '1': 20>, so metadata was not modified.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: fmin=2.500 Hz corresponds to 3.500 < 5 cycles based on the epoch length 1.400 sec, need at least 2.000 sec epochs or fmin=3.571. Spectrum estimate will be unreliable.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 20, Task: BWcontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: There were no Annotations stored in <EpochsArray | 20 events (all good), 0 – 1.399 s (baseline off), ~1.7 MB, data loaded,\n",
      " '1': 20>, so metadata was not modified.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: fmin=2.500 Hz corresponds to 3.500 < 5 cycles based on the epoch length 1.400 sec, need at least 2.000 sec epochs or fmin=3.571. Spectrum estimate will be unreliable.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk5, Experiment: 21, Task: BWnocontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: There were no Annotations stored in <EpochsArray | 20 events (all good), 0 – 1.399 s (baseline off), ~1.7 MB, data loaded,\n",
      " '1': 20>, so metadata was not modified.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: fmin=2.500 Hz corresponds to 3.500 < 5 cycles based on the epoch length 1.400 sec, need at least 2.000 sec epochs or fmin=3.571. Spectrum estimate will be unreliable.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk6, Experiment: 22, Task: BWnocontext\n",
      "(array([[0]]), array([[1, 2]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: There were no Annotations stored in <EpochsArray | 20 events (all good), 0 – 1.399 s (baseline off), ~1.7 MB, data loaded,\n",
      " '1': 20>, so metadata was not modified.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: fmin=2.500 Hz corresponds to 3.500 < 5 cycles based on the epoch length 1.400 sec, need at least 2.000 sec epochs or fmin=3.571. Spectrum estimate will be unreliable.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk5, Experiment: 32, Task: BWnocontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: There were no Annotations stored in <EpochsArray | 20 events (all good), 0 – 1.399 s (baseline off), ~1.7 MB, data loaded,\n",
      " '1': 20>, so metadata was not modified.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: fmin=2.500 Hz corresponds to 3.500 < 5 cycles based on the epoch length 1.400 sec, need at least 2.000 sec epochs or fmin=3.571. Spectrum estimate will be unreliable.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk5, Experiment: 33, Task: BWnocontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: There were no Annotations stored in <EpochsArray | 20 events (all good), 0 – 1.399 s (baseline off), ~1.7 MB, data loaded,\n",
      " '1': 20>, so metadata was not modified.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: fmin=2.500 Hz corresponds to 3.500 < 5 cycles based on the epoch length 1.400 sec, need at least 2.000 sec epochs or fmin=3.571. Spectrum estimate will be unreliable.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 34, Task: BWcontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: There were no Annotations stored in <EpochsArray | 20 events (all good), 0 – 1.399 s (baseline off), ~1.7 MB, data loaded,\n",
      " '1': 20>, so metadata was not modified.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: fmin=2.500 Hz corresponds to 3.500 < 5 cycles based on the epoch length 1.400 sec, need at least 2.000 sec epochs or fmin=3.571. Spectrum estimate will be unreliable.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 35, Task: BWcontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: There were no Annotations stored in <EpochsArray | 20 events (all good), 0 – 1.399 s (baseline off), ~1.7 MB, data loaded,\n",
      " '1': 20>, so metadata was not modified.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: fmin=2.500 Hz corresponds to 3.500 < 5 cycles based on the epoch length 1.400 sec, need at least 2.000 sec epochs or fmin=3.571. Spectrum estimate will be unreliable.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Rat: dk1, Experiment: 36, Task: BWcontext\n",
      "(array([[0, 1]]), array([[2, 3]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: There were no Annotations stored in <EpochsArray | 20 events (all good), 0 – 1.399 s (baseline off), ~1.7 MB, data loaded,\n",
      " '1': 20>, so metadata was not modified.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n",
      "C:\\Users\\CPLab\\AppData\\Local\\Temp\\ipykernel_16892\\2332103863.py:34: RuntimeWarning: fmin=2.500 Hz corresponds to 3.500 < 5 cycles based on the epoch length 1.400 sec, need at least 2.000 sec epochs or fmin=3.571. Spectrum estimate will be unreliable.\n",
      "  con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n"
     ]
    }
   ],
   "source": [
    "con_data_df_clean=pd.read_pickle(savepath+'mne_epochs_array_df_truncated_1400.pkl')\n",
    "all_gc_data=[]\n",
    "for row in con_data_df_clean.itertuples(index=False):\n",
    "    experiment = row.experiment\n",
    "    rat_id = row.rat_id\n",
    "    task = row.task\n",
    "    mne_epoch = row.mne_epoch_door_before\n",
    "    data_around_dig = row.mne_epoch_around_dig\n",
    "    data_before_dig = row.mne_epoch_dig_before\n",
    "    data_after_dig = row.mne_epoch_dig_after\n",
    "    data_before_door = row.mne_epoch_door_before\n",
    "    data_after_door = row.mne_epoch_door_after\n",
    "\n",
    "    event_of_interest = data_around_dig  ### CHANGE THIS TO THE DESIRED EVENT\n",
    "\n",
    "    print(f'Processing Rat: {rat_id}, Experiment: {experiment}, Task: {task}')\n",
    "    #print(event_of_interest.get_data().shape)  # Should be (n_epochs, n_channels,n_times)\n",
    "\n",
    "    aon_signals=[\n",
    "        idx\n",
    "        for idx, ch_info in enumerate(event_of_interest.info[\"chs\"])\n",
    "        if \"AON\" in ch_info[\"ch_name\"]\n",
    "        ]\n",
    "    #print(aon_signals)\n",
    "    vhp_signals=[\n",
    "        idx\n",
    "        for idx, ch_info in enumerate(event_of_interest.info[\"chs\"])\n",
    "        if \"vHp\" in ch_info[\"ch_name\"]\n",
    "    ]\n",
    "    #print(vhp_signals)\n",
    "\n",
    "    indices_aon_vhp = (np.array([aon_signals]), np.array([vhp_signals]))\n",
    "    print(indices_aon_vhp)\n",
    "    con = mne_connectivity.spectral_connectivity_epochs(event_of_interest, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n",
    "                                        mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                        cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False, gc_n_lags=20, n_jobs=-1)\n",
    "    coh = con.get_data()[0,:,:]\n",
    "    new_row = [experiment, rat_id, task, coh]\n",
    "    all_gc_data.append(new_row)\n",
    "all_gc_data_df = pd.DataFrame(all_gc_data, columns=['experiment', 'rat_id', 'task', 'gc_around_dig'])\n",
    "all_gc_data_df.to_pickle(savepath+'gc_around_dig.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df_clean=pd.read_pickle(savepath+'mne_epochs_array_df.pkl')\n",
    "\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "\n",
    "print(event_list)\n",
    "BWcontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWcontext')]\n",
    "BWnocontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWnocontext')]\n",
    "task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "\n",
    "rat_list=np.unique(con_data_df_clean['rat_id'])\n",
    "print(rat_list)\n",
    "all_con_data=[]\n",
    "all_con_data_mean=[]\n",
    "for task_num,task_name in enumerate(task_data_dict.keys()):\n",
    "        task_data=task_data_dict[task_name]\n",
    "    #print(task_name)\n",
    "    # for rat_num, rat_name in enumerate(rat_list):\n",
    "    #     rat_task_data=task_data[task_data['rat_id']==rat_name]\n",
    "        row=[task_name]\n",
    "    #     #print(row)\n",
    "        row_2=[task_name]\n",
    "        for event in event_list:\n",
    "            #print(event)\n",
    "            event_epoch_list=task_data[event]\n",
    "            aon_vHp_con=[]\n",
    "            for event_epoch in event_epoch_list:\n",
    "                    #print(row,event, event_epoch) \n",
    "                    fmin=1\n",
    "                    fmax=100\n",
    "                    freqs = np.arange(fmin,fmax)\n",
    "                    n_cycles = freqs/2\n",
    "                    \n",
    "                    ###Specifying the Indices for AON and vHp channels\n",
    "                    aon_signals=[\n",
    "                    idx\n",
    "                    for idx, ch_info in enumerate(event_epoch.info[\"chs\"])\n",
    "                    if \"AON\" in ch_info[\"ch_name\"]\n",
    "                    ]\n",
    "                    print(aon_signals)\n",
    "                    vhp_signals=[\n",
    "                        idx\n",
    "                        for idx, ch_info in enumerate(event_epoch.info[\"chs\"])\n",
    "                        if \"vHp\" in ch_info[\"ch_name\"]\n",
    "                    ]\n",
    "                    print(vhp_signals)\n",
    "\n",
    "                    indices_aon_vhp = (np.array([aon_signals]), np.array([vhp_signals]))\n",
    "                    indices_vhp_aon = (np.array([vhp_signals]), np.array([aon_signals]))      \n",
    "                    gc_ab = mne_connectivity.spectral_connectivity_epochs(event_epoch, method=[\"gc\"], indices=indices_aon_vhp, fmin=2.5, fmax=100, rank=None,gc_n_lags=20)\n",
    "                    gc_ba= mne_connectivity.spectral_connectivity_epochs(event_epoch, method=[\"gc\"], indices=indices_vhp_aon, fmin=2.5, fmax=100, rank=None,gc_n_lags=20)\n",
    "                    net_gc= gc_ab.get_data() - gc_ba.get_data()\n",
    "                    print(net_gc.shape)\n",
    "\n",
    "                    coh = net_gc[0]\n",
    "                    #coh=np.abs(coh)\n",
    "                    print(coh.shape)\n",
    "                    indices = coh.names\n",
    "                    print(indices)\n",
    "\n",
    "                    for i in range(coh.shape[0]):\n",
    "                        for j in range(coh.shape[1]):\n",
    "                            if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                                aon_vHp_con.append(coh[i,j,:,:])\n",
    "            row.append(np.mean(aon_vHp_con, axis=0))\n",
    "            row_2.append(np.mean(aon_vHp_con))\n",
    "        all_con_data.append(row)                    \n",
    "        all_con_data_mean.append(row_2)\n",
    "# Convert all_con_data to a DataFrame for easier manipulation\n",
    "all_con_data_df = pd.DataFrame(all_con_data, columns=['task'] + event_list)\n",
    "#all_con_data_df.to_pickle(savepath+'coherence_spectrogram_around_door_dig.pkl')\n",
    "fs=2000\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "fs=2000\n",
    "times=np.arange(-2, 2, 1/fs)\n",
    "fig, axs=plt.subplots(2,2, figsize=(20,10), sharey=True)\n",
    "vmin = all_con_data_df[event_list].applymap(np.min).min().min()\n",
    "vmax = all_con_data_df[event_list].applymap(np.max).max().max()\n",
    "event_names=['Around Door','Around Dig']\n",
    "for i, event in enumerate(event_list):\n",
    "    axs[0,i].imshow(all_con_data_df[event][0], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[0,i].set_xlabel('')\n",
    "\n",
    "    axs[0,i].set_ylabel('Frequency (Hz)', fontsize=20)\n",
    "    axs[0,i].set_title(event_names[i], fontsize=20)\n",
    "    axs[0,i].vlines(0, 0, 100, color='k', linestyle='--')\n",
    "    axs[0,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    axs[1,i].imshow(all_con_data_df[event][1], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[1,i].set_xlabel('Time (s)', fontsize=20)\n",
    "    axs[1,i].set_ylabel('Frequency (Hz)', fontsize=20)\n",
    "    axs[1,i].set_title(event_names[i], fontsize=20)\n",
    "    axs[1,i].vlines(0, 0, 100, color='k', linestyle='--')\n",
    "\n",
    "    axs[0,0].text(-0.2, 0.5, 'Context', transform=axs[0,0].transAxes, fontsize=18, verticalalignment='center', rotation=90)\n",
    "    axs[1,0].text(-0.2, 0.5, 'No Context', transform=axs[1,0].transAxes, fontsize=18, verticalalignment='center', rotation=90)\n",
    "    axs[0,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    axs[1,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    axs[0,i].set_xticks(np.arange(-2, 3, 1))  # Set x-ticks from -2 to 2 seconds\n",
    "    axs[0,i].set_xticklabels(np.arange(-2, 3, 1))  # Set x-tick labels from -2 to 2 seconds\n",
    "    axs[1,i].set_xticks(np.arange(-2, 3, 1))  # Set x-ticks from -2 to 2 seconds\n",
    "    axs[1,i].set_xticklabels(np.arange(-2, 3, 1))  # Set x-tick labels from -2 to 2 seconds\n",
    "\n",
    "    # Add a colorbar\n",
    "cbar = fig.colorbar(axs[0,0].images[0], ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "cbar.set_label('Coherence', loc='center', fontsize=20, labelpad=10)\n",
    "cbar.ax.tick_params(labelsize=20)  # Set colorbar tick label size\n",
    "\n",
    "#fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\aon_vhp_coherence_event_spectrogram.png',format='png', dpi=600, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating complex coherence values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated Coherence with Quiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df_clean=pd.read_pickle(savepath+'mne_epochs_array_df_shuffled_truncated.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "fs=2000\n",
    "print(event_list)\n",
    "BWcontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWcontext')]\n",
    "BWnocontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWnocontext')]\n",
    "task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "\n",
    "rat_list=np.unique(con_data_df_clean['rat_id'])\n",
    "print(rat_list)\n",
    "all_coh_abs_data=[]\n",
    "all_coh_abs_data_mean=[]\n",
    "all_coh_phase_data=[]\n",
    "all_coh_phase_data_mean=[]\n",
    "for task_num,task_name in enumerate(task_data_dict.keys()):\n",
    "        task_data=task_data_dict[task_name]\n",
    "    #print(task_name)\n",
    "    # for rat_num, rat_name in enumerate(rat_list):\n",
    "    #     rat_task_data=task_data[task_data['rat_id']==rat_name]\n",
    "        row_coh_abs=[task_name]\n",
    "        row_coh_abs_mean=[task_name]\n",
    "        row_coh_phase=[task_name]\n",
    "        row_coh_phase_mean=[task_name]\n",
    "        for event in event_list:\n",
    "            #print(event)\n",
    "            event_epoch_list=task_data[event]\n",
    "            aon_vhp_coh_abs=[]\n",
    "            aon_vhp_coh_phase=[]\n",
    "            for event_epoch in event_epoch_list:\n",
    "                    #print(row,event, event_epoch) \n",
    "                    fmin=1\n",
    "                    fmax=100\n",
    "                    freqs = np.arange(fmin,fmax)\n",
    "                    n_cycles = freqs/3\n",
    "                           \n",
    "                    con = mne_connectivity.spectral_connectivity_epochs(event_epoch, method='cohy', sfreq=int(fs),\n",
    "                                                         mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                                         cwt_n_cycles=n_cycles, verbose=False, fmin=1, fmax=100, faverage=False)\n",
    "                    coh = con.get_data(output='dense')\n",
    "                    coh_abs = np.abs(coh)\n",
    "                    coh_phase = np.angle(coh)\n",
    "\n",
    "                    indices = con.names\n",
    "                    print(indices)\n",
    "                    print(coh.shape)\n",
    "                    print(coh_abs.shape)\n",
    "                    print(coh_phase.shape)\n",
    "\n",
    "                    for i in range(coh.shape[0]):\n",
    "                        for j in range(coh.shape[1]):\n",
    "                            if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                                coherence_abs=coh_abs[i,j,:,:]\n",
    "                                coherence_abs=np.arctanh(coherence_abs)  # Apply Fisher transformation\n",
    "                                aon_vhp_coh_abs.append(coherence_abs)\n",
    "                                aon_vhp_coh_phase.append(coh_phase[i,j,:,:])\n",
    "\n",
    "            row_coh_abs.append(np.mean(aon_vhp_coh_abs, axis=0))\n",
    "            row_coh_abs_mean.append(np.mean(aon_vhp_coh_abs))\n",
    "            row_coh_phase.append(np.mean(aon_vhp_coh_phase, axis=0))\n",
    "            row_coh_phase_mean.append(np.mean(aon_vhp_coh_phase))\n",
    "        all_coh_abs_data.append(row_coh_abs)\n",
    "        all_coh_abs_data_mean.append(row_coh_abs_mean)\n",
    "        all_coh_phase_data.append(row_coh_phase)\n",
    "        all_coh_phase_data_mean.append(row_coh_phase_mean)\n",
    "        \n",
    "\n",
    "all_coh_abs_data_df = pd.DataFrame(all_coh_abs_data, columns=['task'] + event_list)\n",
    "all_coh_abs_data_df.to_pickle(savepath+'coherence_abs_spectrogram_around_door_dig_shuffled_truncated.pkl')\n",
    "\n",
    "all_coh_phase_data_df = pd.DataFrame(all_coh_phase_data, columns=['task'] + event_list)\n",
    "all_coh_phase_data_df.to_pickle(savepath+'coherence_phase_spectrogram_around_door_dig_shuffled_truncated.pkl')\n",
    "\n",
    "\n",
    "fs=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_coh_abs_data_df=pd.read_pickle(savepath+'coherence_abs_spectrogram_around_door_dig_truncated.pkl')\n",
    "all_coh_phase_data_df=pd.read_pickle(savepath+'coherence_phase_spectrogram_around_door_dig_truncated.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract phase data for each condition\n",
    "phase_context = np.array(all_coh_phase_data_df.loc[all_coh_phase_data_df['task'] == 'BWcontext', 'mne_epoch_around_dig'].iloc[0]).flatten()\n",
    "phase_nocontext = np.array(all_coh_phase_data_df.loc[all_coh_phase_data_df['task'] == 'BWnocontext', 'mne_epoch_around_dig'].iloc[0]).flatten()\n",
    "\n",
    "# Convert phase to [0, 2pi]\n",
    "phase_context = np.mod(phase_context, 2 * np.pi)\n",
    "phase_nocontext = np.mod(phase_nocontext, 2 * np.pi)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, subplot_kw={'projection': 'polar'}, figsize=(12, 6))\n",
    "\n",
    "# Plot BW Context\n",
    "axs[0].hist(phase_context, bins=100, density=True, color=colors['BWcontext'], alpha=0.7)\n",
    "axs[0].set_title('Phase Difference Histogram\\nBW Context', fontsize=16)\n",
    "axs[0].set_xticks(np.linspace(0, 2 * np.pi, 8, endpoint=False))\n",
    "axs[0].set_xlim(0, 2 * np.pi)\n",
    "\n",
    "# Plot BW No Context\n",
    "axs[1].hist(phase_nocontext, bins=100, density=True, color=colors['BWnocontext'], alpha=0.7)\n",
    "axs[1].set_title('Phase Difference Histogram\\nBW No Context', fontsize=16)\n",
    "axs[1].set_xticks(np.linspace(0, 2 * np.pi, 8, endpoint=False))\n",
    "axs[1].set_xlim(0, 2 * np.pi)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Statistical test for difference in phase distributions between tasks\n",
    "\n",
    "# Use the Kolmogorov-Smirnov test to compare the two phase distributions\n",
    "ks_stat, p_value = ks_2samp(phase_context, phase_nocontext)\n",
    "print(f\"KS statistic: {ks_stat:.4f}, p-value: {p_value:.4e}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"There is a significant difference between the phase distributions of the two tasks.\")\n",
    "else:\n",
    "    print(\"No significant difference between the phase distributions of the two tasks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "fs=2000\n",
    "times=np.arange(-0.7, 0.7, 1/fs)\n",
    "fig, axs=plt.subplots(2,2, figsize=(20,10), sharey=True)\n",
    "all_con_data_df=all_coh_abs_data_df\n",
    "aon_vhp_phase=all_coh_phase_data_df\n",
    "vmin = all_con_data_df[event_list].applymap(np.min).min().min()\n",
    "vmax = all_con_data_df[event_list].applymap(np.max).max().max()\n",
    "event_names=['Around Door','Around Dig']\n",
    "freqs=np.arange(2.5, 100, 0.5)\n",
    "for i, event in enumerate(event_list):\n",
    "    axs[0,i].imshow(all_con_data_df[event][0], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    coh_phase = aon_vhp_phase[event][0]\n",
    "    X, Y = np.meshgrid(np.linspace(times[0], times[-1], coh_phase.shape[1]), np.linspace(freqs[0], freqs[-1], coh_phase.shape[0]))\n",
    "\n",
    "    U = np.cos(coh_phase)\n",
    "    V = np.sin(coh_phase)\n",
    "    f_x = 100\n",
    "    f_y = 5\n",
    "    #axs[0, i].quiver(X[2::f_y, ::f_x], Y[2::f_y, ::f_x], U[2::f_y, ::f_x], V[2::f_y, ::f_x], angles='uv', scale=40, alpha=0.7)\n",
    "\n",
    "    axs[0,i].set_xlabel('')\n",
    "\n",
    "    axs[0,i].set_ylabel('Frequency (Hz)', fontsize=20)\n",
    "    axs[0,i].set_title(event_names[i], fontsize=20)\n",
    "    axs[0,i].vlines(0, 0, 100, color='k', linestyle='--')\n",
    "    axs[0,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    \n",
    "    axs[1,i].imshow(all_con_data_df[event][1], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    coh_phase = aon_vhp_phase[event][1]\n",
    "    X, Y = np.meshgrid(np.linspace(times[0], times[-1], coh_phase.shape[1]), np.linspace(freqs[0], freqs[-1], coh_phase.shape[0]))\n",
    "\n",
    "    U = np.cos(coh_phase)\n",
    "    V = np.sin(coh_phase)\n",
    "    f_x = 100\n",
    "    f_y = 5\n",
    "    #axs[1, i].quiver(X[2::f_y, ::f_x], Y[2::f_y, ::f_x], U[2::f_y, ::f_x], V[2::f_y, ::f_x], angles='uv', scale=40, alpha=0.7)\n",
    "\n",
    "    \n",
    "    axs[1,i].set_xlabel('Time (s)', fontsize=20)\n",
    "    axs[1,i].set_ylabel('Frequency (Hz)', fontsize=20)\n",
    "    axs[1,i].set_title(event_names[i], fontsize=20)\n",
    "    axs[1,i].vlines(0, 0, 100, color='k', linestyle='--')\n",
    "\n",
    "    axs[0,0].text(-0.2, 0.5, 'Context', transform=axs[0,0].transAxes, fontsize=18, verticalalignment='center', rotation=90)\n",
    "    axs[1,0].text(-0.2, 0.5, 'No Context', transform=axs[1,0].transAxes, fontsize=18, verticalalignment='center', rotation=90)\n",
    "    axs[0,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    axs[1,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    # axs[0,i].set_xticks(np.arange(-2, 3, 1))  # Set x-ticks from -2 to 2 seconds\n",
    "    # axs[0,i].set_xticklabels(np.arange(-2, 3, 1))  # Set x-tick labels from -2 to 2 seconds\n",
    "    # axs[1,i].set_xticks(np.arange(-2, 3, 1))  # Set x-ticks from -2 to 2 seconds\n",
    "    # axs[1,i].set_xticklabels(np.arange(-2, 3, 1))  # Set x-tick labels from -2 to 2 seconds\n",
    "\n",
    "    # Add a colorbar\n",
    "cbar = fig.colorbar(axs[0,0].images[0], ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "cbar.set_label('Coherence (Z-transformed)', loc='center', fontsize=20, labelpad=10)\n",
    "cbar.ax.tick_params(labelsize=20)  # Set colorbar tick label size\n",
    "\n",
    "fig.savefig(savepath+'aon_vhp_coherogram.png',format='png', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Behavior Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Behavior Correlation with Power\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_df=pd.read_pickle(savepath+'compiled_data_all_epochs.pkl')\n",
    "behavior_df.iloc[:,-5:]=behavior_df.iloc[:,-5:].applymap(lambda x: scipy.signal.welch(x, fs=2000, nperseg=2000)[1])\n",
    "\n",
    "bands_dict = {'beta': [12, 30], 'gamma': [30, 80], 'theta': [4, 12], 'total': [1, 100]}\n",
    "for col in behavior_df.columns[-7:]:\n",
    "    for band, (band_start, band_end) in bands_dict.items():\n",
    "        behavior_df[band + '_' + col] = behavior_df[col].apply(lambda x: functions.get_band_power(x, band_start, band_end))\n",
    "\n",
    "behavior_df['channel'] = behavior_df['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "\n",
    "behavior_df_grouped=behavior_df.groupby(['task', 'channel'])\n",
    "writer=pd.ExcelWriter(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\behavior_power_correlation.xlsx')\n",
    "for (task, channel), group in behavior_df_grouped:\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(20, 10), sharex=True, constrained_layout=True)\n",
    "    axs = axs.flatten()\n",
    "    group=behavior_df[(behavior_df['channel']==channel) & (behavior_df['task']==task)]\n",
    "\n",
    "    power_columns=group.columns[17:]\n",
    "    print(power_columns)\n",
    "    group_melted=pd.melt(group, id_vars=['rat', 'task', 'channel', 'correct?'], value_vars=power_columns, var_name='band_event', value_name='power')\n",
    "    group_melted['band']=group_melted['band_event'].apply(lambda x: x.split('_')[0])\n",
    "    group_melted['event']=group_melted['band_event'].apply(lambda x: x.split('_')[1:])\n",
    "    group_melted['event']=group_melted['event'].apply(lambda x: x[0]+'_'+x[1])\n",
    "    group_melted['correct?']=group_melted['correct?'].apply(lambda x: 'Incorrect' if x=='0' else 'Correct')\n",
    "    \n",
    "    group_melted.to_excel(writer, sheet_name=f'{channel}_{task}')\n",
    "\n",
    "\n",
    "    correct_counts = group_melted[group_melted['correct?'] == 'Correct'].shape[0]\n",
    "    incorrect_counts = group_melted[group_melted['correct?'] == 'Incorrect'].shape[0]\n",
    "    print(f\"Number of Corrects: {correct_counts}\")\n",
    "    print(f\"Number of Incorrects: {incorrect_counts}\")\n",
    "    events_list=['pre_door','post_door','pre_dig','post_dig']\n",
    "    for i, event in enumerate(events_list):\n",
    "        ax=axs[i]\n",
    "        sns.boxplot(x='band', y='power', hue='correct?', data=group_melted[group_melted['event']==event], showfliers=False, ax=ax)\n",
    "        #sns.stripplot(x='band', y='power', hue='correct?', data=aon_behavior_df_melted[aon_behavior_df_melted['event']==event], dodge=True, edgecolor='black', linewidth=1, jitter=True, ax=ax)\n",
    "        ax.set_title(event)\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('Power')\n",
    "        ax.legend(title='Correct?')\n",
    "    fig.suptitle(f'{channel} {task}')\n",
    "    fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\behavior_power_{channel}_{task}.png')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Behavior Correlation with Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "behavior_coherence_df=pd.read_pickle(savepath+'compiled_data_all_epochs.pkl')\n",
    "\n",
    "behavior_coherence_df['unique_id']=behavior_coherence_df['rat']+'_'+behavior_coherence_df['task']+behavior_coherence_df['date']\n",
    "behavior_coherence_df_grouped=behavior_coherence_df.groupby(['unique_id', 'trial'])\n",
    "behavior_coherence_compiled_data_df=[]\n",
    "time_window=  0.7\n",
    "fs=2000\n",
    "for (unique_id, trial), group in behavior_coherence_df_grouped:\n",
    "    print(unique_id, trial)\n",
    "    channels_list=list(group['channel'].unique())\n",
    "    print(channels_list)\n",
    "    info=mne.create_info(ch_names=channels_list, sfreq=fs, ch_types='eeg')\n",
    "\n",
    "    mne_epoch_door_before=np.zeros((1,len(channels_list),int(time_window*fs)))\n",
    "    mne_epoch_door_after=np.zeros((1,len(channels_list),int(time_window*fs)))\n",
    "    mne_epoch_dig_before=np.zeros((1,len(channels_list),int(time_window*fs)))\n",
    "    mne_epoch_dig_after=np.zeros((1,len(channels_list),int(time_window*fs)))\n",
    "    mne_epoch_around_door=np.zeros((1,len(channels_list),int(time_window*fs)*2))\n",
    "    mne_epoch_around_dig=np.zeros((1,len(channels_list),int(time_window*fs)*2))\n",
    "\n",
    "    for channel_num, channel_id in enumerate(channels_list):\n",
    "        data=group[group['channel']==channel_id]\n",
    "        mne_epoch_door_before[0,channel_num,:]=data['pre_door'].values[0][:int(time_window*fs)]\n",
    "        mne_epoch_door_after[0,channel_num,:]=data['post_door'].values[0][:int(time_window*fs)]\n",
    "        mne_epoch_dig_before[0,channel_num,:]=data['pre_dig'].values[0][:int(time_window*fs)]\n",
    "        mne_epoch_dig_after[0,channel_num,:]=data['post_dig'].values[0][:int(time_window*fs)]\n",
    "        mid_point = int(len(data['around_door'].values[0])/2)\n",
    "        mne_epoch_around_door[0,channel_num,:]=data['around_door'].values[0][mid_point-int(time_window*fs):mid_point+int(time_window*fs)]\n",
    "        mne_epoch_around_dig[0,channel_num,:]=data['around_dig'].values[0][mid_point-int(time_window*fs):mid_point+int(time_window*fs)]\n",
    "\n",
    "    # mne_epoch_around_door_truncated = mne_epoch_around_door[:, :, 3000:5000]\n",
    "    # mne_epoch_around_dig_truncated = mne_epoch_around_dig[:, :, 3000:5000]\n",
    "    mne_epoch_door_before = mne.EpochsArray(mne_epoch_door_before, info)\n",
    "    mne_epoch_door_after= mne.EpochsArray(mne_epoch_door_after, info)\n",
    "    mne_epoch_dig_before = mne.EpochsArray(mne_epoch_dig_before, info)\n",
    "    mne_epoch_dig_after = mne.EpochsArray(mne_epoch_dig_after, info)\n",
    "    mne_epoch_around_door = mne.EpochsArray(mne_epoch_around_door, info)\n",
    "    mne_epoch_around_dig = mne.EpochsArray(mne_epoch_around_dig, info)\n",
    "    \n",
    "    behavior_coherence_compiled_data={\n",
    "        'rat': group['rat'].values[0],\n",
    "        'task': group['task'].values[0],\n",
    "        'date': group['date'].values[0],\n",
    "        'unique_id': unique_id,\n",
    "        'trial': trial,\n",
    "        'side': group['side'].values[0],\n",
    "        'correct?': group['correct?'].values[0],\n",
    "        'time_to_dig': group['timestamps'].iloc[0][1] - group['timestamps'].iloc[0][0],\n",
    "        'pre_door': mne_epoch_door_before,\n",
    "        'post_door': mne_epoch_door_after,\n",
    "        'pre_dig': mne_epoch_dig_before,\n",
    "        'post_dig': mne_epoch_dig_after,\n",
    "        'around_door': mne_epoch_around_door,\n",
    "        'around_dig': mne_epoch_around_dig\n",
    "        ,'around_door_truncated': mne_epoch_around_door,\n",
    "        'around_dig_truncated': mne_epoch_around_dig}\n",
    "    \n",
    "    behavior_coherence_compiled_data_df.append(behavior_coherence_compiled_data)\n",
    "behavior_coherence_compiled_data_df=pd.DataFrame(behavior_coherence_compiled_data_df)\n",
    "behavior_coherence_compiled_data_df.to_pickle(savepath+f'behavior_coherence_single_epochs_mne_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "# def lfp_to_mne_epoch(lfp_data):\n",
    "#     fs=2000\n",
    "#     freqs = np.arange(1,100)\n",
    "#     n_cycles = freqs/2\n",
    "#     empty_array=np.zeros((1,1,len(lfp_data)))\n",
    "#     empty_array[0,0,:]=lfp_data\n",
    "#     info = mne.create_info(ch_names=['1'], sfreq=fs, ch_types='eeg')\n",
    "#     mne_epoch = mne.EpochsArray(empty_array, info)\n",
    "#     return mne_epoch\n",
    "\n",
    "# behavior_coherence_df[['pre_door','post_door', 'pre_dig', 'post_dig']]=behavior_coherence_df[['pre_door','post_door', 'pre_dig', 'post_dig']].applymap(lambda x: lfp_to_mne_epoch(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the short MNE Epochs to coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 0.7  # seconds\n",
    "fs = 2000  # Sampling frequency\n",
    "behavior_coherence_compiled_data_df_truncated=pd.read_pickle(savepath+f'behavior_coherence_single_epochs_mne_truncated_{int(time_window*fs)}.pkl')\n",
    "print(behavior_coherence_compiled_data_df_truncated.head())\n",
    "importlib.reload(coherence_functions)\n",
    "bands_dict = {'beta': [12, 30], 'gamma': [30, 80],'theta':[4,12], 'total': [1, 100]}\n",
    "for col in ['pre_door', 'post_door', 'pre_dig', 'post_dig', 'around_door','around_dig']:\n",
    "    print(col)\n",
    "    for band, (band_start, band_end) in bands_dict.items():\n",
    "        behavior_coherence_compiled_data_df_truncated[band + '_' + col] = behavior_coherence_compiled_data_df_truncated[col].apply(lambda x: coherence_functions.convert_epoch_to_coherence_behavior(x, band_start=band_start, band_end=band_end))\n",
    "behavior_coherence_compiled_data_df_truncated.drop(columns=['pre_door', 'post_door', 'pre_dig', 'post_dig', 'around_door','around_dig'], inplace=True)\n",
    "behavior_coherence_compiled_data_df_truncated.to_pickle(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\behavior_coherence_compiled_data_df_truncated_{int(time_window*fs)}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Coherence vs Time to Dig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window=  0.7\n",
    "fs=2000\n",
    "from scipy.stats import linregress\n",
    "behavior_coherence_compiled_data_df_truncated=pd.read_pickle(savepath+f'behavior_coherence_compiled_data_df_truncated_{int(time_window*fs)}.pkl')\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(3,2,figsize=(20, 20), sharey=True, constrained_layout=True)\n",
    "axs = axs.flatten()\n",
    "\n",
    "fig_hist, axs_hist = plt.subplots(3, 2, figsize=(20, 20), sharey=True, constrained_layout=True)\n",
    "axs_hist = axs_hist.flatten()\n",
    "\n",
    "grouped_df=behavior_coherence_compiled_data_df_truncated.groupby(['task'])\n",
    "band='beta'\n",
    "event='around_dig'\n",
    "task_dict = {'BWcontext': 'Context', 'BWnocontext': 'No Context'}\n",
    "\n",
    "events_dict ={'pre_door':'Pre Door', 'post_door':'Post Door', 'pre_dig':'Pre Dig', 'post_dig':'Post Dig', 'around_door':'Around Door', 'around_dig':'Around Dig'}\n",
    "for i, event in enumerate(events_dict.keys()):\n",
    "    ax = axs[i]\n",
    "    ax_hist = axs_hist[i]\n",
    "    print(np.where(behavior_coherence_compiled_data_df_truncated['{}_{}'.format(band,event)]==0))\n",
    "    for task, group in grouped_df:\n",
    "        print(task)\n",
    "        print('{}_{}'.format(band,event))\n",
    "        group = group[(np.abs(group['{}_{}'.format(band,event)]))>0]\n",
    "        print(group['{}_{}'.format(band,event)].mean())\n",
    "        group = group[(np.abs(group['time_to_dig'] - group['time_to_dig'].mean()) <= (3 * group['time_to_dig'].std()))] # Removing Outliers from Time\n",
    "        group = group[(np.abs(group['{}_{}'.format(band,event)] - group['{}_{}'.format(band,event)].mean()) <= (3 * group['{}_{}'.format(band,event)].std()))]  #Removing Outliers from Coherence\n",
    "        \n",
    "        #Plotting Regression\n",
    "        sns.regplot(x='time_to_dig', y='{}_{}'.format(band,event), data=group, ax=ax, label=task[0])\n",
    "        x= group['time_to_dig'].values\n",
    "        y= group['{}_{}'.format(band,event)].values\n",
    "        slope, intercept, r, p, se = linregress(x, y)\n",
    "        print(f'{task[0]}: Slope: {slope}, Intercept: {intercept}, R-squared: {r**2}, P-value: {p}')\n",
    "        \n",
    "        ## Plotting Histogram\n",
    "        sns.histplot(group['{}_{}'.format(band,event)], bins=30, kde=True, ax=ax_hist, label=task[0], color=colors[task[0]], stat='density', alpha=0.5)\n",
    "        ax_hist.axvline(group['{}_{}'.format(band,event)].mean(), color=colors[task[0]], linestyle='--', label=f'{task[0]} Mean')\n",
    "        ax_hist.axvline(group['{}_{}'.format(band,event)].median(), color=colors[task[0]], linestyle=':', label=f'{task[0]} Median')\n",
    "    \n",
    "    #Setting Histogram axis labels and title and legend\n",
    "    ax_hist.set_title(f'{events_dict[event]} - {band} Coherence Histogram', fontsize=16)\n",
    "    ax_hist.set_xlabel('Beta Coherence (Z-transformed)', fontsize=14)            \n",
    "    handles, labels = ax_hist.get_legend_handles_labels()\n",
    "    ax_hist.legend()\n",
    "    #ax_hist.legend(handles, [task_dict[l] for l in labels], title='Task', loc='upper right', fontsize=12)    \n",
    "    \n",
    "    \n",
    "    ax.set_title(f'{events_dict[event]}', fontsize=16)\n",
    "    ax.set_ylabel('Beta Coherence (Z-transformed)', fontsize=14)\n",
    "    ax.set_xlabel('Time to Dig (s)', fontsize=14)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, [task_dict[l] for l in labels], title='Task', loc='upper right', fontsize=12)\n",
    "    #ax.legend(title='Task')\n",
    "    \n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.tight_layout()\n",
    "fig.suptitle(f'Beta Coherence vs Time to Dig', fontsize=20, y=1.02)\n",
    "fig.savefig(savepath+f'{band}_coherence_vs_time_to_dig_{int(time_window*fs)}.png', format='png', dpi=600, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherence vs Time to Dig per rat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window=  0.4\n",
    "fs=2000\n",
    "event_of_interest = 'pre_dig'\n",
    "band_of_interest = 'beta'\n",
    "\n",
    "event_band = f'{band_of_interest}_{event_of_interest}'\n",
    "\n",
    "from scipy.stats import linregress\n",
    "behavior_coherence_compiled_data_df_truncated=pd.read_pickle(savepath+f'behavior_coherence_compiled_data_df_truncated_{int(time_window*fs)}.pkl')\n",
    "task_experiments = behavior_coherence_compiled_data_df_truncated.groupby(['task','unique_id'])\n",
    "task_list= behavior_coherence_compiled_data_df_truncated['task'].unique()\n",
    "print(task_list)\n",
    "for task in task_list:\n",
    "    task_data = behavior_coherence_compiled_data_df_truncated[behavior_coherence_compiled_data_df_truncated['task'] == task]\n",
    "    unique_experiments = task_data['unique_id'].unique()\n",
    "\n",
    "    fig, axs = plt.subplots(4, 5, figsize=(20, 20))\n",
    "    fig.suptitle(f'{task} - Coherence vs Time to Dig', fontsize=20, y=1.02)\n",
    "    dk1_i=0\n",
    "    dk3_i=0\n",
    "    dk5_i=0\n",
    "    dk6_i=0\n",
    "    for experiment in unique_experiments:\n",
    "        print(task, experiment)    \n",
    "        task_exp_data =  task_data[task_data['unique_id'] == experiment]\n",
    "        time_to_dig = task_exp_data['time_to_dig']\n",
    "        coherence_value = task_exp_data[event_band]\n",
    "        \n",
    "        rat_id = task_exp_data['rat'].values[0]\n",
    "        if rat_id == 'dk1':\n",
    "            ax = axs[0, dk1_i]\n",
    "            dk1_i += 1\n",
    "        elif rat_id == 'dk3':\n",
    "            ax = axs[1, dk3_i]\n",
    "            dk3_i += 1\n",
    "        elif rat_id == 'dk5':\n",
    "            ax = axs[2, dk5_i]\n",
    "            dk5_i += 1\n",
    "        elif rat_id == 'dk6':\n",
    "            ax = axs[3, dk6_i]\n",
    "            dk6_i += 1\n",
    "        else:\n",
    "            continue  # Skip if rat_id is not one of the specified rats\n",
    "        ax.scatter(time_to_dig, coherence_value, label=experiment)\n",
    "        \n",
    "        ## Plotting Regression\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(time_to_dig, coherence_value)\n",
    "        ax.plot(time_to_dig, intercept + slope * time_to_dig, color='red', label=f'Fit: y={slope:.2f}x+{intercept:.2f}\\nR²={r_value**2:.2f}, p={p_value:.4f}')\n",
    "        ax.set_title(f'Rat: {rat_id}', fontsize=16)\n",
    "                \n",
    "        \n",
    "        ax.set_title(f'Rat: {rat_id}', fontsize=16)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Coherence through trials as experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_of_interest = 'pre_dig' \n",
    "band_of_interest = 'beta'\n",
    "time_window = 0.4  # seconds\n",
    "fs = 2000  # Sampling frequency\n",
    "\n",
    "behavior_coherence_compiled_data_df_truncated=pd.read_pickle(savepath+f'behavior_coherence_compiled_data_df_truncated_{int(time_window*fs)}.pkl')\n",
    "power_per_trial_df = pd.read_excel(savepath+'power_per_trial_mt.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "band_event = band_of_interest+'_'+event_of_interest\n",
    "\n",
    "vmin = behavior_coherence_compiled_data_df_truncated[band_event].min()\n",
    "vmax = behavior_coherence_compiled_data_df_truncated[band_event].max()\n",
    "\n",
    "coherence_bwcontext_data = behavior_coherence_compiled_data_df_truncated[behavior_coherence_compiled_data_df_truncated['task'] == 'BWcontext']\n",
    "coherence_bwnocontext_data = behavior_coherence_compiled_data_df_truncated[behavior_coherence_compiled_data_df_truncated['task'] == 'BWnocontext']\n",
    "\n",
    "power_bwcontext_data = power_per_trial_df[power_per_trial_df['task'] == 'BWcontext']\n",
    "power_bwnocontext_data = power_per_trial_df[power_per_trial_df['task'] == 'BWnocontext']\n",
    "\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "# axs = axs.flatten()\n",
    "#task_data_dict = {'BWcontext': coherence_bwcontext_data, 'BWnocontext': coherence_bwnocontext_data}\n",
    "# task_list =[ 'BWcontext', 'BWnocontext']\n",
    "# for axi,(task_name, task_data) in enumerate(task_data_dict.items()):\n",
    "#     print(f\"Task: {task_name}\")\n",
    "#     experiment_ids = task_data['unique_id'].unique()\n",
    "#     print(f\"Number of unique IDs for {task_name}: {len(experiment_ids)}\")\n",
    "#     task_data_dict = {}\n",
    "#     for experiment_idi in experiment_ids:\n",
    "#         experiment_data=task_data[task_data['unique_id'] == experiment_idi]\n",
    "#         rat_date = experiment_idi.split('_')[0] +'_'+experiment_idi.split('_')[1][-8:]\n",
    "#         task_data_dict[rat_date] = experiment_data[band_event].values\n",
    "#     task_data_dict['trials'] = np.arange(start=1,stop=21,step=1, dtype=int)  # Assuming 20 trials per unique ID\n",
    "#     task_data_df = pd.DataFrame.from_dict(task_data_dict, orient='index').T\n",
    "#     task_data_df = task_data_df.fillna(0)  # Fill NaN values with 0\n",
    "#     task_data_df = task_data_df.loc[:, (task_data_df != 0).any(axis=0)]\n",
    "#     ax = axs[axi]\n",
    "#     ax.set_title(f'{task_name} - {band_of_interest} {event_of_interest}', fontsize=16)\n",
    "#     ax.set_xlabel('Trials', fontsize=14)\n",
    "#     ax.set_ylabel(f'{band_of_interest} Coherence (Z-transformed)', fontsize=14)\n",
    "#     sns.heatmap(task_data_df.set_index('trials').T, cmap='Purples', ax=ax, cbar_kws={'label': f'{band_of_interest} Coherence (Z-transformed)'}, vmin=vmin, vmax=vmax)\n",
    "    # If you want to see the unique IDs themselves, uncomment the next line\n",
    "    # print(f\"Unique IDs: {unique_ids}\")\n",
    "aon_power_per_trial_df = power_per_trial_df[power_per_trial_df['channel'] == 'AON']\n",
    "vHp_power_per_trial_df = power_per_trial_df[power_per_trial_df['channel'] == 'vHp']\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(20, 10), sharex=True, constrained_layout=True)    \n",
    "task_list = ['BWcontext', 'BWnocontext']\n",
    "for axi, task_name in enumerate(task_list):\n",
    "    print(f\"Task: {task_name}\")\n",
    "    coherence_task_data = behavior_coherence_compiled_data_df_truncated[behavior_coherence_compiled_data_df_truncated['task'] == task_name]\n",
    "    aon_power_task_data = aon_power_per_trial_df[aon_power_per_trial_df['task'] == task_name]\n",
    "    vhp_power_task_data = vHp_power_per_trial_df[vHp_power_per_trial_df['task'] == task_name]    \n",
    "    coherence_task_dict = {}\n",
    "    aon_power_task_dict = {}\n",
    "    vhp_power_task_dict = {}\n",
    "    experiment_ids = coherence_task_data['unique_id'].unique()\n",
    "    for experiment_idi in experiment_ids:\n",
    "        \n",
    "        rat_idi = experiment_idi.split('_')[0]\n",
    "        date_idi = experiment_idi.split('_')[1][-8:]\n",
    "        rat_date = rat_idi + '_' + date_idi\n",
    "\n",
    "        coherence_experiment_data=coherence_task_data[coherence_task_data['unique_id'] == experiment_idi]\n",
    "        \n",
    "        ## Power Data\n",
    "        aon_power_experiment_data = aon_power_task_data[aon_power_task_data['unique_id'] == rat_idi+\"_\"+task_name+'_'+date_idi]\n",
    "        vhp_power_experiment_data = vhp_power_task_data[vhp_power_task_data['unique_id'] == rat_idi+\"_\"+task_name+'_'+date_idi]\n",
    "        \n",
    "        aon_power_per_trial_list=[]\n",
    "        vhp_power_per_trial_list=[]\n",
    "        for triali in range(0, 20):\n",
    "            if triali not in aon_power_experiment_data['trial'].values:\n",
    "                print(f\"Trial {triali} not found in AON power data for {experiment_idi}. Skipping...\")\n",
    "                aon_power_per_trial_list.append(0)\n",
    "            else:\n",
    "                aon_power_trial = aon_power_experiment_data[aon_power_experiment_data['trial'] == triali][f'{band_event}_mt'].values\n",
    "                aon_power_per_trial_list.append(aon_power_trial.mean())\n",
    "            \n",
    "            if triali not in vhp_power_experiment_data['trial'].values:\n",
    "                print(f\"Trial {triali} not found in vHp power data for {experiment_idi}. Skipping...\")\n",
    "                vhp_power_per_trial_list.append(0)\n",
    "            else:\n",
    "                vhp_power_trial = vhp_power_experiment_data[vhp_power_experiment_data['trial'] == triali][f'{band_event}_mt'].values\n",
    "                vhp_power_per_trial_list.append(vhp_power_trial.mean())\n",
    "        aon_power_per_trial_list = np.array(aon_power_per_trial_list)\n",
    "        vhp_power_per_trial_list = np.array(vhp_power_per_trial_list)\n",
    "        \n",
    "        coherence_task_dict[rat_date] = coherence_experiment_data[band_event].values\n",
    "        aon_power_task_dict[rat_date] = aon_power_per_trial_list\n",
    "        vhp_power_task_dict[rat_date] = vhp_power_per_trial_list\n",
    "        \n",
    "    def dict_to_df(task_data_dict):\n",
    "        # Exclude 'trials' key from min/max calculation\n",
    "        arrays = [v for k, v in task_data_dict.items() if k != 'trials']\n",
    "        vmin = np.min(np.concatenate(arrays))\n",
    "        vmax = np.max(np.concatenate(arrays))\n",
    "        task_data_dict['trials'] = np.arange(start=1,stop=21,step=1, dtype=int)\n",
    "        task_data_df = pd.DataFrame.from_dict(task_data_dict, orient='index').T\n",
    "        task_data_df = task_data_df.fillna(0)  # Fill NaN values with 0\n",
    "        #task_data_df = task_data_df.loc[:, (task_data_df != 0).any(axis=0)]\n",
    "        return task_data_df, vmin, vmax\n",
    "    task_data_dicts ={ 'Coherence' : coherence_task_dict,\n",
    "                        'AON Power': aon_power_task_dict,\n",
    "                        'vHp Power': vhp_power_task_dict}\n",
    "    for j, (task_data_name, task_data_dict) in enumerate(task_data_dicts.items()):\n",
    "        task_data_df,vmin,vmax = dict_to_df(task_data_dict)\n",
    "        print(f\"Task Data for {task_data_name}:\", vmin, vmax)\n",
    "        ax = axs[j, axi]\n",
    "        ax.set_title(f'{task_name} - {band_of_interest} {event_of_interest} - {task_data_name}', fontsize=16)\n",
    "        ax.set_xlabel('Trials', fontsize=14)\n",
    "        ax.set_ylabel(f'{task_data_name} (Z-transformed)', fontsize=14)\n",
    "        sns.heatmap(task_data_df.set_index('trials').T, cmap='Purples', ax=ax, cbar_kws={'label': f'{task_data_name} (Z-transformed)'}, vmin=vmin, vmax=vmax)\n",
    "        ax.set_xticklabels(task_data_df['trials'], rotation=45)\n",
    "fig.savefig(savepath+f'{band_of_interest}_coherence_power_vs_trials_{event_of_interest}.png', format='png', dpi=600, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing Mann Whitney U Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window=0.4\n",
    "fs=2000\n",
    "behavior_coherence_compiled_data_df_truncated=pd.read_pickle(savepath+f'behavior_coherence_compiled_data_df_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "event_of_interest = 'around_dig' \n",
    "band_of_interest = 'beta'\n",
    "\n",
    "band_event = band_of_interest+'_'+event_of_interest\n",
    "\n",
    "vmin = behavior_coherence_compiled_data_df_truncated[band_event].min()\n",
    "vmax = behavior_coherence_compiled_data_df_truncated[band_event].max()\n",
    "\n",
    "bwcontext_data = behavior_coherence_compiled_data_df_truncated[behavior_coherence_compiled_data_df_truncated['task'] == 'BWcontext']\n",
    "bwnocontext_data = behavior_coherence_compiled_data_df_truncated[behavior_coherence_compiled_data_df_truncated['task'] == 'BWnocontext']\n",
    "\n",
    "t,p = scipy.stats.mannwhitneyu(bwcontext_data[band_event], bwnocontext_data[band_event])\n",
    "print(f\"Mann - Whitney U test between BWcontext and BWNocontext for {band_event}: t={t}, p={p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherence Phase Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the short MNE Epochs to Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window=0.7\n",
    "fs=2000\n",
    "behavior_coherence_compiled_data_df_truncated=pd.read_pickle(savepath+f'behavior_coherence_single_epochs_mne_truncated_{int(time_window*fs)}.pkl')\n",
    "print(behavior_coherence_compiled_data_df_truncated.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(coherence_functions)\n",
    "bands_dict = {'beta': [12, 30]}#, 'gamma': [30, 80],'theta':[4,12], 'total': [1, 100]}\n",
    "for col in ['pre_door', 'post_door', 'pre_dig', 'post_dig', 'around_door','around_dig']:\n",
    "    print(col)\n",
    "    for band, (band_start, band_end) in bands_dict.items():\n",
    "        behavior_coherence_compiled_data_df_truncated[band + '_' + col] = behavior_coherence_compiled_data_df_truncated[col].apply(lambda x: coherence_functions.convert_epoch_to_phase_behavior(x, band_start=band_start, band_end=band_end))\n",
    "behavior_coherence_compiled_data_df_truncated.drop(columns=['pre_door', 'post_door', 'pre_dig', 'post_dig', 'around_door','around_dig'], inplace=True)\n",
    "behavior_coherence_compiled_data_df_truncated.to_pickle(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\behavior_phase_compiled_data_df_truncated_{int(time_window*fs)}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_coherence_compiled_data_df_truncated=pd.read_pickle(savepath+f'behavior_phase_compiled_data_df_truncated_{int(time_window*fs)}.pkl')\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(3,2,figsize=(20, 20), sharey=True, constrained_layout=True)\n",
    "axs = axs.flatten()\n",
    "\n",
    "grouped_df=behavior_coherence_compiled_data_df_truncated.groupby(['task'])\n",
    "band='beta'\n",
    "event='around_dig'\n",
    "events_dict ={'pre_door':'Pre Door', 'post_door':'Post Door', 'pre_dig':'Pre Dig', 'post_dig':'Post Dig', 'around_door':'Around Door', 'around_dig':'Around Dig'}\n",
    "for i, event in enumerate(events_dict.keys()):\n",
    "    ax = axs[i]\n",
    "    ax.set_title(f'{events_dict[event]}', fontsize=16)\n",
    "    print(np.where(behavior_coherence_compiled_data_df_truncated['{}_{}'.format(band,event)]==0))\n",
    "    for task, group in grouped_df:\n",
    "        print(task)\n",
    "        print('{}_{}'.format(band,event))\n",
    "        group = group[(np.abs(group['{}_{}'.format(band,event)]))>0]\n",
    "        print(group['{}_{}'.format(band,event)].mean())\n",
    "        group['{}_{}'.format(band,event)] = group['{}_{}'.format(band,event)].apply(lambda x: np.arctanh(x))\n",
    "        print(group['{}_{}'.format(band,event)].mean())\n",
    "        group = group[(np.abs(group['time_to_dig'] - group['time_to_dig'].mean()) <= (3 * group['time_to_dig'].std()))]\n",
    "        group = group[(np.abs(group['{}_{}'.format(band,event)] - group['{}_{}'.format(band,event)].mean()) <= (3 * group['{}_{}'.format(band,event)].std()))]\n",
    "        sns.regplot(x='time_to_dig', y='{}_{}'.format(band,event), data=group, ax=ax, label=task)\n",
    "    ax.set_ylabel('Beta PLI', fontsize=14)\n",
    "    ax.set_xlabel('Time to Dig (s)', fontsize=14)\n",
    "    ax.legend(title='Task')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.tight_layout()\n",
    "fig.suptitle(f'Beta pli vs Time to Dig', fontsize=20, y=1.02)\n",
    "#fig.savefig(savepath+'beta_coherence_vs_time_to_dig.png', format='png', dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Plot histogram of beta values for each event, comparing BW Context and BW No Context\n",
    "fig, axs = plt.subplots(3, 2, figsize=(20, 20), sharey=True, constrained_layout=True)\n",
    "axs = axs.flatten()\n",
    "events = list(events_dict.keys())\n",
    "for i, event in enumerate(events):\n",
    "    ax = axs[i]\n",
    "    for task in ['BWcontext', 'BWnocontext']:\n",
    "        data = behavior_coherence_compiled_data_df_truncated[\n",
    "            behavior_coherence_compiled_data_df_truncated['task'] == task\n",
    "        ]['{}_{}'.format(band, event)].dropna()\n",
    "        ax.hist(data, bins=30, alpha=0.6, label=task, density=True)\n",
    "    ax.set_title(f'{events_dict[event]}', fontsize=16)\n",
    "    ax.set_xlabel('Beta Value', fontsize=14)\n",
    "    ax.set_ylabel('Density', fontsize=14)\n",
    "    ax.legend(title='Task')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.tight_layout()\n",
    "fig.suptitle('Histogram of Beta Values per Event', fontsize=20, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Behavior Coherence Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window=  0.7\n",
    "fs=2000\n",
    "\n",
    "loaded_df=pd.read_pickle(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\behavior_coherence_compiled_data_df_truncated_{int(time_window*fs)}.pkl')\n",
    "print(loaded_df.head())\n",
    "loaded_df=loaded_df[loaded_df['rat']!='dk3']\n",
    "print(loaded_df.head())\n",
    "coherence_band_event_df=loaded_df.loc[:,'beta_pre_door':]\n",
    "print(coherence_band_event_df.columns)\n",
    "group_melted=pd.melt(loaded_df, id_vars=['rat', 'task', 'date', 'trial','correct?', 'time_to_dig'], value_vars=coherence_band_event_df.columns, var_name='band_event', value_name='coherence')\n",
    "group_melted=group_melted[group_melted['rat']!='dk3']\n",
    "group_melted['band']=group_melted['band_event'].apply(lambda x: x.split('_')[0])\n",
    "group_melted['event']=group_melted['band_event'].apply(lambda x: x.split('_')[1:])\n",
    "group_melted['event']=group_melted['event'].apply(lambda x: x[0]+'_'+x[1])\n",
    "group_melted.drop(columns=['band_event'], inplace=True)\n",
    "group_melted['correct?']=group_melted['correct?'].apply(lambda x: 'Incorrect' if x=='0' else 'Correct')\n",
    "events_list=['pre_door','post_door','pre_dig','post_dig', 'around_door', 'around_dig']\n",
    "writer=pd.ExcelWriter(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\behavior_coherence_compiled_data_df_truncated_{int(time_window*fs)}.xlsx')\n",
    "for event in events_list:\n",
    "    event_df=group_melted[group_melted['event']==event]\n",
    "    event_df.to_excel(writer, sheet_name=event)\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This plots the number of correct vs incorrect trials and the coherence. The idea is to check if the correct trials in general had a higher coherence than incorrect trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bwcontext_df=group_melted[group_melted['task']=='BWcontext']\n",
    "correct_counts = bwcontext_df[bwcontext_df['correct?'] == 'Correct'].shape[0]\n",
    "incorrect_counts = bwcontext_df[bwcontext_df['correct?'] == 'Incorrect'].shape[0]\n",
    "print(f\"Number of Corrects: {correct_counts}\", f\"Number of Incorrects: {incorrect_counts}\", 'bwcontext')\n",
    "bwnocontext_df=group_melted[group_melted['task']=='BWnocontext']\n",
    "correct_counts = bwnocontext_df[bwnocontext_df['correct?'] == 'Correct'].shape[0]\n",
    "incorrect_counts = bwnocontext_df[bwnocontext_df['correct?'] == 'Incorrect'].shape[0]\n",
    "print(f\"Number of Corrects: {correct_counts}\", f\"Number of Incorrects: {incorrect_counts}\", 'bwnocontext')\n",
    "%matplotlib inline\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20, 10), sharex=True, constrained_layout=True)\n",
    "axs = axs.flatten()\n",
    "for i, event in enumerate(events_list):\n",
    "    ax=axs[i]\n",
    "    sns.boxplot(x='band', y='coherence', hue='correct?',hue_order=['Correct', 'Incorrect'], data=bwcontext_df[group_melted['event']==event], showfliers=False, ax=ax)\n",
    "    sns.stripplot(x='band', y='coherence', hue='correct?',hue_order=['Correct', 'Incorrect'], data=bwcontext_df[group_melted['event']==event], dodge=True, edgecolor='black', linewidth=1, jitter=True, ax=ax, size=1, legend=False)\n",
    "    ax.set_title(event)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Coherence')\n",
    "    ax.legend(title='Correct?')\n",
    "fig.suptitle(f'BW Context Coherence and Correctness')\n",
    "#fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\behavior_coherence_BWcontext.png')\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20, 10), sharex=True, constrained_layout=True)\n",
    "axs = axs.flatten()\n",
    "for i, event in enumerate(events_list):\n",
    "    ax=axs[i]\n",
    "    sns.boxplot(x='band', y='coherence', hue='correct?',hue_order=['Correct', 'Incorrect'], data=bwnocontext_df[group_melted['event']==event], showfliers=False, ax=ax)\n",
    "    sns.stripplot(x='band', y='coherence', hue='correct?',hue_order=['Correct', 'Incorrect'], data=bwnocontext_df[group_melted['event']==event], dodge=True, edgecolor='black', linewidth=1, jitter=True, ax=ax, size=1, legend=False)\n",
    "    ax.set_title(event)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Coherence')\n",
    "    ax.legend(title='Correct?')\n",
    "fig.suptitle(f'BW No Context Coherence and Correctness')\n",
    "#fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\behavior_coherence_BWnocontext.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks=['BWcontext','BWnocontext']\n",
    "loaded_df['correct?']=loaded_df['correct?'].apply(lambda x: 'Correct' if x=='1' else 'Incorrect')\n",
    "correctness=['Correct','Incorrect']\n",
    "fig, axs=plt.subplots(2,2, figsize=(20,10))\n",
    "axs=axs.flatten()\n",
    "axi=0\n",
    "for task in tasks:\n",
    "    for dig_type in correctness:\n",
    "\n",
    "        ax=axs[axi]\n",
    "        task_df=loaded_df[(loaded_df['task']==task) & (loaded_df['correct?']==dig_type)]\n",
    "        events_list=['pre_door','post_door','pre_dig','post_dig']\n",
    "        bands=['total','beta','theta','gamma']\n",
    "        \n",
    "        correlation_matrix = np.zeros((len(events_list), len(bands)))\n",
    "\n",
    "        for i, event in enumerate(events_list):\n",
    "            for j, band in enumerate(bands):\n",
    "                column_name = f'{band}_{event}'\n",
    "                correlation_matrix[i, j] = task_df['time_to_dig'].corr(task_df[column_name])\n",
    "        \n",
    "        cax = ax.matshow(correlation_matrix, cmap='coolwarm')\n",
    "        fig.colorbar(cax, ax=ax)\n",
    "\n",
    "        ax.set_xticks(np.arange(len(bands)))\n",
    "        ax.set_yticks(np.arange(len(events_list)))\n",
    "        ax.set_xticklabels(bands)\n",
    "        ax.set_yticklabels(events_list)\n",
    "        ax.set_title(f'{task} {dig_type}')\n",
    "        axi=axi+1\n",
    "fig.tight_layout()\n",
    "# plt.show()\n",
    "# plt.xlabel('Bands')\n",
    "# plt.ylabel('Events')\n",
    "# plt.title('Correlation Matrix Heatmap')\n",
    "# plt.show()        \n",
    "# bwcontext_df=loaded_df[(loaded_df['task']=='BWcontext') & (loaded_df['correct?']=='0')]\n",
    "# events_list=['pre_door','post_door','pre_dig','post_dig']\n",
    "# bands=['total','beta','theta','gamma']\n",
    "\n",
    "# correlation_matrix = np.zeros((len(events_list), len(bands)))\n",
    "\n",
    "# for i, event in enumerate(events_list):\n",
    "#     for j, band in enumerate(bands):\n",
    "#         column_name = f'{band}_{event}'\n",
    "#         correlation_matrix[i, j] = bwcontext_df['time_to_dig'].corr(bwcontext_df[column_name])\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(10, 8))\n",
    "# cax = ax.matshow(correlation_matrix, cmap='coolwarm')\n",
    "# fig.colorbar(cax)\n",
    "\n",
    "# ax.set_xticks(np.arange(len(bands)))\n",
    "# ax.set_yticks(np.arange(len(events_list)))\n",
    "# ax.set_xticklabels(bands)\n",
    "# ax.set_yticklabels(events_list)\n",
    "\n",
    "# plt.xlabel('Bands')\n",
    "# plt.ylabel('Events')\n",
    "# plt.title('Correlation Matrix Heatmap')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import statsmodels\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "loaded_df=pd.read_pickle(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\behavior_coherence_compiled_data_df_truncated_{int(time_window*fs)}.pkl')\n",
    "print(loaded_df.head())\n",
    "loaded_df=loaded_df[loaded_df['rat']!='dk3']\n",
    "print(loaded_df.head())\n",
    "\n",
    "fig, ax=plt.subplots(1,1, figsize=(10,10))\n",
    "tasks=['BWcontext','BWnocontext']\n",
    "loaded_df['correct?']=loaded_df['correct?'].apply(lambda x: 'Correct' if x=='1' else 'Incorrect')\n",
    "correctness=['Correct','Incorrect']\n",
    "bwcontext_incorrect_df=loaded_df[(loaded_df['task']=='BWcontext')]\n",
    "bwnocontext_incorrect_df=loaded_df[(loaded_df['task']=='BWnocontext')]\n",
    "x=bwcontext_incorrect_df['time_to_dig']\n",
    "y=bwcontext_incorrect_df['beta_pre_dig']\n",
    "df = pd.DataFrame({'coherence': y, 'time': x})\n",
    "df.drop(df[df['coherence'] == 0].index, inplace=True)  # Remove rows with coherence = 0\n",
    "\n",
    "try:\n",
    "    correlation_coefficient, p_value = pearsonr(df['coherence'], df['time'])\n",
    "\n",
    "    print(f\"Pearson Correlation Coefficient (r): {correlation_coefficient:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "    correlation_coefficient_s, p_value_s = spearmanr(df['coherence'], df['time'])\n",
    "\n",
    "    print(f\"\\nSpearman Rank Correlation Coefficient (rs): {correlation_coefficient_s:.4f}\")\n",
    "    print(f\"P-value: {p_value_s:.4f}\")\n",
    "\n",
    "    # --- Interpretation ---\n",
    "    alpha = 0.05 # Set your significance level\n",
    "    print(f\"\\nSignificance Level (alpha): {alpha}\")\n",
    "\n",
    "    if p_value <= alpha:\n",
    "        print(\"Result: Reject the null hypothesis (H0).\")\n",
    "        print(\"Conclusion: There is a statistically significant linear relationship between the variables.\")\n",
    "    else:\n",
    "        print(\"Result: Fail to reject the null hypothesis (H0).\")\n",
    "        print(\"Conclusion: There is NOT enough evidence for a statistically significant linear relationship.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"Please ensure 'coherence_data' and 'time_data' are populated correctly with numerical lists or arrays of the same length.\")\n",
    "# --- Create a Pandas DataFrame ---\n",
    "\n",
    "print(f\"Original number of data points: {len(df)}\")\n",
    "\n",
    "# --- Calculate IQR Fences for BOTH variables ---\n",
    "Q1_coherence = df['coherence'].quantile(0.25)\n",
    "Q3_coherence = df['coherence'].quantile(0.75)\n",
    "IQR_coherence = Q3_coherence - Q1_coherence\n",
    "lower_fence_coherence = Q1_coherence - 1.5 * IQR_coherence\n",
    "upper_fence_coherence = Q3_coherence + 1.5 * IQR_coherence\n",
    "\n",
    "Q1_time = df['time'].quantile(0.25)\n",
    "Q3_time = df['time'].quantile(0.75)\n",
    "IQR_time = Q3_time - Q1_time\n",
    "lower_fence_time = Q1_time - 1.5 * IQR_time\n",
    "upper_fence_time = Q3_time + 1.5 * IQR_time\n",
    "\n",
    "\n",
    "print(\"\\n--- Outlier Fences ---\")\n",
    "print(f\"Coherence: Lower={lower_fence_coherence:.2f}, Upper={upper_fence_coherence:.2f}\")\n",
    "print(f\"Time:      Lower={lower_fence_time:.2f}, Upper={upper_fence_time:.2f}\")\n",
    "\n",
    "# --- Identify outliers (points outside fences for EITHER variable) ---\n",
    "outlier_condition = (\n",
    "    (df['coherence'] < lower_fence_coherence) | (df['coherence'] > upper_fence_coherence) |\n",
    "    (df['time'] < lower_fence_time) | (df['time'] > upper_fence_time)\n",
    ")\n",
    "\n",
    "outliers = df[outlier_condition]\n",
    "print(f\"\\nIdentified {len(outliers)} potential outliers:\")\n",
    "print(outliers)\n",
    "# --- Filter out the outliers ---\n",
    "df_filtered = df[~outlier_condition] # Use ~ to negate the condition, keeping non-outliers\n",
    "print(f\"\\nNumber of data points after removing outliers: {len(df_filtered)}\")\n",
    "\n",
    "\n",
    "# --- Recalculate Correlation on Filtered Data ---\n",
    "if len(df_filtered) > 1: # Need at least 2 points to calculate correlation\n",
    "    # Extract the filtered data columns\n",
    "    coherence_filtered = df_filtered['coherence']\n",
    "    time_filtered = df_filtered['time']\n",
    "\n",
    "    # Calculate original correlation (optional comparison)\n",
    "    try:\n",
    "      original_r, original_p = pearsonr(df['coherence'], df['time'])\n",
    "      print(f\"\\nOriginal Correlation (r): {original_r:.4f}, p-value: {original_p:.4f}\")\n",
    "    except Exception as e:\n",
    "      print(f\"\\nCould not calculate original correlation: {e}\")\n",
    "\n",
    "\n",
    "    # Calculate filtered correlation\n",
    "    try:\n",
    "      filtered_r, filtered_p = pearsonr(coherence_filtered, time_filtered)\n",
    "      print(f\"Filtered Correlation (r): {filtered_r:.4f}, p-value: {filtered_p:.4f}\")\n",
    "\n",
    "      # Interpretation (using alpha = 0.05)\n",
    "      alpha = 0.05\n",
    "      if filtered_p <= alpha:\n",
    "          print(\"Result (Filtered): Reject H0. Statistically significant linear relationship found.\")\n",
    "      else:\n",
    "          print(\"Result (Filtered): Fail to reject H0. No statistically significant linear relationship found.\")\n",
    "    except Exception as e:\n",
    "      print(f\"Could not calculate filtered correlation: {e}\")\n",
    "\n",
    "sns.regplot(y=df['coherence'], x=df['time'], label='Context', ax=ax)\n",
    "sns.regplot(x=bwnocontext_incorrect_df['time_to_dig'], y=bwnocontext_incorrect_df['beta_pre_dig'], label='No context', ax=ax)\n",
    "plt.xlabel('Time to Dig (s)', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.ylabel('Coherence', fontsize=20)\n",
    "plt.title('AON-VHP Beta Pre Dig Coherence vs Time to Dig', fontsize=20)\n",
    "plt.legend(fontsize=20) \n",
    "plt.tight_layout()\n",
    "#fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\beta_pre_dig_vs_time_to_dig.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# bwcontext_correct_df=loaded_df[(loaded_df['task']=='BWcontext') & (loaded_df['correct?']=='Correct')]\n",
    "# bwnocontext_correct_df=loaded_df[(loaded_df['task']=='BWnocontext') & (loaded_df['correct?']=='Correct')]\n",
    "# sns.regplot(x=bwcontext_correct_df['time_to_dig'], y=bwcontext_correct_df['gamma_pre_dig'], label='BWcontext',robust=True, order=2)\n",
    "# sns.regplot(x=bwnocontext_correct_df['time_to_dig'], y=bwnocontext_correct_df['gamma_pre_dig'], label='BWnocontext',robust=True, order=2)\n",
    "# plt.xlabel('Time to Dig')\n",
    "# plt.ylabel('Beta Pre Dig')\n",
    "# plt.title('Beta Pre Dig vs Time to Dig for Correct Trials')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# --- Plot SVM regression for BWcontext and BWnocontext separately ---\n",
    "\n",
    "fig2, ax2 = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "# Prepare data for each task\n",
    "for task_name, color in colors.items():\n",
    "  task_df = loaded_df[(loaded_df['task'] == task_name) & (~outlier_condition)]\n",
    "  if task_df.empty:\n",
    "    print(f\"Skipping {task_name}: no data after outlier removal.\")\n",
    "    continue\n",
    "  X_task = task_df['time_to_dig'].values.reshape(-1, 1)\n",
    "  y_task = task_df['beta_pre_dig'].values\n",
    "\n",
    "  # Fit SVM regression\n",
    "  svm_poly_task = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVR(kernel='poly', degree=3, C=1.0, epsilon=0.1)\n",
    "  )\n",
    "  svm_poly_task.fit(X_task, y_task)\n",
    "  x_range_task = np.linspace(X_task.min(), X_task.max(), 200).reshape(-1, 1)\n",
    "  y_pred_task = svm_poly_task.predict(x_range_task)\n",
    "\n",
    "  # Scatter and SVM fit\n",
    "  ax2.scatter(X_task, y_task, color=color, alpha=0.5, label=f'{task_name} data')\n",
    "  ax2.plot(x_range_task, y_pred_task, color=color, linestyle='-', linewidth=2, label=f'{task_name} SVM fit')\n",
    "\n",
    "ax2.set_xlabel('Time to Dig (s)', fontsize=20)\n",
    "ax2.set_ylabel('Coherence', fontsize=20)\n",
    "ax2.set_title('SVM Poly Fit: BWcontext vs BWnocontext', fontsize=20)\n",
    "ax2.legend(fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Get residuals for each group\n",
    "residuals = {}\n",
    "for task_name in colors.keys():\n",
    "  task_df = loaded_df[(loaded_df['task'] == task_name) & (~outlier_condition)]\n",
    "  if task_df.empty:\n",
    "    print(f\"Skipping {task_name} residuals: no data after outlier removal.\")\n",
    "    continue\n",
    "  X_task = task_df['time_to_dig'].values.reshape(-1, 1)\n",
    "  y_task = task_df['beta_pre_dig'].values\n",
    "  svm_poly_task = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVR(kernel='poly', degree=3, C=1.0, epsilon=0.1)\n",
    "  )\n",
    "  svm_poly_task.fit(X_task, y_task)\n",
    "  y_pred_task = svm_poly_task.predict(X_task)\n",
    "  residuals[task_name] = y_task - y_pred_task\n",
    "\n",
    "# t-test on residuals (only if both groups have data)\n",
    "if all(k in residuals and len(residuals[k]) > 0 for k in ['BWcontext', 'BWnocontext']):\n",
    "  t_stat, p_val = ttest_ind(residuals['BWcontext'], residuals['BWnocontext'], equal_var=False)\n",
    "  print(f\"Residuals t-test: t={t_stat:.4f}, p={p_val:.4g}\")\n",
    "  if p_val < 0.05:\n",
    "    print(\"Statistically significant difference in SVM fit residuals between BWcontext and BWnocontext.\")\n",
    "  else:\n",
    "    print(\"No statistically significant difference in SVM fit residuals between BWcontext and BWnocontext.\")\n",
    "else:\n",
    "  print(\"Not enough data for both groups to perform residuals t-test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "print(f\"Number of data points: {len(df)}\")\n",
    "coherence_data = df['coherence']\n",
    "time_data = df['time']\n",
    "# --- Perform the Spearman Rank Correlation Test ---\n",
    "try:\n",
    "    correlation_coefficient_s, p_value_s = spearmanr(coherence_data, time_data)\n",
    "\n",
    "    print(f\"\\nSpearman Rank Correlation Coefficient (rs): {correlation_coefficient_s:.4f}\")\n",
    "    print(f\"P-value: {p_value_s:.4f}\")\n",
    "\n",
    "    # --- Interpretation ---\n",
    "    alpha = 0.05 # Set your significance level\n",
    "    print(f\"\\nSignificance Level (alpha): {alpha}\")\n",
    "\n",
    "    if p_value_s <= alpha:\n",
    "        print(\"Result: Reject the null hypothesis (H0).\")\n",
    "        print(\"Conclusion: There is a statistically significant monotonic relationship between the variables.\")\n",
    "    else:\n",
    "        print(\"Result: Fail to reject the null hypothesis (H0).\")\n",
    "        print(\"Conclusion: There is NOT enough evidence for a statistically significant monotonic relationship.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"Please ensure 'coherence_data' and 'time_data' are populated correctly with numerical lists or arrays of the same length.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lfp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
