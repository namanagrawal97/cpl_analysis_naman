{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Importing packages and the functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import getpass\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import functions\n",
    "import lfp_pre_processing_functions\n",
    "import power_functions\n",
    "import coherence_functions\n",
    "import spectrogram_plotting_functions\n",
    "import plotting_styles\n",
    "import scipy.stats\n",
    "import mne_connectivity\n",
    "import mne\n",
    "importlib.reload(functions) #loads our custom made functions.py file\n",
    "importlib.reload(spectrogram_plotting_functions)\n",
    "importlib.reload(plotting_styles)\n",
    "\n",
    "linestyle = plotting_styles.linestyles\n",
    "colors = plotting_styles.colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Loading the data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code fetches the current 'user' by using getpass. Then it sets the basepath, loads the files and specifies the savepath. Note that the basepath, files and savepath need to be changed depending on where you have kept the files and where you want the results to be stored. In this case, I have set it up to be in a particular folder in my Dropbox account, which is stored locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello sinha\n",
      "Base path: C:\\Users\\sinha\\Dropbox\\CPLab\n",
      "Save path: C:\\Users\\sinha\\Dropbox\\CPLab\\results\\\n",
      "['C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230529_dk1_nocontext.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230529_dk3_nocontext.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230529_dk5_nocontext.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230529_dk6_nocontext.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230531_dk1_nocontext_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230531_dk3_nocontext_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230531_dk5_nocontext_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230531_dk6_nocontext_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230609_dk1_BW_nocontext_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230609_dk3_BW_nocontext_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230610_dk1_BW_nocontext_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230610_dk3_BW_nocontext_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230615_dk5_BW_context_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230615_dk6_BW_context_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230616_dk5_BW_context_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230616_dk6_BW_context_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230623_dk1_BW_context_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230626_dk1_BW_context_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230626_dk5_BW_nocontext_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230626_dk6_BW_nocontext_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230627_dk1_BW_context_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230627_dk5_BW_nocontext_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230628_dk6_BW_nocontext_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230718_dk1_nocontext_os2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230718_dk5_nocontext_os2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230718_dk6_nocontext_os2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230719_dk1_nocontext_os2_day2_part1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230719_dk1_nocontext_os2_day2_part2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230719_dk5_nocontext_os2_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230719_dk6_nocontext_os2_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230807_dk3_BW_context_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230808_dk3_BW_context_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230808_dk5_BW_nocontext_day1_os2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230810_dk5_BW_nocontext_day2_os2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230817_dk1_BW_context_os2_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230818_dk1_BW_context_os2_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230818_dk3_BW_context_os2_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230821_dk3_BW_context_os2_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230821_dk5_BW_context_day1_os2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230822_dk1_BW_nocontext_os2_day1.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230823_dk1_BW_nocontext_os2_day2.mat', 'C:\\\\Users\\\\sinha\\\\Dropbox\\\\CPLab\\\\all_data_mat_250825\\\\20230823_dk5_BW_context_day2_os2.mat']\n"
     ]
    }
   ],
   "source": [
    "#Fetch the current user\n",
    "user= (getpass.getuser())\n",
    "print(\"Hello\", user)\n",
    "\n",
    "if user == 'CPLab':\n",
    "    base='D:\\\\Dropbox\\\\CPLab'\n",
    "else:\n",
    "    base='C:\\\\Users\\\\{}\\\\Dropbox\\\\CPLab'.format(user)\n",
    "#Set the basepath, savepath and load the data files\n",
    "files = glob.glob(base+'\\\\all_data_mat_250825\\\\*.mat')\n",
    "savepath = base+'\\\\results\\\\'\n",
    "print(\"Base path:\", base)\n",
    "print(\"Save path:\", savepath)\n",
    "print(files)\n",
    "\n",
    "\n",
    "all_bands_dict = {'total':[1,100], 'beta':[12,30], 'gamma':[30,80], 'theta':[4,12]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting LFP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20230529 dk1 nocontext\n",
      "20230529 dk3 nocontext\n",
      "20230529 dk5 nocontext\n",
      "20230529 dk6 nocontext\n",
      "20230531 dk1 nocontextday2\n",
      "20230531 dk3 nocontextday2\n",
      "20230531 dk5 nocontextday2\n",
      "20230531 dk6 nocontextday2\n",
      "20230609 dk1 BWnocontext\n",
      "['LFP1_vHp', 'LFP2_vHp', 'LFP3_AON', 'LFP4_AON', 'Memory', 'Ref', 'Respirat']\n",
      "Global start time: 0.0, Global end time: 1980.2432749999998\n",
      "3960487\n",
      "start_index: 0, end_index: 3960486\n",
      "LFP1_vHp\n",
      "(3960487,) (3960487,) 2000\n",
      "3960487\n",
      "start_index: 0, end_index: 3960487\n",
      "notch filter applied\n",
      "normalizing data\n",
      "101.713685 98.0 115.92368499999999 48.0\n",
      "333.79768499999994 98.0 338.87168499999996 49.0\n",
      "444.175685 98.0 446.85367999999994 49.0\n",
      "501.75368 98.0 504.30367999999993 49.0\n",
      "760.9496849999999 119.0 762.7096849999999 49.0\n",
      "860.7156849999999 119.0 862.9756849999999 49.0\n",
      "950.9996849999999 119.0 954.8776899999999 49.0\n",
      "1030.4496849999998 98.0 1032.059685 49.0\n",
      "1118.8436849999998 98.0 1120.763685 49.0\n",
      "1222.2376849999998 98.0 1224.1076799999998 49.0\n",
      "1292.675685 98.0 1297.08369 48.0\n",
      "1376.743685 119.0 1380.1036849999998 49.0\n",
      "1467.2696799999999 119.0 1469.7196849999998 49.0\n",
      "1533.645685 98.0 1536.311685 49.0\n",
      "1603.7656849999998 119.0 1606.5956849999998 49.0\n",
      "1650.251685 119.0 1652.089685 49.0\n",
      "1710.44968 119.0 1712.839685 49.0\n",
      "1752.8636849999998 119.0 1758.0296849999997 48.0\n",
      "1852.0356849999998 98.0 1854.6236849999998 48.0\n",
      "1911.945685 119.0 1913.2856799999997 49.0\n",
      "LFP2_vHp\n",
      "(3960487,) (3960487,) 2000\n",
      "3960487\n",
      "start_index: 0, end_index: 3960487\n",
      "notch filter applied\n",
      "normalizing data\n",
      "101.713685 98.0 115.92368499999999 48.0\n",
      "333.79768499999994 98.0 338.87168499999996 49.0\n",
      "444.175685 98.0 446.85367999999994 49.0\n",
      "501.75368 98.0 504.30367999999993 49.0\n",
      "760.9496849999999 119.0 762.7096849999999 49.0\n",
      "860.7156849999999 119.0 862.9756849999999 49.0\n",
      "950.9996849999999 119.0 954.8776899999999 49.0\n",
      "1030.4496849999998 98.0 1032.059685 49.0\n",
      "1118.8436849999998 98.0 1120.763685 49.0\n",
      "1222.2376849999998 98.0 1224.1076799999998 49.0\n",
      "1292.675685 98.0 1297.08369 48.0\n",
      "1376.743685 119.0 1380.1036849999998 49.0\n",
      "1467.2696799999999 119.0 1469.7196849999998 49.0\n",
      "1533.645685 98.0 1536.311685 49.0\n",
      "1603.7656849999998 119.0 1606.5956849999998 49.0\n",
      "1650.251685 119.0 1652.089685 49.0\n",
      "1710.44968 119.0 1712.839685 49.0\n",
      "1752.8636849999998 119.0 1758.0296849999997 48.0\n",
      "1852.0356849999998 98.0 1854.6236849999998 48.0\n",
      "1911.945685 119.0 1913.2856799999997 49.0\n",
      "LFP3_AON\n",
      "(3960487,) (3960487,) 2000\n",
      "3960487\n",
      "start_index: 0, end_index: 3960487\n",
      "notch filter applied\n",
      "normalizing data\n",
      "101.713685 98.0 115.92368499999999 48.0\n",
      "333.79768499999994 98.0 338.87168499999996 49.0\n",
      "444.175685 98.0 446.85367999999994 49.0\n",
      "501.75368 98.0 504.30367999999993 49.0\n",
      "760.9496849999999 119.0 762.7096849999999 49.0\n",
      "860.7156849999999 119.0 862.9756849999999 49.0\n",
      "950.9996849999999 119.0 954.8776899999999 49.0\n",
      "1030.4496849999998 98.0 1032.059685 49.0\n",
      "1118.8436849999998 98.0 1120.763685 49.0\n",
      "1222.2376849999998 98.0 1224.1076799999998 49.0\n",
      "1292.675685 98.0 1297.08369 48.0\n",
      "1376.743685 119.0 1380.1036849999998 49.0\n",
      "1467.2696799999999 119.0 1469.7196849999998 49.0\n",
      "1533.645685 98.0 1536.311685 49.0\n",
      "1603.7656849999998 119.0 1606.5956849999998 49.0\n",
      "1650.251685 119.0 1652.089685 49.0\n",
      "1710.44968 119.0 1712.839685 49.0\n",
      "1752.8636849999998 119.0 1758.0296849999997 48.0\n",
      "1852.0356849999998 98.0 1854.6236849999998 48.0\n",
      "1911.945685 119.0 1913.2856799999997 49.0\n",
      "LFP4_AON\n",
      "(3960487,) (3960487,) 2000\n",
      "3960487\n",
      "start_index: 0, end_index: 3960487\n",
      "notch filter applied\n",
      "normalizing data\n",
      "101.713685 98.0 115.92368499999999 48.0\n",
      "333.79768499999994 98.0 338.87168499999996 49.0\n",
      "444.175685 98.0 446.85367999999994 49.0\n",
      "501.75368 98.0 504.30367999999993 49.0\n",
      "760.9496849999999 119.0 762.7096849999999 49.0\n",
      "860.7156849999999 119.0 862.9756849999999 49.0\n",
      "950.9996849999999 119.0 954.8776899999999 49.0\n",
      "1030.4496849999998 98.0 1032.059685 49.0\n",
      "1118.8436849999998 98.0 1120.763685 49.0\n",
      "1222.2376849999998 98.0 1224.1076799999998 49.0\n",
      "1292.675685 98.0 1297.08369 48.0\n",
      "1376.743685 119.0 1380.1036849999998 49.0\n",
      "1467.2696799999999 119.0 1469.7196849999998 49.0\n",
      "1533.645685 98.0 1536.311685 49.0\n",
      "1603.7656849999998 119.0 1606.5956849999998 49.0\n",
      "1650.251685 119.0 1652.089685 49.0\n",
      "1710.44968 119.0 1712.839685 49.0\n",
      "1752.8636849999998 119.0 1758.0296849999997 48.0\n",
      "1852.0356849999998 98.0 1854.6236849999998 48.0\n",
      "1911.945685 119.0 1913.2856799999997 49.0\n",
      "20230609 dk3 BWnocontext\n",
      "['Keyboard', 'LFP1_AON', 'LFP1_vHp', 'LFP2_AON', 'LFP2_vHp', 'LFP3_AON', 'LFP4_AON', 'Ref', 'Respirat']\n",
      "Global start time: 0.0, Global end time: 1731.6383749999998\n",
      "3463277\n",
      "start_index: 0, end_index: 3463277\n",
      "LFP1_AON\n",
      "(3463277,) (3463277,) 2000\n",
      "3463277\n",
      "start_index: 0, end_index: 3463277\n",
      "notch filter applied\n",
      "normalizing data\n",
      "122.26209499999999 98.0 148.04408999999998 49.0\n",
      "280.62809 98.0 281.96808999999996 49.0\n",
      "330.65808999999996 98.0 339.67609 48.0\n",
      "406.986095 98.0 407.92609 49.0\n",
      "457.23608999999993 119.0 458.676095 49.0\n",
      "529.43009 119.0 530.93809 49.0\n",
      "923.1780949999999 119.0 930.9780949999999 49.0\n",
      "998.7060949999999 98.0 1000.64609 49.0\n",
      "1085.73409 98.0 1089.1140899999998 49.0\n",
      "1153.09009 98.0 1155.86009 49.0\n",
      "1199.248095 98.0 1207.1980899999999 49.0\n",
      "1274.1620899999998 119.0 1277.38809 49.0\n",
      "1322.460095 119.0 1325.640095 49.0\n",
      "1371.9940949999998 98.0 1373.27409 49.0\n",
      "1434.1700899999998 119.0 1435.56009 49.0\n",
      "1473.4160949999998 119.0 1475.736095 49.0\n",
      "1514.6660949999998 119.0 1516.112095 49.0\n",
      "1557.5780949999998 119.0 1560.2360899999999 49.0\n",
      "1606.412095 98.0 1608.1420899999998 49.0\n",
      "1644.88609 119.0 1649.89609 49.0\n",
      "LFP1_vHp\n",
      "(3463277,) (3463277,) 2000\n",
      "3463277\n",
      "start_index: 0, end_index: 3463277\n",
      "notch filter applied\n",
      "normalizing data\n",
      "122.26209499999999 98.0 148.04408999999998 49.0\n",
      "280.62809 98.0 281.96808999999996 49.0\n",
      "330.65808999999996 98.0 339.67609 48.0\n",
      "406.986095 98.0 407.92609 49.0\n",
      "457.23608999999993 119.0 458.676095 49.0\n",
      "529.43009 119.0 530.93809 49.0\n",
      "923.1780949999999 119.0 930.9780949999999 49.0\n",
      "998.7060949999999 98.0 1000.64609 49.0\n",
      "1085.73409 98.0 1089.1140899999998 49.0\n",
      "1153.09009 98.0 1155.86009 49.0\n",
      "1199.248095 98.0 1207.1980899999999 49.0\n",
      "1274.1620899999998 119.0 1277.38809 49.0\n",
      "1322.460095 119.0 1325.640095 49.0\n",
      "1371.9940949999998 98.0 1373.27409 49.0\n",
      "1434.1700899999998 119.0 1435.56009 49.0\n",
      "1473.4160949999998 119.0 1475.736095 49.0\n",
      "1514.6660949999998 119.0 1516.112095 49.0\n",
      "1557.5780949999998 119.0 1560.2360899999999 49.0\n",
      "1606.412095 98.0 1608.1420899999998 49.0\n",
      "1644.88609 119.0 1649.89609 49.0\n",
      "LFP2_AON\n",
      "(3463277,) (3463277,) 2000\n",
      "3463277\n",
      "start_index: 0, end_index: 3463277\n",
      "notch filter applied\n",
      "normalizing data\n",
      "122.26209499999999 98.0 148.04408999999998 49.0\n",
      "280.62809 98.0 281.96808999999996 49.0\n",
      "330.65808999999996 98.0 339.67609 48.0\n",
      "406.986095 98.0 407.92609 49.0\n",
      "457.23608999999993 119.0 458.676095 49.0\n",
      "529.43009 119.0 530.93809 49.0\n",
      "923.1780949999999 119.0 930.9780949999999 49.0\n",
      "998.7060949999999 98.0 1000.64609 49.0\n",
      "1085.73409 98.0 1089.1140899999998 49.0\n",
      "1153.09009 98.0 1155.86009 49.0\n",
      "1199.248095 98.0 1207.1980899999999 49.0\n",
      "1274.1620899999998 119.0 1277.38809 49.0\n",
      "1322.460095 119.0 1325.640095 49.0\n",
      "1371.9940949999998 98.0 1373.27409 49.0\n",
      "1434.1700899999998 119.0 1435.56009 49.0\n",
      "1473.4160949999998 119.0 1475.736095 49.0\n",
      "1514.6660949999998 119.0 1516.112095 49.0\n",
      "1557.5780949999998 119.0 1560.2360899999999 49.0\n",
      "1606.412095 98.0 1608.1420899999998 49.0\n",
      "1644.88609 119.0 1649.89609 49.0\n",
      "LFP2_vHp\n",
      "(3463277,) (3463277,) 2000\n",
      "3463277\n",
      "start_index: 0, end_index: 3463277\n",
      "notch filter applied\n",
      "normalizing data\n",
      "122.26209499999999 98.0 148.04408999999998 49.0\n",
      "280.62809 98.0 281.96808999999996 49.0\n",
      "330.65808999999996 98.0 339.67609 48.0\n",
      "406.986095 98.0 407.92609 49.0\n",
      "457.23608999999993 119.0 458.676095 49.0\n",
      "529.43009 119.0 530.93809 49.0\n",
      "923.1780949999999 119.0 930.9780949999999 49.0\n",
      "998.7060949999999 98.0 1000.64609 49.0\n",
      "1085.73409 98.0 1089.1140899999998 49.0\n",
      "1153.09009 98.0 1155.86009 49.0\n",
      "1199.248095 98.0 1207.1980899999999 49.0\n",
      "1274.1620899999998 119.0 1277.38809 49.0\n",
      "1322.460095 119.0 1325.640095 49.0\n",
      "1371.9940949999998 98.0 1373.27409 49.0\n",
      "1434.1700899999998 119.0 1435.56009 49.0\n",
      "1473.4160949999998 119.0 1475.736095 49.0\n",
      "1514.6660949999998 119.0 1516.112095 49.0\n",
      "1557.5780949999998 119.0 1560.2360899999999 49.0\n",
      "1606.412095 98.0 1608.1420899999998 49.0\n",
      "1644.88609 119.0 1649.89609 49.0\n",
      "LFP3_AON\n",
      "(3463277,) (3463277,) 2000\n",
      "3463277\n",
      "start_index: 0, end_index: 3463277\n",
      "notch filter applied\n",
      "normalizing data\n",
      "122.26209499999999 98.0 148.04408999999998 49.0\n",
      "280.62809 98.0 281.96808999999996 49.0\n",
      "330.65808999999996 98.0 339.67609 48.0\n",
      "406.986095 98.0 407.92609 49.0\n",
      "457.23608999999993 119.0 458.676095 49.0\n",
      "529.43009 119.0 530.93809 49.0\n",
      "923.1780949999999 119.0 930.9780949999999 49.0\n",
      "998.7060949999999 98.0 1000.64609 49.0\n",
      "1085.73409 98.0 1089.1140899999998 49.0\n",
      "1153.09009 98.0 1155.86009 49.0\n",
      "1199.248095 98.0 1207.1980899999999 49.0\n",
      "1274.1620899999998 119.0 1277.38809 49.0\n",
      "1322.460095 119.0 1325.640095 49.0\n",
      "1371.9940949999998 98.0 1373.27409 49.0\n",
      "1434.1700899999998 119.0 1435.56009 49.0\n",
      "1473.4160949999998 119.0 1475.736095 49.0\n",
      "1514.6660949999998 119.0 1516.112095 49.0\n",
      "1557.5780949999998 119.0 1560.2360899999999 49.0\n",
      "1606.412095 98.0 1608.1420899999998 49.0\n",
      "1644.88609 119.0 1649.89609 49.0\n",
      "LFP4_AON\n",
      "(3463277,) (3463277,) 2000\n",
      "3463277\n",
      "start_index: 0, end_index: 3463277\n",
      "notch filter applied\n",
      "normalizing data\n",
      "122.26209499999999 98.0 148.04408999999998 49.0\n",
      "280.62809 98.0 281.96808999999996 49.0\n",
      "330.65808999999996 98.0 339.67609 48.0\n",
      "406.986095 98.0 407.92609 49.0\n",
      "457.23608999999993 119.0 458.676095 49.0\n",
      "529.43009 119.0 530.93809 49.0\n",
      "923.1780949999999 119.0 930.9780949999999 49.0\n",
      "998.7060949999999 98.0 1000.64609 49.0\n",
      "1085.73409 98.0 1089.1140899999998 49.0\n",
      "1153.09009 98.0 1155.86009 49.0\n",
      "1199.248095 98.0 1207.1980899999999 49.0\n",
      "1274.1620899999998 119.0 1277.38809 49.0\n",
      "1322.460095 119.0 1325.640095 49.0\n",
      "1371.9940949999998 98.0 1373.27409 49.0\n",
      "1434.1700899999998 119.0 1435.56009 49.0\n",
      "1473.4160949999998 119.0 1475.736095 49.0\n",
      "1514.6660949999998 119.0 1516.112095 49.0\n",
      "1557.5780949999998 119.0 1560.2360899999999 49.0\n",
      "1606.412095 98.0 1608.1420899999998 49.0\n",
      "1644.88609 119.0 1649.89609 49.0\n",
      "20230610 dk1 BWnocontext\n",
      "['Keyboard', 'LFP1_vHp', 'LFP2_vHp', 'LFP3_AON', 'Ref', 'Respirat']\n",
      "Global start time: 0.0, Global end time: 1598.8853749999998\n",
      "3197771\n",
      "start_index: 0, end_index: 3197771\n",
      "LFP1_vHp\n",
      "(3197771,) (3197771,) 2000\n",
      "3197771\n",
      "start_index: 0, end_index: 3197771\n",
      "notch filter applied\n",
      "normalizing data\n",
      "136.95642999999998 119.0 141.87443 49.0\n",
      "195.78042499999998 98.0 198.710425 49.0\n",
      "247.90842499999997 119.0 249.61842499999997 49.0\n",
      "352.96842999999996 119.0 354.85842499999995 49.0\n",
      "444.698425 98.0 446.89842999999996 49.0\n",
      "497.97842499999996 119.0 500.30642499999993 49.0\n",
      "574.7464299999999 119.0 576.6564249999999 48.0\n",
      "687.052425 119.0 688.832425 49.0\n",
      "754.048425 98.0 756.50843 49.0\n",
      "813.2964249999999 98.0 814.9164249999999 49.0\n",
      "877.302425 119.0 881.240425 49.0\n",
      "946.2504299999999 98.0 948.9004249999999 49.0\n",
      "1021.5084249999999 98.0 1024.6144299999999 49.0\n",
      "1090.644425 119.0 1092.714425 49.0\n",
      "1136.722425 119.0 1139.982425 49.0\n",
      "1188.4764249999998 98.0 1190.6064299999998 49.0\n",
      "1240.796425 98.0 1244.06443 49.0\n",
      "1388.702425 119.0 1391.452425 49.0\n",
      "1467.01443 119.0 1468.732425 49.0\n",
      "1548.490425 119.0 1551.0384199999999 49.0\n",
      "LFP2_vHp\n",
      "(3197771,) (3197771,) 2000\n",
      "3197771\n",
      "start_index: 0, end_index: 3197771\n",
      "notch filter applied\n",
      "normalizing data\n",
      "136.95642999999998 119.0 141.87443 49.0\n",
      "195.78042499999998 98.0 198.710425 49.0\n",
      "247.90842499999997 119.0 249.61842499999997 49.0\n",
      "352.96842999999996 119.0 354.85842499999995 49.0\n",
      "444.698425 98.0 446.89842999999996 49.0\n",
      "497.97842499999996 119.0 500.30642499999993 49.0\n",
      "574.7464299999999 119.0 576.6564249999999 48.0\n",
      "687.052425 119.0 688.832425 49.0\n",
      "754.048425 98.0 756.50843 49.0\n",
      "813.2964249999999 98.0 814.9164249999999 49.0\n",
      "877.302425 119.0 881.240425 49.0\n",
      "946.2504299999999 98.0 948.9004249999999 49.0\n",
      "1021.5084249999999 98.0 1024.6144299999999 49.0\n",
      "1090.644425 119.0 1092.714425 49.0\n",
      "1136.722425 119.0 1139.982425 49.0\n",
      "1188.4764249999998 98.0 1190.6064299999998 49.0\n",
      "1240.796425 98.0 1244.06443 49.0\n",
      "1388.702425 119.0 1391.452425 49.0\n",
      "1467.01443 119.0 1468.732425 49.0\n",
      "1548.490425 119.0 1551.0384199999999 49.0\n",
      "LFP3_AON\n",
      "(3197771,) (3197771,) 2000\n",
      "3197771\n",
      "start_index: 0, end_index: 3197771\n",
      "notch filter applied\n",
      "normalizing data\n",
      "136.95642999999998 119.0 141.87443 49.0\n",
      "195.78042499999998 98.0 198.710425 49.0\n",
      "247.90842499999997 119.0 249.61842499999997 49.0\n",
      "352.96842999999996 119.0 354.85842499999995 49.0\n",
      "444.698425 98.0 446.89842999999996 49.0\n",
      "497.97842499999996 119.0 500.30642499999993 49.0\n",
      "574.7464299999999 119.0 576.6564249999999 48.0\n",
      "687.052425 119.0 688.832425 49.0\n",
      "754.048425 98.0 756.50843 49.0\n",
      "813.2964249999999 98.0 814.9164249999999 49.0\n",
      "877.302425 119.0 881.240425 49.0\n",
      "946.2504299999999 98.0 948.9004249999999 49.0\n",
      "1021.5084249999999 98.0 1024.6144299999999 49.0\n",
      "1090.644425 119.0 1092.714425 49.0\n",
      "1136.722425 119.0 1139.982425 49.0\n",
      "1188.4764249999998 98.0 1190.6064299999998 49.0\n",
      "1240.796425 98.0 1244.06443 49.0\n",
      "1388.702425 119.0 1391.452425 49.0\n",
      "1467.01443 119.0 1468.732425 49.0\n",
      "1548.490425 119.0 1551.0384199999999 49.0\n",
      "20230610 dk3 BWnocontext\n",
      "['LFP1_vHp', 'LFP2_vHp', 'LFP4_AON', 'Memory', 'Ref', 'Respirat']\n",
      "Global start time: 2.4999999999999998e-05, Global end time: 1607.4207749999998\n",
      "3214842\n",
      "start_index: 0, end_index: 3214841\n",
      "LFP1_vHp\n",
      "(3214842,) (3214842,) 2000\n",
      "3214842\n",
      "start_index: 0, end_index: 3214842\n",
      "notch filter applied\n",
      "normalizing data\n",
      "100.64680999999999 119.0 109.60880999999999 49.0\n",
      "268.520805 98.0 271.09880499999997 49.0\n",
      "321.48880499999996 119.0 325.13881 48.0\n",
      "370.850805 119.0 372.36080499999997 48.0\n",
      "427.956805 98.0 429.946805 49.0\n",
      "507.61480499999993 119.0 509.51280499999996 48.0\n",
      "560.218805 119.0 561.96881 49.0\n",
      "607.0428099999999 119.0 608.940805 48.0\n",
      "660.3228099999999 98.0 661.522805 49.0\n",
      "718.4288099999999 98.0 720.1588099999999 49.0\n",
      "772.8228099999999 119.0 774.892805 48.0\n",
      "824.95481 98.0 826.4148099999999 49.0\n",
      "941.2708049999999 98.0 943.2588099999999 48.0\n",
      "1019.4768099999999 119.0 1022.474805 49.0\n",
      "1161.12881 119.0 1163.7068049999998 49.0\n",
      "1211.700805 98.0 1226.978805 49.0\n",
      "1322.8228049999998 98.0 1327.6588049999998 48.0\n",
      "1441.2748049999998 98.0 1449.7148049999998 49.0\n",
      "1492.48481 119.0 1494.4648049999998 49.0\n",
      "1544.3908099999999 119.0 1547.7568099999999 49.0\n",
      "LFP2_vHp\n",
      "(3214842,) (3214842,) 2000\n",
      "3214842\n",
      "start_index: 0, end_index: 3214842\n",
      "notch filter applied\n",
      "normalizing data\n",
      "100.64680999999999 119.0 109.60880999999999 49.0\n",
      "268.520805 98.0 271.09880499999997 49.0\n",
      "321.48880499999996 119.0 325.13881 48.0\n",
      "370.850805 119.0 372.36080499999997 48.0\n",
      "427.956805 98.0 429.946805 49.0\n",
      "507.61480499999993 119.0 509.51280499999996 48.0\n",
      "560.218805 119.0 561.96881 49.0\n",
      "607.0428099999999 119.0 608.940805 48.0\n",
      "660.3228099999999 98.0 661.522805 49.0\n",
      "718.4288099999999 98.0 720.1588099999999 49.0\n",
      "772.8228099999999 119.0 774.892805 48.0\n",
      "824.95481 98.0 826.4148099999999 49.0\n",
      "941.2708049999999 98.0 943.2588099999999 48.0\n",
      "1019.4768099999999 119.0 1022.474805 49.0\n",
      "1161.12881 119.0 1163.7068049999998 49.0\n",
      "1211.700805 98.0 1226.978805 49.0\n",
      "1322.8228049999998 98.0 1327.6588049999998 48.0\n",
      "1441.2748049999998 98.0 1449.7148049999998 49.0\n",
      "1492.48481 119.0 1494.4648049999998 49.0\n",
      "1544.3908099999999 119.0 1547.7568099999999 49.0\n",
      "LFP4_AON\n",
      "(3214842,) (3214842,) 2000\n",
      "3214842\n",
      "start_index: 0, end_index: 3214842\n",
      "notch filter applied\n",
      "normalizing data\n",
      "100.64680999999999 119.0 109.60880999999999 49.0\n",
      "268.520805 98.0 271.09880499999997 49.0\n",
      "321.48880499999996 119.0 325.13881 48.0\n",
      "370.850805 119.0 372.36080499999997 48.0\n",
      "427.956805 98.0 429.946805 49.0\n",
      "507.61480499999993 119.0 509.51280499999996 48.0\n",
      "560.218805 119.0 561.96881 49.0\n",
      "607.0428099999999 119.0 608.940805 48.0\n",
      "660.3228099999999 98.0 661.522805 49.0\n",
      "718.4288099999999 98.0 720.1588099999999 49.0\n",
      "772.8228099999999 119.0 774.892805 48.0\n",
      "824.95481 98.0 826.4148099999999 49.0\n",
      "941.2708049999999 98.0 943.2588099999999 48.0\n",
      "1019.4768099999999 119.0 1022.474805 49.0\n",
      "1161.12881 119.0 1163.7068049999998 49.0\n",
      "1211.700805 98.0 1226.978805 49.0\n",
      "1322.8228049999998 98.0 1327.6588049999998 48.0\n",
      "1441.2748049999998 98.0 1449.7148049999998 49.0\n",
      "1492.48481 119.0 1494.4648049999998 49.0\n",
      "1544.3908099999999 119.0 1547.7568099999999 49.0\n",
      "20230615 dk5 BWcontext\n",
      "['Keyboard', 'LFP1_vHp', 'LFP2_vHp', 'LFP3_AON', 'LFP4_AON', 'Ref', 'Respirat']\n",
      "Global start time: 0.0, Global end time: 1199.0873749999998\n",
      "2398175\n",
      "start_index: 0, end_index: 2398175\n",
      "LFP1_vHp\n",
      "(2398175,) (2398175,) 2000\n",
      "2398175\n",
      "start_index: 0, end_index: 2398175\n",
      "notch filter applied\n",
      "normalizing data\n",
      "80.972805 98.0 84.172805 49.0\n",
      "167.40880499999997 98.0 172.96880499999997 49.0\n",
      "236.53879999999998 119.0 239.75679999999997 48.0\n",
      "297.51280499999996 98.0 299.52279999999996 49.0\n",
      "350.1828 98.0 351.782805 49.0\n",
      "424.194805 98.0 428.10279999999995 49.0\n",
      "499.28279999999995 98.0 501.01280499999996 49.0\n",
      "568.4327999999999 119.0 571.4227999999999 48.0\n",
      "625.974805 98.0 628.5047999999999 49.0\n",
      "676.3008 119.0 679.3408049999999 48.0\n",
      "731.872805 119.0 735.550805 49.0\n",
      "781.7608049999999 98.0 784.7588049999999 49.0\n",
      "830.8987999999999 119.0 834.5788049999999 49.0\n",
      "876.5908049999999 119.0 880.9488049999999 49.0\n",
      "930.3208 119.0 932.1607999999999 49.0\n",
      "976.2028049999999 119.0 978.6908 49.0\n",
      "1021.1208049999999 98.0 1022.8408049999999 49.0\n",
      "1063.6668 119.0 1065.606805 49.0\n",
      "1116.384805 98.0 1119.664805 49.0\n",
      "1166.3267999999998 98.0 1168.4768049999998 49.0\n",
      "LFP2_vHp\n",
      "(2398175,) (2398175,) 2000\n",
      "2398175\n",
      "start_index: 0, end_index: 2398175\n",
      "notch filter applied\n",
      "normalizing data\n",
      "80.972805 98.0 84.172805 49.0\n",
      "167.40880499999997 98.0 172.96880499999997 49.0\n",
      "236.53879999999998 119.0 239.75679999999997 48.0\n",
      "297.51280499999996 98.0 299.52279999999996 49.0\n",
      "350.1828 98.0 351.782805 49.0\n",
      "424.194805 98.0 428.10279999999995 49.0\n",
      "499.28279999999995 98.0 501.01280499999996 49.0\n",
      "568.4327999999999 119.0 571.4227999999999 48.0\n",
      "625.974805 98.0 628.5047999999999 49.0\n",
      "676.3008 119.0 679.3408049999999 48.0\n",
      "731.872805 119.0 735.550805 49.0\n",
      "781.7608049999999 98.0 784.7588049999999 49.0\n",
      "830.8987999999999 119.0 834.5788049999999 49.0\n",
      "876.5908049999999 119.0 880.9488049999999 49.0\n",
      "930.3208 119.0 932.1607999999999 49.0\n",
      "976.2028049999999 119.0 978.6908 49.0\n",
      "1021.1208049999999 98.0 1022.8408049999999 49.0\n",
      "1063.6668 119.0 1065.606805 49.0\n",
      "1116.384805 98.0 1119.664805 49.0\n",
      "1166.3267999999998 98.0 1168.4768049999998 49.0\n",
      "LFP3_AON\n",
      "(2398175,) (2398175,) 2000\n",
      "2398175\n",
      "start_index: 0, end_index: 2398175\n",
      "notch filter applied\n",
      "normalizing data\n",
      "80.972805 98.0 84.172805 49.0\n",
      "167.40880499999997 98.0 172.96880499999997 49.0\n",
      "236.53879999999998 119.0 239.75679999999997 48.0\n",
      "297.51280499999996 98.0 299.52279999999996 49.0\n",
      "350.1828 98.0 351.782805 49.0\n",
      "424.194805 98.0 428.10279999999995 49.0\n",
      "499.28279999999995 98.0 501.01280499999996 49.0\n",
      "568.4327999999999 119.0 571.4227999999999 48.0\n",
      "625.974805 98.0 628.5047999999999 49.0\n",
      "676.3008 119.0 679.3408049999999 48.0\n",
      "731.872805 119.0 735.550805 49.0\n",
      "781.7608049999999 98.0 784.7588049999999 49.0\n",
      "830.8987999999999 119.0 834.5788049999999 49.0\n",
      "876.5908049999999 119.0 880.9488049999999 49.0\n",
      "930.3208 119.0 932.1607999999999 49.0\n",
      "976.2028049999999 119.0 978.6908 49.0\n",
      "1021.1208049999999 98.0 1022.8408049999999 49.0\n",
      "1063.6668 119.0 1065.606805 49.0\n",
      "1116.384805 98.0 1119.664805 49.0\n",
      "1166.3267999999998 98.0 1168.4768049999998 49.0\n",
      "LFP4_AON\n",
      "(2398175,) (2398175,) 2000\n",
      "2398175\n",
      "start_index: 0, end_index: 2398175\n",
      "notch filter applied\n",
      "normalizing data\n",
      "80.972805 98.0 84.172805 49.0\n",
      "167.40880499999997 98.0 172.96880499999997 49.0\n",
      "236.53879999999998 119.0 239.75679999999997 48.0\n",
      "297.51280499999996 98.0 299.52279999999996 49.0\n",
      "350.1828 98.0 351.782805 49.0\n",
      "424.194805 98.0 428.10279999999995 49.0\n",
      "499.28279999999995 98.0 501.01280499999996 49.0\n",
      "568.4327999999999 119.0 571.4227999999999 48.0\n",
      "625.974805 98.0 628.5047999999999 49.0\n",
      "676.3008 119.0 679.3408049999999 48.0\n",
      "731.872805 119.0 735.550805 49.0\n",
      "781.7608049999999 98.0 784.7588049999999 49.0\n",
      "830.8987999999999 119.0 834.5788049999999 49.0\n",
      "876.5908049999999 119.0 880.9488049999999 49.0\n",
      "930.3208 119.0 932.1607999999999 49.0\n",
      "976.2028049999999 119.0 978.6908 49.0\n",
      "1021.1208049999999 98.0 1022.8408049999999 49.0\n",
      "1063.6668 119.0 1065.606805 49.0\n",
      "1116.384805 98.0 1119.664805 49.0\n",
      "1166.3267999999998 98.0 1168.4768049999998 49.0\n",
      "20230615 dk6 BWcontext\n",
      "['Keyboard', 'LFP1_vHp', 'LFP2_vHp', 'LFP4_AON', 'Ref', 'Respirat']\n",
      "Global start time: 2.4999999999999998e-05, Global end time: 1347.6775249999998\n",
      "2695355\n",
      "start_index: 0, end_index: 2695355\n",
      "LFP1_vHp\n",
      "(2695355,) (2695355,) 2000\n",
      "2695355\n",
      "start_index: 0, end_index: 2695355\n",
      "notch filter applied\n",
      "normalizing data\n",
      "89.50689499999999 98.0 98.550895 48.0\n",
      "328.924895 98.0 331.3649 49.0\n",
      "467.01489 119.0 468.922895 48.0\n",
      "532.810895 98.0 535.420895 49.0\n",
      "565.8368999999999 98.0 569.386895 49.0\n",
      "614.4149 98.0 617.084895 49.0\n",
      "681.4808999999999 98.0 684.19089 49.0\n",
      "722.5308899999999 119.0 726.4288949999999 48.0\n",
      "773.4628949999999 98.0 774.1928899999999 49.0\n",
      "811.8508999999999 119.0 817.098895 49.0\n",
      "850.4668999999999 119.0 852.5868949999999 49.0\n",
      "886.7948999999999 98.0 889.4628999999999 49.0\n",
      "925.3228949999999 119.0 927.7508949999999 49.0\n",
      "969.7848999999999 119.0 972.1048949999999 49.0\n",
      "1024.966895 119.0 1027.9069049999998 49.0\n",
      "1066.682895 119.0 1068.488895 49.0\n",
      "1143.6569 98.0 1146.1348899999998 49.0\n",
      "1209.6688949999998 119.0 1210.9188949999998 49.0\n",
      "1254.150895 98.0 1256.1108949999998 49.0\n",
      "1306.5348949999998 98.0 1320.662895 48.0\n",
      "LFP2_vHp\n",
      "(2695355,) (2695355,) 2000\n",
      "2695355\n",
      "start_index: 0, end_index: 2695355\n",
      "notch filter applied\n",
      "normalizing data\n",
      "89.50689499999999 98.0 98.550895 48.0\n",
      "328.924895 98.0 331.3649 49.0\n",
      "467.01489 119.0 468.922895 48.0\n",
      "532.810895 98.0 535.420895 49.0\n",
      "565.8368999999999 98.0 569.386895 49.0\n",
      "614.4149 98.0 617.084895 49.0\n",
      "681.4808999999999 98.0 684.19089 49.0\n",
      "722.5308899999999 119.0 726.4288949999999 48.0\n",
      "773.4628949999999 98.0 774.1928899999999 49.0\n",
      "811.8508999999999 119.0 817.098895 49.0\n",
      "850.4668999999999 119.0 852.5868949999999 49.0\n",
      "886.7948999999999 98.0 889.4628999999999 49.0\n",
      "925.3228949999999 119.0 927.7508949999999 49.0\n",
      "969.7848999999999 119.0 972.1048949999999 49.0\n",
      "1024.966895 119.0 1027.9069049999998 49.0\n",
      "1066.682895 119.0 1068.488895 49.0\n",
      "1143.6569 98.0 1146.1348899999998 49.0\n",
      "1209.6688949999998 119.0 1210.9188949999998 49.0\n",
      "1254.150895 98.0 1256.1108949999998 49.0\n",
      "1306.5348949999998 98.0 1320.662895 48.0\n",
      "LFP4_AON\n",
      "(2695356,) (2695356,) 2000\n",
      "2695355\n",
      "Warning: Data length (2695356) exceeds available space. Truncating.\n",
      "start_index: 0, end_index: 2695355\n",
      "notch filter applied\n",
      "normalizing data\n",
      "89.50689499999999 98.0 98.550895 48.0\n",
      "328.924895 98.0 331.3649 49.0\n",
      "467.01489 119.0 468.922895 48.0\n",
      "532.810895 98.0 535.420895 49.0\n",
      "565.8368999999999 98.0 569.386895 49.0\n",
      "614.4149 98.0 617.084895 49.0\n",
      "681.4808999999999 98.0 684.19089 49.0\n",
      "722.5308899999999 119.0 726.4288949999999 48.0\n",
      "773.4628949999999 98.0 774.1928899999999 49.0\n",
      "811.8508999999999 119.0 817.098895 49.0\n",
      "850.4668999999999 119.0 852.5868949999999 49.0\n",
      "886.7948999999999 98.0 889.4628999999999 49.0\n",
      "925.3228949999999 119.0 927.7508949999999 49.0\n",
      "969.7848999999999 119.0 972.1048949999999 49.0\n",
      "1024.966895 119.0 1027.9069049999998 49.0\n",
      "1066.682895 119.0 1068.488895 49.0\n",
      "1143.6569 98.0 1146.1348899999998 49.0\n",
      "1209.6688949999998 119.0 1210.9188949999998 49.0\n",
      "1254.150895 98.0 1256.1108949999998 49.0\n",
      "1306.5348949999998 98.0 1320.662895 48.0\n",
      "20230616 dk5 BWcontext\n",
      "['Keyboard', 'LFP1_vHp', 'LFP2_vHp', 'LFP3_AON', 'LFP4_AON', 'Ref', 'Respirat']\n",
      "Global start time: 0.0, Global end time: 1129.4820249999998\n",
      "2258965\n",
      "start_index: 0, end_index: 2258964\n",
      "LFP1_vHp\n",
      "(2258964,) (2258964,) 2000\n",
      "2258965\n",
      "start_index: 0, end_index: 2258964\n",
      "notch filter applied\n",
      "normalizing data\n",
      "64.741165 119.0 70.26916 49.0\n",
      "141.17116 98.0 146.141165 49.0\n",
      "280.81116 98.0 283.02115999999995 48.0\n",
      "325.35116 119.0 326.98716499999995 49.0\n",
      "365.06316 98.0 366.673165 49.0\n",
      "410.39116499999994 119.0 413.00115999999997 49.0\n",
      "456.30315999999993 119.0 459.07316 49.0\n",
      "495.58915999999994 98.0 497.87915999999996 49.0\n",
      "541.2311599999999 119.0 543.83116 49.0\n",
      "594.901165 98.0 599.71516 49.0\n",
      "640.06516 119.0 642.2551649999999 49.0\n",
      "681.7371649999999 98.0 684.047165 49.0\n",
      "746.989165 98.0 749.9491599999999 49.0\n",
      "795.821165 119.0 802.43716 49.0\n",
      "855.0011649999999 98.0 858.5911649999999 49.0\n",
      "897.8491599999999 119.0 900.3671599999999 49.0\n",
      "954.2791599999999 119.0 955.4591649999999 49.0\n",
      "999.8771599999999 119.0 1001.657165 49.0\n",
      "1052.02516 119.0 1055.5951599999999 49.0\n",
      "1102.309165 98.0 1103.9691599999999 49.0\n",
      "LFP2_vHp\n",
      "(2258964,) (2258964,) 2000\n",
      "2258965\n",
      "start_index: 0, end_index: 2258964\n",
      "notch filter applied\n",
      "normalizing data\n",
      "64.741165 119.0 70.26916 49.0\n",
      "141.17116 98.0 146.141165 49.0\n",
      "280.81116 98.0 283.02115999999995 48.0\n",
      "325.35116 119.0 326.98716499999995 49.0\n",
      "365.06316 98.0 366.673165 49.0\n",
      "410.39116499999994 119.0 413.00115999999997 49.0\n",
      "456.30315999999993 119.0 459.07316 49.0\n",
      "495.58915999999994 98.0 497.87915999999996 49.0\n",
      "541.2311599999999 119.0 543.83116 49.0\n",
      "594.901165 98.0 599.71516 49.0\n",
      "640.06516 119.0 642.2551649999999 49.0\n",
      "681.7371649999999 98.0 684.047165 49.0\n",
      "746.989165 98.0 749.9491599999999 49.0\n",
      "795.821165 119.0 802.43716 49.0\n",
      "855.0011649999999 98.0 858.5911649999999 49.0\n",
      "897.8491599999999 119.0 900.3671599999999 49.0\n",
      "954.2791599999999 119.0 955.4591649999999 49.0\n",
      "999.8771599999999 119.0 1001.657165 49.0\n",
      "1052.02516 119.0 1055.5951599999999 49.0\n",
      "1102.309165 98.0 1103.9691599999999 49.0\n",
      "LFP3_AON\n",
      "(2258965,) (2258965,) 2000\n",
      "2258965\n",
      "start_index: 0, end_index: 2258965\n",
      "notch filter applied\n",
      "normalizing data\n",
      "64.741165 119.0 70.26916 49.0\n",
      "141.17116 98.0 146.141165 49.0\n",
      "280.81116 98.0 283.02115999999995 48.0\n",
      "325.35116 119.0 326.98716499999995 49.0\n",
      "365.06316 98.0 366.673165 49.0\n",
      "410.39116499999994 119.0 413.00115999999997 49.0\n",
      "456.30315999999993 119.0 459.07316 49.0\n",
      "495.58915999999994 98.0 497.87915999999996 49.0\n",
      "541.2311599999999 119.0 543.83116 49.0\n",
      "594.901165 98.0 599.71516 49.0\n",
      "640.06516 119.0 642.2551649999999 49.0\n",
      "681.7371649999999 98.0 684.047165 49.0\n",
      "746.989165 98.0 749.9491599999999 49.0\n",
      "795.821165 119.0 802.43716 49.0\n",
      "855.0011649999999 98.0 858.5911649999999 49.0\n",
      "897.8491599999999 119.0 900.3671599999999 49.0\n",
      "954.2791599999999 119.0 955.4591649999999 49.0\n",
      "999.8771599999999 119.0 1001.657165 49.0\n",
      "1052.02516 119.0 1055.5951599999999 49.0\n",
      "1102.309165 98.0 1103.9691599999999 49.0\n",
      "LFP4_AON\n",
      "(2258965,) (2258965,) 2000\n",
      "2258965\n",
      "start_index: 0, end_index: 2258965\n",
      "notch filter applied\n",
      "normalizing data\n",
      "64.741165 119.0 70.26916 49.0\n",
      "141.17116 98.0 146.141165 49.0\n",
      "280.81116 98.0 283.02115999999995 48.0\n",
      "325.35116 119.0 326.98716499999995 49.0\n",
      "365.06316 98.0 366.673165 49.0\n",
      "410.39116499999994 119.0 413.00115999999997 49.0\n",
      "456.30315999999993 119.0 459.07316 49.0\n",
      "495.58915999999994 98.0 497.87915999999996 49.0\n",
      "541.2311599999999 119.0 543.83116 49.0\n",
      "594.901165 98.0 599.71516 49.0\n",
      "640.06516 119.0 642.2551649999999 49.0\n",
      "681.7371649999999 98.0 684.047165 49.0\n",
      "746.989165 98.0 749.9491599999999 49.0\n",
      "795.821165 119.0 802.43716 49.0\n",
      "855.0011649999999 98.0 858.5911649999999 49.0\n",
      "897.8491599999999 119.0 900.3671599999999 49.0\n",
      "954.2791599999999 119.0 955.4591649999999 49.0\n",
      "999.8771599999999 119.0 1001.657165 49.0\n",
      "1052.02516 119.0 1055.5951599999999 49.0\n",
      "1102.309165 98.0 1103.9691599999999 49.0\n",
      "20230616 dk6 BWcontext\n",
      "['Keyboard', 'LFP1_AON', 'LFP1_vHp', 'LFP2_vHp', 'LFP3_AON', 'LFP4_AON', 'Ref', 'Respirat']\n",
      "Global start time: 0.0, Global end time: 1105.6840499999998\n",
      "2211369\n",
      "start_index: 0, end_index: 2211368\n",
      "LFP1_AON\n",
      "(2211369,) (2211369,) 2000\n",
      "2211369\n",
      "start_index: 0, end_index: 2211369\n",
      "notch filter applied\n",
      "normalizing data\n",
      "94.81371499999999 119.0 104.29370999999999 48.0\n",
      "161.73570999999998 98.0 163.735715 49.0\n",
      "206.56170999999998 98.0 208.35170499999998 49.0\n",
      "264.17370999999997 119.0 266.55371499999995 49.0\n",
      "315.42971 98.0 317.28970999999996 49.0\n",
      "365.38570999999996 119.0 368.97571 49.0\n",
      "402.80771 119.0 406.283715 49.0\n",
      "514.4897149999999 98.0 517.73771 48.0\n",
      "555.82571 119.0 558.045715 49.0\n",
      "614.73771 98.0 617.347715 49.0\n",
      "656.917705 119.0 659.7077149999999 49.0\n",
      "695.55571 98.0 697.0537099999999 49.0\n",
      "740.2357149999999 98.0 743.0917099999999 49.0\n",
      "785.7797099999999 119.0 787.339715 49.0\n",
      "835.963715 98.0 838.1117099999999 49.0\n",
      "892.0877149999999 119.0 896.7177099999999 49.0\n",
      "937.1557149999999 119.0 939.7037099999999 49.0\n",
      "979.99371 119.0 981.473715 49.0\n",
      "1040.82971 119.0 1042.63771 49.0\n",
      "1080.7297099999998 98.0 1082.96771 49.0\n",
      "LFP1_vHp\n",
      "(2211368,) (2211368,) 2000\n",
      "2211369\n",
      "start_index: 0, end_index: 2211368\n",
      "notch filter applied\n",
      "normalizing data\n",
      "94.81371499999999 119.0 104.29370999999999 48.0\n",
      "161.73570999999998 98.0 163.735715 49.0\n",
      "206.56170999999998 98.0 208.35170499999998 49.0\n",
      "264.17370999999997 119.0 266.55371499999995 49.0\n",
      "315.42971 98.0 317.28970999999996 49.0\n",
      "365.38570999999996 119.0 368.97571 49.0\n",
      "402.80771 119.0 406.283715 49.0\n",
      "514.4897149999999 98.0 517.73771 48.0\n",
      "555.82571 119.0 558.045715 49.0\n",
      "614.73771 98.0 617.347715 49.0\n",
      "656.917705 119.0 659.7077149999999 49.0\n",
      "695.55571 98.0 697.0537099999999 49.0\n",
      "740.2357149999999 98.0 743.0917099999999 49.0\n",
      "785.7797099999999 119.0 787.339715 49.0\n",
      "835.963715 98.0 838.1117099999999 49.0\n",
      "892.0877149999999 119.0 896.7177099999999 49.0\n",
      "937.1557149999999 119.0 939.7037099999999 49.0\n",
      "979.99371 119.0 981.473715 49.0\n",
      "1040.82971 119.0 1042.63771 49.0\n",
      "1080.7297099999998 98.0 1082.96771 49.0\n",
      "LFP2_vHp\n",
      "(2211368,) (2211368,) 2000\n",
      "2211369\n",
      "start_index: 0, end_index: 2211368\n",
      "notch filter applied\n",
      "normalizing data\n",
      "94.81371499999999 119.0 104.29370999999999 48.0\n",
      "161.73570999999998 98.0 163.735715 49.0\n",
      "206.56170999999998 98.0 208.35170499999998 49.0\n",
      "264.17370999999997 119.0 266.55371499999995 49.0\n",
      "315.42971 98.0 317.28970999999996 49.0\n",
      "365.38570999999996 119.0 368.97571 49.0\n",
      "402.80771 119.0 406.283715 49.0\n",
      "514.4897149999999 98.0 517.73771 48.0\n",
      "555.82571 119.0 558.045715 49.0\n",
      "614.73771 98.0 617.347715 49.0\n",
      "656.917705 119.0 659.7077149999999 49.0\n",
      "695.55571 98.0 697.0537099999999 49.0\n",
      "740.2357149999999 98.0 743.0917099999999 49.0\n",
      "785.7797099999999 119.0 787.339715 49.0\n",
      "835.963715 98.0 838.1117099999999 49.0\n",
      "892.0877149999999 119.0 896.7177099999999 49.0\n",
      "937.1557149999999 119.0 939.7037099999999 49.0\n",
      "979.99371 119.0 981.473715 49.0\n",
      "1040.82971 119.0 1042.63771 49.0\n",
      "1080.7297099999998 98.0 1082.96771 49.0\n",
      "LFP3_AON\n",
      "(2211369,) (2211369,) 2000\n",
      "2211369\n",
      "start_index: 0, end_index: 2211369\n",
      "notch filter applied\n",
      "normalizing data\n",
      "94.81371499999999 119.0 104.29370999999999 48.0\n",
      "161.73570999999998 98.0 163.735715 49.0\n",
      "206.56170999999998 98.0 208.35170499999998 49.0\n",
      "264.17370999999997 119.0 266.55371499999995 49.0\n",
      "315.42971 98.0 317.28970999999996 49.0\n",
      "365.38570999999996 119.0 368.97571 49.0\n",
      "402.80771 119.0 406.283715 49.0\n",
      "514.4897149999999 98.0 517.73771 48.0\n",
      "555.82571 119.0 558.045715 49.0\n",
      "614.73771 98.0 617.347715 49.0\n",
      "656.917705 119.0 659.7077149999999 49.0\n",
      "695.55571 98.0 697.0537099999999 49.0\n",
      "740.2357149999999 98.0 743.0917099999999 49.0\n",
      "785.7797099999999 119.0 787.339715 49.0\n",
      "835.963715 98.0 838.1117099999999 49.0\n",
      "892.0877149999999 119.0 896.7177099999999 49.0\n",
      "937.1557149999999 119.0 939.7037099999999 49.0\n",
      "979.99371 119.0 981.473715 49.0\n",
      "1040.82971 119.0 1042.63771 49.0\n",
      "1080.7297099999998 98.0 1082.96771 49.0\n",
      "LFP4_AON\n",
      "(2211369,) (2211369,) 2000\n",
      "2211369\n",
      "start_index: 0, end_index: 2211369\n",
      "notch filter applied\n",
      "normalizing data\n",
      "94.81371499999999 119.0 104.29370999999999 48.0\n",
      "161.73570999999998 98.0 163.735715 49.0\n",
      "206.56170999999998 98.0 208.35170499999998 49.0\n",
      "264.17370999999997 119.0 266.55371499999995 49.0\n",
      "315.42971 98.0 317.28970999999996 49.0\n",
      "365.38570999999996 119.0 368.97571 49.0\n",
      "402.80771 119.0 406.283715 49.0\n",
      "514.4897149999999 98.0 517.73771 48.0\n",
      "555.82571 119.0 558.045715 49.0\n",
      "614.73771 98.0 617.347715 49.0\n",
      "656.917705 119.0 659.7077149999999 49.0\n",
      "695.55571 98.0 697.0537099999999 49.0\n",
      "740.2357149999999 98.0 743.0917099999999 49.0\n",
      "785.7797099999999 119.0 787.339715 49.0\n",
      "835.963715 98.0 838.1117099999999 49.0\n",
      "892.0877149999999 119.0 896.7177099999999 49.0\n",
      "937.1557149999999 119.0 939.7037099999999 49.0\n",
      "979.99371 119.0 981.473715 49.0\n",
      "1040.82971 119.0 1042.63771 49.0\n",
      "1080.7297099999998 98.0 1082.96771 49.0\n",
      "20230623 dk1 BWcontext\n",
      "['Keyboard', 'LFP1_vHp', 'LFP2_vHp', 'LFP3_AON', 'LFP4_AON', 'Ref', 'Respirat']\n",
      "Global start time: 0.0, Global end time: 1347.990875\n",
      "2695982\n",
      "start_index: 0, end_index: 2695982\n",
      "LFP1_vHp\n",
      "(2695982,) (2695982,) 2000\n",
      "2695982\n",
      "start_index: 0, end_index: 2695982\n",
      "notch filter applied\n",
      "normalizing data\n",
      "87.728145 119.0 89.388145 49.0\n",
      "178.53615 98.0 180.194145 49.0\n",
      "238.44214999999997 119.0 243.01014999999998 49.0\n",
      "311.47414999999995 119.0 315.11414499999995 49.0\n",
      "363.31814499999996 98.0 366.19814999999994 48.0\n",
      "411.14214999999996 119.0 415.57214999999997 49.0\n",
      "489.84414499999997 119.0 491.62414999999993 49.0\n",
      "552.812145 98.0 554.432145 49.0\n",
      "601.076145 98.0 602.72615 49.0\n",
      "648.9261449999999 119.0 650.47615 49.0\n",
      "740.55215 98.0 741.8721449999999 49.0\n",
      "799.5501499999999 119.0 801.990145 49.0\n",
      "850.5321449999999 119.0 856.9721499999999 49.0\n",
      "908.222145 98.0 911.1621499999999 49.0\n",
      "967.9281449999999 119.0 969.698145 49.0\n",
      "1015.0221499999999 98.0 1019.2221499999999 49.0\n",
      "1129.9521499999998 98.0 1131.94015 49.0\n",
      "1182.916145 98.0 1184.836145 49.0\n",
      "1232.070145 98.0 1234.000145 49.0\n",
      "1293.9701449999998 119.0 1295.540145 49.0\n",
      "LFP2_vHp\n",
      "(2695982,) (2695982,) 2000\n",
      "2695982\n",
      "start_index: 0, end_index: 2695982\n",
      "notch filter applied\n",
      "normalizing data\n",
      "87.728145 119.0 89.388145 49.0\n",
      "178.53615 98.0 180.194145 49.0\n",
      "238.44214999999997 119.0 243.01014999999998 49.0\n",
      "311.47414999999995 119.0 315.11414499999995 49.0\n",
      "363.31814499999996 98.0 366.19814999999994 48.0\n",
      "411.14214999999996 119.0 415.57214999999997 49.0\n",
      "489.84414499999997 119.0 491.62414999999993 49.0\n",
      "552.812145 98.0 554.432145 49.0\n",
      "601.076145 98.0 602.72615 49.0\n",
      "648.9261449999999 119.0 650.47615 49.0\n",
      "740.55215 98.0 741.8721449999999 49.0\n",
      "799.5501499999999 119.0 801.990145 49.0\n",
      "850.5321449999999 119.0 856.9721499999999 49.0\n",
      "908.222145 98.0 911.1621499999999 49.0\n",
      "967.9281449999999 119.0 969.698145 49.0\n",
      "1015.0221499999999 98.0 1019.2221499999999 49.0\n",
      "1129.9521499999998 98.0 1131.94015 49.0\n",
      "1182.916145 98.0 1184.836145 49.0\n",
      "1232.070145 98.0 1234.000145 49.0\n",
      "1293.9701449999998 119.0 1295.540145 49.0\n",
      "LFP3_AON\n",
      "(2695982,) (2695982,) 2000\n",
      "2695982\n",
      "start_index: 0, end_index: 2695982\n",
      "notch filter applied\n",
      "normalizing data\n",
      "87.728145 119.0 89.388145 49.0\n",
      "178.53615 98.0 180.194145 49.0\n",
      "238.44214999999997 119.0 243.01014999999998 49.0\n",
      "311.47414999999995 119.0 315.11414499999995 49.0\n",
      "363.31814499999996 98.0 366.19814999999994 48.0\n",
      "411.14214999999996 119.0 415.57214999999997 49.0\n",
      "489.84414499999997 119.0 491.62414999999993 49.0\n",
      "552.812145 98.0 554.432145 49.0\n",
      "601.076145 98.0 602.72615 49.0\n",
      "648.9261449999999 119.0 650.47615 49.0\n",
      "740.55215 98.0 741.8721449999999 49.0\n",
      "799.5501499999999 119.0 801.990145 49.0\n",
      "850.5321449999999 119.0 856.9721499999999 49.0\n",
      "908.222145 98.0 911.1621499999999 49.0\n",
      "967.9281449999999 119.0 969.698145 49.0\n",
      "1015.0221499999999 98.0 1019.2221499999999 49.0\n",
      "1129.9521499999998 98.0 1131.94015 49.0\n",
      "1182.916145 98.0 1184.836145 49.0\n",
      "1232.070145 98.0 1234.000145 49.0\n",
      "1293.9701449999998 119.0 1295.540145 49.0\n",
      "LFP4_AON\n",
      "(2695982,) (2695982,) 2000\n",
      "2695982\n",
      "start_index: 0, end_index: 2695982\n",
      "notch filter applied\n",
      "normalizing data\n",
      "87.728145 119.0 89.388145 49.0\n",
      "178.53615 98.0 180.194145 49.0\n",
      "238.44214999999997 119.0 243.01014999999998 49.0\n",
      "311.47414999999995 119.0 315.11414499999995 49.0\n",
      "363.31814499999996 98.0 366.19814999999994 48.0\n",
      "411.14214999999996 119.0 415.57214999999997 49.0\n",
      "489.84414499999997 119.0 491.62414999999993 49.0\n",
      "552.812145 98.0 554.432145 49.0\n",
      "601.076145 98.0 602.72615 49.0\n",
      "648.9261449999999 119.0 650.47615 49.0\n",
      "740.55215 98.0 741.8721449999999 49.0\n",
      "799.5501499999999 119.0 801.990145 49.0\n",
      "850.5321449999999 119.0 856.9721499999999 49.0\n",
      "908.222145 98.0 911.1621499999999 49.0\n",
      "967.9281449999999 119.0 969.698145 49.0\n",
      "1015.0221499999999 98.0 1019.2221499999999 49.0\n",
      "1129.9521499999998 98.0 1131.94015 49.0\n",
      "1182.916145 98.0 1184.836145 49.0\n",
      "1232.070145 98.0 1234.000145 49.0\n",
      "1293.9701449999998 119.0 1295.540145 49.0\n",
      "20230626 dk1 BWcontext\n",
      "['Keyboard', 'LFP1_vHp', 'LFP2_vHp', 'LFP3_AON', 'LFP4_AON', 'Ref', 'Respirat']\n",
      "Global start time: 0.0, Global end time: 1236.2558749999998\n",
      "2472512\n",
      "start_index: 0, end_index: 2472512\n",
      "LFP1_vHp\n",
      "(2472512,) (2472512,) 2000\n",
      "2472512\n",
      "start_index: 0, end_index: 2472512\n",
      "notch filter applied\n",
      "normalizing data\n",
      "177.11349499999997 98.0 182.66349499999998 48.0\n",
      "244.40749499999998 119.0 249.087495 49.0\n",
      "326.015495 119.0 329.325495 49.0\n",
      "375.003495 119.0 378.21349499999997 49.0\n",
      "415.11749499999996 119.0 418.527495 49.0\n",
      "464.96349499999997 119.0 467.443495 49.0\n",
      "539.3454949999999 98.0 542.7494949999999 48.0\n",
      "588.9314949999999 98.0 591.651495 49.0\n",
      "634.4694949999999 98.0 637.2594949999999 49.0\n",
      "676.1234949999999 119.0 678.871495 49.0\n",
      "733.5334949999999 98.0 735.4814949999999 49.0\n",
      "770.7214949999999 119.0 773.3614949999999 49.0\n",
      "813.2694949999999 119.0 814.8594949999999 49.0\n",
      "847.4694949999999 98.0 848.4794949999999 49.0\n",
      "890.5534949999999 119.0 893.3034949999999 49.0\n",
      "940.103495 119.0 942.1534949999999 49.0\n",
      "979.2194949999999 98.0 982.6954949999999 49.0\n",
      "1029.973495 98.0 1035.541495 49.0\n",
      "1108.0174949999998 119.0 1109.415495 49.0\n",
      "1191.745495 98.0 1196.745495 49.0\n",
      "LFP2_vHp\n",
      "(2472512,) (2472512,) 2000\n",
      "2472512\n",
      "start_index: 0, end_index: 2472512\n",
      "notch filter applied\n",
      "normalizing data\n",
      "177.11349499999997 98.0 182.66349499999998 48.0\n",
      "244.40749499999998 119.0 249.087495 49.0\n",
      "326.015495 119.0 329.325495 49.0\n",
      "375.003495 119.0 378.21349499999997 49.0\n",
      "415.11749499999996 119.0 418.527495 49.0\n",
      "464.96349499999997 119.0 467.443495 49.0\n",
      "539.3454949999999 98.0 542.7494949999999 48.0\n",
      "588.9314949999999 98.0 591.651495 49.0\n",
      "634.4694949999999 98.0 637.2594949999999 49.0\n",
      "676.1234949999999 119.0 678.871495 49.0\n",
      "733.5334949999999 98.0 735.4814949999999 49.0\n",
      "770.7214949999999 119.0 773.3614949999999 49.0\n",
      "813.2694949999999 119.0 814.8594949999999 49.0\n",
      "847.4694949999999 98.0 848.4794949999999 49.0\n",
      "890.5534949999999 119.0 893.3034949999999 49.0\n",
      "940.103495 119.0 942.1534949999999 49.0\n",
      "979.2194949999999 98.0 982.6954949999999 49.0\n",
      "1029.973495 98.0 1035.541495 49.0\n",
      "1108.0174949999998 119.0 1109.415495 49.0\n",
      "1191.745495 98.0 1196.745495 49.0\n",
      "LFP3_AON\n",
      "(2472512,) (2472512,) 2000\n",
      "2472512\n",
      "start_index: 0, end_index: 2472512\n",
      "notch filter applied\n",
      "normalizing data\n",
      "177.11349499999997 98.0 182.66349499999998 48.0\n",
      "244.40749499999998 119.0 249.087495 49.0\n",
      "326.015495 119.0 329.325495 49.0\n",
      "375.003495 119.0 378.21349499999997 49.0\n",
      "415.11749499999996 119.0 418.527495 49.0\n",
      "464.96349499999997 119.0 467.443495 49.0\n",
      "539.3454949999999 98.0 542.7494949999999 48.0\n",
      "588.9314949999999 98.0 591.651495 49.0\n",
      "634.4694949999999 98.0 637.2594949999999 49.0\n",
      "676.1234949999999 119.0 678.871495 49.0\n",
      "733.5334949999999 98.0 735.4814949999999 49.0\n",
      "770.7214949999999 119.0 773.3614949999999 49.0\n",
      "813.2694949999999 119.0 814.8594949999999 49.0\n",
      "847.4694949999999 98.0 848.4794949999999 49.0\n",
      "890.5534949999999 119.0 893.3034949999999 49.0\n",
      "940.103495 119.0 942.1534949999999 49.0\n",
      "979.2194949999999 98.0 982.6954949999999 49.0\n",
      "1029.973495 98.0 1035.541495 49.0\n",
      "1108.0174949999998 119.0 1109.415495 49.0\n",
      "1191.745495 98.0 1196.745495 49.0\n",
      "LFP4_AON\n",
      "(2472512,) (2472512,) 2000\n",
      "2472512\n",
      "start_index: 0, end_index: 2472512\n",
      "notch filter applied\n",
      "normalizing data\n",
      "177.11349499999997 98.0 182.66349499999998 48.0\n",
      "244.40749499999998 119.0 249.087495 49.0\n",
      "326.015495 119.0 329.325495 49.0\n",
      "375.003495 119.0 378.21349499999997 49.0\n",
      "415.11749499999996 119.0 418.527495 49.0\n",
      "464.96349499999997 119.0 467.443495 49.0\n",
      "539.3454949999999 98.0 542.7494949999999 48.0\n",
      "588.9314949999999 98.0 591.651495 49.0\n",
      "634.4694949999999 98.0 637.2594949999999 49.0\n",
      "676.1234949999999 119.0 678.871495 49.0\n",
      "733.5334949999999 98.0 735.4814949999999 49.0\n",
      "770.7214949999999 119.0 773.3614949999999 49.0\n",
      "813.2694949999999 119.0 814.8594949999999 49.0\n",
      "847.4694949999999 98.0 848.4794949999999 49.0\n",
      "890.5534949999999 119.0 893.3034949999999 49.0\n",
      "940.103495 119.0 942.1534949999999 49.0\n",
      "979.2194949999999 98.0 982.6954949999999 49.0\n",
      "1029.973495 98.0 1035.541495 49.0\n",
      "1108.0174949999998 119.0 1109.415495 49.0\n",
      "1191.745495 98.0 1196.745495 49.0\n",
      "20230626 dk5 BWnocontext\n",
      "['Keyboard', 'LFP1_vHp', 'LFP2_vHp', 'LFP3_AON', 'LFP4_AON', 'Ref', 'Respirat']\n",
      "Global start time: 0.0, Global end time: 1486.8456749999998\n",
      "2973692\n",
      "start_index: 0, end_index: 2973691\n",
      "LFP1_vHp\n",
      "(2973692,) (2973692,) 2000\n",
      "2973692\n",
      "start_index: 0, end_index: 2973692\n",
      "notch filter applied\n",
      "normalizing data\n",
      "158.44735 98.0 160.82735 49.0\n",
      "234.67534999999998 119.0 236.055345 49.0\n",
      "321.06334999999996 119.0 322.793345 49.0\n",
      "377.58135 119.0 378.651345 49.0\n",
      "426.96135499999997 119.0 428.36934999999994 49.0\n",
      "486.46134499999994 119.0 487.96134499999994 49.0\n",
      "534.5293499999999 98.0 537.35935 49.0\n",
      "590.5393449999999 98.0 593.71935 49.0\n",
      "639.65735 98.0 641.6253499999999 49.0\n",
      "717.19535 119.0 720.2853499999999 49.0\n",
      "767.8813449999999 98.0 770.96535 49.0\n",
      "815.9373499999999 119.0 819.5353449999999 49.0\n",
      "881.27735 119.0 892.75335 49.0\n",
      "1052.46735 98.0 1054.33735 49.0\n",
      "1100.1853449999999 119.0 1101.44535 49.0\n",
      "1150.13935 119.0 1151.479355 49.0\n",
      "1206.603345 98.0 1208.683345 49.0\n",
      "1267.7293499999998 98.0 1269.31935 49.0\n",
      "1377.2233449999999 119.0 1385.4613499999998 49.0\n",
      "1442.72735 98.0 1445.96535 49.0\n",
      "LFP2_vHp\n",
      "(2973691,) (2973691,) 2000\n",
      "2973692\n",
      "start_index: 0, end_index: 2973691\n",
      "notch filter applied\n",
      "normalizing data\n",
      "158.44735 98.0 160.82735 49.0\n",
      "234.67534999999998 119.0 236.055345 49.0\n",
      "321.06334999999996 119.0 322.793345 49.0\n",
      "377.58135 119.0 378.651345 49.0\n",
      "426.96135499999997 119.0 428.36934999999994 49.0\n",
      "486.46134499999994 119.0 487.96134499999994 49.0\n",
      "534.5293499999999 98.0 537.35935 49.0\n",
      "590.5393449999999 98.0 593.71935 49.0\n",
      "639.65735 98.0 641.6253499999999 49.0\n",
      "717.19535 119.0 720.2853499999999 49.0\n",
      "767.8813449999999 98.0 770.96535 49.0\n",
      "815.9373499999999 119.0 819.5353449999999 49.0\n",
      "881.27735 119.0 892.75335 49.0\n",
      "1052.46735 98.0 1054.33735 49.0\n",
      "1100.1853449999999 119.0 1101.44535 49.0\n",
      "1150.13935 119.0 1151.479355 49.0\n",
      "1206.603345 98.0 1208.683345 49.0\n",
      "1267.7293499999998 98.0 1269.31935 49.0\n",
      "1377.2233449999999 119.0 1385.4613499999998 49.0\n",
      "1442.72735 98.0 1445.96535 49.0\n",
      "LFP3_AON\n",
      "(2973692,) (2973692,) 2000\n",
      "2973692\n",
      "start_index: 0, end_index: 2973692\n",
      "notch filter applied\n",
      "normalizing data\n",
      "158.44735 98.0 160.82735 49.0\n",
      "234.67534999999998 119.0 236.055345 49.0\n",
      "321.06334999999996 119.0 322.793345 49.0\n",
      "377.58135 119.0 378.651345 49.0\n",
      "426.96135499999997 119.0 428.36934999999994 49.0\n",
      "486.46134499999994 119.0 487.96134499999994 49.0\n",
      "534.5293499999999 98.0 537.35935 49.0\n",
      "590.5393449999999 98.0 593.71935 49.0\n",
      "639.65735 98.0 641.6253499999999 49.0\n",
      "717.19535 119.0 720.2853499999999 49.0\n",
      "767.8813449999999 98.0 770.96535 49.0\n",
      "815.9373499999999 119.0 819.5353449999999 49.0\n",
      "881.27735 119.0 892.75335 49.0\n",
      "1052.46735 98.0 1054.33735 49.0\n",
      "1100.1853449999999 119.0 1101.44535 49.0\n",
      "1150.13935 119.0 1151.479355 49.0\n",
      "1206.603345 98.0 1208.683345 49.0\n",
      "1267.7293499999998 98.0 1269.31935 49.0\n",
      "1377.2233449999999 119.0 1385.4613499999998 49.0\n",
      "1442.72735 98.0 1445.96535 49.0\n",
      "LFP4_AON\n",
      "(2973692,) (2973692,) 2000\n",
      "2973692\n",
      "start_index: 0, end_index: 2973692\n",
      "notch filter applied\n",
      "normalizing data\n",
      "158.44735 98.0 160.82735 49.0\n",
      "234.67534999999998 119.0 236.055345 49.0\n",
      "321.06334999999996 119.0 322.793345 49.0\n",
      "377.58135 119.0 378.651345 49.0\n",
      "426.96135499999997 119.0 428.36934999999994 49.0\n",
      "486.46134499999994 119.0 487.96134499999994 49.0\n",
      "534.5293499999999 98.0 537.35935 49.0\n",
      "590.5393449999999 98.0 593.71935 49.0\n",
      "639.65735 98.0 641.6253499999999 49.0\n",
      "717.19535 119.0 720.2853499999999 49.0\n",
      "767.8813449999999 98.0 770.96535 49.0\n",
      "815.9373499999999 119.0 819.5353449999999 49.0\n",
      "881.27735 119.0 892.75335 49.0\n",
      "1052.46735 98.0 1054.33735 49.0\n",
      "1100.1853449999999 119.0 1101.44535 49.0\n",
      "1150.13935 119.0 1151.479355 49.0\n",
      "1206.603345 98.0 1208.683345 49.0\n",
      "1267.7293499999998 98.0 1269.31935 49.0\n",
      "1377.2233449999999 119.0 1385.4613499999998 49.0\n",
      "1442.72735 98.0 1445.96535 49.0\n",
      "20230626 dk6 BWnocontext\n",
      "['Keyboard', 'LFP1_AON', 'LFP1_vHp', 'LFP2_AON', 'LFP2_vHp', 'LFP3_AON', 'LFP4_AON', 'Ref', 'Respirat']\n",
      "Global start time: 0.0, Global end time: 1035.330375\n",
      "2070661\n",
      "start_index: 0, end_index: 2070661\n",
      "LFP1_AON\n",
      "(2070661,) (2070661,) 2000\n",
      "2070661\n",
      "start_index: 0, end_index: 2070661\n",
      "notch filter applied\n",
      "normalizing data\n",
      "125.56925499999998 98.0 136.45925 49.0\n",
      "195.95125499999997 119.0 199.35125499999998 49.0\n",
      "240.01324999999997 119.0 241.61324999999997 49.0\n",
      "288.01525499999997 119.0 290.25525 49.0\n",
      "325.671255 119.0 327.41925 49.0\n",
      "369.30725499999994 119.0 371.12724999999995 49.0\n",
      "415.34725499999996 98.0 417.20725 49.0\n",
      "460.34324999999995 98.0 462.85324999999995 49.0\n",
      "508.19125499999996 98.0 509.88124999999997 49.0\n",
      "547.73325 119.0 550.273255 49.0\n",
      "584.0352549999999 98.0 591.39325 49.0\n",
      "629.01325 119.0 631.67125 49.0\n",
      "673.199255 119.0 674.725255 49.0\n",
      "722.67725 98.0 725.2072499999999 49.0\n",
      "761.1832499999999 119.0 763.4532499999999 49.0\n",
      "819.12925 119.0 821.7792499999999 49.0\n",
      "863.4092549999999 98.0 865.5392499999999 49.0\n",
      "903.6732549999999 98.0 906.1712499999999 49.0\n",
      "951.7432499999999 119.0 953.6032499999999 49.0\n",
      "1002.3352499999999 98.0 1004.1132549999999 49.0\n",
      "LFP1_vHp\n",
      "(2070661,) (2070661,) 2000\n",
      "2070661\n",
      "start_index: 0, end_index: 2070661\n",
      "notch filter applied\n",
      "normalizing data\n",
      "125.56925499999998 98.0 136.45925 49.0\n",
      "195.95125499999997 119.0 199.35125499999998 49.0\n",
      "240.01324999999997 119.0 241.61324999999997 49.0\n",
      "288.01525499999997 119.0 290.25525 49.0\n",
      "325.671255 119.0 327.41925 49.0\n",
      "369.30725499999994 119.0 371.12724999999995 49.0\n",
      "415.34725499999996 98.0 417.20725 49.0\n",
      "460.34324999999995 98.0 462.85324999999995 49.0\n",
      "508.19125499999996 98.0 509.88124999999997 49.0\n",
      "547.73325 119.0 550.273255 49.0\n",
      "584.0352549999999 98.0 591.39325 49.0\n",
      "629.01325 119.0 631.67125 49.0\n",
      "673.199255 119.0 674.725255 49.0\n",
      "722.67725 98.0 725.2072499999999 49.0\n",
      "761.1832499999999 119.0 763.4532499999999 49.0\n",
      "819.12925 119.0 821.7792499999999 49.0\n",
      "863.4092549999999 98.0 865.5392499999999 49.0\n",
      "903.6732549999999 98.0 906.1712499999999 49.0\n",
      "951.7432499999999 119.0 953.6032499999999 49.0\n",
      "1002.3352499999999 98.0 1004.1132549999999 49.0\n",
      "LFP2_AON\n",
      "(2070661,) (2070661,) 2000\n",
      "2070661\n",
      "start_index: 0, end_index: 2070661\n",
      "notch filter applied\n",
      "normalizing data\n",
      "125.56925499999998 98.0 136.45925 49.0\n",
      "195.95125499999997 119.0 199.35125499999998 49.0\n",
      "240.01324999999997 119.0 241.61324999999997 49.0\n",
      "288.01525499999997 119.0 290.25525 49.0\n",
      "325.671255 119.0 327.41925 49.0\n",
      "369.30725499999994 119.0 371.12724999999995 49.0\n",
      "415.34725499999996 98.0 417.20725 49.0\n",
      "460.34324999999995 98.0 462.85324999999995 49.0\n",
      "508.19125499999996 98.0 509.88124999999997 49.0\n",
      "547.73325 119.0 550.273255 49.0\n",
      "584.0352549999999 98.0 591.39325 49.0\n",
      "629.01325 119.0 631.67125 49.0\n",
      "673.199255 119.0 674.725255 49.0\n",
      "722.67725 98.0 725.2072499999999 49.0\n",
      "761.1832499999999 119.0 763.4532499999999 49.0\n",
      "819.12925 119.0 821.7792499999999 49.0\n",
      "863.4092549999999 98.0 865.5392499999999 49.0\n",
      "903.6732549999999 98.0 906.1712499999999 49.0\n",
      "951.7432499999999 119.0 953.6032499999999 49.0\n",
      "1002.3352499999999 98.0 1004.1132549999999 49.0\n",
      "LFP2_vHp\n",
      "(2070661,) (2070661,) 2000\n",
      "2070661\n",
      "start_index: 0, end_index: 2070661\n",
      "notch filter applied\n",
      "normalizing data\n",
      "125.56925499999998 98.0 136.45925 49.0\n",
      "195.95125499999997 119.0 199.35125499999998 49.0\n",
      "240.01324999999997 119.0 241.61324999999997 49.0\n",
      "288.01525499999997 119.0 290.25525 49.0\n",
      "325.671255 119.0 327.41925 49.0\n",
      "369.30725499999994 119.0 371.12724999999995 49.0\n",
      "415.34725499999996 98.0 417.20725 49.0\n",
      "460.34324999999995 98.0 462.85324999999995 49.0\n",
      "508.19125499999996 98.0 509.88124999999997 49.0\n",
      "547.73325 119.0 550.273255 49.0\n",
      "584.0352549999999 98.0 591.39325 49.0\n",
      "629.01325 119.0 631.67125 49.0\n",
      "673.199255 119.0 674.725255 49.0\n",
      "722.67725 98.0 725.2072499999999 49.0\n",
      "761.1832499999999 119.0 763.4532499999999 49.0\n",
      "819.12925 119.0 821.7792499999999 49.0\n",
      "863.4092549999999 98.0 865.5392499999999 49.0\n",
      "903.6732549999999 98.0 906.1712499999999 49.0\n",
      "951.7432499999999 119.0 953.6032499999999 49.0\n",
      "1002.3352499999999 98.0 1004.1132549999999 49.0\n",
      "LFP3_AON\n",
      "(2070661,) (2070661,) 2000\n",
      "2070661\n",
      "start_index: 0, end_index: 2070661\n",
      "notch filter applied\n",
      "normalizing data\n",
      "125.56925499999998 98.0 136.45925 49.0\n",
      "195.95125499999997 119.0 199.35125499999998 49.0\n",
      "240.01324999999997 119.0 241.61324999999997 49.0\n",
      "288.01525499999997 119.0 290.25525 49.0\n",
      "325.671255 119.0 327.41925 49.0\n",
      "369.30725499999994 119.0 371.12724999999995 49.0\n",
      "415.34725499999996 98.0 417.20725 49.0\n",
      "460.34324999999995 98.0 462.85324999999995 49.0\n",
      "508.19125499999996 98.0 509.88124999999997 49.0\n",
      "547.73325 119.0 550.273255 49.0\n",
      "584.0352549999999 98.0 591.39325 49.0\n",
      "629.01325 119.0 631.67125 49.0\n",
      "673.199255 119.0 674.725255 49.0\n",
      "722.67725 98.0 725.2072499999999 49.0\n",
      "761.1832499999999 119.0 763.4532499999999 49.0\n",
      "819.12925 119.0 821.7792499999999 49.0\n",
      "863.4092549999999 98.0 865.5392499999999 49.0\n",
      "903.6732549999999 98.0 906.1712499999999 49.0\n",
      "951.7432499999999 119.0 953.6032499999999 49.0\n",
      "1002.3352499999999 98.0 1004.1132549999999 49.0\n",
      "LFP4_AON\n",
      "(2070661,) (2070661,) 2000\n",
      "2070661\n",
      "start_index: 0, end_index: 2070661\n",
      "notch filter applied\n",
      "normalizing data\n",
      "125.56925499999998 98.0 136.45925 49.0\n",
      "195.95125499999997 119.0 199.35125499999998 49.0\n",
      "240.01324999999997 119.0 241.61324999999997 49.0\n",
      "288.01525499999997 119.0 290.25525 49.0\n",
      "325.671255 119.0 327.41925 49.0\n",
      "369.30725499999994 119.0 371.12724999999995 49.0\n",
      "415.34725499999996 98.0 417.20725 49.0\n",
      "460.34324999999995 98.0 462.85324999999995 49.0\n",
      "508.19125499999996 98.0 509.88124999999997 49.0\n",
      "547.73325 119.0 550.273255 49.0\n",
      "584.0352549999999 98.0 591.39325 49.0\n",
      "629.01325 119.0 631.67125 49.0\n",
      "673.199255 119.0 674.725255 49.0\n",
      "722.67725 98.0 725.2072499999999 49.0\n",
      "761.1832499999999 119.0 763.4532499999999 49.0\n",
      "819.12925 119.0 821.7792499999999 49.0\n",
      "863.4092549999999 98.0 865.5392499999999 49.0\n",
      "903.6732549999999 98.0 906.1712499999999 49.0\n",
      "951.7432499999999 119.0 953.6032499999999 49.0\n",
      "1002.3352499999999 98.0 1004.1132549999999 49.0\n",
      "20230627 dk1 BWcontext\n",
      "['Keyboard', 'LFP1_AON', 'LFP1_vHp', 'LFP2_AON', 'LFP2_vHp', 'LFP3_AON', 'LFP4_AON', 'Ref', 'Respirat']\n",
      "Global start time: 0.0, Global end time: 1781.621575\n",
      "3563244\n",
      "start_index: 0, end_index: 3563243\n",
      "LFP1_AON\n",
      "(3563244,) (3563244,) 2000\n",
      "3563244\n",
      "start_index: 0, end_index: 3563244\n",
      "notch filter applied\n",
      "normalizing data\n",
      "125.25896999999999 98.0 128.69697 48.0\n",
      "183.39496499999998 119.0 186.484965 48.0\n",
      "245.794965 119.0 248.61296499999997 48.0\n",
      "325.92897 98.0 328.05897 48.0\n",
      "390.080965 119.0 392.55096999999995 48.0\n",
      "455.78296499999993 98.0 457.792965 49.0\n",
      "510.98896499999995 98.0 513.158965 49.0\n",
      "616.192965 98.0 617.782965 49.0\n",
      "732.6369699999999 98.0 734.9169599999999 49.0\n",
      "785.7469649999999 98.0 790.2569649999999 49.0\n",
      "872.244965 119.0 875.0949649999999 49.0\n",
      "1005.9549649999999 98.0 1008.9149649999999 49.0\n",
      "1074.9249699999998 119.0 1077.9329699999998 49.0\n",
      "1149.0269649999998 119.0 1151.4669649999998 49.0\n",
      "1296.20897 98.0 1299.3549699999999 49.0\n",
      "1427.21096 119.0 1429.6709649999998 49.0\n",
      "1552.6309649999998 119.0 1553.9709699999999 49.0\n",
      "1602.1229649999998 98.0 1604.4229649999997 49.0\n",
      "1666.2549649999999 98.0 1668.25497 49.0\n",
      "1755.9489649999998 119.0 1760.0289699999998 49.0\n",
      "LFP1_vHp\n",
      "(3563243,) (3563243,) 2000\n",
      "3563244\n",
      "start_index: 0, end_index: 3563243\n",
      "notch filter applied\n",
      "normalizing data\n",
      "125.25896999999999 98.0 128.69697 48.0\n",
      "183.39496499999998 119.0 186.484965 48.0\n",
      "245.794965 119.0 248.61296499999997 48.0\n",
      "325.92897 98.0 328.05897 48.0\n",
      "390.080965 119.0 392.55096999999995 48.0\n",
      "455.78296499999993 98.0 457.792965 49.0\n",
      "510.98896499999995 98.0 513.158965 49.0\n",
      "616.192965 98.0 617.782965 49.0\n",
      "732.6369699999999 98.0 734.9169599999999 49.0\n",
      "785.7469649999999 98.0 790.2569649999999 49.0\n",
      "872.244965 119.0 875.0949649999999 49.0\n",
      "1005.9549649999999 98.0 1008.9149649999999 49.0\n",
      "1074.9249699999998 119.0 1077.9329699999998 49.0\n",
      "1149.0269649999998 119.0 1151.4669649999998 49.0\n",
      "1296.20897 98.0 1299.3549699999999 49.0\n",
      "1427.21096 119.0 1429.6709649999998 49.0\n",
      "1552.6309649999998 119.0 1553.9709699999999 49.0\n",
      "1602.1229649999998 98.0 1604.4229649999997 49.0\n",
      "1666.2549649999999 98.0 1668.25497 49.0\n",
      "1755.9489649999998 119.0 1760.0289699999998 49.0\n",
      "LFP2_AON\n",
      "(3563244,) (3563244,) 2000\n",
      "3563244\n",
      "start_index: 0, end_index: 3563244\n",
      "notch filter applied\n",
      "normalizing data\n",
      "125.25896999999999 98.0 128.69697 48.0\n",
      "183.39496499999998 119.0 186.484965 48.0\n",
      "245.794965 119.0 248.61296499999997 48.0\n",
      "325.92897 98.0 328.05897 48.0\n",
      "390.080965 119.0 392.55096999999995 48.0\n",
      "455.78296499999993 98.0 457.792965 49.0\n",
      "510.98896499999995 98.0 513.158965 49.0\n",
      "616.192965 98.0 617.782965 49.0\n",
      "732.6369699999999 98.0 734.9169599999999 49.0\n",
      "785.7469649999999 98.0 790.2569649999999 49.0\n",
      "872.244965 119.0 875.0949649999999 49.0\n",
      "1005.9549649999999 98.0 1008.9149649999999 49.0\n",
      "1074.9249699999998 119.0 1077.9329699999998 49.0\n",
      "1149.0269649999998 119.0 1151.4669649999998 49.0\n",
      "1296.20897 98.0 1299.3549699999999 49.0\n",
      "1427.21096 119.0 1429.6709649999998 49.0\n",
      "1552.6309649999998 119.0 1553.9709699999999 49.0\n",
      "1602.1229649999998 98.0 1604.4229649999997 49.0\n",
      "1666.2549649999999 98.0 1668.25497 49.0\n",
      "1755.9489649999998 119.0 1760.0289699999998 49.0\n",
      "LFP2_vHp\n",
      "(3563243,) (3563243,) 2000\n",
      "3563244\n",
      "start_index: 0, end_index: 3563243\n",
      "notch filter applied\n",
      "normalizing data\n",
      "125.25896999999999 98.0 128.69697 48.0\n",
      "183.39496499999998 119.0 186.484965 48.0\n",
      "245.794965 119.0 248.61296499999997 48.0\n",
      "325.92897 98.0 328.05897 48.0\n",
      "390.080965 119.0 392.55096999999995 48.0\n",
      "455.78296499999993 98.0 457.792965 49.0\n",
      "510.98896499999995 98.0 513.158965 49.0\n",
      "616.192965 98.0 617.782965 49.0\n",
      "732.6369699999999 98.0 734.9169599999999 49.0\n",
      "785.7469649999999 98.0 790.2569649999999 49.0\n",
      "872.244965 119.0 875.0949649999999 49.0\n",
      "1005.9549649999999 98.0 1008.9149649999999 49.0\n",
      "1074.9249699999998 119.0 1077.9329699999998 49.0\n",
      "1149.0269649999998 119.0 1151.4669649999998 49.0\n",
      "1296.20897 98.0 1299.3549699999999 49.0\n",
      "1427.21096 119.0 1429.6709649999998 49.0\n",
      "1552.6309649999998 119.0 1553.9709699999999 49.0\n",
      "1602.1229649999998 98.0 1604.4229649999997 49.0\n",
      "1666.2549649999999 98.0 1668.25497 49.0\n",
      "1755.9489649999998 119.0 1760.0289699999998 49.0\n",
      "LFP3_AON\n",
      "(3563244,) (3563244,) 2000\n",
      "3563244\n",
      "start_index: 0, end_index: 3563244\n",
      "notch filter applied\n",
      "normalizing data\n",
      "125.25896999999999 98.0 128.69697 48.0\n",
      "183.39496499999998 119.0 186.484965 48.0\n",
      "245.794965 119.0 248.61296499999997 48.0\n",
      "325.92897 98.0 328.05897 48.0\n",
      "390.080965 119.0 392.55096999999995 48.0\n",
      "455.78296499999993 98.0 457.792965 49.0\n",
      "510.98896499999995 98.0 513.158965 49.0\n",
      "616.192965 98.0 617.782965 49.0\n",
      "732.6369699999999 98.0 734.9169599999999 49.0\n",
      "785.7469649999999 98.0 790.2569649999999 49.0\n",
      "872.244965 119.0 875.0949649999999 49.0\n",
      "1005.9549649999999 98.0 1008.9149649999999 49.0\n",
      "1074.9249699999998 119.0 1077.9329699999998 49.0\n",
      "1149.0269649999998 119.0 1151.4669649999998 49.0\n",
      "1296.20897 98.0 1299.3549699999999 49.0\n",
      "1427.21096 119.0 1429.6709649999998 49.0\n",
      "1552.6309649999998 119.0 1553.9709699999999 49.0\n",
      "1602.1229649999998 98.0 1604.4229649999997 49.0\n",
      "1666.2549649999999 98.0 1668.25497 49.0\n",
      "1755.9489649999998 119.0 1760.0289699999998 49.0\n",
      "LFP4_AON\n",
      "(3563244,) (3563244,) 2000\n",
      "3563244\n",
      "start_index: 0, end_index: 3563244\n",
      "notch filter applied\n",
      "normalizing data\n",
      "125.25896999999999 98.0 128.69697 48.0\n",
      "183.39496499999998 119.0 186.484965 48.0\n",
      "245.794965 119.0 248.61296499999997 48.0\n",
      "325.92897 98.0 328.05897 48.0\n",
      "390.080965 119.0 392.55096999999995 48.0\n",
      "455.78296499999993 98.0 457.792965 49.0\n",
      "510.98896499999995 98.0 513.158965 49.0\n",
      "616.192965 98.0 617.782965 49.0\n",
      "732.6369699999999 98.0 734.9169599999999 49.0\n",
      "785.7469649999999 98.0 790.2569649999999 49.0\n",
      "872.244965 119.0 875.0949649999999 49.0\n",
      "1005.9549649999999 98.0 1008.9149649999999 49.0\n",
      "1074.9249699999998 119.0 1077.9329699999998 49.0\n",
      "1149.0269649999998 119.0 1151.4669649999998 49.0\n",
      "1296.20897 98.0 1299.3549699999999 49.0\n",
      "1427.21096 119.0 1429.6709649999998 49.0\n",
      "1552.6309649999998 119.0 1553.9709699999999 49.0\n",
      "1602.1229649999998 98.0 1604.4229649999997 49.0\n",
      "1666.2549649999999 98.0 1668.25497 49.0\n",
      "1755.9489649999998 119.0 1760.0289699999998 49.0\n",
      "20230627 dk5 BWnocontext\n",
      "['Keyboard', 'LFP1_vHp', 'LFP2_vHp', 'LFP3_AON', 'LFP4_AON', 'Ref', 'Respirat']\n",
      "Global start time: 0.0, Global end time: 1211.2410249999998\n",
      "2422483\n",
      "start_index: 0, end_index: 2422482\n",
      "LFP1_vHp\n",
      "(2422482,) (2422482,) 2000\n",
      "2422483\n",
      "start_index: 0, end_index: 2422482\n",
      "notch filter applied\n",
      "normalizing data\n",
      "134.539225 98.0 136.88922499999998 49.0\n",
      "172.285225 119.0 173.81521999999998 49.0\n",
      "218.23322 119.0 220.58322499999997 49.0\n",
      "275.205225 98.0 276.93521999999996 49.0\n",
      "339.64322 119.0 341.74321999999995 49.0\n",
      "409.67122499999994 98.0 410.751225 49.0\n",
      "465.12322499999993 98.0 466.26321999999993 49.0\n",
      "515.8212249999999 98.0 517.40122 49.0\n",
      "564.553225 98.0 565.6632249999999 49.0\n",
      "614.685225 98.0 615.5652249999999 49.0\n",
      "652.179225 119.0 653.597225 49.0\n",
      "698.111225 98.0 699.9512249999999 49.0\n",
      "765.665225 119.0 769.655235 49.0\n",
      "817.0472249999999 119.0 821.1752299999999 49.0\n",
      "871.14722 98.0 873.1452249999999 49.0\n",
      "925.2012249999999 119.0 927.0912249999999 49.0\n",
      "1002.1552249999999 119.0 1004.1352249999999 49.0\n",
      "1050.539225 98.0 1053.30722 49.0\n",
      "1093.26722 98.0 1095.847225 49.0\n",
      "1137.857225 119.0 1140.02522 49.0\n",
      "LFP2_vHp\n",
      "(2422482,) (2422482,) 2000\n",
      "2422483\n",
      "start_index: 0, end_index: 2422482\n",
      "notch filter applied\n",
      "normalizing data\n",
      "134.539225 98.0 136.88922499999998 49.0\n",
      "172.285225 119.0 173.81521999999998 49.0\n",
      "218.23322 119.0 220.58322499999997 49.0\n",
      "275.205225 98.0 276.93521999999996 49.0\n",
      "339.64322 119.0 341.74321999999995 49.0\n",
      "409.67122499999994 98.0 410.751225 49.0\n",
      "465.12322499999993 98.0 466.26321999999993 49.0\n",
      "515.8212249999999 98.0 517.40122 49.0\n",
      "564.553225 98.0 565.6632249999999 49.0\n",
      "614.685225 98.0 615.5652249999999 49.0\n",
      "652.179225 119.0 653.597225 49.0\n",
      "698.111225 98.0 699.9512249999999 49.0\n",
      "765.665225 119.0 769.655235 49.0\n",
      "817.0472249999999 119.0 821.1752299999999 49.0\n",
      "871.14722 98.0 873.1452249999999 49.0\n",
      "925.2012249999999 119.0 927.0912249999999 49.0\n",
      "1002.1552249999999 119.0 1004.1352249999999 49.0\n",
      "1050.539225 98.0 1053.30722 49.0\n",
      "1093.26722 98.0 1095.847225 49.0\n",
      "1137.857225 119.0 1140.02522 49.0\n",
      "LFP3_AON\n",
      "(2422483,) (2422483,) 2000\n",
      "2422483\n",
      "start_index: 0, end_index: 2422483\n",
      "notch filter applied\n",
      "normalizing data\n",
      "134.539225 98.0 136.88922499999998 49.0\n",
      "172.285225 119.0 173.81521999999998 49.0\n",
      "218.23322 119.0 220.58322499999997 49.0\n",
      "275.205225 98.0 276.93521999999996 49.0\n",
      "339.64322 119.0 341.74321999999995 49.0\n",
      "409.67122499999994 98.0 410.751225 49.0\n",
      "465.12322499999993 98.0 466.26321999999993 49.0\n",
      "515.8212249999999 98.0 517.40122 49.0\n",
      "564.553225 98.0 565.6632249999999 49.0\n",
      "614.685225 98.0 615.5652249999999 49.0\n",
      "652.179225 119.0 653.597225 49.0\n",
      "698.111225 98.0 699.9512249999999 49.0\n",
      "765.665225 119.0 769.655235 49.0\n",
      "817.0472249999999 119.0 821.1752299999999 49.0\n",
      "871.14722 98.0 873.1452249999999 49.0\n",
      "925.2012249999999 119.0 927.0912249999999 49.0\n",
      "1002.1552249999999 119.0 1004.1352249999999 49.0\n",
      "1050.539225 98.0 1053.30722 49.0\n",
      "1093.26722 98.0 1095.847225 49.0\n",
      "1137.857225 119.0 1140.02522 49.0\n",
      "LFP4_AON\n",
      "(2422483,) (2422483,) 2000\n",
      "2422483\n",
      "start_index: 0, end_index: 2422483\n",
      "notch filter applied\n",
      "normalizing data\n",
      "134.539225 98.0 136.88922499999998 49.0\n",
      "172.285225 119.0 173.81521999999998 49.0\n",
      "218.23322 119.0 220.58322499999997 49.0\n",
      "275.205225 98.0 276.93521999999996 49.0\n",
      "339.64322 119.0 341.74321999999995 49.0\n",
      "409.67122499999994 98.0 410.751225 49.0\n",
      "465.12322499999993 98.0 466.26321999999993 49.0\n",
      "515.8212249999999 98.0 517.40122 49.0\n",
      "564.553225 98.0 565.6632249999999 49.0\n",
      "614.685225 98.0 615.5652249999999 49.0\n",
      "652.179225 119.0 653.597225 49.0\n",
      "698.111225 98.0 699.9512249999999 49.0\n",
      "765.665225 119.0 769.655235 49.0\n",
      "817.0472249999999 119.0 821.1752299999999 49.0\n",
      "871.14722 98.0 873.1452249999999 49.0\n",
      "925.2012249999999 119.0 927.0912249999999 49.0\n",
      "1002.1552249999999 119.0 1004.1352249999999 49.0\n",
      "1050.539225 98.0 1053.30722 49.0\n",
      "1093.26722 98.0 1095.847225 49.0\n",
      "1137.857225 119.0 1140.02522 49.0\n",
      "20230628 dk6 BWnocontext\n",
      "['Keyboard', 'LFP1_vHp', 'LFP2_vHp', 'LFP3_AON', 'Ref', 'Respirat']\n",
      "Global start time: 0.0, Global end time: 1692.4622749999999\n",
      "3384925\n",
      "start_index: 0, end_index: 3384924\n",
      "LFP1_vHp\n",
      "(3384925,) (3384925,) 2000\n",
      "3384925\n",
      "start_index: 0, end_index: 3384925\n",
      "notch filter applied\n",
      "normalizing data\n",
      "150.22528499999999 98.0 152.14529 49.0\n",
      "272.02928499999996 119.0 273.66929 49.0\n",
      "323.961285 119.0 328.19129 49.0\n",
      "380.22328 119.0 385.17328999999995 49.0\n",
      "440.521285 119.0 444.41128999999995 49.0\n",
      "504.27328499999993 119.0 515.513285 49.0\n",
      "562.731285 98.0 565.08129 49.0\n",
      "610.639285 98.0 612.4392899999999 49.0\n",
      "940.813285 119.0 943.183285 49.0\n",
      "995.3932849999999 98.0 997.043285 49.0\n",
      "1050.1252849999998 119.0 1052.6652849999998 49.0\n",
      "1117.8112899999999 98.0 1120.921285 49.0\n",
      "1188.697285 98.0 1196.217285 49.0\n",
      "1257.853285 119.0 1259.553285 49.0\n",
      "1316.795285 119.0 1317.8052899999998 49.0\n",
      "1409.217285 98.0 1413.467285 49.0\n",
      "1484.755285 119.0 1487.285285 49.0\n",
      "1536.159285 119.0 1538.467285 49.0\n",
      "1577.591285 98.0 1579.7312849999998 49.0\n",
      "LFP2_vHp\n",
      "(3384925,) (3384925,) 2000\n",
      "3384925\n",
      "start_index: 0, end_index: 3384925\n",
      "notch filter applied\n",
      "normalizing data\n",
      "150.22528499999999 98.0 152.14529 49.0\n",
      "272.02928499999996 119.0 273.66929 49.0\n",
      "323.961285 119.0 328.19129 49.0\n",
      "380.22328 119.0 385.17328999999995 49.0\n",
      "440.521285 119.0 444.41128999999995 49.0\n",
      "504.27328499999993 119.0 515.513285 49.0\n",
      "562.731285 98.0 565.08129 49.0\n",
      "610.639285 98.0 612.4392899999999 49.0\n",
      "940.813285 119.0 943.183285 49.0\n",
      "995.3932849999999 98.0 997.043285 49.0\n",
      "1050.1252849999998 119.0 1052.6652849999998 49.0\n",
      "1117.8112899999999 98.0 1120.921285 49.0\n",
      "1188.697285 98.0 1196.217285 49.0\n",
      "1257.853285 119.0 1259.553285 49.0\n",
      "1316.795285 119.0 1317.8052899999998 49.0\n",
      "1409.217285 98.0 1413.467285 49.0\n",
      "1484.755285 119.0 1487.285285 49.0\n",
      "1536.159285 119.0 1538.467285 49.0\n",
      "1577.591285 98.0 1579.7312849999998 49.0\n",
      "LFP3_AON\n",
      "(3384925,) (3384925,) 2000\n",
      "3384925\n",
      "start_index: 0, end_index: 3384925\n",
      "notch filter applied\n",
      "normalizing data\n",
      "150.22528499999999 98.0 152.14529 49.0\n",
      "272.02928499999996 119.0 273.66929 49.0\n",
      "323.961285 119.0 328.19129 49.0\n",
      "380.22328 119.0 385.17328999999995 49.0\n",
      "440.521285 119.0 444.41128999999995 49.0\n",
      "504.27328499999993 119.0 515.513285 49.0\n",
      "562.731285 98.0 565.08129 49.0\n",
      "610.639285 98.0 612.4392899999999 49.0\n",
      "940.813285 119.0 943.183285 49.0\n",
      "995.3932849999999 98.0 997.043285 49.0\n",
      "1050.1252849999998 119.0 1052.6652849999998 49.0\n",
      "1117.8112899999999 98.0 1120.921285 49.0\n",
      "1188.697285 98.0 1196.217285 49.0\n",
      "1257.853285 119.0 1259.553285 49.0\n",
      "1316.795285 119.0 1317.8052899999998 49.0\n",
      "1409.217285 98.0 1413.467285 49.0\n",
      "1484.755285 119.0 1487.285285 49.0\n",
      "1536.159285 119.0 1538.467285 49.0\n",
      "1577.591285 98.0 1579.7312849999998 49.0\n",
      "20230718 dk1 nocontextos2\n",
      "20230718 dk5 nocontextos2\n",
      "20230718 dk6 nocontextos2\n",
      "20230719 dk1 nocontextos2\n",
      "20230719 dk1 nocontextos2\n",
      "20230719 dk5 nocontextos2\n",
      "20230719 dk6 nocontextos2\n",
      "20230807 dk3 BWcontext\n",
      "['Keyboard', 'LFP1_AON', 'LFP1_vHp', 'LFP2_AON', 'LFP3_AON', 'LFP4_AON', 'Ref', 'Respirat']\n",
      "Global start time: 0.0, Global end time: 1692.7728749999999\n",
      "3385546\n",
      "start_index: 0, end_index: 3385546\n",
      "LFP1_AON\n",
      "(3385546,) (3385546,) 2000\n",
      "3385546\n",
      "start_index: 0, end_index: 3385546\n",
      "notch filter applied\n",
      "normalizing data\n",
      "175.06719999999999 98.0 183.993205 49.0\n",
      "324.373205 119.0 325.7732 49.0\n",
      "396.609205 119.0 401.00919999999996 49.0\n",
      "465.74319999999994 98.0 469.931205 49.0\n",
      "559.5391999999999 98.0 562.657205 49.0\n",
      "633.7091999999999 119.0 635.277205 49.0\n",
      "730.7112 98.0 732.211205 49.0\n",
      "806.4071999999999 98.0 808.1171999999999 49.0\n",
      "855.225205 98.0 858.0651999999999 49.0\n",
      "919.8012049999999 119.0 921.989205 49.0\n",
      "1011.225205 119.0 1013.7332049999999 49.0\n",
      "1081.639205 98.0 1083.6191999999999 49.0\n",
      "1121.725205 119.0 1123.9751999999999 49.0\n",
      "1167.455205 119.0 1169.7251999999999 49.0\n",
      "1197.0231999999999 98.0 1199.4432049999998 49.0\n",
      "1292.9112049999999 98.0 1294.9312 49.0\n",
      "1352.4912049999998 119.0 1355.091205 49.0\n",
      "1471.8411999999998 119.0 1473.3292049999998 49.0\n",
      "1536.177205 98.0 1537.6771999999999 49.0\n",
      "1607.121205 119.0 1608.8292049999998 49.0\n",
      "LFP1_vHp\n",
      "(3385546,) (3385546,) 2000\n",
      "3385546\n",
      "start_index: 0, end_index: 3385546\n",
      "notch filter applied\n",
      "normalizing data\n",
      "175.06719999999999 98.0 183.993205 49.0\n",
      "324.373205 119.0 325.7732 49.0\n",
      "396.609205 119.0 401.00919999999996 49.0\n",
      "465.74319999999994 98.0 469.931205 49.0\n",
      "559.5391999999999 98.0 562.657205 49.0\n",
      "633.7091999999999 119.0 635.277205 49.0\n",
      "730.7112 98.0 732.211205 49.0\n",
      "806.4071999999999 98.0 808.1171999999999 49.0\n",
      "855.225205 98.0 858.0651999999999 49.0\n",
      "919.8012049999999 119.0 921.989205 49.0\n",
      "1011.225205 119.0 1013.7332049999999 49.0\n",
      "1081.639205 98.0 1083.6191999999999 49.0\n",
      "1121.725205 119.0 1123.9751999999999 49.0\n",
      "1167.455205 119.0 1169.7251999999999 49.0\n",
      "1197.0231999999999 98.0 1199.4432049999998 49.0\n",
      "1292.9112049999999 98.0 1294.9312 49.0\n",
      "1352.4912049999998 119.0 1355.091205 49.0\n",
      "1471.8411999999998 119.0 1473.3292049999998 49.0\n",
      "1536.177205 98.0 1537.6771999999999 49.0\n",
      "1607.121205 119.0 1608.8292049999998 49.0\n",
      "LFP2_AON\n",
      "(3385546,) (3385546,) 2000\n",
      "3385546\n",
      "start_index: 0, end_index: 3385546\n",
      "notch filter applied\n",
      "normalizing data\n",
      "175.06719999999999 98.0 183.993205 49.0\n",
      "324.373205 119.0 325.7732 49.0\n",
      "396.609205 119.0 401.00919999999996 49.0\n",
      "465.74319999999994 98.0 469.931205 49.0\n",
      "559.5391999999999 98.0 562.657205 49.0\n",
      "633.7091999999999 119.0 635.277205 49.0\n",
      "730.7112 98.0 732.211205 49.0\n",
      "806.4071999999999 98.0 808.1171999999999 49.0\n",
      "855.225205 98.0 858.0651999999999 49.0\n",
      "919.8012049999999 119.0 921.989205 49.0\n",
      "1011.225205 119.0 1013.7332049999999 49.0\n",
      "1081.639205 98.0 1083.6191999999999 49.0\n",
      "1121.725205 119.0 1123.9751999999999 49.0\n",
      "1167.455205 119.0 1169.7251999999999 49.0\n",
      "1197.0231999999999 98.0 1199.4432049999998 49.0\n",
      "1292.9112049999999 98.0 1294.9312 49.0\n",
      "1352.4912049999998 119.0 1355.091205 49.0\n",
      "1471.8411999999998 119.0 1473.3292049999998 49.0\n",
      "1536.177205 98.0 1537.6771999999999 49.0\n",
      "1607.121205 119.0 1608.8292049999998 49.0\n",
      "LFP3_AON\n",
      "(3385546,) (3385546,) 2000\n",
      "3385546\n",
      "start_index: 0, end_index: 3385546\n",
      "notch filter applied\n",
      "normalizing data\n",
      "175.06719999999999 98.0 183.993205 49.0\n",
      "324.373205 119.0 325.7732 49.0\n",
      "396.609205 119.0 401.00919999999996 49.0\n",
      "465.74319999999994 98.0 469.931205 49.0\n",
      "559.5391999999999 98.0 562.657205 49.0\n",
      "633.7091999999999 119.0 635.277205 49.0\n",
      "730.7112 98.0 732.211205 49.0\n",
      "806.4071999999999 98.0 808.1171999999999 49.0\n",
      "855.225205 98.0 858.0651999999999 49.0\n",
      "919.8012049999999 119.0 921.989205 49.0\n",
      "1011.225205 119.0 1013.7332049999999 49.0\n",
      "1081.639205 98.0 1083.6191999999999 49.0\n",
      "1121.725205 119.0 1123.9751999999999 49.0\n",
      "1167.455205 119.0 1169.7251999999999 49.0\n",
      "1197.0231999999999 98.0 1199.4432049999998 49.0\n",
      "1292.9112049999999 98.0 1294.9312 49.0\n",
      "1352.4912049999998 119.0 1355.091205 49.0\n",
      "1471.8411999999998 119.0 1473.3292049999998 49.0\n",
      "1536.177205 98.0 1537.6771999999999 49.0\n",
      "1607.121205 119.0 1608.8292049999998 49.0\n",
      "LFP4_AON\n",
      "(3385546,) (3385546,) 2000\n",
      "3385546\n",
      "start_index: 0, end_index: 3385546\n",
      "notch filter applied\n",
      "normalizing data\n",
      "175.06719999999999 98.0 183.993205 49.0\n",
      "324.373205 119.0 325.7732 49.0\n",
      "396.609205 119.0 401.00919999999996 49.0\n",
      "465.74319999999994 98.0 469.931205 49.0\n",
      "559.5391999999999 98.0 562.657205 49.0\n",
      "633.7091999999999 119.0 635.277205 49.0\n",
      "730.7112 98.0 732.211205 49.0\n",
      "806.4071999999999 98.0 808.1171999999999 49.0\n",
      "855.225205 98.0 858.0651999999999 49.0\n",
      "919.8012049999999 119.0 921.989205 49.0\n",
      "1011.225205 119.0 1013.7332049999999 49.0\n",
      "1081.639205 98.0 1083.6191999999999 49.0\n",
      "1121.725205 119.0 1123.9751999999999 49.0\n",
      "1167.455205 119.0 1169.7251999999999 49.0\n",
      "1197.0231999999999 98.0 1199.4432049999998 49.0\n",
      "1292.9112049999999 98.0 1294.9312 49.0\n",
      "1352.4912049999998 119.0 1355.091205 49.0\n",
      "1471.8411999999998 119.0 1473.3292049999998 49.0\n",
      "1536.177205 98.0 1537.6771999999999 49.0\n",
      "1607.121205 119.0 1608.8292049999998 49.0\n",
      "20230808 dk3 BWcontext\n",
      "['Keyboard', 'LFP1_AON', 'LFP1_vHp', 'LFP2_AON', 'LFP2_vHp', 'LFP3_AON', 'Ref', 'Respirat']\n",
      "Global start time: 0.0, Global end time: 2298.041375\n",
      "4596083\n",
      "start_index: 0, end_index: 4596083\n",
      "LFP1_AON\n",
      "(4596083,) (4596083,) 2000\n",
      "4596083\n",
      "start_index: 0, end_index: 4596083\n",
      "notch filter applied\n",
      "normalizing data\n",
      "101.314855 119.0 110.57885499999999 48.0\n",
      "282.65085999999997 119.0 302.76086 48.0\n",
      "400.98886 98.0 404.65685999999994 49.0\n",
      "632.88686 98.0 637.082865 49.0\n",
      "851.26086 119.0 852.6108599999999 49.0\n",
      "931.7448549999999 98.0 933.73286 49.0\n",
      "1037.99886 98.0 1041.40886 49.0\n",
      "1114.0988599999998 98.0 1115.9068599999998 49.0\n",
      "1193.3588599999998 119.0 1194.568855 49.0\n",
      "1272.8248549999998 98.0 1274.2948549999999 49.0\n",
      "1346.3028599999998 119.0 1348.44086 49.0\n",
      "1446.264855 119.0 1447.2748599999998 49.0\n",
      "1580.0448549999999 119.0 1581.252855 49.0\n",
      "1651.5468649999998 119.0 1653.2368549999999 49.0\n",
      "1710.8948549999998 119.0 1714.38486 49.0\n",
      "1772.8768599999999 119.0 1774.1468549999997 49.0\n",
      "1869.7508599999999 98.0 1871.22086 49.0\n",
      "2084.2708599999996 98.0 2085.970855 49.0\n",
      "2136.186855 119.0 2138.2148549999997 49.0\n",
      "2233.3828599999997 119.0 2234.532855 49.0\n",
      "LFP1_vHp\n",
      "(4596083,) (4596083,) 2000\n",
      "4596083\n",
      "start_index: 0, end_index: 4596083\n",
      "notch filter applied\n",
      "normalizing data\n",
      "101.314855 119.0 110.57885499999999 48.0\n",
      "282.65085999999997 119.0 302.76086 48.0\n",
      "400.98886 98.0 404.65685999999994 49.0\n",
      "632.88686 98.0 637.082865 49.0\n",
      "851.26086 119.0 852.6108599999999 49.0\n",
      "931.7448549999999 98.0 933.73286 49.0\n",
      "1037.99886 98.0 1041.40886 49.0\n",
      "1114.0988599999998 98.0 1115.9068599999998 49.0\n",
      "1193.3588599999998 119.0 1194.568855 49.0\n",
      "1272.8248549999998 98.0 1274.2948549999999 49.0\n",
      "1346.3028599999998 119.0 1348.44086 49.0\n",
      "1446.264855 119.0 1447.2748599999998 49.0\n",
      "1580.0448549999999 119.0 1581.252855 49.0\n",
      "1651.5468649999998 119.0 1653.2368549999999 49.0\n",
      "1710.8948549999998 119.0 1714.38486 49.0\n",
      "1772.8768599999999 119.0 1774.1468549999997 49.0\n",
      "1869.7508599999999 98.0 1871.22086 49.0\n",
      "2084.2708599999996 98.0 2085.970855 49.0\n",
      "2136.186855 119.0 2138.2148549999997 49.0\n",
      "2233.3828599999997 119.0 2234.532855 49.0\n",
      "LFP2_AON\n",
      "(4596083,) (4596083,) 2000\n",
      "4596083\n",
      "start_index: 0, end_index: 4596083\n",
      "notch filter applied\n",
      "normalizing data\n",
      "101.314855 119.0 110.57885499999999 48.0\n",
      "282.65085999999997 119.0 302.76086 48.0\n",
      "400.98886 98.0 404.65685999999994 49.0\n",
      "632.88686 98.0 637.082865 49.0\n",
      "851.26086 119.0 852.6108599999999 49.0\n",
      "931.7448549999999 98.0 933.73286 49.0\n",
      "1037.99886 98.0 1041.40886 49.0\n",
      "1114.0988599999998 98.0 1115.9068599999998 49.0\n",
      "1193.3588599999998 119.0 1194.568855 49.0\n",
      "1272.8248549999998 98.0 1274.2948549999999 49.0\n",
      "1346.3028599999998 119.0 1348.44086 49.0\n",
      "1446.264855 119.0 1447.2748599999998 49.0\n",
      "1580.0448549999999 119.0 1581.252855 49.0\n",
      "1651.5468649999998 119.0 1653.2368549999999 49.0\n",
      "1710.8948549999998 119.0 1714.38486 49.0\n",
      "1772.8768599999999 119.0 1774.1468549999997 49.0\n",
      "1869.7508599999999 98.0 1871.22086 49.0\n",
      "2084.2708599999996 98.0 2085.970855 49.0\n",
      "2136.186855 119.0 2138.2148549999997 49.0\n",
      "2233.3828599999997 119.0 2234.532855 49.0\n",
      "LFP2_vHp\n",
      "(4596083,) (4596083,) 2000\n",
      "4596083\n",
      "start_index: 0, end_index: 4596083\n",
      "notch filter applied\n",
      "normalizing data\n",
      "101.314855 119.0 110.57885499999999 48.0\n",
      "282.65085999999997 119.0 302.76086 48.0\n",
      "400.98886 98.0 404.65685999999994 49.0\n",
      "632.88686 98.0 637.082865 49.0\n",
      "851.26086 119.0 852.6108599999999 49.0\n",
      "931.7448549999999 98.0 933.73286 49.0\n",
      "1037.99886 98.0 1041.40886 49.0\n",
      "1114.0988599999998 98.0 1115.9068599999998 49.0\n",
      "1193.3588599999998 119.0 1194.568855 49.0\n",
      "1272.8248549999998 98.0 1274.2948549999999 49.0\n",
      "1346.3028599999998 119.0 1348.44086 49.0\n",
      "1446.264855 119.0 1447.2748599999998 49.0\n",
      "1580.0448549999999 119.0 1581.252855 49.0\n",
      "1651.5468649999998 119.0 1653.2368549999999 49.0\n",
      "1710.8948549999998 119.0 1714.38486 49.0\n",
      "1772.8768599999999 119.0 1774.1468549999997 49.0\n",
      "1869.7508599999999 98.0 1871.22086 49.0\n",
      "2084.2708599999996 98.0 2085.970855 49.0\n",
      "2136.186855 119.0 2138.2148549999997 49.0\n",
      "2233.3828599999997 119.0 2234.532855 49.0\n",
      "LFP3_AON\n",
      "(4596083,) (4596083,) 2000\n",
      "4596083\n",
      "start_index: 0, end_index: 4596083\n",
      "notch filter applied\n",
      "normalizing data\n",
      "101.314855 119.0 110.57885499999999 48.0\n",
      "282.65085999999997 119.0 302.76086 48.0\n",
      "400.98886 98.0 404.65685999999994 49.0\n",
      "632.88686 98.0 637.082865 49.0\n",
      "851.26086 119.0 852.6108599999999 49.0\n",
      "931.7448549999999 98.0 933.73286 49.0\n",
      "1037.99886 98.0 1041.40886 49.0\n",
      "1114.0988599999998 98.0 1115.9068599999998 49.0\n",
      "1193.3588599999998 119.0 1194.568855 49.0\n",
      "1272.8248549999998 98.0 1274.2948549999999 49.0\n",
      "1346.3028599999998 119.0 1348.44086 49.0\n",
      "1446.264855 119.0 1447.2748599999998 49.0\n",
      "1580.0448549999999 119.0 1581.252855 49.0\n",
      "1651.5468649999998 119.0 1653.2368549999999 49.0\n",
      "1710.8948549999998 119.0 1714.38486 49.0\n",
      "1772.8768599999999 119.0 1774.1468549999997 49.0\n",
      "1869.7508599999999 98.0 1871.22086 49.0\n",
      "2084.2708599999996 98.0 2085.970855 49.0\n",
      "2136.186855 119.0 2138.2148549999997 49.0\n",
      "2233.3828599999997 119.0 2234.532855 49.0\n",
      "20230808 dk5 BWnocontext\n",
      "['Keyboard', 'LFP1_vHp', 'LFP2_vHp', 'LFP3_AON', 'LFP4_AON', 'Ref', 'Respirat']\n",
      "Global start time: 0.0, Global end time: 1655.629025\n",
      "3311259\n",
      "start_index: 0, end_index: 3311258\n",
      "LFP1_vHp\n",
      "(3311258,) (3311258,) 2000\n",
      "3311259\n",
      "start_index: 0, end_index: 3311258\n",
      "notch filter applied\n",
      "normalizing data\n",
      "64.898905 119.0 68.48890499999999 49.0\n",
      "138.418905 119.0 139.27890499999998 49.0\n",
      "230.73289999999997 98.0 233.21290499999998 49.0\n",
      "302.9049 98.0 303.964905 49.0\n",
      "390.7949 119.0 392.154905 49.0\n",
      "472.48490499999997 98.0 473.66090499999996 49.0\n",
      "539.346905 98.0 540.9469049999999 49.0\n",
      "611.838905 98.0 614.2988999999999 49.0\n",
      "752.6909049999999 98.0 755.1108999999999 49.0\n",
      "813.9989049999999 98.0 815.818905 49.0\n",
      "872.5669049999999 98.0 880.0968999999999 49.0\n",
      "953.9349 119.0 955.4628999999999 49.0\n",
      "1009.6889049999999 119.0 1010.8689049999999 49.0\n",
      "1074.586905 119.0 1076.4269049999998 49.0\n",
      "1162.084905 119.0 1163.6048999999998 49.0\n",
      "1273.6448999999998 98.0 1275.874905 49.0\n",
      "1338.7649 119.0 1339.814905 49.0\n",
      "1404.9169 119.0 1405.9649 49.0\n",
      "1516.2029049999999 98.0 1517.6528999999998 49.0\n",
      "1588.0448999999999 119.0 1590.2749049999998 49.0\n",
      "LFP2_vHp\n",
      "(3311258,) (3311258,) 2000\n",
      "3311259\n",
      "start_index: 0, end_index: 3311258\n",
      "notch filter applied\n",
      "normalizing data\n",
      "64.898905 119.0 68.48890499999999 49.0\n",
      "138.418905 119.0 139.27890499999998 49.0\n",
      "230.73289999999997 98.0 233.21290499999998 49.0\n",
      "302.9049 98.0 303.964905 49.0\n",
      "390.7949 119.0 392.154905 49.0\n",
      "472.48490499999997 98.0 473.66090499999996 49.0\n",
      "539.346905 98.0 540.9469049999999 49.0\n",
      "611.838905 98.0 614.2988999999999 49.0\n",
      "752.6909049999999 98.0 755.1108999999999 49.0\n",
      "813.9989049999999 98.0 815.818905 49.0\n",
      "872.5669049999999 98.0 880.0968999999999 49.0\n",
      "953.9349 119.0 955.4628999999999 49.0\n",
      "1009.6889049999999 119.0 1010.8689049999999 49.0\n",
      "1074.586905 119.0 1076.4269049999998 49.0\n",
      "1162.084905 119.0 1163.6048999999998 49.0\n",
      "1273.6448999999998 98.0 1275.874905 49.0\n",
      "1338.7649 119.0 1339.814905 49.0\n",
      "1404.9169 119.0 1405.9649 49.0\n",
      "1516.2029049999999 98.0 1517.6528999999998 49.0\n",
      "1588.0448999999999 119.0 1590.2749049999998 49.0\n",
      "LFP3_AON\n",
      "(3311259,) (3311259,) 2000\n",
      "3311259\n",
      "start_index: 0, end_index: 3311259\n",
      "notch filter applied\n",
      "normalizing data\n",
      "64.898905 119.0 68.48890499999999 49.0\n",
      "138.418905 119.0 139.27890499999998 49.0\n",
      "230.73289999999997 98.0 233.21290499999998 49.0\n",
      "302.9049 98.0 303.964905 49.0\n",
      "390.7949 119.0 392.154905 49.0\n",
      "472.48490499999997 98.0 473.66090499999996 49.0\n",
      "539.346905 98.0 540.9469049999999 49.0\n",
      "611.838905 98.0 614.2988999999999 49.0\n",
      "752.6909049999999 98.0 755.1108999999999 49.0\n",
      "813.9989049999999 98.0 815.818905 49.0\n",
      "872.5669049999999 98.0 880.0968999999999 49.0\n",
      "953.9349 119.0 955.4628999999999 49.0\n",
      "1009.6889049999999 119.0 1010.8689049999999 49.0\n",
      "1074.586905 119.0 1076.4269049999998 49.0\n",
      "1162.084905 119.0 1163.6048999999998 49.0\n",
      "1273.6448999999998 98.0 1275.874905 49.0\n",
      "1338.7649 119.0 1339.814905 49.0\n",
      "1404.9169 119.0 1405.9649 49.0\n",
      "1516.2029049999999 98.0 1517.6528999999998 49.0\n",
      "1588.0448999999999 119.0 1590.2749049999998 49.0\n",
      "LFP4_AON\n",
      "(3311259,) (3311259,) 2000\n",
      "3311259\n",
      "start_index: 0, end_index: 3311259\n",
      "notch filter applied\n",
      "normalizing data\n",
      "64.898905 119.0 68.48890499999999 49.0\n",
      "138.418905 119.0 139.27890499999998 49.0\n",
      "230.73289999999997 98.0 233.21290499999998 49.0\n",
      "302.9049 98.0 303.964905 49.0\n",
      "390.7949 119.0 392.154905 49.0\n",
      "472.48490499999997 98.0 473.66090499999996 49.0\n",
      "539.346905 98.0 540.9469049999999 49.0\n",
      "611.838905 98.0 614.2988999999999 49.0\n",
      "752.6909049999999 98.0 755.1108999999999 49.0\n",
      "813.9989049999999 98.0 815.818905 49.0\n",
      "872.5669049999999 98.0 880.0968999999999 49.0\n",
      "953.9349 119.0 955.4628999999999 49.0\n",
      "1009.6889049999999 119.0 1010.8689049999999 49.0\n",
      "1074.586905 119.0 1076.4269049999998 49.0\n",
      "1162.084905 119.0 1163.6048999999998 49.0\n",
      "1273.6448999999998 98.0 1275.874905 49.0\n",
      "1338.7649 119.0 1339.814905 49.0\n",
      "1404.9169 119.0 1405.9649 49.0\n",
      "1516.2029049999999 98.0 1517.6528999999998 49.0\n",
      "1588.0448999999999 119.0 1590.2749049999998 49.0\n",
      "20230810 dk5 BWnocontext\n",
      "['Keyboard', 'LFP1_AON', 'LFP1_vHp', 'LFP2_AON', 'LFP2_vHp', 'LFP3_AON', 'LFP4_AON', 'Ref', 'Respirat']\n",
      "Global start time: 0.0, Global end time: 1459.5658749999998\n",
      "2919132\n",
      "start_index: 0, end_index: 2919132\n",
      "LFP1_AON\n",
      "(2919132,) (2919132,) 2000\n",
      "2919132\n",
      "start_index: 0, end_index: 2919132\n",
      "notch filter applied\n",
      "normalizing data\n",
      "66.826495 119.0 73.66249499999999 49.0\n",
      "139.588495 98.0 140.91849499999998 49.0\n",
      "199.58649499999999 98.0 201.31649499999997 49.0\n",
      "269.02649499999995 98.0 270.51649499999996 49.0\n",
      "341.93049499999995 98.0 343.830495 49.0\n",
      "435.83849499999997 98.0 437.67849499999994 49.0\n",
      "504.70849499999997 119.0 506.3184949999999 49.0\n",
      "561.9704949999999 98.0 563.140495 49.0\n",
      "631.044495 98.0 633.002495 49.0\n",
      "710.9264949999999 119.0 712.036495 49.0\n",
      "764.4164949999999 119.0 765.4164949999999 49.0\n",
      "831.7104949999999 119.0 832.900495 49.0\n",
      "891.5224949999999 98.0 892.982495 49.0\n",
      "951.6384949999999 98.0 954.7884949999999 49.0\n",
      "1012.290495 119.0 1013.968495 49.0\n",
      "1069.818495 98.0 1071.578495 49.0\n",
      "1126.2464949999999 119.0 1127.954495 49.0\n",
      "1202.462495 119.0 1204.1304949999999 49.0\n",
      "1284.106495 119.0 1285.636495 49.0\n",
      "1361.014495 119.0 1362.5844949999998 49.0\n",
      "LFP1_vHp\n",
      "(2919132,) (2919132,) 2000\n",
      "2919132\n",
      "start_index: 0, end_index: 2919132\n",
      "notch filter applied\n",
      "normalizing data\n",
      "66.826495 119.0 73.66249499999999 49.0\n",
      "139.588495 98.0 140.91849499999998 49.0\n",
      "199.58649499999999 98.0 201.31649499999997 49.0\n",
      "269.02649499999995 98.0 270.51649499999996 49.0\n",
      "341.93049499999995 98.0 343.830495 49.0\n",
      "435.83849499999997 98.0 437.67849499999994 49.0\n",
      "504.70849499999997 119.0 506.3184949999999 49.0\n",
      "561.9704949999999 98.0 563.140495 49.0\n",
      "631.044495 98.0 633.002495 49.0\n",
      "710.9264949999999 119.0 712.036495 49.0\n",
      "764.4164949999999 119.0 765.4164949999999 49.0\n",
      "831.7104949999999 119.0 832.900495 49.0\n",
      "891.5224949999999 98.0 892.982495 49.0\n",
      "951.6384949999999 98.0 954.7884949999999 49.0\n",
      "1012.290495 119.0 1013.968495 49.0\n",
      "1069.818495 98.0 1071.578495 49.0\n",
      "1126.2464949999999 119.0 1127.954495 49.0\n",
      "1202.462495 119.0 1204.1304949999999 49.0\n",
      "1284.106495 119.0 1285.636495 49.0\n",
      "1361.014495 119.0 1362.5844949999998 49.0\n",
      "LFP2_AON\n",
      "(2919132,) (2919132,) 2000\n",
      "2919132\n",
      "start_index: 0, end_index: 2919132\n",
      "notch filter applied\n",
      "normalizing data\n",
      "66.826495 119.0 73.66249499999999 49.0\n",
      "139.588495 98.0 140.91849499999998 49.0\n",
      "199.58649499999999 98.0 201.31649499999997 49.0\n",
      "269.02649499999995 98.0 270.51649499999996 49.0\n",
      "341.93049499999995 98.0 343.830495 49.0\n",
      "435.83849499999997 98.0 437.67849499999994 49.0\n",
      "504.70849499999997 119.0 506.3184949999999 49.0\n",
      "561.9704949999999 98.0 563.140495 49.0\n",
      "631.044495 98.0 633.002495 49.0\n",
      "710.9264949999999 119.0 712.036495 49.0\n",
      "764.4164949999999 119.0 765.4164949999999 49.0\n",
      "831.7104949999999 119.0 832.900495 49.0\n",
      "891.5224949999999 98.0 892.982495 49.0\n",
      "951.6384949999999 98.0 954.7884949999999 49.0\n",
      "1012.290495 119.0 1013.968495 49.0\n",
      "1069.818495 98.0 1071.578495 49.0\n",
      "1126.2464949999999 119.0 1127.954495 49.0\n",
      "1202.462495 119.0 1204.1304949999999 49.0\n",
      "1284.106495 119.0 1285.636495 49.0\n",
      "1361.014495 119.0 1362.5844949999998 49.0\n",
      "LFP2_vHp\n",
      "(2919132,) (2919132,) 2000\n",
      "2919132\n",
      "start_index: 0, end_index: 2919132\n",
      "notch filter applied\n",
      "normalizing data\n",
      "66.826495 119.0 73.66249499999999 49.0\n",
      "139.588495 98.0 140.91849499999998 49.0\n",
      "199.58649499999999 98.0 201.31649499999997 49.0\n",
      "269.02649499999995 98.0 270.51649499999996 49.0\n",
      "341.93049499999995 98.0 343.830495 49.0\n",
      "435.83849499999997 98.0 437.67849499999994 49.0\n",
      "504.70849499999997 119.0 506.3184949999999 49.0\n",
      "561.9704949999999 98.0 563.140495 49.0\n",
      "631.044495 98.0 633.002495 49.0\n",
      "710.9264949999999 119.0 712.036495 49.0\n",
      "764.4164949999999 119.0 765.4164949999999 49.0\n",
      "831.7104949999999 119.0 832.900495 49.0\n",
      "891.5224949999999 98.0 892.982495 49.0\n",
      "951.6384949999999 98.0 954.7884949999999 49.0\n",
      "1012.290495 119.0 1013.968495 49.0\n",
      "1069.818495 98.0 1071.578495 49.0\n",
      "1126.2464949999999 119.0 1127.954495 49.0\n",
      "1202.462495 119.0 1204.1304949999999 49.0\n",
      "1284.106495 119.0 1285.636495 49.0\n",
      "1361.014495 119.0 1362.5844949999998 49.0\n",
      "LFP3_AON\n",
      "(2919132,) (2919132,) 2000\n",
      "2919132\n",
      "start_index: 0, end_index: 2919132\n",
      "notch filter applied\n",
      "normalizing data\n",
      "66.826495 119.0 73.66249499999999 49.0\n",
      "139.588495 98.0 140.91849499999998 49.0\n",
      "199.58649499999999 98.0 201.31649499999997 49.0\n",
      "269.02649499999995 98.0 270.51649499999996 49.0\n",
      "341.93049499999995 98.0 343.830495 49.0\n",
      "435.83849499999997 98.0 437.67849499999994 49.0\n",
      "504.70849499999997 119.0 506.3184949999999 49.0\n",
      "561.9704949999999 98.0 563.140495 49.0\n",
      "631.044495 98.0 633.002495 49.0\n",
      "710.9264949999999 119.0 712.036495 49.0\n",
      "764.4164949999999 119.0 765.4164949999999 49.0\n",
      "831.7104949999999 119.0 832.900495 49.0\n",
      "891.5224949999999 98.0 892.982495 49.0\n",
      "951.6384949999999 98.0 954.7884949999999 49.0\n",
      "1012.290495 119.0 1013.968495 49.0\n",
      "1069.818495 98.0 1071.578495 49.0\n",
      "1126.2464949999999 119.0 1127.954495 49.0\n",
      "1202.462495 119.0 1204.1304949999999 49.0\n",
      "1284.106495 119.0 1285.636495 49.0\n",
      "1361.014495 119.0 1362.5844949999998 49.0\n",
      "LFP4_AON\n",
      "(2919132,) (2919132,) 2000\n",
      "2919132\n",
      "start_index: 0, end_index: 2919132\n",
      "notch filter applied\n",
      "normalizing data\n",
      "66.826495 119.0 73.66249499999999 49.0\n",
      "139.588495 98.0 140.91849499999998 49.0\n",
      "199.58649499999999 98.0 201.31649499999997 49.0\n",
      "269.02649499999995 98.0 270.51649499999996 49.0\n",
      "341.93049499999995 98.0 343.830495 49.0\n",
      "435.83849499999997 98.0 437.67849499999994 49.0\n",
      "504.70849499999997 119.0 506.3184949999999 49.0\n",
      "561.9704949999999 98.0 563.140495 49.0\n",
      "631.044495 98.0 633.002495 49.0\n",
      "710.9264949999999 119.0 712.036495 49.0\n",
      "764.4164949999999 119.0 765.4164949999999 49.0\n",
      "831.7104949999999 119.0 832.900495 49.0\n",
      "891.5224949999999 98.0 892.982495 49.0\n",
      "951.6384949999999 98.0 954.7884949999999 49.0\n",
      "1012.290495 119.0 1013.968495 49.0\n",
      "1069.818495 98.0 1071.578495 49.0\n",
      "1126.2464949999999 119.0 1127.954495 49.0\n",
      "1202.462495 119.0 1204.1304949999999 49.0\n",
      "1284.106495 119.0 1285.636495 49.0\n",
      "1361.014495 119.0 1362.5844949999998 49.0\n",
      "20230817 dk1 BWcontext\n",
      "['#refs#', 'Keyboard', 'LFP1_AON', 'LFP1_vHp', 'LFP2_AON', 'LFP2_vHp', 'LFP3_AON', 'LFP4_AON', 'Ref', 'Respirat', 'concatenation_info']\n",
      "Global start time: 0.0, Global end time: 1899.6833749999998\n",
      "3799367\n",
      "start_index: 0, end_index: 3799367\n",
      "LFP1_AON\n",
      "(3799367,) (3799367,) 2000\n",
      "3799367\n",
      "start_index: 0, end_index: 3799367\n",
      "notch filter applied\n",
      "normalizing data\n",
      "136.26361 98.0 138.819605 48.0\n",
      "269.92960999999997 119.0 273.61960999999997 48.0\n",
      "369.18760999999995 119.0 370.96560999999997 49.0\n",
      "460.04421999999994 98.0 462.074215 48.0\n",
      "549.0922149999999 98.0 551.132215 49.0\n",
      "627.12222 98.0 629.7362149999999 49.0\n",
      "813.2002149999998 119.0 815.7502199999999 49.0\n",
      "989.26222 119.0 991.0322199999999 49.0\n",
      "1086.45822 119.0 1088.2562199999998 49.0\n",
      "1143.01622 119.0 1145.31622 49.0\n",
      "1233.3762149999998 119.0 1234.8862199999999 49.0\n",
      "1298.32422 98.0 1302.2942199999998 49.0\n",
      "1360.8402149999997 119.0 1362.1402199999998 49.0\n",
      "1432.484215 119.0 1433.7222149999998 49.0\n",
      "1503.1602199999998 119.0 1504.7082149999997 49.0\n",
      "1578.05022 119.0 1579.6182199999998 49.0\n",
      "1637.85822 98.0 1640.99822 49.0\n",
      "1705.1442199999997 98.0 1706.82422 49.0\n",
      "1773.0122149999997 119.0 1774.6222199999997 49.0\n",
      "1839.9822199999999 98.0 1842.2802199999996 49.0\n",
      "LFP1_vHp\n",
      "(3799367,) (3799367,) 2000\n",
      "3799367\n",
      "start_index: 0, end_index: 3799367\n",
      "notch filter applied\n",
      "normalizing data\n",
      "136.26361 98.0 138.819605 48.0\n",
      "269.92960999999997 119.0 273.61960999999997 48.0\n",
      "369.18760999999995 119.0 370.96560999999997 49.0\n",
      "460.04421999999994 98.0 462.074215 48.0\n",
      "549.0922149999999 98.0 551.132215 49.0\n",
      "627.12222 98.0 629.7362149999999 49.0\n",
      "813.2002149999998 119.0 815.7502199999999 49.0\n",
      "989.26222 119.0 991.0322199999999 49.0\n",
      "1086.45822 119.0 1088.2562199999998 49.0\n",
      "1143.01622 119.0 1145.31622 49.0\n",
      "1233.3762149999998 119.0 1234.8862199999999 49.0\n",
      "1298.32422 98.0 1302.2942199999998 49.0\n",
      "1360.8402149999997 119.0 1362.1402199999998 49.0\n",
      "1432.484215 119.0 1433.7222149999998 49.0\n",
      "1503.1602199999998 119.0 1504.7082149999997 49.0\n",
      "1578.05022 119.0 1579.6182199999998 49.0\n",
      "1637.85822 98.0 1640.99822 49.0\n",
      "1705.1442199999997 98.0 1706.82422 49.0\n",
      "1773.0122149999997 119.0 1774.6222199999997 49.0\n",
      "1839.9822199999999 98.0 1842.2802199999996 49.0\n",
      "LFP2_AON\n",
      "(3799367,) (3799367,) 2000\n",
      "3799367\n",
      "start_index: 0, end_index: 3799367\n",
      "notch filter applied\n",
      "normalizing data\n",
      "136.26361 98.0 138.819605 48.0\n",
      "269.92960999999997 119.0 273.61960999999997 48.0\n",
      "369.18760999999995 119.0 370.96560999999997 49.0\n",
      "460.04421999999994 98.0 462.074215 48.0\n",
      "549.0922149999999 98.0 551.132215 49.0\n",
      "627.12222 98.0 629.7362149999999 49.0\n",
      "813.2002149999998 119.0 815.7502199999999 49.0\n",
      "989.26222 119.0 991.0322199999999 49.0\n",
      "1086.45822 119.0 1088.2562199999998 49.0\n",
      "1143.01622 119.0 1145.31622 49.0\n",
      "1233.3762149999998 119.0 1234.8862199999999 49.0\n",
      "1298.32422 98.0 1302.2942199999998 49.0\n",
      "1360.8402149999997 119.0 1362.1402199999998 49.0\n",
      "1432.484215 119.0 1433.7222149999998 49.0\n",
      "1503.1602199999998 119.0 1504.7082149999997 49.0\n",
      "1578.05022 119.0 1579.6182199999998 49.0\n",
      "1637.85822 98.0 1640.99822 49.0\n",
      "1705.1442199999997 98.0 1706.82422 49.0\n",
      "1773.0122149999997 119.0 1774.6222199999997 49.0\n",
      "1839.9822199999999 98.0 1842.2802199999996 49.0\n",
      "LFP2_vHp\n",
      "(3799367,) (3799367,) 2000\n",
      "3799367\n",
      "start_index: 0, end_index: 3799367\n",
      "notch filter applied\n",
      "normalizing data\n",
      "136.26361 98.0 138.819605 48.0\n",
      "269.92960999999997 119.0 273.61960999999997 48.0\n",
      "369.18760999999995 119.0 370.96560999999997 49.0\n",
      "460.04421999999994 98.0 462.074215 48.0\n",
      "549.0922149999999 98.0 551.132215 49.0\n",
      "627.12222 98.0 629.7362149999999 49.0\n",
      "813.2002149999998 119.0 815.7502199999999 49.0\n",
      "989.26222 119.0 991.0322199999999 49.0\n",
      "1086.45822 119.0 1088.2562199999998 49.0\n",
      "1143.01622 119.0 1145.31622 49.0\n",
      "1233.3762149999998 119.0 1234.8862199999999 49.0\n",
      "1298.32422 98.0 1302.2942199999998 49.0\n",
      "1360.8402149999997 119.0 1362.1402199999998 49.0\n",
      "1432.484215 119.0 1433.7222149999998 49.0\n",
      "1503.1602199999998 119.0 1504.7082149999997 49.0\n",
      "1578.05022 119.0 1579.6182199999998 49.0\n",
      "1637.85822 98.0 1640.99822 49.0\n",
      "1705.1442199999997 98.0 1706.82422 49.0\n",
      "1773.0122149999997 119.0 1774.6222199999997 49.0\n",
      "1839.9822199999999 98.0 1842.2802199999996 49.0\n",
      "LFP3_AON\n",
      "(3799367,) (3799367,) 2000\n",
      "3799367\n",
      "start_index: 0, end_index: 3799367\n",
      "notch filter applied\n",
      "normalizing data\n",
      "136.26361 98.0 138.819605 48.0\n",
      "269.92960999999997 119.0 273.61960999999997 48.0\n",
      "369.18760999999995 119.0 370.96560999999997 49.0\n",
      "460.04421999999994 98.0 462.074215 48.0\n",
      "549.0922149999999 98.0 551.132215 49.0\n",
      "627.12222 98.0 629.7362149999999 49.0\n",
      "813.2002149999998 119.0 815.7502199999999 49.0\n",
      "989.26222 119.0 991.0322199999999 49.0\n",
      "1086.45822 119.0 1088.2562199999998 49.0\n",
      "1143.01622 119.0 1145.31622 49.0\n",
      "1233.3762149999998 119.0 1234.8862199999999 49.0\n",
      "1298.32422 98.0 1302.2942199999998 49.0\n",
      "1360.8402149999997 119.0 1362.1402199999998 49.0\n",
      "1432.484215 119.0 1433.7222149999998 49.0\n",
      "1503.1602199999998 119.0 1504.7082149999997 49.0\n",
      "1578.05022 119.0 1579.6182199999998 49.0\n",
      "1637.85822 98.0 1640.99822 49.0\n",
      "1705.1442199999997 98.0 1706.82422 49.0\n",
      "1773.0122149999997 119.0 1774.6222199999997 49.0\n",
      "1839.9822199999999 98.0 1842.2802199999996 49.0\n",
      "LFP4_AON\n",
      "(3799367,) (3799367,) 2000\n",
      "3799367\n",
      "start_index: 0, end_index: 3799367\n",
      "notch filter applied\n",
      "normalizing data\n",
      "136.26361 98.0 138.819605 48.0\n",
      "269.92960999999997 119.0 273.61960999999997 48.0\n",
      "369.18760999999995 119.0 370.96560999999997 49.0\n",
      "460.04421999999994 98.0 462.074215 48.0\n",
      "549.0922149999999 98.0 551.132215 49.0\n",
      "627.12222 98.0 629.7362149999999 49.0\n",
      "813.2002149999998 119.0 815.7502199999999 49.0\n",
      "989.26222 119.0 991.0322199999999 49.0\n",
      "1086.45822 119.0 1088.2562199999998 49.0\n",
      "1143.01622 119.0 1145.31622 49.0\n",
      "1233.3762149999998 119.0 1234.8862199999999 49.0\n",
      "1298.32422 98.0 1302.2942199999998 49.0\n",
      "1360.8402149999997 119.0 1362.1402199999998 49.0\n",
      "1432.484215 119.0 1433.7222149999998 49.0\n",
      "1503.1602199999998 119.0 1504.7082149999997 49.0\n",
      "1578.05022 119.0 1579.6182199999998 49.0\n",
      "1637.85822 98.0 1640.99822 49.0\n",
      "1705.1442199999997 98.0 1706.82422 49.0\n",
      "1773.0122149999997 119.0 1774.6222199999997 49.0\n",
      "1839.9822199999999 98.0 1842.2802199999996 49.0\n",
      "20230818 dk1 BWcontext\n",
      "['Keyboard', 'LFP1_AON', 'LFP1_vHp', 'LFP2_AON', 'LFP2_vHp', 'LFP3_AON', 'LFP4_AON', 'Ref', 'Respirat']\n",
      "Global start time: 0.0, Global end time: 2279.7355749999997\n",
      "4559472\n",
      "start_index: 0, end_index: 4559471\n",
      "LFP1_AON\n",
      "(4559472,) (4559472,) 2000\n",
      "4559472\n",
      "start_index: 0, end_index: 4559472\n",
      "notch filter applied\n",
      "normalizing data\n",
      "78.64780999999999 119.0 79.83780999999999 49.0\n",
      "179.923805 98.0 181.46380499999998 49.0\n",
      "319.73981 119.0 321.15980499999995 49.0\n",
      "421.989805 119.0 425.53980499999994 48.0\n",
      "499.08380999999997 98.0 500.57380499999994 49.0\n",
      "576.4418049999999 98.0 578.42981 49.0\n",
      "706.5658099999999 98.0 708.65581 49.0\n",
      "811.0518099999999 98.0 813.401805 49.0\n",
      "1035.4558049999998 119.0 1036.9658049999998 49.0\n",
      "1204.5858099999998 98.0 1208.385805 49.0\n",
      "1315.04181 119.0 1316.5618049999998 49.0\n",
      "1381.1698099999999 119.0 1382.7798099999998 49.0\n",
      "1456.2978099999998 119.0 1458.497805 49.0\n",
      "1529.5098099999998 119.0 1531.7678099999998 49.0\n",
      "1605.7258049999998 119.0 1613.0518049999998 49.0\n",
      "1673.4878049999998 98.0 1676.5878099999998 49.0\n",
      "1775.02381 98.0 1777.3918049999997 49.0\n",
      "1857.8778049999999 98.0 1862.4478049999998 49.0\n",
      "1929.0318099999997 119.0 1931.481805 49.0\n",
      "2112.2978049999997 98.0 2114.66781 49.0\n",
      "LFP1_vHp\n",
      "(4559471,) (4559471,) 2000\n",
      "4559472\n",
      "start_index: 0, end_index: 4559471\n",
      "notch filter applied\n",
      "normalizing data\n",
      "78.64780999999999 119.0 79.83780999999999 49.0\n",
      "179.923805 98.0 181.46380499999998 49.0\n",
      "319.73981 119.0 321.15980499999995 49.0\n",
      "421.989805 119.0 425.53980499999994 48.0\n",
      "499.08380999999997 98.0 500.57380499999994 49.0\n",
      "576.4418049999999 98.0 578.42981 49.0\n",
      "706.5658099999999 98.0 708.65581 49.0\n",
      "811.0518099999999 98.0 813.401805 49.0\n",
      "1035.4558049999998 119.0 1036.9658049999998 49.0\n",
      "1204.5858099999998 98.0 1208.385805 49.0\n",
      "1315.04181 119.0 1316.5618049999998 49.0\n",
      "1381.1698099999999 119.0 1382.7798099999998 49.0\n",
      "1456.2978099999998 119.0 1458.497805 49.0\n",
      "1529.5098099999998 119.0 1531.7678099999998 49.0\n",
      "1605.7258049999998 119.0 1613.0518049999998 49.0\n",
      "1673.4878049999998 98.0 1676.5878099999998 49.0\n",
      "1775.02381 98.0 1777.3918049999997 49.0\n",
      "1857.8778049999999 98.0 1862.4478049999998 49.0\n",
      "1929.0318099999997 119.0 1931.481805 49.0\n",
      "2112.2978049999997 98.0 2114.66781 49.0\n",
      "LFP2_AON\n",
      "(4559472,) (4559472,) 2000\n",
      "4559472\n",
      "start_index: 0, end_index: 4559472\n",
      "notch filter applied\n",
      "normalizing data\n",
      "78.64780999999999 119.0 79.83780999999999 49.0\n",
      "179.923805 98.0 181.46380499999998 49.0\n",
      "319.73981 119.0 321.15980499999995 49.0\n",
      "421.989805 119.0 425.53980499999994 48.0\n",
      "499.08380999999997 98.0 500.57380499999994 49.0\n",
      "576.4418049999999 98.0 578.42981 49.0\n",
      "706.5658099999999 98.0 708.65581 49.0\n",
      "811.0518099999999 98.0 813.401805 49.0\n",
      "1035.4558049999998 119.0 1036.9658049999998 49.0\n",
      "1204.5858099999998 98.0 1208.385805 49.0\n",
      "1315.04181 119.0 1316.5618049999998 49.0\n",
      "1381.1698099999999 119.0 1382.7798099999998 49.0\n",
      "1456.2978099999998 119.0 1458.497805 49.0\n",
      "1529.5098099999998 119.0 1531.7678099999998 49.0\n",
      "1605.7258049999998 119.0 1613.0518049999998 49.0\n",
      "1673.4878049999998 98.0 1676.5878099999998 49.0\n",
      "1775.02381 98.0 1777.3918049999997 49.0\n",
      "1857.8778049999999 98.0 1862.4478049999998 49.0\n",
      "1929.0318099999997 119.0 1931.481805 49.0\n",
      "2112.2978049999997 98.0 2114.66781 49.0\n",
      "LFP2_vHp\n",
      "(4559471,) (4559471,) 2000\n",
      "4559472\n",
      "start_index: 0, end_index: 4559471\n",
      "notch filter applied\n",
      "normalizing data\n",
      "78.64780999999999 119.0 79.83780999999999 49.0\n",
      "179.923805 98.0 181.46380499999998 49.0\n",
      "319.73981 119.0 321.15980499999995 49.0\n",
      "421.989805 119.0 425.53980499999994 48.0\n",
      "499.08380999999997 98.0 500.57380499999994 49.0\n",
      "576.4418049999999 98.0 578.42981 49.0\n",
      "706.5658099999999 98.0 708.65581 49.0\n",
      "811.0518099999999 98.0 813.401805 49.0\n",
      "1035.4558049999998 119.0 1036.9658049999998 49.0\n",
      "1204.5858099999998 98.0 1208.385805 49.0\n",
      "1315.04181 119.0 1316.5618049999998 49.0\n",
      "1381.1698099999999 119.0 1382.7798099999998 49.0\n",
      "1456.2978099999998 119.0 1458.497805 49.0\n",
      "1529.5098099999998 119.0 1531.7678099999998 49.0\n",
      "1605.7258049999998 119.0 1613.0518049999998 49.0\n",
      "1673.4878049999998 98.0 1676.5878099999998 49.0\n",
      "1775.02381 98.0 1777.3918049999997 49.0\n",
      "1857.8778049999999 98.0 1862.4478049999998 49.0\n",
      "1929.0318099999997 119.0 1931.481805 49.0\n",
      "2112.2978049999997 98.0 2114.66781 49.0\n",
      "LFP3_AON\n",
      "(4559472,) (4559472,) 2000\n",
      "4559472\n",
      "start_index: 0, end_index: 4559472\n",
      "notch filter applied\n",
      "normalizing data\n",
      "78.64780999999999 119.0 79.83780999999999 49.0\n",
      "179.923805 98.0 181.46380499999998 49.0\n",
      "319.73981 119.0 321.15980499999995 49.0\n",
      "421.989805 119.0 425.53980499999994 48.0\n",
      "499.08380999999997 98.0 500.57380499999994 49.0\n",
      "576.4418049999999 98.0 578.42981 49.0\n",
      "706.5658099999999 98.0 708.65581 49.0\n",
      "811.0518099999999 98.0 813.401805 49.0\n",
      "1035.4558049999998 119.0 1036.9658049999998 49.0\n",
      "1204.5858099999998 98.0 1208.385805 49.0\n",
      "1315.04181 119.0 1316.5618049999998 49.0\n",
      "1381.1698099999999 119.0 1382.7798099999998 49.0\n",
      "1456.2978099999998 119.0 1458.497805 49.0\n",
      "1529.5098099999998 119.0 1531.7678099999998 49.0\n",
      "1605.7258049999998 119.0 1613.0518049999998 49.0\n",
      "1673.4878049999998 98.0 1676.5878099999998 49.0\n",
      "1775.02381 98.0 1777.3918049999997 49.0\n",
      "1857.8778049999999 98.0 1862.4478049999998 49.0\n",
      "1929.0318099999997 119.0 1931.481805 49.0\n",
      "2112.2978049999997 98.0 2114.66781 49.0\n",
      "LFP4_AON\n",
      "(4559472,) (4559472,) 2000\n",
      "4559472\n",
      "start_index: 0, end_index: 4559472\n",
      "notch filter applied\n",
      "normalizing data\n",
      "78.64780999999999 119.0 79.83780999999999 49.0\n",
      "179.923805 98.0 181.46380499999998 49.0\n",
      "319.73981 119.0 321.15980499999995 49.0\n",
      "421.989805 119.0 425.53980499999994 48.0\n",
      "499.08380999999997 98.0 500.57380499999994 49.0\n",
      "576.4418049999999 98.0 578.42981 49.0\n",
      "706.5658099999999 98.0 708.65581 49.0\n",
      "811.0518099999999 98.0 813.401805 49.0\n",
      "1035.4558049999998 119.0 1036.9658049999998 49.0\n",
      "1204.5858099999998 98.0 1208.385805 49.0\n",
      "1315.04181 119.0 1316.5618049999998 49.0\n",
      "1381.1698099999999 119.0 1382.7798099999998 49.0\n",
      "1456.2978099999998 119.0 1458.497805 49.0\n",
      "1529.5098099999998 119.0 1531.7678099999998 49.0\n",
      "1605.7258049999998 119.0 1613.0518049999998 49.0\n",
      "1673.4878049999998 98.0 1676.5878099999998 49.0\n",
      "1775.02381 98.0 1777.3918049999997 49.0\n",
      "1857.8778049999999 98.0 1862.4478049999998 49.0\n",
      "1929.0318099999997 119.0 1931.481805 49.0\n",
      "2112.2978049999997 98.0 2114.66781 49.0\n",
      "20230818 dk3 BWcontext\n",
      "['Keyboard', 'LFP1_AON', 'LFP1_vHp', 'LFP2_AON', 'LFP2_vHp', 'LFP3_AON', 'LFP4_AON', 'Ref', 'Respirat']\n",
      "Global start time: 0.0, Global end time: 1814.608675\n",
      "3629218\n",
      "start_index: 0, end_index: 3629217\n",
      "LFP1_AON\n",
      "(3629218,) (3629218,) 2000\n",
      "3629218\n",
      "start_index: 0, end_index: 3629218\n",
      "notch filter applied\n",
      "normalizing data\n",
      "129.769835 98.0 135.24584 49.0\n",
      "260.73184 119.0 262.11184 49.0\n",
      "332.70984 119.0 335.26784 49.0\n",
      "405.553835 98.0 407.993835 49.0\n",
      "503.84183499999995 98.0 505.27984 49.0\n",
      "582.02184 98.0 583.53184 48.0\n",
      "668.343845 119.0 671.9838349999999 49.0\n",
      "752.3818399999999 119.0 754.57184 49.0\n",
      "814.9098449999999 119.0 817.027835 49.0\n",
      "862.7938399999999 119.0 864.3738349999999 49.0\n",
      "945.6758399999999 119.0 947.0458399999999 49.0\n",
      "1000.8418399999999 98.0 1002.0418399999999 49.0\n",
      "1111.0138399999998 119.0 1119.37384 49.0\n",
      "1219.9038349999998 119.0 1222.033835 49.0\n",
      "1354.6678399999998 119.0 1355.8778349999998 49.0\n",
      "1462.93384 119.0 1464.9738349999998 49.0\n",
      "1536.72984 98.0 1538.28784 49.0\n",
      "1623.9478399999998 98.0 1625.9978399999998 49.0\n",
      "1703.38584 119.0 1705.6758399999999 49.0\n",
      "1765.9598399999998 98.0 1767.48984 49.0\n",
      "LFP1_vHp\n",
      "(3629218,) (3629218,) 2000\n",
      "3629218\n",
      "start_index: 0, end_index: 3629218\n",
      "notch filter applied\n",
      "normalizing data\n",
      "129.769835 98.0 135.24584 49.0\n",
      "260.73184 119.0 262.11184 49.0\n",
      "332.70984 119.0 335.26784 49.0\n",
      "405.553835 98.0 407.993835 49.0\n",
      "503.84183499999995 98.0 505.27984 49.0\n",
      "582.02184 98.0 583.53184 48.0\n",
      "668.343845 119.0 671.9838349999999 49.0\n",
      "752.3818399999999 119.0 754.57184 49.0\n",
      "814.9098449999999 119.0 817.027835 49.0\n",
      "862.7938399999999 119.0 864.3738349999999 49.0\n",
      "945.6758399999999 119.0 947.0458399999999 49.0\n",
      "1000.8418399999999 98.0 1002.0418399999999 49.0\n",
      "1111.0138399999998 119.0 1119.37384 49.0\n",
      "1219.9038349999998 119.0 1222.033835 49.0\n",
      "1354.6678399999998 119.0 1355.8778349999998 49.0\n",
      "1462.93384 119.0 1464.9738349999998 49.0\n",
      "1536.72984 98.0 1538.28784 49.0\n",
      "1623.9478399999998 98.0 1625.9978399999998 49.0\n",
      "1703.38584 119.0 1705.6758399999999 49.0\n",
      "1765.9598399999998 98.0 1767.48984 49.0\n",
      "LFP2_AON\n",
      "(3629218,) (3629218,) 2000\n",
      "3629218\n",
      "start_index: 0, end_index: 3629218\n",
      "notch filter applied\n",
      "normalizing data\n",
      "129.769835 98.0 135.24584 49.0\n",
      "260.73184 119.0 262.11184 49.0\n",
      "332.70984 119.0 335.26784 49.0\n",
      "405.553835 98.0 407.993835 49.0\n",
      "503.84183499999995 98.0 505.27984 49.0\n",
      "582.02184 98.0 583.53184 48.0\n",
      "668.343845 119.0 671.9838349999999 49.0\n",
      "752.3818399999999 119.0 754.57184 49.0\n",
      "814.9098449999999 119.0 817.027835 49.0\n",
      "862.7938399999999 119.0 864.3738349999999 49.0\n",
      "945.6758399999999 119.0 947.0458399999999 49.0\n",
      "1000.8418399999999 98.0 1002.0418399999999 49.0\n",
      "1111.0138399999998 119.0 1119.37384 49.0\n",
      "1219.9038349999998 119.0 1222.033835 49.0\n",
      "1354.6678399999998 119.0 1355.8778349999998 49.0\n",
      "1462.93384 119.0 1464.9738349999998 49.0\n",
      "1536.72984 98.0 1538.28784 49.0\n",
      "1623.9478399999998 98.0 1625.9978399999998 49.0\n",
      "1703.38584 119.0 1705.6758399999999 49.0\n",
      "1765.9598399999998 98.0 1767.48984 49.0\n",
      "LFP2_vHp\n",
      "(3629217,) (3629217,) 2000\n",
      "3629218\n",
      "start_index: 0, end_index: 3629217\n",
      "notch filter applied\n",
      "normalizing data\n",
      "129.769835 98.0 135.24584 49.0\n",
      "260.73184 119.0 262.11184 49.0\n",
      "332.70984 119.0 335.26784 49.0\n",
      "405.553835 98.0 407.993835 49.0\n",
      "503.84183499999995 98.0 505.27984 49.0\n",
      "582.02184 98.0 583.53184 48.0\n",
      "668.343845 119.0 671.9838349999999 49.0\n",
      "752.3818399999999 119.0 754.57184 49.0\n",
      "814.9098449999999 119.0 817.027835 49.0\n",
      "862.7938399999999 119.0 864.3738349999999 49.0\n",
      "945.6758399999999 119.0 947.0458399999999 49.0\n",
      "1000.8418399999999 98.0 1002.0418399999999 49.0\n",
      "1111.0138399999998 119.0 1119.37384 49.0\n",
      "1219.9038349999998 119.0 1222.033835 49.0\n",
      "1354.6678399999998 119.0 1355.8778349999998 49.0\n",
      "1462.93384 119.0 1464.9738349999998 49.0\n",
      "1536.72984 98.0 1538.28784 49.0\n",
      "1623.9478399999998 98.0 1625.9978399999998 49.0\n",
      "1703.38584 119.0 1705.6758399999999 49.0\n",
      "1765.9598399999998 98.0 1767.48984 49.0\n",
      "LFP3_AON\n",
      "(3629218,) (3629218,) 2000\n",
      "3629218\n",
      "start_index: 0, end_index: 3629218\n",
      "notch filter applied\n",
      "normalizing data\n",
      "129.769835 98.0 135.24584 49.0\n",
      "260.73184 119.0 262.11184 49.0\n",
      "332.70984 119.0 335.26784 49.0\n",
      "405.553835 98.0 407.993835 49.0\n",
      "503.84183499999995 98.0 505.27984 49.0\n",
      "582.02184 98.0 583.53184 48.0\n",
      "668.343845 119.0 671.9838349999999 49.0\n",
      "752.3818399999999 119.0 754.57184 49.0\n",
      "814.9098449999999 119.0 817.027835 49.0\n",
      "862.7938399999999 119.0 864.3738349999999 49.0\n",
      "945.6758399999999 119.0 947.0458399999999 49.0\n",
      "1000.8418399999999 98.0 1002.0418399999999 49.0\n",
      "1111.0138399999998 119.0 1119.37384 49.0\n",
      "1219.9038349999998 119.0 1222.033835 49.0\n",
      "1354.6678399999998 119.0 1355.8778349999998 49.0\n",
      "1462.93384 119.0 1464.9738349999998 49.0\n",
      "1536.72984 98.0 1538.28784 49.0\n",
      "1623.9478399999998 98.0 1625.9978399999998 49.0\n",
      "1703.38584 119.0 1705.6758399999999 49.0\n",
      "1765.9598399999998 98.0 1767.48984 49.0\n",
      "LFP4_AON\n",
      "(3629218,) (3629218,) 2000\n",
      "3629218\n",
      "start_index: 0, end_index: 3629218\n",
      "notch filter applied\n",
      "normalizing data\n",
      "129.769835 98.0 135.24584 49.0\n",
      "260.73184 119.0 262.11184 49.0\n",
      "332.70984 119.0 335.26784 49.0\n",
      "405.553835 98.0 407.993835 49.0\n",
      "503.84183499999995 98.0 505.27984 49.0\n",
      "582.02184 98.0 583.53184 48.0\n",
      "668.343845 119.0 671.9838349999999 49.0\n",
      "752.3818399999999 119.0 754.57184 49.0\n",
      "814.9098449999999 119.0 817.027835 49.0\n",
      "862.7938399999999 119.0 864.3738349999999 49.0\n",
      "945.6758399999999 119.0 947.0458399999999 49.0\n",
      "1000.8418399999999 98.0 1002.0418399999999 49.0\n",
      "1111.0138399999998 119.0 1119.37384 49.0\n",
      "1219.9038349999998 119.0 1222.033835 49.0\n",
      "1354.6678399999998 119.0 1355.8778349999998 49.0\n",
      "1462.93384 119.0 1464.9738349999998 49.0\n",
      "1536.72984 98.0 1538.28784 49.0\n",
      "1623.9478399999998 98.0 1625.9978399999998 49.0\n",
      "1703.38584 119.0 1705.6758399999999 49.0\n",
      "1765.9598399999998 98.0 1767.48984 49.0\n",
      "20230821 dk3 BWcontext\n",
      "['Keyboard', 'LFP1_AON', 'LFP1_vHp', 'LFP2_AON', 'LFP2_vHp', 'LFP3_AON', 'LFP4_AON', 'Ref', 'Respirat']\n",
      "Global start time: 0.0, Global end time: 1276.568675\n",
      "2553138\n",
      "start_index: 0, end_index: 2553137\n",
      "LFP1_AON\n",
      "(2553138,) (2553138,) 2000\n",
      "2553138\n",
      "start_index: 0, end_index: 2553138\n",
      "notch filter applied\n",
      "normalizing data\n",
      "139.25281999999999 119.0 142.69281999999998 48.0\n",
      "193.274815 98.0 196.02281499999998 48.0\n",
      "343.56482 119.0 347.95482 49.0\n",
      "397.14482 98.0 398.85081499999995 49.0\n",
      "447.31282 98.0 449.97081999999995 49.0\n",
      "489.29681999999997 119.0 491.65682 49.0\n",
      "536.390815 119.0 537.5708199999999 49.0\n",
      "583.6128199999999 119.0 584.85082 49.0\n",
      "629.8008199999999 98.0 631.2108199999999 49.0\n",
      "669.41482 119.0 670.9748199999999 49.0\n",
      "710.7528149999999 98.0 712.28082 49.0\n",
      "769.39482 119.0 771.04482 49.0\n",
      "814.8468149999999 98.0 816.13682 49.0\n",
      "883.7988149999999 98.0 885.2788199999999 49.0\n",
      "934.6148199999999 119.0 937.0628149999999 49.0\n",
      "978.8148199999999 119.0 979.8948199999999 49.0\n",
      "1020.39082 98.0 1022.920815 49.0\n",
      "1079.77282 119.0 1082.3728199999998 48.0\n",
      "1171.4988199999998 119.0 1173.216815 49.0\n",
      "1219.5268199999998 98.0 1221.4068149999998 49.0\n",
      "LFP1_vHp\n",
      "(2553138,) (2553138,) 2000\n",
      "2553138\n",
      "start_index: 0, end_index: 2553138\n",
      "notch filter applied\n",
      "normalizing data\n",
      "139.25281999999999 119.0 142.69281999999998 48.0\n",
      "193.274815 98.0 196.02281499999998 48.0\n",
      "343.56482 119.0 347.95482 49.0\n",
      "397.14482 98.0 398.85081499999995 49.0\n",
      "447.31282 98.0 449.97081999999995 49.0\n",
      "489.29681999999997 119.0 491.65682 49.0\n",
      "536.390815 119.0 537.5708199999999 49.0\n",
      "583.6128199999999 119.0 584.85082 49.0\n",
      "629.8008199999999 98.0 631.2108199999999 49.0\n",
      "669.41482 119.0 670.9748199999999 49.0\n",
      "710.7528149999999 98.0 712.28082 49.0\n",
      "769.39482 119.0 771.04482 49.0\n",
      "814.8468149999999 98.0 816.13682 49.0\n",
      "883.7988149999999 98.0 885.2788199999999 49.0\n",
      "934.6148199999999 119.0 937.0628149999999 49.0\n",
      "978.8148199999999 119.0 979.8948199999999 49.0\n",
      "1020.39082 98.0 1022.920815 49.0\n",
      "1079.77282 119.0 1082.3728199999998 48.0\n",
      "1171.4988199999998 119.0 1173.216815 49.0\n",
      "1219.5268199999998 98.0 1221.4068149999998 49.0\n",
      "LFP2_AON\n",
      "(2553138,) (2553138,) 2000\n",
      "2553138\n",
      "start_index: 0, end_index: 2553138\n",
      "notch filter applied\n",
      "normalizing data\n",
      "139.25281999999999 119.0 142.69281999999998 48.0\n",
      "193.274815 98.0 196.02281499999998 48.0\n",
      "343.56482 119.0 347.95482 49.0\n",
      "397.14482 98.0 398.85081499999995 49.0\n",
      "447.31282 98.0 449.97081999999995 49.0\n",
      "489.29681999999997 119.0 491.65682 49.0\n",
      "536.390815 119.0 537.5708199999999 49.0\n",
      "583.6128199999999 119.0 584.85082 49.0\n",
      "629.8008199999999 98.0 631.2108199999999 49.0\n",
      "669.41482 119.0 670.9748199999999 49.0\n",
      "710.7528149999999 98.0 712.28082 49.0\n",
      "769.39482 119.0 771.04482 49.0\n",
      "814.8468149999999 98.0 816.13682 49.0\n",
      "883.7988149999999 98.0 885.2788199999999 49.0\n",
      "934.6148199999999 119.0 937.0628149999999 49.0\n",
      "978.8148199999999 119.0 979.8948199999999 49.0\n",
      "1020.39082 98.0 1022.920815 49.0\n",
      "1079.77282 119.0 1082.3728199999998 48.0\n",
      "1171.4988199999998 119.0 1173.216815 49.0\n",
      "1219.5268199999998 98.0 1221.4068149999998 49.0\n",
      "LFP2_vHp\n",
      "(2553137,) (2553137,) 2000\n",
      "2553138\n",
      "start_index: 0, end_index: 2553137\n",
      "notch filter applied\n",
      "normalizing data\n",
      "139.25281999999999 119.0 142.69281999999998 48.0\n",
      "193.274815 98.0 196.02281499999998 48.0\n",
      "343.56482 119.0 347.95482 49.0\n",
      "397.14482 98.0 398.85081499999995 49.0\n",
      "447.31282 98.0 449.97081999999995 49.0\n",
      "489.29681999999997 119.0 491.65682 49.0\n",
      "536.390815 119.0 537.5708199999999 49.0\n",
      "583.6128199999999 119.0 584.85082 49.0\n",
      "629.8008199999999 98.0 631.2108199999999 49.0\n",
      "669.41482 119.0 670.9748199999999 49.0\n",
      "710.7528149999999 98.0 712.28082 49.0\n",
      "769.39482 119.0 771.04482 49.0\n",
      "814.8468149999999 98.0 816.13682 49.0\n",
      "883.7988149999999 98.0 885.2788199999999 49.0\n",
      "934.6148199999999 119.0 937.0628149999999 49.0\n",
      "978.8148199999999 119.0 979.8948199999999 49.0\n",
      "1020.39082 98.0 1022.920815 49.0\n",
      "1079.77282 119.0 1082.3728199999998 48.0\n",
      "1171.4988199999998 119.0 1173.216815 49.0\n",
      "1219.5268199999998 98.0 1221.4068149999998 49.0\n",
      "LFP3_AON\n",
      "(2553138,) (2553138,) 2000\n",
      "2553138\n",
      "start_index: 0, end_index: 2553138\n",
      "notch filter applied\n",
      "normalizing data\n",
      "139.25281999999999 119.0 142.69281999999998 48.0\n",
      "193.274815 98.0 196.02281499999998 48.0\n",
      "343.56482 119.0 347.95482 49.0\n",
      "397.14482 98.0 398.85081499999995 49.0\n",
      "447.31282 98.0 449.97081999999995 49.0\n",
      "489.29681999999997 119.0 491.65682 49.0\n",
      "536.390815 119.0 537.5708199999999 49.0\n",
      "583.6128199999999 119.0 584.85082 49.0\n",
      "629.8008199999999 98.0 631.2108199999999 49.0\n",
      "669.41482 119.0 670.9748199999999 49.0\n",
      "710.7528149999999 98.0 712.28082 49.0\n",
      "769.39482 119.0 771.04482 49.0\n",
      "814.8468149999999 98.0 816.13682 49.0\n",
      "883.7988149999999 98.0 885.2788199999999 49.0\n",
      "934.6148199999999 119.0 937.0628149999999 49.0\n",
      "978.8148199999999 119.0 979.8948199999999 49.0\n",
      "1020.39082 98.0 1022.920815 49.0\n",
      "1079.77282 119.0 1082.3728199999998 48.0\n",
      "1171.4988199999998 119.0 1173.216815 49.0\n",
      "1219.5268199999998 98.0 1221.4068149999998 49.0\n",
      "LFP4_AON\n",
      "(2553138,) (2553138,) 2000\n",
      "2553138\n",
      "start_index: 0, end_index: 2553138\n",
      "notch filter applied\n",
      "normalizing data\n",
      "139.25281999999999 119.0 142.69281999999998 48.0\n",
      "193.274815 98.0 196.02281499999998 48.0\n",
      "343.56482 119.0 347.95482 49.0\n",
      "397.14482 98.0 398.85081499999995 49.0\n",
      "447.31282 98.0 449.97081999999995 49.0\n",
      "489.29681999999997 119.0 491.65682 49.0\n",
      "536.390815 119.0 537.5708199999999 49.0\n",
      "583.6128199999999 119.0 584.85082 49.0\n",
      "629.8008199999999 98.0 631.2108199999999 49.0\n",
      "669.41482 119.0 670.9748199999999 49.0\n",
      "710.7528149999999 98.0 712.28082 49.0\n",
      "769.39482 119.0 771.04482 49.0\n",
      "814.8468149999999 98.0 816.13682 49.0\n",
      "883.7988149999999 98.0 885.2788199999999 49.0\n",
      "934.6148199999999 119.0 937.0628149999999 49.0\n",
      "978.8148199999999 119.0 979.8948199999999 49.0\n",
      "1020.39082 98.0 1022.920815 49.0\n",
      "1079.77282 119.0 1082.3728199999998 48.0\n",
      "1171.4988199999998 119.0 1173.216815 49.0\n",
      "1219.5268199999998 98.0 1221.4068149999998 49.0\n",
      "20230821 dk5 BWcontext\n",
      "['Keyboard', 'LFP1_AON', 'LFP1_vHp', 'LFP2_AON', 'LFP2_vHp', 'LFP3_AON', 'LFP4_AON', 'Ref', 'Respirat']\n",
      "Global start time: 0.0, Global end time: 1273.110375\n",
      "2546221\n",
      "start_index: 0, end_index: 2546221\n",
      "LFP1_AON\n",
      "(2546221,) (2546221,) 2000\n",
      "2546221\n",
      "start_index: 0, end_index: 2546221\n",
      "notch filter applied\n",
      "normalizing data\n",
      "61.460719999999995 119.0 63.80071999999999 49.0\n",
      "104.870725 98.0 106.98071999999999 49.0\n",
      "149.83272 119.0 150.77071999999998 49.0\n",
      "211.66671999999997 98.0 213.34671999999998 49.0\n",
      "255.01071999999996 119.0 255.89072 49.0\n",
      "300.158715 119.0 301.588715 48.0\n",
      "337.62471999999997 98.0 339.72472 49.0\n",
      "396.86671499999994 98.0 398.89471999999995 49.0\n",
      "483.32872 119.0 484.72871499999997 49.0\n",
      "580.7167199999999 98.0 582.81472 49.0\n",
      "697.0387149999999 119.0 699.19872 49.0\n",
      "738.4167199999999 98.0 742.27672 49.0\n",
      "797.7247199999999 119.0 799.1547199999999 49.0\n",
      "842.8847199999999 98.0 844.5947199999999 49.0\n",
      "888.6207199999999 98.0 891.1007149999999 49.0\n",
      "936.2487199999999 98.0 938.3087149999999 49.0\n",
      "981.9687149999999 98.0 983.5787149999999 49.0\n",
      "1035.212715 119.0 1036.3827199999998 49.0\n",
      "1086.43472 119.0 1089.0907149999998 49.0\n",
      "1137.58872 119.0 1139.2387199999998 49.0\n",
      "1187.49472 98.0 1190.94472 49.0\n",
      "1230.5627149999998 98.0 1232.162715 49.0\n",
      "LFP1_vHp\n",
      "(2546221,) (2546221,) 2000\n",
      "2546221\n",
      "start_index: 0, end_index: 2546221\n",
      "notch filter applied\n",
      "normalizing data\n",
      "61.460719999999995 119.0 63.80071999999999 49.0\n",
      "104.870725 98.0 106.98071999999999 49.0\n",
      "149.83272 119.0 150.77071999999998 49.0\n",
      "211.66671999999997 98.0 213.34671999999998 49.0\n",
      "255.01071999999996 119.0 255.89072 49.0\n",
      "300.158715 119.0 301.588715 48.0\n",
      "337.62471999999997 98.0 339.72472 49.0\n",
      "396.86671499999994 98.0 398.89471999999995 49.0\n",
      "483.32872 119.0 484.72871499999997 49.0\n",
      "580.7167199999999 98.0 582.81472 49.0\n",
      "697.0387149999999 119.0 699.19872 49.0\n",
      "738.4167199999999 98.0 742.27672 49.0\n",
      "797.7247199999999 119.0 799.1547199999999 49.0\n",
      "842.8847199999999 98.0 844.5947199999999 49.0\n",
      "888.6207199999999 98.0 891.1007149999999 49.0\n",
      "936.2487199999999 98.0 938.3087149999999 49.0\n",
      "981.9687149999999 98.0 983.5787149999999 49.0\n",
      "1035.212715 119.0 1036.3827199999998 49.0\n",
      "1086.43472 119.0 1089.0907149999998 49.0\n",
      "1137.58872 119.0 1139.2387199999998 49.0\n",
      "1187.49472 98.0 1190.94472 49.0\n",
      "1230.5627149999998 98.0 1232.162715 49.0\n",
      "LFP2_AON\n",
      "(2546221,) (2546221,) 2000\n",
      "2546221\n",
      "start_index: 0, end_index: 2546221\n",
      "notch filter applied\n",
      "normalizing data\n",
      "61.460719999999995 119.0 63.80071999999999 49.0\n",
      "104.870725 98.0 106.98071999999999 49.0\n",
      "149.83272 119.0 150.77071999999998 49.0\n",
      "211.66671999999997 98.0 213.34671999999998 49.0\n",
      "255.01071999999996 119.0 255.89072 49.0\n",
      "300.158715 119.0 301.588715 48.0\n",
      "337.62471999999997 98.0 339.72472 49.0\n",
      "396.86671499999994 98.0 398.89471999999995 49.0\n",
      "483.32872 119.0 484.72871499999997 49.0\n",
      "580.7167199999999 98.0 582.81472 49.0\n",
      "697.0387149999999 119.0 699.19872 49.0\n",
      "738.4167199999999 98.0 742.27672 49.0\n",
      "797.7247199999999 119.0 799.1547199999999 49.0\n",
      "842.8847199999999 98.0 844.5947199999999 49.0\n",
      "888.6207199999999 98.0 891.1007149999999 49.0\n",
      "936.2487199999999 98.0 938.3087149999999 49.0\n",
      "981.9687149999999 98.0 983.5787149999999 49.0\n",
      "1035.212715 119.0 1036.3827199999998 49.0\n",
      "1086.43472 119.0 1089.0907149999998 49.0\n",
      "1137.58872 119.0 1139.2387199999998 49.0\n",
      "1187.49472 98.0 1190.94472 49.0\n",
      "1230.5627149999998 98.0 1232.162715 49.0\n",
      "LFP2_vHp\n",
      "(2546221,) (2546221,) 2000\n",
      "2546221\n",
      "start_index: 0, end_index: 2546221\n",
      "notch filter applied\n",
      "normalizing data\n",
      "61.460719999999995 119.0 63.80071999999999 49.0\n",
      "104.870725 98.0 106.98071999999999 49.0\n",
      "149.83272 119.0 150.77071999999998 49.0\n",
      "211.66671999999997 98.0 213.34671999999998 49.0\n",
      "255.01071999999996 119.0 255.89072 49.0\n",
      "300.158715 119.0 301.588715 48.0\n",
      "337.62471999999997 98.0 339.72472 49.0\n",
      "396.86671499999994 98.0 398.89471999999995 49.0\n",
      "483.32872 119.0 484.72871499999997 49.0\n",
      "580.7167199999999 98.0 582.81472 49.0\n",
      "697.0387149999999 119.0 699.19872 49.0\n",
      "738.4167199999999 98.0 742.27672 49.0\n",
      "797.7247199999999 119.0 799.1547199999999 49.0\n",
      "842.8847199999999 98.0 844.5947199999999 49.0\n",
      "888.6207199999999 98.0 891.1007149999999 49.0\n",
      "936.2487199999999 98.0 938.3087149999999 49.0\n",
      "981.9687149999999 98.0 983.5787149999999 49.0\n",
      "1035.212715 119.0 1036.3827199999998 49.0\n",
      "1086.43472 119.0 1089.0907149999998 49.0\n",
      "1137.58872 119.0 1139.2387199999998 49.0\n",
      "1187.49472 98.0 1190.94472 49.0\n",
      "1230.5627149999998 98.0 1232.162715 49.0\n",
      "LFP3_AON\n",
      "(2546221,) (2546221,) 2000\n",
      "2546221\n",
      "start_index: 0, end_index: 2546221\n",
      "notch filter applied\n",
      "normalizing data\n",
      "61.460719999999995 119.0 63.80071999999999 49.0\n",
      "104.870725 98.0 106.98071999999999 49.0\n",
      "149.83272 119.0 150.77071999999998 49.0\n",
      "211.66671999999997 98.0 213.34671999999998 49.0\n",
      "255.01071999999996 119.0 255.89072 49.0\n",
      "300.158715 119.0 301.588715 48.0\n",
      "337.62471999999997 98.0 339.72472 49.0\n",
      "396.86671499999994 98.0 398.89471999999995 49.0\n",
      "483.32872 119.0 484.72871499999997 49.0\n",
      "580.7167199999999 98.0 582.81472 49.0\n",
      "697.0387149999999 119.0 699.19872 49.0\n",
      "738.4167199999999 98.0 742.27672 49.0\n",
      "797.7247199999999 119.0 799.1547199999999 49.0\n",
      "842.8847199999999 98.0 844.5947199999999 49.0\n",
      "888.6207199999999 98.0 891.1007149999999 49.0\n",
      "936.2487199999999 98.0 938.3087149999999 49.0\n",
      "981.9687149999999 98.0 983.5787149999999 49.0\n",
      "1035.212715 119.0 1036.3827199999998 49.0\n",
      "1086.43472 119.0 1089.0907149999998 49.0\n",
      "1137.58872 119.0 1139.2387199999998 49.0\n",
      "1187.49472 98.0 1190.94472 49.0\n",
      "1230.5627149999998 98.0 1232.162715 49.0\n",
      "LFP4_AON\n",
      "(2546221,) (2546221,) 2000\n",
      "2546221\n",
      "start_index: 0, end_index: 2546221\n",
      "notch filter applied\n",
      "normalizing data\n",
      "61.460719999999995 119.0 63.80071999999999 49.0\n",
      "104.870725 98.0 106.98071999999999 49.0\n",
      "149.83272 119.0 150.77071999999998 49.0\n",
      "211.66671999999997 98.0 213.34671999999998 49.0\n",
      "255.01071999999996 119.0 255.89072 49.0\n",
      "300.158715 119.0 301.588715 48.0\n",
      "337.62471999999997 98.0 339.72472 49.0\n",
      "396.86671499999994 98.0 398.89471999999995 49.0\n",
      "483.32872 119.0 484.72871499999997 49.0\n",
      "580.7167199999999 98.0 582.81472 49.0\n",
      "697.0387149999999 119.0 699.19872 49.0\n",
      "738.4167199999999 98.0 742.27672 49.0\n",
      "797.7247199999999 119.0 799.1547199999999 49.0\n",
      "842.8847199999999 98.0 844.5947199999999 49.0\n",
      "888.6207199999999 98.0 891.1007149999999 49.0\n",
      "936.2487199999999 98.0 938.3087149999999 49.0\n",
      "981.9687149999999 98.0 983.5787149999999 49.0\n",
      "1035.212715 119.0 1036.3827199999998 49.0\n",
      "1086.43472 119.0 1089.0907149999998 49.0\n",
      "1137.58872 119.0 1139.2387199999998 49.0\n",
      "1187.49472 98.0 1190.94472 49.0\n",
      "1230.5627149999998 98.0 1232.162715 49.0\n",
      "20230822 dk1 BWnocontext\n",
      "['Keyboard', 'LFP1_vHp', 'LFP2_vHp', 'LFP3_AON', 'LFP4_AON', 'Ref', 'Respirat']\n",
      "Global start time: 0.0, Global end time: 1463.4690249999999\n",
      "2926939\n",
      "start_index: 0, end_index: 2926938\n",
      "LFP1_vHp\n",
      "(2926938,) (2926938,) 2000\n",
      "2926939\n",
      "start_index: 0, end_index: 2926938\n",
      "notch filter applied\n",
      "normalizing data\n",
      "75.99422 119.0 83.10021499999999 49.0\n",
      "157.452215 98.0 160.02221999999998 49.0\n",
      "252.23021999999997 98.0 254.01821999999999 49.0\n",
      "316.56622 98.0 318.95421 49.0\n",
      "368.06221999999997 119.0 369.65021499999995 49.0\n",
      "412.45021999999994 119.0 414.70021499999996 49.0\n",
      "510.42621499999996 98.0 512.9562149999999 49.0\n",
      "565.006215 98.0 567.3262149999999 49.0\n",
      "612.9802149999999 119.0 616.66022 49.0\n",
      "670.344215 98.0 673.1922149999999 49.0\n",
      "764.3802199999999 119.0 767.5002149999999 49.0\n",
      "828.940215 119.0 830.19022 49.0\n",
      "905.0422199999999 98.0 907.3982199999999 49.0\n",
      "1017.7502199999999 98.0 1019.2202199999999 49.0\n",
      "1094.7002149999998 119.0 1098.02622 49.0\n",
      "1155.696215 98.0 1157.494215 49.0\n",
      "1210.6162149999998 98.0 1212.2762149999999 49.0\n",
      "1266.0282149999998 119.0 1270.0842149999999 49.0\n",
      "1316.880215 98.0 1318.6702149999999 49.0\n",
      "1412.6922149999998 119.0 1415.6902149999999 49.0\n",
      "LFP2_vHp\n",
      "(2926938,) (2926938,) 2000\n",
      "2926939\n",
      "start_index: 0, end_index: 2926938\n",
      "notch filter applied\n",
      "normalizing data\n",
      "75.99422 119.0 83.10021499999999 49.0\n",
      "157.452215 98.0 160.02221999999998 49.0\n",
      "252.23021999999997 98.0 254.01821999999999 49.0\n",
      "316.56622 98.0 318.95421 49.0\n",
      "368.06221999999997 119.0 369.65021499999995 49.0\n",
      "412.45021999999994 119.0 414.70021499999996 49.0\n",
      "510.42621499999996 98.0 512.9562149999999 49.0\n",
      "565.006215 98.0 567.3262149999999 49.0\n",
      "612.9802149999999 119.0 616.66022 49.0\n",
      "670.344215 98.0 673.1922149999999 49.0\n",
      "764.3802199999999 119.0 767.5002149999999 49.0\n",
      "828.940215 119.0 830.19022 49.0\n",
      "905.0422199999999 98.0 907.3982199999999 49.0\n",
      "1017.7502199999999 98.0 1019.2202199999999 49.0\n",
      "1094.7002149999998 119.0 1098.02622 49.0\n",
      "1155.696215 98.0 1157.494215 49.0\n",
      "1210.6162149999998 98.0 1212.2762149999999 49.0\n",
      "1266.0282149999998 119.0 1270.0842149999999 49.0\n",
      "1316.880215 98.0 1318.6702149999999 49.0\n",
      "1412.6922149999998 119.0 1415.6902149999999 49.0\n",
      "LFP3_AON\n",
      "(2926939,) (2926939,) 2000\n",
      "2926939\n",
      "start_index: 0, end_index: 2926939\n",
      "notch filter applied\n",
      "normalizing data\n",
      "75.99422 119.0 83.10021499999999 49.0\n",
      "157.452215 98.0 160.02221999999998 49.0\n",
      "252.23021999999997 98.0 254.01821999999999 49.0\n",
      "316.56622 98.0 318.95421 49.0\n",
      "368.06221999999997 119.0 369.65021499999995 49.0\n",
      "412.45021999999994 119.0 414.70021499999996 49.0\n",
      "510.42621499999996 98.0 512.9562149999999 49.0\n",
      "565.006215 98.0 567.3262149999999 49.0\n",
      "612.9802149999999 119.0 616.66022 49.0\n",
      "670.344215 98.0 673.1922149999999 49.0\n",
      "764.3802199999999 119.0 767.5002149999999 49.0\n",
      "828.940215 119.0 830.19022 49.0\n",
      "905.0422199999999 98.0 907.3982199999999 49.0\n",
      "1017.7502199999999 98.0 1019.2202199999999 49.0\n",
      "1094.7002149999998 119.0 1098.02622 49.0\n",
      "1155.696215 98.0 1157.494215 49.0\n",
      "1210.6162149999998 98.0 1212.2762149999999 49.0\n",
      "1266.0282149999998 119.0 1270.0842149999999 49.0\n",
      "1316.880215 98.0 1318.6702149999999 49.0\n",
      "1412.6922149999998 119.0 1415.6902149999999 49.0\n",
      "LFP4_AON\n",
      "(2926939,) (2926939,) 2000\n",
      "2926939\n",
      "start_index: 0, end_index: 2926939\n",
      "notch filter applied\n",
      "normalizing data\n",
      "75.99422 119.0 83.10021499999999 49.0\n",
      "157.452215 98.0 160.02221999999998 49.0\n",
      "252.23021999999997 98.0 254.01821999999999 49.0\n",
      "316.56622 98.0 318.95421 49.0\n",
      "368.06221999999997 119.0 369.65021499999995 49.0\n",
      "412.45021999999994 119.0 414.70021499999996 49.0\n",
      "510.42621499999996 98.0 512.9562149999999 49.0\n",
      "565.006215 98.0 567.3262149999999 49.0\n",
      "612.9802149999999 119.0 616.66022 49.0\n",
      "670.344215 98.0 673.1922149999999 49.0\n",
      "764.3802199999999 119.0 767.5002149999999 49.0\n",
      "828.940215 119.0 830.19022 49.0\n",
      "905.0422199999999 98.0 907.3982199999999 49.0\n",
      "1017.7502199999999 98.0 1019.2202199999999 49.0\n",
      "1094.7002149999998 119.0 1098.02622 49.0\n",
      "1155.696215 98.0 1157.494215 49.0\n",
      "1210.6162149999998 98.0 1212.2762149999999 49.0\n",
      "1266.0282149999998 119.0 1270.0842149999999 49.0\n",
      "1316.880215 98.0 1318.6702149999999 49.0\n",
      "1412.6922149999998 119.0 1415.6902149999999 49.0\n",
      "20230823 dk1 BWnocontext\n",
      "['Keyboard', 'LFP1_AON', 'LFP1_vHp', 'LFP2_AON', 'LFP2_vHp', 'LFP3_AON', 'LFP4_AON', 'Ref', 'Respirat']\n",
      "Global start time: 0.0, Global end time: 1530.5920749999998\n",
      "3061185\n",
      "start_index: 0, end_index: 3061184\n",
      "LFP1_AON\n",
      "(3061185,) (3061185,) 2000\n",
      "3061185\n",
      "start_index: 0, end_index: 3061185\n",
      "notch filter applied\n",
      "normalizing data\n",
      "100.39765499999999 98.0 102.79764999999999 48.0\n",
      "235.61764999999997 98.0 238.59765 48.0\n",
      "294.96164999999996 98.0 298.73165 49.0\n",
      "416.92564999999996 119.0 418.43364999999994 49.0\n",
      "484.30764999999997 119.0 486.67764999999997 49.0\n",
      "529.34965 119.0 533.38765 49.0\n",
      "575.05365 119.0 578.163645 49.0\n",
      "640.32565 98.0 642.03365 49.0\n",
      "690.6856499999999 98.0 692.91365 49.0\n",
      "744.1096449999999 119.0 745.5596499999999 49.0\n",
      "787.4896449999999 119.0 788.7396449999999 49.0\n",
      "834.41365 98.0 844.0936499999999 49.0\n",
      "918.35165 119.0 919.6796499999999 49.0\n",
      "968.5856499999999 98.0 970.98565 49.0\n",
      "1094.925655 98.0 1097.12565 49.0\n",
      "1164.21965 119.0 1165.8796499999999 49.0\n",
      "1271.18365 119.0 1273.093645 49.0\n",
      "1331.77165 98.0 1333.21165 49.0\n",
      "1404.77965 98.0 1407.7196499999998 49.0\n",
      "1458.8836499999998 98.0 1460.8936449999999 49.0\n",
      "LFP1_vHp\n",
      "(3061184,) (3061184,) 2000\n",
      "3061185\n",
      "start_index: 0, end_index: 3061184\n",
      "notch filter applied\n",
      "normalizing data\n",
      "100.39765499999999 98.0 102.79764999999999 48.0\n",
      "235.61764999999997 98.0 238.59765 48.0\n",
      "294.96164999999996 98.0 298.73165 49.0\n",
      "416.92564999999996 119.0 418.43364999999994 49.0\n",
      "484.30764999999997 119.0 486.67764999999997 49.0\n",
      "529.34965 119.0 533.38765 49.0\n",
      "575.05365 119.0 578.163645 49.0\n",
      "640.32565 98.0 642.03365 49.0\n",
      "690.6856499999999 98.0 692.91365 49.0\n",
      "744.1096449999999 119.0 745.5596499999999 49.0\n",
      "787.4896449999999 119.0 788.7396449999999 49.0\n",
      "834.41365 98.0 844.0936499999999 49.0\n",
      "918.35165 119.0 919.6796499999999 49.0\n",
      "968.5856499999999 98.0 970.98565 49.0\n",
      "1094.925655 98.0 1097.12565 49.0\n",
      "1164.21965 119.0 1165.8796499999999 49.0\n",
      "1271.18365 119.0 1273.093645 49.0\n",
      "1331.77165 98.0 1333.21165 49.0\n",
      "1404.77965 98.0 1407.7196499999998 49.0\n",
      "1458.8836499999998 98.0 1460.8936449999999 49.0\n",
      "LFP2_AON\n",
      "(3061185,) (3061185,) 2000\n",
      "3061185\n",
      "start_index: 0, end_index: 3061185\n",
      "notch filter applied\n",
      "normalizing data\n",
      "100.39765499999999 98.0 102.79764999999999 48.0\n",
      "235.61764999999997 98.0 238.59765 48.0\n",
      "294.96164999999996 98.0 298.73165 49.0\n",
      "416.92564999999996 119.0 418.43364999999994 49.0\n",
      "484.30764999999997 119.0 486.67764999999997 49.0\n",
      "529.34965 119.0 533.38765 49.0\n",
      "575.05365 119.0 578.163645 49.0\n",
      "640.32565 98.0 642.03365 49.0\n",
      "690.6856499999999 98.0 692.91365 49.0\n",
      "744.1096449999999 119.0 745.5596499999999 49.0\n",
      "787.4896449999999 119.0 788.7396449999999 49.0\n",
      "834.41365 98.0 844.0936499999999 49.0\n",
      "918.35165 119.0 919.6796499999999 49.0\n",
      "968.5856499999999 98.0 970.98565 49.0\n",
      "1094.925655 98.0 1097.12565 49.0\n",
      "1164.21965 119.0 1165.8796499999999 49.0\n",
      "1271.18365 119.0 1273.093645 49.0\n",
      "1331.77165 98.0 1333.21165 49.0\n",
      "1404.77965 98.0 1407.7196499999998 49.0\n",
      "1458.8836499999998 98.0 1460.8936449999999 49.0\n",
      "LFP2_vHp\n",
      "(3061184,) (3061184,) 2000\n",
      "3061185\n",
      "start_index: 0, end_index: 3061184\n",
      "notch filter applied\n",
      "normalizing data\n",
      "100.39765499999999 98.0 102.79764999999999 48.0\n",
      "235.61764999999997 98.0 238.59765 48.0\n",
      "294.96164999999996 98.0 298.73165 49.0\n",
      "416.92564999999996 119.0 418.43364999999994 49.0\n",
      "484.30764999999997 119.0 486.67764999999997 49.0\n",
      "529.34965 119.0 533.38765 49.0\n",
      "575.05365 119.0 578.163645 49.0\n",
      "640.32565 98.0 642.03365 49.0\n",
      "690.6856499999999 98.0 692.91365 49.0\n",
      "744.1096449999999 119.0 745.5596499999999 49.0\n",
      "787.4896449999999 119.0 788.7396449999999 49.0\n",
      "834.41365 98.0 844.0936499999999 49.0\n",
      "918.35165 119.0 919.6796499999999 49.0\n",
      "968.5856499999999 98.0 970.98565 49.0\n",
      "1094.925655 98.0 1097.12565 49.0\n",
      "1164.21965 119.0 1165.8796499999999 49.0\n",
      "1271.18365 119.0 1273.093645 49.0\n",
      "1331.77165 98.0 1333.21165 49.0\n",
      "1404.77965 98.0 1407.7196499999998 49.0\n",
      "1458.8836499999998 98.0 1460.8936449999999 49.0\n",
      "LFP3_AON\n",
      "(3061185,) (3061185,) 2000\n",
      "3061185\n",
      "start_index: 0, end_index: 3061185\n",
      "notch filter applied\n",
      "normalizing data\n",
      "100.39765499999999 98.0 102.79764999999999 48.0\n",
      "235.61764999999997 98.0 238.59765 48.0\n",
      "294.96164999999996 98.0 298.73165 49.0\n",
      "416.92564999999996 119.0 418.43364999999994 49.0\n",
      "484.30764999999997 119.0 486.67764999999997 49.0\n",
      "529.34965 119.0 533.38765 49.0\n",
      "575.05365 119.0 578.163645 49.0\n",
      "640.32565 98.0 642.03365 49.0\n",
      "690.6856499999999 98.0 692.91365 49.0\n",
      "744.1096449999999 119.0 745.5596499999999 49.0\n",
      "787.4896449999999 119.0 788.7396449999999 49.0\n",
      "834.41365 98.0 844.0936499999999 49.0\n",
      "918.35165 119.0 919.6796499999999 49.0\n",
      "968.5856499999999 98.0 970.98565 49.0\n",
      "1094.925655 98.0 1097.12565 49.0\n",
      "1164.21965 119.0 1165.8796499999999 49.0\n",
      "1271.18365 119.0 1273.093645 49.0\n",
      "1331.77165 98.0 1333.21165 49.0\n",
      "1404.77965 98.0 1407.7196499999998 49.0\n",
      "1458.8836499999998 98.0 1460.8936449999999 49.0\n",
      "LFP4_AON\n",
      "(3061185,) (3061185,) 2000\n",
      "3061185\n",
      "start_index: 0, end_index: 3061185\n",
      "notch filter applied\n",
      "normalizing data\n",
      "100.39765499999999 98.0 102.79764999999999 48.0\n",
      "235.61764999999997 98.0 238.59765 48.0\n",
      "294.96164999999996 98.0 298.73165 49.0\n",
      "416.92564999999996 119.0 418.43364999999994 49.0\n",
      "484.30764999999997 119.0 486.67764999999997 49.0\n",
      "529.34965 119.0 533.38765 49.0\n",
      "575.05365 119.0 578.163645 49.0\n",
      "640.32565 98.0 642.03365 49.0\n",
      "690.6856499999999 98.0 692.91365 49.0\n",
      "744.1096449999999 119.0 745.5596499999999 49.0\n",
      "787.4896449999999 119.0 788.7396449999999 49.0\n",
      "834.41365 98.0 844.0936499999999 49.0\n",
      "918.35165 119.0 919.6796499999999 49.0\n",
      "968.5856499999999 98.0 970.98565 49.0\n",
      "1094.925655 98.0 1097.12565 49.0\n",
      "1164.21965 119.0 1165.8796499999999 49.0\n",
      "1271.18365 119.0 1273.093645 49.0\n",
      "1331.77165 98.0 1333.21165 49.0\n",
      "1404.77965 98.0 1407.7196499999998 49.0\n",
      "1458.8836499999998 98.0 1460.8936449999999 49.0\n",
      "20230823 dk5 BWcontext\n",
      "['Keyboard', 'LFP1_AON', 'LFP1_vHp', 'LFP2_AON', 'LFP2_vHp', 'LFP3_AON', 'LFP4_AON', 'Ref', 'Respirat']\n",
      "Global start time: 0.0, Global end time: 1212.287875\n",
      "2424576\n",
      "start_index: 0, end_index: 2424576\n",
      "LFP1_AON\n",
      "(2424576,) (2424576,) 2000\n",
      "2424576\n",
      "start_index: 0, end_index: 2424576\n",
      "notch filter applied\n",
      "normalizing data\n",
      "172.962945 98.0 174.57294499999998 49.0\n",
      "256.06294499999996 98.0 258.342945 49.0\n",
      "309.29695499999997 119.0 310.69694499999997 49.0\n",
      "358.926945 119.0 361.76494499999995 49.0\n",
      "410.69494499999996 119.0 412.64494499999995 49.0\n",
      "456.59494499999994 119.0 457.75494499999996 49.0\n",
      "496.81693999999993 98.0 498.61694499999993 49.0\n",
      "544.3989449999999 119.0 544.8789499999999 49.0\n",
      "619.540945 98.0 620.810945 49.0\n",
      "658.616945 98.0 660.74695 49.0\n",
      "699.8609449999999 98.0 701.160945 49.0\n",
      "739.4569499999999 98.0 740.536945 49.0\n",
      "788.0949449999999 119.0 789.98295 49.0\n",
      "890.9909499999999 119.0 892.2709449999999 49.0\n",
      "940.4149449999999 98.0 941.444945 49.0\n",
      "983.1329499999999 119.0 985.0729449999999 49.0\n",
      "1037.9409449999998 98.0 1039.8809449999999 49.0\n",
      "1085.9009449999999 119.0 1087.030945 49.0\n",
      "1129.8409399999998 119.0 1132.6109399999998 49.0\n",
      "1175.476945 119.0 1177.00695 49.0\n",
      "LFP1_vHp\n",
      "(2424576,) (2424576,) 2000\n",
      "2424576\n",
      "start_index: 0, end_index: 2424576\n",
      "notch filter applied\n",
      "normalizing data\n",
      "172.962945 98.0 174.57294499999998 49.0\n",
      "256.06294499999996 98.0 258.342945 49.0\n",
      "309.29695499999997 119.0 310.69694499999997 49.0\n",
      "358.926945 119.0 361.76494499999995 49.0\n",
      "410.69494499999996 119.0 412.64494499999995 49.0\n",
      "456.59494499999994 119.0 457.75494499999996 49.0\n",
      "496.81693999999993 98.0 498.61694499999993 49.0\n",
      "544.3989449999999 119.0 544.8789499999999 49.0\n",
      "619.540945 98.0 620.810945 49.0\n",
      "658.616945 98.0 660.74695 49.0\n",
      "699.8609449999999 98.0 701.160945 49.0\n",
      "739.4569499999999 98.0 740.536945 49.0\n",
      "788.0949449999999 119.0 789.98295 49.0\n",
      "890.9909499999999 119.0 892.2709449999999 49.0\n",
      "940.4149449999999 98.0 941.444945 49.0\n",
      "983.1329499999999 119.0 985.0729449999999 49.0\n",
      "1037.9409449999998 98.0 1039.8809449999999 49.0\n",
      "1085.9009449999999 119.0 1087.030945 49.0\n",
      "1129.8409399999998 119.0 1132.6109399999998 49.0\n",
      "1175.476945 119.0 1177.00695 49.0\n",
      "LFP2_AON\n",
      "(2424576,) (2424576,) 2000\n",
      "2424576\n",
      "start_index: 0, end_index: 2424576\n",
      "notch filter applied\n",
      "normalizing data\n",
      "172.962945 98.0 174.57294499999998 49.0\n",
      "256.06294499999996 98.0 258.342945 49.0\n",
      "309.29695499999997 119.0 310.69694499999997 49.0\n",
      "358.926945 119.0 361.76494499999995 49.0\n",
      "410.69494499999996 119.0 412.64494499999995 49.0\n",
      "456.59494499999994 119.0 457.75494499999996 49.0\n",
      "496.81693999999993 98.0 498.61694499999993 49.0\n",
      "544.3989449999999 119.0 544.8789499999999 49.0\n",
      "619.540945 98.0 620.810945 49.0\n",
      "658.616945 98.0 660.74695 49.0\n",
      "699.8609449999999 98.0 701.160945 49.0\n",
      "739.4569499999999 98.0 740.536945 49.0\n",
      "788.0949449999999 119.0 789.98295 49.0\n",
      "890.9909499999999 119.0 892.2709449999999 49.0\n",
      "940.4149449999999 98.0 941.444945 49.0\n",
      "983.1329499999999 119.0 985.0729449999999 49.0\n",
      "1037.9409449999998 98.0 1039.8809449999999 49.0\n",
      "1085.9009449999999 119.0 1087.030945 49.0\n",
      "1129.8409399999998 119.0 1132.6109399999998 49.0\n",
      "1175.476945 119.0 1177.00695 49.0\n",
      "LFP2_vHp\n",
      "(2424576,) (2424576,) 2000\n",
      "2424576\n",
      "start_index: 0, end_index: 2424576\n",
      "notch filter applied\n",
      "normalizing data\n",
      "172.962945 98.0 174.57294499999998 49.0\n",
      "256.06294499999996 98.0 258.342945 49.0\n",
      "309.29695499999997 119.0 310.69694499999997 49.0\n",
      "358.926945 119.0 361.76494499999995 49.0\n",
      "410.69494499999996 119.0 412.64494499999995 49.0\n",
      "456.59494499999994 119.0 457.75494499999996 49.0\n",
      "496.81693999999993 98.0 498.61694499999993 49.0\n",
      "544.3989449999999 119.0 544.8789499999999 49.0\n",
      "619.540945 98.0 620.810945 49.0\n",
      "658.616945 98.0 660.74695 49.0\n",
      "699.8609449999999 98.0 701.160945 49.0\n",
      "739.4569499999999 98.0 740.536945 49.0\n",
      "788.0949449999999 119.0 789.98295 49.0\n",
      "890.9909499999999 119.0 892.2709449999999 49.0\n",
      "940.4149449999999 98.0 941.444945 49.0\n",
      "983.1329499999999 119.0 985.0729449999999 49.0\n",
      "1037.9409449999998 98.0 1039.8809449999999 49.0\n",
      "1085.9009449999999 119.0 1087.030945 49.0\n",
      "1129.8409399999998 119.0 1132.6109399999998 49.0\n",
      "1175.476945 119.0 1177.00695 49.0\n",
      "LFP3_AON\n",
      "(2424576,) (2424576,) 2000\n",
      "2424576\n",
      "start_index: 0, end_index: 2424576\n",
      "notch filter applied\n",
      "normalizing data\n",
      "172.962945 98.0 174.57294499999998 49.0\n",
      "256.06294499999996 98.0 258.342945 49.0\n",
      "309.29695499999997 119.0 310.69694499999997 49.0\n",
      "358.926945 119.0 361.76494499999995 49.0\n",
      "410.69494499999996 119.0 412.64494499999995 49.0\n",
      "456.59494499999994 119.0 457.75494499999996 49.0\n",
      "496.81693999999993 98.0 498.61694499999993 49.0\n",
      "544.3989449999999 119.0 544.8789499999999 49.0\n",
      "619.540945 98.0 620.810945 49.0\n",
      "658.616945 98.0 660.74695 49.0\n",
      "699.8609449999999 98.0 701.160945 49.0\n",
      "739.4569499999999 98.0 740.536945 49.0\n",
      "788.0949449999999 119.0 789.98295 49.0\n",
      "890.9909499999999 119.0 892.2709449999999 49.0\n",
      "940.4149449999999 98.0 941.444945 49.0\n",
      "983.1329499999999 119.0 985.0729449999999 49.0\n",
      "1037.9409449999998 98.0 1039.8809449999999 49.0\n",
      "1085.9009449999999 119.0 1087.030945 49.0\n",
      "1129.8409399999998 119.0 1132.6109399999998 49.0\n",
      "1175.476945 119.0 1177.00695 49.0\n",
      "LFP4_AON\n",
      "(2424576,) (2424576,) 2000\n",
      "2424576\n",
      "start_index: 0, end_index: 2424576\n",
      "notch filter applied\n",
      "normalizing data\n",
      "172.962945 98.0 174.57294499999998 49.0\n",
      "256.06294499999996 98.0 258.342945 49.0\n",
      "309.29695499999997 119.0 310.69694499999997 49.0\n",
      "358.926945 119.0 361.76494499999995 49.0\n",
      "410.69494499999996 119.0 412.64494499999995 49.0\n",
      "456.59494499999994 119.0 457.75494499999996 49.0\n",
      "496.81693999999993 98.0 498.61694499999993 49.0\n",
      "544.3989449999999 119.0 544.8789499999999 49.0\n",
      "619.540945 98.0 620.810945 49.0\n",
      "658.616945 98.0 660.74695 49.0\n",
      "699.8609449999999 98.0 701.160945 49.0\n",
      "739.4569499999999 98.0 740.536945 49.0\n",
      "788.0949449999999 119.0 789.98295 49.0\n",
      "890.9909499999999 119.0 892.2709449999999 49.0\n",
      "940.4149449999999 98.0 941.444945 49.0\n",
      "983.1329499999999 119.0 985.0729449999999 49.0\n",
      "1037.9409449999998 98.0 1039.8809449999999 49.0\n",
      "1085.9009449999999 119.0 1087.030945 49.0\n",
      "1129.8409399999998 119.0 1132.6109399999998 49.0\n",
      "1175.476945 119.0 1177.00695 49.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "keyboard_dict={'98':'b','119':'w','120':'nc','49':'1','48':'0'} #specifying the map of keyboard annotations to their meanings.\n",
    "all_bands={'total':[1,100],'beta':[12,30], 'gamma':[30,80], 'theta':[4,12]}\n",
    "importlib.reload(lfp_pre_processing_functions) #Reloading the lfp_pre_processing_functions module to ensure we have the latest version\n",
    "#files=[f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230615_dk6_BW_context_day1.mat', f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230626_dk6_BW_nocontext_day1.mat'] #This is just for testing purposes\n",
    "time_window=0.4\n",
    "fs=2000\n",
    "#Initializing a few empty things to store data\n",
    "events_codes_all = {}\n",
    "compiled_data_all_epochs = []\n",
    "compiled_data_list=[]\n",
    "compiled_shuffled_data_list = []\n",
    "baseline_lfp_all = []\n",
    "normalization_comparison_all = []\n",
    "for file in files: #Looping through data files\n",
    "    \n",
    "    ## Get the date, mouse_id and task from the file name\n",
    "    base_name = os.path.basename(file)\n",
    "    base_name, _ = os.path.splitext(base_name)\n",
    "    date, mouse_id, task=lfp_pre_processing_functions.exp_params(base_name) #Using a custom made function [see functions.py]\n",
    "    print(date, mouse_id, task)\n",
    "    if task == 'nocontextday2' or task == 'nocontextos2':\n",
    "        task = 'nocontext'\n",
    "    if task =='nocontext':\n",
    "        continue\n",
    "    f=h5py.File(file, 'r')  ## Open the data file\n",
    "    channels = list(f.keys()) ## Extract channels list from the data file\n",
    "    print(channels)\n",
    "    if not any(\"AON\" in channel or \"vHp\" in channel for channel in channels):\n",
    "        continue\n",
    "    events,reference_electrode=lfp_pre_processing_functions.get_keyboard_and_ref_channels(f,channels)\n",
    "\n",
    "    events_codes=np.array(events['codes'][0]) #saving the keyboard annotations of the events (door open, door close etc.)\n",
    "    events_times=np.array(events['times'][0]) #saving when the events happened\n",
    "    events_codes_all[base_name] = events_codes #saving the codes in a dictionary to be analyzed later for events other than the ones in our keyboard_dict map\n",
    "    \n",
    "    #Generating epochs from events (epochs are basically start of a trial and end of a trial)\n",
    "    epochs=lfp_pre_processing_functions.generate_epochs_with_first_event(events_codes, events_times)\n",
    "\n",
    "    # task Start time\n",
    "    first_event=events_times[0]\n",
    "    #finding global start and end time of all channels, since they start and end recordings at different times\n",
    "    global_start_time, global_end_time=lfp_pre_processing_functions.find_global_start_end_times(f,channels)\n",
    "    \n",
    "    ## Reference electrode finding and padding\n",
    "    reference_time = np.array(reference_electrode['times']).flatten()\n",
    "    reference_value = np.array(reference_electrode['values']).flatten()\n",
    "    padd_ref_data,padded_ref_time=lfp_pre_processing_functions.pad_raw_data_raw_time(reference_value,reference_time,global_start_time,global_end_time,sampling_rate=2000)\n",
    "\n",
    "\n",
    "    for channeli in channels:\n",
    "        if \"AON\" in channeli or  \"vHp\" in channeli :\n",
    "            \n",
    "            channel_id=channeli\n",
    "            # Extracting raw data and time\n",
    "            data_all=f[channeli]\n",
    "            raw_data=np.array(data_all['values']).flatten()\n",
    "            raw_time = np.array(data_all['times']).flatten()\n",
    "            sampling_rate = 2000\n",
    "            print(channel_id)\n",
    "            print(raw_data.shape, raw_time.shape, sampling_rate)\n",
    "            \n",
    "            padded_data,padded_time=lfp_pre_processing_functions.pad_raw_data_raw_time(raw_data,raw_time,global_start_time,global_end_time,sampling_rate)\n",
    "            subtracted_data = padded_data - padd_ref_data\n",
    "            raw_data=subtracted_data\n",
    "            notch_filtered_data = lfp_pre_processing_functions.iir_notch(raw_data, sampling_rate, 60)\n",
    "            \n",
    "            data_before, time, baseline_mean, baseline_std=lfp_pre_processing_functions.baseline_data_normalization(notch_filtered_data, raw_time, first_event, sampling_rate)\n",
    "            first_event_index=np.where(raw_time>first_event)[0][0]\n",
    "\n",
    "            baseline_row=[mouse_id,task,channel_id,np.array(data_before)]\n",
    "            baseline_lfp_all.append(baseline_row)\n",
    "            normalized_data=notch_filtered_data\n",
    "\n",
    "            #Saving non-normalized data and normalized data for plotting\n",
    "            normalization_row=[mouse_id,task,channel_id,[notch_filtered_data[first_event_index:first_event_index+30*sampling_rate]],np.mean(data_before),np.std(data_before),[normalized_data[first_event_index:first_event_index+30*sampling_rate]]]\n",
    "            normalization_comparison_all.append(normalization_row)\n",
    "\n",
    "\n",
    "            for i,epochi in enumerate(epochs):\n",
    "                \n",
    "                compiled_data = pd.DataFrame() # Initializing a dataframe to store the data of a single epoch\n",
    "                compiled_shuffled_data = pd.DataFrame() # Initializing a dataframe to store the shuffled data of a single epoch\n",
    "                door_timestamp = epochi[0][0]\n",
    "                trial_type = epochi[0][1]\n",
    "                dig_type = epochi[1, 1]\n",
    "                dig_timestamp = epochi[1, 0]\n",
    "                print(door_timestamp,trial_type,dig_timestamp,dig_type)\n",
    "                \n",
    "                \n",
    "                data_complete_trial=lfp_pre_processing_functions.extract_complete_trial_data(notch_filtered_data,time,door_timestamp,dig_timestamp,sampling_rate,time_window)\n",
    "                data_trial_before, data_trial_after=lfp_pre_processing_functions.extract_event_data(notch_filtered_data,time,door_timestamp,sampling_rate,time_window)\n",
    "                data_dig_before, data_dig_after=lfp_pre_processing_functions.extract_event_data(notch_filtered_data,time,dig_timestamp,sampling_rate,time_window)\n",
    "                data_door_around=np.append(data_trial_before, data_trial_after)\n",
    "                data_dig_around=np.append(data_dig_before, data_dig_after)\n",
    "                epoch_data = [data_complete_trial, data_trial_before, data_trial_after, data_dig_before, data_dig_after, data_door_around, data_dig_around]\n",
    "                epoch_data = [lfp_pre_processing_functions.zscore_event_data(x, baseline_std) for x in epoch_data]\n",
    "                shuffled_epoch_data = [np.random.permutation(x) for x in epoch_data]  # Shuffle the epoch data\n",
    "                compiled_data = dict(rat=mouse_id, date=date, task=task, channel=channel_id, trial=i, timestamps=[door_timestamp, dig_timestamp],\n",
    "                                     side=keyboard_dict.get(str(int(trial_type)), ''), correct=keyboard_dict.get(str(int(dig_type)), ''), time=time,\n",
    "                                     **dict(zip(['complete_trial', 'pre_door', 'post_door', 'pre_dig', 'post_dig', 'around_door', 'around_dig'], epoch_data)))\n",
    "                compiled_shuffled_data = dict(rat=mouse_id, date=date, task=task, channel=channel_id, trial=i, timestamps=[door_timestamp, dig_timestamp],\n",
    "                                     side=keyboard_dict.get(str(int(trial_type)), ''), correct=keyboard_dict.get(str(int(dig_type)), ''), time=time,\n",
    "                                     **dict(zip(['complete_trial', 'pre_door', 'post_door', 'pre_dig', 'post_dig', 'around_door', 'around_dig'], shuffled_epoch_data)))\n",
    "                compiled_data_list.append(compiled_data)\n",
    "                compiled_shuffled_data_list.append(compiled_shuffled_data)\n",
    "def combine_and_save_data(data_list, name):\n",
    "    compiled_data_all_epochs = []\n",
    "    compiled_data_all_epochs.extend(data_list)\n",
    "    compiled_data_all_epochs = pd.DataFrame(compiled_data_all_epochs)\n",
    "    compiled_data_all_epochs= compiled_data_all_epochs[compiled_data_all_epochs['task']!='nocontext']\n",
    "    compiled_data_all_epochs.to_pickle(savepath+'{}.pkl'.format(name))\n",
    "\n",
    "combine_and_save_data(compiled_data_list, f'compiled_data_all_epochs_truncated_{int(time_window*fs)}')\n",
    "combine_and_save_data(compiled_shuffled_data_list, f'compiled_shuffled_data_all_epochs_truncated_{int(time_window*fs)}')\n",
    "\n",
    "baseline_lfp_all = pd.DataFrame(baseline_lfp_all, columns=['rat', 'task', 'channel', 'data'])\n",
    "baseline_lfp_all.to_pickle(savepath+'baseline_lfp_all.pkl')\n",
    "normalization_comparison_all = pd.DataFrame(normalization_comparison_all, columns=['rat', 'task', 'channel', 'non_normalized_data', 'baseline_mean', 'baseline_std', 'normalized_data'])   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Waveform Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Rat 1-100Hz around door and digging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_data_all_epochs=pd.read_pickle(savepath+'compiled_data_all_epochs.pkl')\n",
    "waveform_data_all = compiled_data_all_epochs.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform_data_all['channel'] = waveform_data_all['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "event_dictionary = {'around_door':'Before and After door open', 'around_dig': 'Before and After Digging'}\n",
    "all_bands_dict = {'total':[1,100], 'beta':[12,30], 'gamma':[30,80], 'theta':[4,12]}\n",
    "rat_list=['dk5']\n",
    "for rat in rat_list:\n",
    "    writer=pd.ExcelWriter(os.path.join(savepath, f'{rat}_waveform_data.xlsx'), engine='xlsxwriter')\n",
    "    \n",
    "    waveform_data = waveform_data_all[waveform_data_all['rat'] == rat]\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(20, 10))\n",
    "    fig.suptitle(f'{rat} LFP (1-100Hz)', fontsize=20)\n",
    "    \n",
    "    subfigs = fig.subfigures(2, 1)\n",
    "    subfigs=subfigs.flatten()\n",
    "    for subfig in subfigs:\n",
    "        subfig.patch.set_edgecolor('black')\n",
    "        subfig.patch.set_linewidth(2)\n",
    "\n",
    "    areas=['AON','vHp']\n",
    "    for outerind, area in enumerate(areas):\n",
    "        subfig=subfigs[outerind]\n",
    "        axs = subfig.subplots(1, 2)\n",
    "        subfig.suptitle(f'{area}', fontsize=16)\n",
    "        waveform_data_area = waveform_data[waveform_data['channel'] == area]\n",
    "        waveform_data_area = waveform_data_area.reset_index(drop=True)\n",
    "\n",
    "        for innerind, col in enumerate(['around_door', 'around_dig']):\n",
    "            data = np.array(waveform_data_area[col].tolist())  # Ensure data is a numpy array\n",
    "            ax = axs[innerind]  # Correct indexing for axs\n",
    "            ax.set_title(f'{event_dictionary[col]}', fontsize=16)            \n",
    "            sheet_dict={}\n",
    "            for task in (['BWcontext', 'BWnocontext']):\n",
    "                task_data = data[waveform_data_area['task'] == task]\n",
    "                \n",
    "                if len(task_data) > 0:\n",
    "                    task_data = np.array([functions.freq_band(row, all_bands_dict['total'][0], all_bands_dict['total'][1], 2000) for row in task_data])\n",
    "                    data_mean = np.mean(task_data, axis=0)\n",
    "                    data_sem = scipy.stats.sem(task_data, axis=0)\n",
    "                    time_axis = np.linspace(-0.7, 0.7, len(data_mean))\n",
    "                    ax.plot(time_axis, data_mean, color=plotting_styles.colors[task])\n",
    "                    ax.fill_between(time_axis, data_mean - data_sem, data_mean + data_sem, alpha=0.2, color=plotting_styles.colors[task])\n",
    "                    sheet_dict[f'{task}_mean'] = data_mean\n",
    "                    sheet_dict[f'{task}_sem'] = data_sem\n",
    "            sheet_dict['time'] = time_axis\n",
    "            sheet_df=pd.DataFrame(sheet_dict)\n",
    "            sheet_df.to_excel(writer, sheet_name=f'{area}_{col}', index=False)\n",
    "            ax.vlines(0, ax.get_ylim()[0], ax.get_ylim()[1], color='k', linestyle='--')\n",
    "            ax.set_xlabel('Time (s)', fontsize=14)\n",
    "            ax.set_ylabel('Amplitude (uV)', fontsize=14)\n",
    "            ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "            #ax.tick_params(axis='both', which='minor', labelsize=10)\n",
    "    #writer.close()\n",
    "    #fig.savefig(os.path.join(savepath,f' LFP_total_waveform_{rat}'), dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All rats alls bands around door and digging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "compiled_data_all_epochs = pd.read_pickle(savepath+'compiled_data_all_epochs.pkl')\n",
    "rat_list=list(np.unique(compiled_data_all_epochs['rat']))\n",
    "window = [-2, 2]  # Set the window for the waveform\n",
    "\n",
    "#band = 'total'  # Insert the band of interest\n",
    "tasks = ['BWcontext', 'BWnocontext']\n",
    "areas=['AON','vHp']\n",
    "compiled_data_all_epochs['around_door'] = compiled_data_all_epochs['pre_door'].apply(lambda x: x.tolist()) + compiled_data_all_epochs['post_door'].apply(lambda x: x.tolist())\n",
    "compiled_data_all_epochs['around_dig'] = compiled_data_all_epochs['pre_dig'].apply(lambda x: x.tolist()) + compiled_data_all_epochs['post_dig'].apply(lambda x: x.tolist())\n",
    "print(np.array(compiled_data_all_epochs['around_door'][0]).shape, np.array(compiled_data_all_epochs['around_dig'][0]).shape)\n",
    "all_bands_dict = {'total':[1,100], 'beta':[12,30], 'gamma':[30,80], 'theta':[4,12]}\n",
    "\n",
    "for rati in rat_list:\n",
    "    rat_dict = {}\n",
    "    rat_data = compiled_data_all_epochs[compiled_data_all_epochs['rat'] == rati]\n",
    "    rat_data['channel']=rat_data['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "    rat_data = rat_data.reset_index(drop=True)\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(10, 10))    \n",
    "    subfigs = fig.subfigures(2, 1)\n",
    "    subfigs=subfigs.flatten()\n",
    "    subfigs[1].set_facecolor('0.85')\n",
    "    fig.suptitle(f'{rati}')\n",
    "    \n",
    "    for outerind, area in enumerate(areas):\n",
    "        subfig=subfigs[outerind]\n",
    "        axs = subfig.subplots(4, 2)\n",
    "        \n",
    "        rat_data_area = rat_data[rat_data['channel'] == area]\n",
    "        rat_data_area = rat_data_area.reset_index(drop=True)   \n",
    "    \n",
    "        for i, band in enumerate(all_bands_dict.keys()):\n",
    "            rat_data_band=rat_data_area.__deepcopy__()\n",
    "            for col in (['around_door', 'around_dig']):\n",
    "                rat_data_band[col] = rat_data_area[col].apply(lambda x: functions.freq_band(x, all_bands_dict[band][0], all_bands_dict[band][1], 2000))\n",
    "\n",
    "            rat_data_band_grouped = rat_data_band.groupby(['task', 'channel'])\n",
    "            for (task, channel), group in rat_data_band_grouped:\n",
    "                group=group.reset_index(drop=True)\n",
    "                print(group.shape)\n",
    "                #group['around_dig']=np.concatenate([group['pre_dig'], group['post_dig']], axis=1)\n",
    "                for j, col in enumerate(['around_door', 'around_dig']):\n",
    "                    data = np.array(group[col])\n",
    "                    data_mean = np.mean(data, axis=0)\n",
    "                    data_sem = scipy.stats.sem(data, axis=0)\n",
    "                    time_axis = np.linspace(-0.7, 0.7, len(data_mean))\n",
    "                    ax = axs[i, j]\n",
    "                    ax.set_title(f'{band} {channel} {col}')\n",
    "                    ax.plot(time_axis, data_mean, color=plotting_styles.colors[task])\n",
    "                    ax.fill_between(time_axis, data_mean - data_sem, data_mean + data_sem, alpha=0.2, color=plotting_styles.colors[task])\n",
    "                    ax.vlines(0, ax.get_ylim()[0], ax.get_ylim()[1], color='k', linestyle='--')\n",
    "    #fig.savefig(os.path.join(savepath,f' LFP_waveform{rati}'), dpi=300)\n",
    "    plt.show()\n",
    "    #plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaged across rats single band "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform_data = compiled_data_all_epochs.copy()\n",
    "waveform_data['channel'] = waveform_data['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "event_dictionary = {'around_door':'Before and After door open', 'around_dig': 'Before and After Digging'}\n",
    "fig = plt.figure(constrained_layout=True, figsize=(20, 10))\n",
    "fig.suptitle(f'raw LFP averaged across rats', fontsize=20)\n",
    "\n",
    "subfigs = fig.subfigures(2, 1)\n",
    "subfigs=subfigs.flatten()\n",
    "for subfig in subfigs:\n",
    "    subfig.patch.set_edgecolor('black')\n",
    "    subfig.patch.set_linewidth(0.5)\n",
    "areas=['AON','vHp']\n",
    "for outerind, area in enumerate(areas):\n",
    "    subfig=subfigs[outerind]\n",
    "    axs = subfig.subplots(1, 2)\n",
    "    subfig.suptitle(f'{area}', fontsize= 16) \n",
    "    waveform_data_area = waveform_data[waveform_data['channel'] == area]\n",
    "    waveform_data_area = waveform_data_area.reset_index(drop=True)\n",
    "\n",
    "    for innerind, col in enumerate(['around_door', 'around_dig']):\n",
    "        data = np.array(waveform_data_area[col].tolist())  # Ensure data is a numpy array\n",
    "        ax = axs[innerind]  # Correct indexing for axs\n",
    "        ax.set_title(f'{event_dictionary[col]}', fontsize=14)\n",
    "        for task in (['BWcontext', 'BWnocontext']):\n",
    "            task_data = data[waveform_data_area['task'] == task]\n",
    "            if len(task_data) > 0:\n",
    "            \n",
    "                data_mean = np.mean(task_data, axis=0)\n",
    "                data_sem = scipy.stats.sem(task_data, axis=0)\n",
    "                time_axis = np.linspace(-2, 2, len(data_mean))\n",
    "                \n",
    "                ax.plot(time_axis, data_mean, color=plotting_styles.colors[task])\n",
    "                ax.fill_between(time_axis, data_mean - data_sem, data_mean + data_sem, alpha=0.2, color=plotting_styles.colors[task])\n",
    "        ax.vlines(0, ax.get_ylim()[0], ax.get_ylim()[1], color='k', linestyle='--')\n",
    "        ax.set_xlabel('Time (s)', fontsize=14)\n",
    "        ax.set_ylabel('Amplitude (uV)', fontsize=14)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "        ax.tick_params(axis='both', which='minor', labelsize=10)\n",
    "#fig.savefig(os.path.join(savepath,f' LFP_raw_waveform_averaged'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaged across rats all bands (To be Deleted later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform_data = compiled_data_all_epochs.copy()\n",
    "waveform_data['channel'] = waveform_data['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "waveform_data = waveform_data.reset_index(drop=True)\n",
    "fig = plt.figure(constrained_layout=True, figsize=(20, 10))\n",
    "subfigs = fig.subfigures(2, 1)\n",
    "subfigs=subfigs.flatten()\n",
    "subfigs[1].set_facecolor('0.85')\n",
    "fig.suptitle(f'Waveform')\n",
    "\n",
    "for i, band in enumerate(all_bands_dict.keys()):\n",
    "    print(band)\n",
    "\n",
    "waveform_data_grouped = waveform_data.groupby(['task', 'channel'])\n",
    "for outerind, area in enumerate(areas):\n",
    "    subfig=subfigs[outerind]\n",
    "    axs = subfig.subplots(4, 2)\n",
    "    waveform_data_area = waveform_data[waveform_data['channel'] == area]\n",
    "    waveform_data_area = waveform_data_area.reset_index(drop=True)\n",
    "    \n",
    "    for i, band in enumerate(all_bands_dict.keys()):\n",
    "        for col in (['around_door', 'around_dig']):\n",
    "            waveform_data_area[col+'_'+band] = waveform_data_area[col].apply(lambda x: functions.freq_band(x, all_bands_dict[band][0], all_bands_dict[band][1], 2000))\n",
    "\n",
    "        data = waveform_data_area[[f'around_door_{band}', f'around_dig_{band}']]\n",
    "        data_mean = data.groupby(waveform_data_area['task']).mean() \n",
    "        data_sem = data.groupby(waveform_data_area['task']).sem()\n",
    "        time_axis = np.linspace(-2, 2, len(data_mean.columns))\n",
    "        for j, task in enumerate(tasks):\n",
    "            ax = axs[i, j]\n",
    "            ax.set_title(f'{band} {task}')\n",
    "            ax.plot(time_axis, data_mean.loc[task], color=plotting_styles.colors[task])\n",
    "            ax.fill_between(time_axis, data_mean.loc[task] - data_sem.loc[task], data_mean.loc[task] + data_sem.loc[task], alpha=0.2, color=plotting_styles.colors[task])\n",
    "            ax.vlines(0, ax.get_ylim()[0], ax.get_ylim()[1], color='k', linestyle='--')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Power Spectra Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Baseline Power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotting_styles\n",
    "from mne.time_frequency import psd_array_multitaper\n",
    "importlib.reload(plotting_styles)\n",
    "importlib.reload(power_functions)\n",
    "linestyle = plotting_styles.linestyles\n",
    "colors = plotting_styles.colors\n",
    "baseline_lfp_all = pd.read_pickle(savepath+'baseline_lfp_all.pkl')\n",
    "df= baseline_lfp_all.__deepcopy__()\n",
    "df['channel']=df['channel'].apply(lambda x:'AON' if 'AON' in x else 'vHp')\n",
    "channel_experiment_group=df.groupby(['task','channel'])\n",
    "channel_dict = {'BWcontext_AON': 'context AON', 'BWcontext_vHp': 'context vHp',\n",
    "                'BWnocontext_AON': 'No context AON', 'BWnocontext_vHp': 'No context vHp'}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "mean_dict={}\n",
    "for channel, data in channel_experiment_group:\n",
    "    print(channel)\n",
    "    data_array=np.vstack(data['data'].to_numpy())\n",
    "    print(data_array.shape)\n",
    "    data_array_welch = np.array([power_functions.apply_welch_transform(row) for row in data_array]) # Applying Welch's method to each row of data_array\n",
    "    print(data_array_welch.shape)\n",
    "    freqs = np.linspace(0,1000,num=int(data_array_welch.shape[1]))  # Assuming the frequency range is 0-1000 Hz\n",
    "    print(freqs.shape)\n",
    "\n",
    "    data_array_welch_mean = np.mean(data_array_welch, axis=0)\n",
    "    data_array_welch_std = np.std(data_array_welch, axis=0)\n",
    "    print(data_array_welch_mean.shape, data_array_welch_std.shape)\n",
    "    mean_dict[channel[0] + '_' + channel[1] + '_mean'] = data_array_welch_mean\n",
    "    mean_dict[channel[0] + '_' + channel[1] + '_std'] = data_array_welch_std\n",
    "    \n",
    "    ax.plot(freqs,data_array_welch_mean, linestyle=linestyle[channel[1]], color=colors[channel[0]], label=f'{channel[0]} {channel[1]}')\n",
    "    ax.fill_between(freqs,data_array_welch_mean-data_array_welch_std,data_array_welch_mean+data_array_welch_std, alpha=0.1, color=colors[channel[0]])\n",
    "    #ax.set_yscale('log')\n",
    "    ax.set_xlim(0,100)\n",
    "    ax.legend(loc='upper right', fontsize=20)\n",
    "    ax.set_title('Baseline Power Spectral Density', fontsize=20)\n",
    "    ax.set_xlabel('Frequency (Hz)', fontsize=20)\n",
    "    ax.set_ylabel('Power (V^2/Hz)', fontsize=20)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "#    ax.set_yscale('log')\n",
    "mean_dict['frequency']=freqs\n",
    "mean_df=pd.DataFrame(mean_dict)\n",
    "#mean_df.to_csv(savepath+'baseline_power_truncated.csv')\n",
    "#plt.savefig(savepath+'baseline_power_truncated.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close(fig)\n",
    "\n",
    "# Calculate multitaper PSD for each group and plot\n",
    "fig_mt, ax_mt = plt.subplots(figsize=(15, 10))\n",
    "mt_mean_dict = {}\n",
    "for channel, data in channel_experiment_group:\n",
    "    data_array = np.vstack(data['data'].to_numpy())\n",
    "    # Multitaper PSD: average across trials\n",
    "    psds = []\n",
    "    for row in data_array:\n",
    "        psd, freqs_mt = psd_array_multitaper(row, sfreq=2000,bandwidth=2, fmin=0, fmax=100, adaptive=True, normalization='full', verbose=0)\n",
    "        psds.append(psd)\n",
    "    psds = np.array(psds)\n",
    "    psd_mean = psds.mean(axis=0)\n",
    "    psd_std = psds.std(axis=0)\n",
    "    mt_mean_dict[channel[0] + '_' + channel[1] + '_mean'] = psd_mean\n",
    "    mt_mean_dict[channel[0] + '_' + channel[1] + '_std'] = psd_std\n",
    "    ax_mt.plot(freqs_mt, psd_mean, linestyle=linestyle[channel[1]], color=colors[channel[0]], label=f'{channel[0]}_{channel[1]}')\n",
    "    ax_mt.fill_between(freqs_mt, psd_mean-psd_std, psd_mean+psd_std, alpha=0.1, color=colors[channel[0]])\n",
    "ax_mt.set_xlim(0, 100)\n",
    "handles, labels = ax_mt.get_legend_handles_labels()\n",
    "ax_mt.legend(handles, [channel_dict[l] for l in labels], loc='upper right', fontsize=20)\n",
    "ax_mt.set_title('Baseline Power Spectral Density (Multitaper)', fontsize=20)\n",
    "ax_mt.set_xlabel('Frequency (Hz)', fontsize=20)\n",
    "ax_mt.set_ylabel('Power (V^2/Hz)', fontsize=20)\n",
    "ax_mt.tick_params(axis='both', which='major', labelsize=20)\n",
    "\n",
    "mt_mean_dict['frequency'] = freqs_mt\n",
    "mt_mean_df = pd.DataFrame(mt_mean_dict)\n",
    "mt_mean_df.to_csv(savepath+'baseline_psd_multitaper.csv')\n",
    "plt.savefig(savepath+'baseline_psd_multitaper.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "plt.close(fig_mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Prepare data for ANOVA: for each frequency, compare power between tasks\n",
    "# We'll do this for both AON and vHp channels\n",
    "\n",
    "results = {'frequency': [], 'AON_F': [], 'AON_p': [], 'vHp_F': [], 'vHp_p': []}\n",
    "tasks = ['BWcontext', 'BWnocontext']\n",
    "def make_welch_data_dfs(data, task, channel):\n",
    "    data_task_channel = data[(data['task'] == task) & (data['channel'] == channel)]\n",
    "    data_array = np.vstack(data_task_channel['data'].to_numpy())\n",
    "    data_array_welch = np.array([power_functions.apply_welch_transform(row) for row in data_array])  # Applying Welch's method to each row of data_array\n",
    "    return data_array_welch\n",
    "\n",
    "aon_context_vals= make_welch_data_dfs(df, 'BWcontext', 'AON')\n",
    "aon_nocontext_vals= make_welch_data_dfs(df, 'BWnocontext', 'AON')\n",
    "vHp_context_vals= make_welch_data_dfs(df, 'BWcontext', 'vHp')\n",
    "vHp_nocontext_vals= make_welch_data_dfs(df, 'BWnocontext', 'vHp')\n",
    "for freq in range(aon_context_vals.shape[1]):\n",
    "    aon_f, aon_p = f_oneway(aon_context_vals[:, freq], aon_nocontext_vals[:, freq])\n",
    "    vHp_f, vHp_p = f_oneway(vHp_context_vals[:, freq], vHp_nocontext_vals[:, freq])\n",
    "    \n",
    "    results['frequency'].append(freq)\n",
    "    results['AON_F'].append(aon_f)\n",
    "    results['AON_p'].append(aon_p)\n",
    "    results['vHp_F'].append(vHp_f)\n",
    "    results['vHp_p'].append(vHp_p)\n",
    "    # Convert results to DataFrame and filter for frequency 1 to 100\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df[(results_df['frequency'] >= 1) & (results_df['frequency'] <= 100)]\n",
    "results_df.to_csv(savepath + 'anova_psd_per_frequency_1_100.csv', index=False)\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# For each frequency, extract power for each task and channel\n",
    "for i, freq in enumerate(mean_df['frequency']):\n",
    "    # For ANOVA, we need the raw values, not the means, so we go back to the original data\n",
    "    # Get all AON/vHp power values at this frequency for each task\n",
    "    aon_mask = (data['channel'] == 'AON')\n",
    "    vhp_mask = (data['channel'] == 'vHp')\n",
    "    context_mask = (data['task'] == 'BWcontext')\n",
    "    nocontext_mask = (data['task'] == 'BWnocontext')\n",
    "\n",
    "    aon_context_vals = data_array_welch[aon_mask & context_mask, i]\n",
    "    aon_nocontext_vals = data_array_welch[aon_mask & nocontext_mask, i]\n",
    "    vhp_context_vals = data_array_welch[vhp_mask & context_mask, i]\n",
    "    vhp_nocontext_vals = data_array_welch[vhp_mask & nocontext_mask, i]\n",
    "\n",
    "    # ANOVA for AON\n",
    "    if len(aon_context_vals) > 1 and len(aon_nocontext_vals) > 1:\n",
    "        F_aon, p_aon = f_oneway(aon_context_vals, aon_nocontext_vals)\n",
    "    else:\n",
    "        F_aon, p_aon = float('nan'), float('nan')\n",
    "\n",
    "    # ANOVA for vHp\n",
    "    if len(vhp_context_vals) > 1 and len(vhp_nocontext_vals) > 1:\n",
    "        F_vhp, p_vhp = f_oneway(vhp_context_vals, vhp_nocontext_vals)\n",
    "    else:\n",
    "        F_vhp, p_vhp = float('nan'), float('nan')\n",
    "\n",
    "    results['frequency'].append(freq)\n",
    "    results['AON_F'].append(F_aon)\n",
    "    results['AON_p'].append(p_aon)\n",
    "    results['vHp_F'].append(F_vhp)\n",
    "    results['vHp_p'].append(p_vhp)\n",
    "\n",
    "anova_df = pd.DataFrame(results)\n",
    "anova_df.to_csv(savepath + 'anova_psd_per_frequency.csv', index=False)\n",
    "print(anova_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BaselinePower Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.time_frequency import psd_array_multitaper\n",
    "\n",
    "baseline_lfp_all = pd.read_pickle(savepath+'baseline_lfp_all.pkl') #Loading the baseline LFP data\n",
    "df= baseline_lfp_all.__deepcopy__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "importlib.reload(plotting_styles)\n",
    "importlib.reload(power_functions)\n",
    "linestyles = plotting_styles.linestyles\n",
    "colors = plotting_styles.colors\n",
    "brain_areas = ['AON','vHp']\n",
    "\n",
    "\n",
    "number_per_segment = 2000\n",
    "tukey_window = scipy.signal.get_window(('tukey', 0.2), number_per_segment)    \n",
    "all_bands_dict = {'total':[1,100], 'beta':[12,30], 'gamma':[30,80], 'theta':[4,12]}\n",
    "task_dict = {'BWcontext': 'Context', 'BWnocontext': 'No Context'}\n",
    "df['data']=df['data'].apply(lambda x:power_functions.apply_welch_transform(x))\n",
    "\n",
    "for band_name, band_values in all_bands_dict.items():\n",
    "    df[band_name+'_power']=df['data'].apply(lambda x:power_functions.get_band_power(x, band_values[0], band_values[1]))\n",
    "\n",
    "writer=pd.ExcelWriter(savepath+'baseline_power_per_band_welch.xlsx')\n",
    "fig, axs = plt.subplots(1,2, figsize=(15, 10), sharey=True)\n",
    "axs=axs.flatten()\n",
    "for i, area in enumerate(brain_areas):\n",
    "    data = df[df['channel'].str.contains(area)]\n",
    "    data_melted = data.melt(id_vars=['rat','task','channel'], value_vars=['total_power','beta_power','gamma_power','theta_power'], var_name='band', value_name='power')\n",
    "    sns.barplot(\n",
    "        data=data_melted, x='band', y='power', hue='task',\n",
    "        hue_order=['BWcontext', 'BWnocontext'], palette=colors, ax=axs[i])\n",
    "    sns.stripplot(data=data_melted, x='band', y='power', hue='task', hue_order=['BWcontext','BWnocontext'], palette=colors, dodge=True, alpha=0.5, jitter=0.2, ax=axs[i], linewidth=1, legend=False )\n",
    "#    axs[i].set_yscale('log')\n",
    "    axs[i].set_title(f'Baseline {area} Power per Band', fontsize=20)\n",
    "    axs[i].set_xlabel('Band', fontsize=20)\n",
    "    axs[i].set_ylabel('Power V^2', fontsize=20)\n",
    "    axs[i].legend(loc='upper right', fontsize=15)\n",
    "    axs[i].set_xticks(([0,1,2,3]),list(all_bands_dict.keys()))\n",
    "    axs[i].tick_params(axis='both', which='major', labelsize=15)\n",
    "    data_melted.to_excel(writer, sheet_name=area)\n",
    "writer.close()\n",
    "plt.savefig(savepath+'baseline_power_per_band_welch.png', format='png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "baseline_lfp_all = pd.read_pickle(savepath+'baseline_lfp_all.pkl') #Loading the baseline LFP data\n",
    "df= baseline_lfp_all.__deepcopy__()\n",
    "\n",
    "# Calculate multitaper PSD and band power for each row\n",
    "df['data_mt'] = df['data'].apply(lambda x: psd_array_multitaper(x, sfreq=2000, fmin=0, fmax=100, adaptive=True,bandwidth=2, normalization='full', verbose=0, max_iter=500)[0])\n",
    "\n",
    "for band_name, band_values in all_bands_dict.items():\n",
    "    # df[band_name + '_power_mt'] = df['data_mt'].apply(lambda x: psd_array_multitaper(x, sfreq=2000, fmin=band_values[0], fmax=band_values[1], adaptive=True,bandwidth=2, normalization='full', verbose=0, max_iter=500,faverage=True)[0])\n",
    "\n",
    "    df[band_name + '_power_mt'] = df['data_mt'].apply(lambda x: power_functions.get_band_power(x, band_values[0], band_values[1]))\n",
    "    epsilon = 1e-12\n",
    "    #df[band_name + '_power_mt'] = df[band_name + '_power_mt'].apply(lambda x: 10*np.log10(x + epsilon))     # Log-normalize multitaper band power, handling log(0) by adding a small epsilon\n",
    "\n",
    "    # Plot multitaper band power\n",
    "writer_mt = pd.ExcelWriter(savepath + 'baseline_power_per_band_multitaper.xlsx')\n",
    "\n",
    "fig_mt, axs_mt = plt.subplots(1, 2, figsize=(15, 10), sharey=True)\n",
    "axs_mt = axs_mt.flatten()\n",
    "for i, area in enumerate(brain_areas):\n",
    "    data_mt = df[df['channel'].str.contains(area)]\n",
    "    data_melted_mt = data_mt.melt(\n",
    "        id_vars=['rat', 'task', 'channel'],\n",
    "        value_vars=['total_power_mt', 'beta_power_mt', 'gamma_power_mt', 'theta_power_mt'],\n",
    "        var_name='band', value_name='power'\n",
    "    )\n",
    "    # Plot log-normalized multitaper band power\n",
    "    sns.barplot(\n",
    "        data=data_melted_mt, x='band', y='power', hue='task',\n",
    "        hue_order=['BWcontext', 'BWnocontext'], palette=colors, ax=axs_mt[i]\n",
    "    )\n",
    "    sns.stripplot(\n",
    "        data=data_melted_mt, x='band', y='power', hue='task',\n",
    "        hue_order=['BWcontext', 'BWnocontext'], palette=colors, dodge=True, alpha=1, jitter=0.2,\n",
    "        ax=axs_mt[i], linewidth=1, legend=False\n",
    "    )\n",
    "    axs_mt[i].set_title(f'Baseline {area} Power per band', fontsize=20)\n",
    "    axs_mt[i].set_xlabel('Band', fontsize=20)\n",
    "    axs_mt[i].set_ylabel('Power (V^2)', fontsize=20)\n",
    "    handles, labels = axs_mt[i].get_legend_handles_labels()\n",
    "    axs_mt[i].legend(handles, [task_dict[l] for l in labels], loc='upper right', fontsize=15)\n",
    "    #axs_mt[i].legend(loc='upper right', fontsize=15)\n",
    "    axs_mt[i].set_xticks([0, 1, 2, 3], list(all_bands_dict.keys()))\n",
    "    axs_mt[i].tick_params(axis='both', which='major', labelsize=15)\n",
    "    data_melted_mt.to_excel(writer_mt, sheet_name=area)\n",
    "writer_mt.close()\n",
    "plt.savefig(savepath + 'baseline_power_per_band_multitaper.png', format='png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Power Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_lfp_all = pd.read_pickle(savepath+'baseline_lfp_all.pkl')\n",
    "df= baseline_lfp_all.__deepcopy__()\n",
    "df['channel']=df['channel'].apply(lambda x:'AON' if 'AON' in x else 'vHp')\n",
    "channel_experiment_group=df.groupby(['task','channel'])\n",
    "channel_dict = {'BWcontext_AON': 'context AON', 'BWcontext_vHp': 'context vHp',\n",
    "                'BWnocontext_AON': 'No context AON', 'BWnocontext_vHp': 'No context vHp'}\n",
    "\n",
    "\"\"\"\n",
    "Doing a test run\n",
    "\n",
    "test_array = df['data'].iloc[0]\n",
    "print(test_array.shape)\n",
    "\n",
    "test_array_new = test_array.reshape((1,1,-1))\n",
    "print(test_array_new.shape)\n",
    "\n",
    "fmin = 1\n",
    "fmax = 100\n",
    "freqs = np.arange(fmin, fmax)\n",
    "n_cycles = freqs / 3.  # different number of cycles per frequency\n",
    "fs =2000\n",
    "\n",
    "tfr_array = tfr_array_morlet(test_array_new, sfreq=fs, freqs=freqs, n_cycles=n_cycles, n_jobs=-1, output='power')\n",
    "\n",
    "print(tfr_array.shape)  # Should be (n_epochs,n_channels, n_freqs, n_times)\n",
    "\n",
    "tfr_array_squeezed = tfr_array.squeeze()\n",
    "print(tfr_array_squeezed.shape)  # Should be (n_freqs, n_times)\n",
    "\n",
    "plt.imshow(tfr_array_squeezed, aspect='auto', origin='lower', extent=[-2, 0, fmin, fmax])\n",
    "plt.colorbar(label='Power')\n",
    "#for task_channel, data in channel_experiment_group:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from mne.time_frequency import tfr_array_morlet\n",
    "\n",
    "def compute_tfr(data_array, fmin=1, fmax=100, fs=2000):\n",
    "    data_array = data_array.reshape((1, 1, -1))\n",
    "    freqs = np.arange(fmin, fmax)\n",
    "    n_cycles = freqs / 3.  # different number of cycles per frequency\n",
    "    tfr_array = tfr_array_morlet(data_array, sfreq=fs, freqs=freqs, n_cycles=n_cycles, n_jobs=1, output='power')\n",
    "    tfr_array_squeezed = tfr_array.squeeze()\n",
    "    #tfr_normalized = scipy.stats.zscore(tfr_array_squeezed, axis=1)\n",
    "    tfr_normalized = 10*np.log10(tfr_array_squeezed) #dB normalization\n",
    "    return tfr_normalized\n",
    "\n",
    "df['data_tfr'] = df['data'].apply(compute_tfr)\n",
    "\n",
    "\n",
    "channel_experiment_group=df.groupby(['task','channel'])\n",
    "channel_dict = {'BWcontext_AON': 'context AON', 'BWcontext_vHp': 'context vHp',\n",
    "                'BWnocontext_AON': 'No context AON', 'BWnocontext_vHp': 'No context vHp'}\n",
    "fig, axs= plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Baseline Power Spectrograms', fontsize=20)\n",
    "axs=axs.flatten()\n",
    "for i, (task_channel, data) in enumerate(channel_experiment_group):\n",
    "    ax = axs[i]\n",
    "    print(task_channel)\n",
    "    data_array_tfr = np.array(data['data_tfr'].tolist())\n",
    "    print(data_array_tfr.shape)  # Should be (n_epochs, n_freqs, n_times)\n",
    "    \n",
    "    data_array_tfr_mean = np.mean(data_array_tfr, axis=0)\n",
    "    print(data_array_tfr_mean.shape)  # Should be (n_freqs, n_times)\n",
    "    ax.imshow(data_array_tfr_mean, aspect='auto', origin='lower', extent=[-2, 0, 1, 100])\n",
    "\n",
    "    ax.set_title(f'{task_channel[0]} {task_channel[1]}')\n",
    "    ax.set_xlabel('Time (s)')\n",
    "    ax.set_ylabel('Frequency (Hz)')\n",
    "fig.colorbar(ax.images[0], ax=axs, label='Power (dB)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will plot the power spectra for each rat and the mean power spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will plot the power spectra for the complete trial # [NOT USED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(20, 10), sharex=True, sharey=True)\n",
    "axs=axs.flatten()\n",
    "fig.suptitle('Power Spectral Density')\n",
    "linestyles = {'AON': '-', 'vHp': '--'}\n",
    "\n",
    "for i,rati in enumerate(rat_list):\n",
    "    rat_data=power_df[power_df['rat']==rati]\n",
    "    rat_data=rat_data.reset_index(drop=True)\n",
    "    rat_data_grouped=rat_data.groupby(['task','channel'])\n",
    "    for (task, channel),group in rat_data_grouped:\n",
    "        print(task, channel)\n",
    "        group=group.reset_index(drop=True)\n",
    "        col='complete_trial'\n",
    "        data = np.array(group[col])\n",
    "        data_mean = np.mean(data, axis=0)\n",
    "        data_sem = scipy.stats.sem(data, axis=0)\n",
    "        freq = np.linspace(0, 1000, len(data_mean))        \n",
    "        ax = axs[i]\n",
    "        ax.set_title(f'{rati}')\n",
    "        ax.plot(freq, data_mean, color=colors[task], linestyle=linestyles[channel])\n",
    "        ax.fill_between(freq, data_mean - data_sem, data_mean + data_sem, alpha=0.2, color=colors[task])\n",
    "        ax.set_xlim(0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 Average Power Spectra across all rats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 Event Power Spectra individual Rats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Events PSD Welch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "\n",
    "time_window=0.7\n",
    "fs=2000\n",
    "\n",
    "##################\n",
    "\n",
    "from mne.time_frequency import psd_array_multitaper\n",
    "\n",
    "importlib.reload(power_functions)\n",
    "compiled_data_all_epochs = pd.read_pickle(savepath+f'compiled_data_all_epochs_truncated_{int(time_window*fs)}.pkl')\n",
    "power_df=compiled_data_all_epochs.__deepcopy__()\n",
    "# number_per_segment = 700\n",
    "# tukey_window = scipy.signal.get_window(('tukey', 0.1), number_per_segment)\n",
    "columns= ['complete_trial','pre_door', 'post_door', 'pre_dig', 'post_dig', 'around_door', 'around_dig']\n",
    "\n",
    "power_df.loc[:,columns]=power_df.loc[:,columns].applymap(lambda x:power_functions.apply_welch_transform(x))\n",
    "events_dict={'pre_door':' Pre Door','post_door':'Post Door','pre_dig':'Pre Dig','post_dig':'Post Dig'}\n",
    "fig, axs=plt.subplots(1,4, figsize=(40,10), sharex=True, sharey=True)\n",
    "axs=axs.flatten()\n",
    "for ax in axs:\n",
    "    ax.tick_params(axis='y', which='both', labelleft=True)\n",
    "writer=pd.ExcelWriter(savepath+'events_power_spectral_density.xlsx')\n",
    "for i, event in enumerate(events_dict.keys()):\n",
    "\n",
    "    data = power_df[['rat','task','channel',event]]\n",
    "    data['channel']=data['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "    data_groups=data.groupby(['task','channel'])\n",
    "    mean_data_dict={}\n",
    "    for (task, channel), group in data_groups:\n",
    "        group=group.reset_index(drop=True)\n",
    "        data = np.array(group[event])\n",
    "        data_mean = np.mean(data, axis=0)\n",
    "        data_sem = scipy.stats.sem(data, axis=0)\n",
    "        mean_data_dict[task+'_'+channel+'_mean']=data_mean\n",
    "        mean_data_dict[task+'_'+channel+'_sem']=data_sem\n",
    "        freq = np.linspace(0, 1000, len(data_mean))\n",
    "        ax = axs[i]\n",
    "        ax.set_title(f'{events_dict[event]}', fontsize=20)\n",
    "        ax.plot(freq, data_mean, color=plotting_styles.colors[task], linestyle=plotting_styles.linestyles[channel])\n",
    "        ax.fill_between(freq, data_mean - data_sem, data_mean + data_sem, alpha=0.2, color=plotting_styles.colors[task])\n",
    "        ax.set_xlim(0, 100)\n",
    "        #ax.set_yscale('log')\n",
    "        ax.set_xlabel('Frequency (Hz)', fontsize=20)\n",
    "        if i == 0:\n",
    "            ax.set_ylabel('Power (V^2/Hz)', fontsize=25)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "    mean_data_dict['frequency'] = freq\n",
    "    mean_df=pd.DataFrame(mean_data_dict)\n",
    "    mean_df.to_excel(writer, sheet_name=event)\n",
    "writer.close()\n",
    "fig.savefig(savepath+f'pow_events_psd{int(time_window*fs/2)}ms.png', format='png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Events PSD MT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "time_window=0.4\n",
    "fs=2000\n",
    "###############\n",
    "\n",
    "\n",
    "importlib.reload(power_functions)\n",
    "compiled_data_all_epochs = pd.read_pickle(savepath+f'compiled_data_all_epochs_truncated_{int(time_window*fs)}.pkl')\n",
    "power_df=compiled_data_all_epochs.__deepcopy__()\n",
    "# number_per_segment = 700\n",
    "# tukey_window = scipy.signal.get_window(('tukey', 0.1), number_per_segment)\n",
    "columns= ['pre_door', 'post_door', 'pre_dig', 'post_dig', 'around_door', 'around_dig']\n",
    "# Apply multitaper PSD to each event column\n",
    "def multitaper_transform(x):\n",
    "    # x is a 1D array or list of values\n",
    "    psd, _ = psd_array_multitaper(x, sfreq=2000, fmin=0, fmax=100, adaptive=True, bandwidth=6, normalization='length', verbose=0, max_iter=1000)\n",
    "    #psd = 10 * np.log10(psd)\n",
    "    return psd\n",
    "\n",
    "power_df.loc[:, columns] = power_df.loc[:, columns].applymap(multitaper_transform)\n",
    "task_dict = {'BWcontext': 'Context', 'BWnocontext': 'No Context'}\n",
    "events_dict = {'pre_door': ' Pre Door', 'post_door': 'Post Door', 'pre_dig': 'Pre Dig', 'post_dig': 'Post Dig'}\n",
    "fig, axs = plt.subplots(1, 4, figsize=(40, 10), sharex=True, sharey=True)\n",
    "axs = axs.flatten()\n",
    "for ax in axs:\n",
    "    ax.tick_params(axis='y', which='both', labelleft=True)\n",
    "writer = pd.ExcelWriter(savepath + f'pow_events_psd_{int(time_window*fs/2)}ms.xlsx')\n",
    "for i, event in enumerate(events_dict.keys()):\n",
    "    data = power_df[['rat', 'task', 'channel', event]]\n",
    "    data['channel'] = data['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "    data_groups = data.groupby(['task', 'channel'])\n",
    "    mean_data_dict = {}\n",
    "    for (task, channel), group in data_groups:\n",
    "        group = group.reset_index(drop=True)\n",
    "        data_arr = np.array(group[event])\n",
    "        data_mean = np.mean(data_arr, axis=0)\n",
    "        data_sem = scipy.stats.sem(data_arr, axis=0)\n",
    "        mean_data_dict[task + '_' + channel + '_mean'] = data_mean\n",
    "        mean_data_dict[task + '_' + channel + '_sem'] = data_sem\n",
    "        freq = np.linspace(0, 100, len(data_mean))\n",
    "        ax = axs[i]\n",
    "        ax.set_title(f'{events_dict[event]}', fontsize=20)\n",
    "        ax.plot(freq, data_mean, color=plotting_styles.colors[task], linestyle=plotting_styles.linestyles[channel], label=f'{task_dict[task]} {channel}')\n",
    "        ax.fill_between(freq, data_mean - data_sem, data_mean + data_sem, alpha=0.2, color=plotting_styles.colors[task])\n",
    "        ax.set_xlim(0, 100)\n",
    "        # ax.set_yscale('log')\n",
    "        ax.set_xlabel('Frequency (Hz)', fontsize=20)\n",
    "        if i == 0:\n",
    "            ax.set_ylabel('Power (V^2/Hz)', fontsize=25)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "    mean_data_dict['frequency'] = freq\n",
    "    mean_df = pd.DataFrame(mean_data_dict)\n",
    "    mean_df.to_excel(writer, sheet_name=event)\n",
    "writer.close()\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, labels, loc='upper right', fontsize=20)\n",
    "fig.savefig(savepath + f'pow_events_psd_{int(time_window*fs/2)}ms.png', format='png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Spectrograms for each each trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window=0.7\n",
    "fs=2000\n",
    "mne_epochs = pd.read_pickle(savepath+f'mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "## Test Epoch\n",
    "\n",
    "test_epoch_pre_door = mne_epochs['mne_epoch_door_before'].iloc[0]\n",
    "test_epoch_pre_dig = mne_epochs['mne_epoch_dig_before'].iloc[0]\n",
    "fmin=2.5\n",
    "fmax=100\n",
    "fs=2000\n",
    "freqs = np.arange(fmin,fmax)\n",
    "n_cycles = freqs/3\n",
    "\n",
    "power_pre_door = test_epoch_pre_door.compute_tfr(\n",
    "    method=\"morlet\", freqs=freqs, n_cycles=n_cycles, return_itc=False, average=False\n",
    ")\n",
    "\n",
    "power_pre_door_data = power_pre_door.get_data()\n",
    "print(power_pre_door_data.shape)\n",
    "power_pre_dig = test_epoch_pre_dig.compute_tfr(\n",
    "    method=\"morlet\", freqs=freqs, n_cycles=n_cycles, return_itc=False, average=False\n",
    ")\n",
    "power_pre_dig_data = power_pre_dig.get_data()\n",
    "print(power_pre_dig_data.shape)\n",
    "channel_names = power_pre_door.ch_names\n",
    "print(channel_names)\n",
    "plt.imshow(power_pre_door_data[0, 0, :, :], aspect='auto', origin='lower', extent=[-0.7, 0, fmin, fmax])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(power_pre_dig_data[0, 0, :, :], aspect='auto', origin='lower', extent=[-0.7, 0, fmin, fmax])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_of_cols = power_pre_door_data.shape[0]\n",
    "num_of_rows = power_pre_door_data.shape[1]\n",
    "\n",
    "fig, axs = plt.subplots(num_of_rows, num_of_cols, figsize=(20, 10), sharex=True, sharey=True)\n",
    "vmin_global = 0\n",
    "vmax_global = 0\n",
    "\n",
    "for i in range(num_of_rows):\n",
    "    for j in range(num_of_cols):\n",
    "        pre_door = power_pre_door_data[j,i, :, :]\n",
    "        pre_dig = power_pre_dig_data[j,i, :, :]\n",
    "        net_power = pre_dig - pre_door\n",
    "        axs[i, j].imshow(pre_door, aspect='auto', origin='lower', extent=[-0.7, 0, fmin, fmax])\n",
    "        if j == 0:\n",
    "            axs[i, j].set_ylabel(f'{channel_names[i]}', fontsize=10)\n",
    "        if i==0:\n",
    "            axs[i, j].set_title(f'trial {j}', fontsize=10)\n",
    "            \n",
    "        vmin_global = min(vmin_global, pre_door.min())\n",
    "        vmax_global = max(vmax_global, pre_door.max())\n",
    "\n",
    "# for ax in axs.flat:\n",
    "#     # Set color limits for all axes to the global min/max\n",
    "#     for im in ax.get_images():\n",
    "#         im.set_clim(vmin_global, vmax_global)\n",
    "fig.colorbar(axs[0, 0].images[0], ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne_epochs = pd.read_pickle(savepath+'mne_epochs_array_df_truncated_1400.pkl')\n",
    "def get_power_tfr(epoch):\n",
    "    fmin=2.5\n",
    "    fmax=100\n",
    "    fs=2000\n",
    "    freqs = np.arange(fmin,fmax)\n",
    "    n_cycles = freqs/3\n",
    "\n",
    "    power = epoch.compute_tfr(\n",
    "        method=\"morlet\", freqs=freqs, n_cycles=n_cycles, return_itc=False, average=False, method_kw\n",
    "        \n",
    "    )\n",
    "\n",
    "    return power\n",
    "results = []\n",
    "for row in mne_epochs.itertuples(index=False):\n",
    "    experiment, rat_id, task = row.experiment, row.rat_id, row.task\n",
    "    door_before,door_after = row.mne_epoch_door_before, row.mne_epoch_door_after\n",
    "    dig_before,dig_after = row.mne_epoch_dig_before, row.mne_epoch_dig_after\n",
    "    around_door, around_dig = row.mne_epoch_around_door, row.mne_epoch_around_dig\n",
    "\n",
    "    power_door_before = get_power_tfr(door_before)\n",
    "    power_door_after = get_power_tfr(door_after)\n",
    "    power_dig_before = get_power_tfr(dig_before)\n",
    "    power_dig_after = get_power_tfr(dig_after)\n",
    "    power_around_door = get_power_tfr(around_door)\n",
    "    power_around_dig = get_power_tfr(around_dig)\n",
    "\n",
    "    net_power = power_dig_before - power_door_before\n",
    "    channel_names = door_before.ch_names\n",
    "    new_row = [experiment, rat_id, task,power_door_before,power_door_after,power_dig_before,power_dig_after, power_around_door, power_around_dig, net_power, channel_names]\n",
    "    results.append(new_row)\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['experiment', 'rat_id', 'task', 'power_pre_door', 'power_post_door','power_pre_dig','power_post_dig','power_around_door','power_around_dig','net_power_pre_dig_pre_door', 'channel_names'])\n",
    "results_df.to_pickle(savepath + 'power_tfr_epochs_mrlt.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_power_epoch = results_df['power_pre_door'].iloc[0]\n",
    "test_power_epoch_2 = results_df['power_pre_door'].iloc[1]\n",
    "def plot_power_spec_from_epochs(test_power_epoch):\n",
    "    print(test_power_epoch.ch_names)\n",
    "    aon_channels = [channel for channel in test_power_epoch.ch_names if \"AON\" in channel]\n",
    "    vhp_channels = [channel for channel in test_power_epoch.ch_names if \"vHp\" in channel]\n",
    "    print(aon_channels,vhp_channels)\n",
    "    averaged_epoch_power = test_power_epoch.average(dim='epochs')\n",
    "    averaged_epoch_power.plot(title=\"auto\", vlim = (0, None))\n",
    "    averaged_epoch_power.plot(picks=aon_channels,title=\"AON power\", combine='mean', vlim = (0, None))\n",
    "    averaged_epoch_power.plot(picks=vhp_channels, title=\"VHP power\", combine='mean', vlim = (0, None))\n",
    "    return averaged_epoch_power, aon_channels,vhp_channels\n",
    "epoch1_avg, aon_1, vhp_1 = plot_power_spec_from_epochs(test_power_epoch)\n",
    "epoch2_avg, aon_2, vhp_2 = plot_power_spec_from_epochs(test_power_epoch_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0,1400,8)\n",
    "list(np.arange(0,1600,200))\n",
    "list(np.round(np.arange(0,0.8,0.1), decimals = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_pickle(savepath+'power_tfr_epochs_mrlt.pkl')\n",
    "\n",
    "def make_averaged_power(epoch, area):\n",
    "    #print(epoch.ch_names)\n",
    "    area_channels = [channel for channel in epoch.ch_names if area in channel]\n",
    "    #print(area_channels)\n",
    "\n",
    "    if len(area_channels)==0:\n",
    "        print(\"Error\")\n",
    "        return None\n",
    "    else:\n",
    "        area_epoch = epoch.copy()\n",
    "        area_epoch.pick(area_channels)\n",
    "        averaged_epoch_power = area_epoch.average(dim='epochs')\n",
    "        print(f\"Data shape before mean: {averaged_epoch_power.shape}\")  # DEBUG\n",
    "        mean_ch_power = np.mean(averaged_epoch_power.get_data(), axis = 0)\n",
    "        print(f\"Data shape after mean: {mean_ch_power.shape}\")  # DEBUG\n",
    "        return mean_ch_power\n",
    "\n",
    "# test_averaged_epoch_power = make_averaged_power(test_power_epoch, \"vHp\")\n",
    "# print(test_averaged_epoch_power.shape)\n",
    "\n",
    "for area in [\"AON\", \"vHp\"]:\n",
    "    area_df = pd.DataFrame()\n",
    "    fig, axs = plt.subplots(2,3, figsize= (15,10))\n",
    "    fig.suptitle(f'Average {area} Power')\n",
    "    for rowi, task in enumerate([\"BWcontext\", \"BWnocontext\"]):\n",
    "        task_data=results_df[results_df['task']==task]\n",
    "        print(f\"\\nTask: {task}, Area: {area}, Rows in task_data: {len(task_data)}\")\n",
    "        for coli, event in enumerate(['power_pre_door', 'power_pre_dig','power_post_dig']):\n",
    "            print(coli,event, task, area)\n",
    "            event_arrays = task_data[event].apply(lambda x: make_averaged_power(x, area))\n",
    "            \n",
    "            valid_arrays = [arr for arr in event_arrays.values if arr is not None]\n",
    "            \n",
    "            print(f\"Valid arrays found: {len(valid_arrays)}\")\n",
    "            \n",
    "            if len(valid_arrays) > 0:\n",
    "                averaged_array = np.mean(np.stack(valid_arrays), axis=0)\n",
    "                print(f\"Averaged array shape: {averaged_array.shape}\")\n",
    "                \n",
    "                ax = axs[rowi, coli]\n",
    "                im = ax.imshow(X= averaged_array, cmap = 'viridis', aspect='auto', origin='lower')\n",
    "                                # Add titles and labels\n",
    "                ax.set_title(f'{event.replace(\"_\", \" \").title()}')\n",
    "                ax.set_xlabel('Time (samples)')\n",
    "                ax.set_ylabel('Frequency (Hz)')\n",
    "                ax.set_xticks(list(np.arange(0,1600,200)))\n",
    "                ax.set_xticklabels(list(np.round(np.arange(0,0.8,0.1), decimals = 1)))\n",
    "                # Add colorbar\n",
    "                plt.colorbar(im, ax=ax, label='Power (mV^2/Hz)')\n",
    "                \n",
    "                # Add row labels\n",
    "                if coli == 0:\n",
    "                    ax.set_ylabel(f'{task}\\nFrequency (Hz)', fontweight='bold')\n",
    "                # Add your plotting code here\n",
    "            else:\n",
    "                print(f\"WARNING: No valid data for {area}, {task}, {event}\")\n",
    "                ax = axs[rowi, coli]\n",
    "                ax.text(0.5, 0.5, 'No data', ha='center', va='center')\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(savepath+f'power_spectrogram_{area}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for resulti in results_df.itertuples(index=False):\n",
    "    net_power = resulti.net_power_pre_dig_pre_door\n",
    "    vmin=net_power.min()\n",
    "    vmax=net_power.max()\n",
    "    num_of_trials = net_power.shape[0]\n",
    "    num_of_channels = net_power.shape[1]\n",
    "    fig, axs = plt.subplots(nrows=num_of_channels, ncols=num_of_trials,figsize=(10, 5))\n",
    "    for channeli in range(num_of_channels):\n",
    "        for triali in range(num_of_trials):\n",
    "            net_power_tfr = net_power[triali, channeli, :, :]\n",
    "            ax= axs[channeli, triali]\n",
    "            ax.imshow(net_power_tfr, aspect='auto', origin='lower', extent=[-0.7, 0, 2.5, 100], vmin=vmin, vmax=vmax)\n",
    "\n",
    "    plt.colorbar(ax.images[0], ax=ax, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(power_around_dig.shape)\n",
    "\n",
    "net_power = power_dig_before - power_door_before\n",
    "\n",
    "print(net_power.shape)\n",
    "print(net_power[0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Power for 1s around digging only [NOT USED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_list=['around_dig','around_door']\n",
    "fig, axs=plt.subplots(2,2, figsize=(20,10), sharex=True, sharey=True)\n",
    "axs=axs.flatten()\n",
    "writer=pd.ExcelWriter(savepath+'events_power_spectral_density.xlsx')\n",
    "for i, event in enumerate(events_list):\n",
    "\n",
    "    data = power_df[['rat','task','channel',event]]\n",
    "    data['channel']=data['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "    data_groups=data.groupby(['task','channel'])\n",
    "    mean_data_dict={}\n",
    "    for (task, channel), group in data_groups:\n",
    "        group=group.reset_index(drop=True)\n",
    "        data = np.array(group[event])\n",
    "        data_mean = np.mean(data, axis=0)\n",
    "        data_sem = scipy.stats.sem(data, axis=0)\n",
    "        mean_data_dict[task+'_'+channel+'_mean']=data_mean\n",
    "        mean_data_dict[task+'_'+channel+'_sem']=data_sem\n",
    "        freq = np.linspace(0, 1000, len(data_mean))\n",
    "        ax = axs[i]\n",
    "        ax.set_title(f'{event}')\n",
    "        ax.plot(freq, data_mean, color=plotting_styles.colors[task], linestyle=plotting_styles.linestyles[channel])\n",
    "        ax.fill_between(freq, data_mean - data_sem, data_mean + data_sem, alpha=0.2, color=plotting_styles.colors[task])\n",
    "        ax.set_xlim(0, 100)\n",
    "        ax.set_xlabel('Frequency (Hz)')\n",
    "        ax.set_ylabel('Power uV^2/Hz')\n",
    "    mean_df=pd.DataFrame(mean_data_dict)\n",
    "    #mean_df.to_excel(writer, sheet_name=event)\n",
    "#writer.close()\n",
    "#plt.savefig(savepath+'events_power_spectral_density.png')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Events Power Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Power Boxplots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per Trial [NOT USED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "compiled_data_all_epochs=pd.read_pickle(savepath+'compiled_data_all_epochs_truncated.pkl')\n",
    "boxplot_df=compiled_data_all_epochs.__deepcopy__()\n",
    "\n",
    "boxplot_df.loc[:,['pre_door','post_door','pre_dig','post_dig']]=boxplot_df.loc[:,['pre_door','post_door','pre_dig','post_dig']].applymap(lambda x: power_functions.apply_welch_transform(x))\n",
    "new_boxplot_df=boxplot_df[['rat', 'task', 'date', 'channel','trial']].copy()\n",
    "bands_dict = {'beta': [12, 30], 'gamma': [30, 80], 'theta': [4, 12], 'total': [1, 100]}\n",
    "for col in ['pre_door','post_door','pre_dig','post_dig']:\n",
    "    for band, (band_start, band_end) in bands_dict.items():\n",
    "        new_boxplot_df[band + '_' + col] = boxplot_df[col].apply(lambda x: power_functions.get_band_power(x, band_start, band_end))\n",
    "\n",
    "new_boxplot_df['unique_id'] = new_boxplot_df['rat'] + '_' + new_boxplot_df['task']+ '_' + new_boxplot_df['date']\n",
    "\n",
    "all_boxplot_df=new_boxplot_df.__deepcopy__()\n",
    "\n",
    "all_boxplot_df['channel'] = all_boxplot_df['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "aon_channels=all_boxplot_df[all_boxplot_df['channel']=='AON']\n",
    "vhp_channels=all_boxplot_df[all_boxplot_df['channel']=='vHp']\n",
    "\n",
    "area_list= ['AON', 'vHp']\n",
    "for area in area_list:\n",
    "    area_channels = all_boxplot_df[all_boxplot_df['channel'] == area]\n",
    "    writer=pd.ExcelWriter(savepath+'events_power_per_band_{}_truncated.xlsx'.format(area), engine='xlsxwriter')\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(40, 10), sharex=True, sharey=True)\n",
    "\n",
    "    axs=axs.flatten()\n",
    "    for ax in axs:\n",
    "        ax.tick_params(axis='y', which='both', labelleft=True)\n",
    "    events_dict={'pre_door':'Pre Door', 'post_door':'Post Door', 'pre_dig':'Pre Dig', 'post_dig':'Post Dig'}\n",
    "    for i, event in enumerate(events_dict.keys()):\n",
    "        area_df=area_channels.__deepcopy__()\n",
    "        event_cols = [col for col in area_df.columns if event in col]\n",
    "        print(event_cols)\n",
    "        event_df = area_df[['rat', 'task', 'channel','trial', *event_cols]]\n",
    "        event_df_melted = pd.melt(event_df, id_vars=['rat', 'task', 'channel','trial'], var_name='band', value_name='power')\n",
    "        event_df_melted['band'] = event_df_melted['band'].apply(lambda x: x.split('_')[0])\n",
    "        ax=axs[i]\n",
    "        #sns.boxplot(data=event_df_melted, x='band', y='power', hue='task', hue_order=['BWcontext','BWnocontext'], palette=colors, showfliers=False, ax=axs[i])\n",
    "        #sns.stripplot(data=event_df_melted, x='band', y='power', hue='task', hue_order=['BWcontext','BWnocontext'], palette=colors, dodge=True, alpha=0.5, jitter=0.2, ax=axs[i], linewidth=1, legend=False )\n",
    "        sns.violinplot(x='band',y='power',hue='task',hue_order=['BWcontext','BWnocontext'],data=event_df_melted, ax=ax)\n",
    "        #sns.stripplot(x='band',y='power',hue='task',hue_order=['BWcontext','BWnocontext'],data=event_df,dodge=True,edgecolor='black',linewidth=1,jitter=True, legend=False, ax=ax, color=\".3\", size=2)\n",
    "\n",
    "        ax.set_title(f'{events_dict[event]} {area}', fontsize=20)\n",
    "        ax.set_xlabel('Band', fontsize=20)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        #axs[i].set_yscale('log')\n",
    "        #axs[i].set_ylim(1e-3, 1e3)\n",
    "        if i == 0:\n",
    "            axs[i].set_ylabel('Power (V^2)', fontsize=25)\n",
    "        event_df_melted.to_excel(writer, sheet_name=event)\n",
    "    writer.close()\n",
    "    plt.savefig(savepath+'events_power_per_band_{}_truncated.png'.format(area), format='png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per Trial Multitaper [NOT USED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from mne.time_frequency import psd_array_multitaper\n",
    "\n",
    "compiled_data_all_epochs = pd.read_pickle(savepath + 'compiled_data_all_epochs_truncated.pkl')\n",
    "boxplot_df = compiled_data_all_epochs.__deepcopy__()\n",
    "\n",
    "event_cols = ['pre_door', 'post_door', 'pre_dig', 'post_dig']\n",
    "bands_dict = {'beta': [12, 30], 'gamma': [30, 80], 'theta': [4, 12], 'total': [1, 100]}\n",
    "sfreq = 2000\n",
    "epsilon = 1e-12  # for log-normalization\n",
    "\n",
    "# Apply multitaper PSD to each event column\n",
    "for col in event_cols:\n",
    "    boxplot_df[col] = boxplot_df[col].apply(lambda x: psd_array_multitaper(x, sfreq=sfreq, fmin=0, fmax=1000, adaptive=True, normalization='full', verbose=0, max_iter=500, bandwidth=4)[0])\n",
    "\n",
    "# Calculate band power from multitaper PSD and log-normalize\n",
    "new_boxplot_df = boxplot_df[['rat', 'task', 'date', 'channel', 'trial']].copy()\n",
    "for col in event_cols:\n",
    "    for band, (band_start, band_end) in bands_dict.items():\n",
    "        new_boxplot_df[f'{band}_{col}_mt'] = boxplot_df[col].apply(lambda x: np.log10(power_functions.get_band_power(x, band_start, band_end) + epsilon))\n",
    "\n",
    "new_boxplot_df['unique_id'] = new_boxplot_df['rat'] + '_' + new_boxplot_df['task'] + '_' + new_boxplot_df['date']\n",
    "all_boxplot_df = new_boxplot_df.__deepcopy__()\n",
    "all_boxplot_df['channel'] = all_boxplot_df['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "all_boxplot_df.to_excel(savepath + 'power_per_trial_mt.xlsx', index=False)\n",
    "area_list = ['AON', 'vHp']\n",
    "for area in area_list:\n",
    "    area_channels = all_boxplot_df[all_boxplot_df['channel'] == area]\n",
    "    writer = pd.ExcelWriter(savepath + f'events_power_per_band_multitaper_log10_{area}_truncated.xlsx', engine='xlsxwriter')\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(40, 10), sharex=True, sharey=True)\n",
    "    axs = axs.flatten()\n",
    "    for ax in axs:\n",
    "        ax.tick_params(axis='y', which='both', labelleft=True)\n",
    "    events_dict = {'pre_door': 'Pre Door', 'post_door': 'Post Door', 'pre_dig': 'Pre Dig', 'post_dig': 'Post Dig'}\n",
    "    for i, event in enumerate(events_dict.keys()):\n",
    "        event_cols_mt = [f'{band}_{event}_mt' for band in bands_dict.keys()]\n",
    "        event_df = area_channels[['rat', 'task', 'channel', 'trial', *event_cols_mt]]\n",
    "        event_df_melted = pd.melt(event_df, id_vars=['rat', 'task', 'channel', 'trial'], var_name='band', value_name='power')\n",
    "        event_df_melted['band'] = event_df_melted['band'].apply(lambda x: x.split('_')[0])\n",
    "        ax = axs[i]\n",
    "        sns.violinplot(x='band', y='power', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=event_df_melted, ax=ax)\n",
    "        ax.set_title(f'{events_dict[event]} {area} (Multitaper, log10)', fontsize=20)\n",
    "        ax.set_xlabel('Band', fontsize=20)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        if i == 0:\n",
    "            ax.set_ylabel('log10 Power (V^2)', fontsize=25)\n",
    "        event_df_melted.to_excel(writer, sheet_name=event)\n",
    "    writer.close()\n",
    "    plt.savefig(savepath + f'events_power_per_band_multitaper_log10_{area}_truncated.png', format='png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean across all trials Welch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(power_functions)\n",
    "compiled_data_all_epochs=pd.read_pickle(savepath+'compiled_data_all_epochs_truncated.pkl')\n",
    "boxplot_df=compiled_data_all_epochs.__deepcopy__()\n",
    "event_list=['pre_door','post_door','pre_dig','post_dig']\n",
    "boxplot_df.loc[:,event_list]=boxplot_df.loc[:,event_list].applymap(lambda x: power_functions.apply_welch_transform(x))\n",
    "new_boxplot_df=power_functions.get_all_band_power_from_welchdf(boxplot_df, event_list)\n",
    "new_boxplot_df['channel'] = new_boxplot_df['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "new_boxplot_df['unique_id'] = new_boxplot_df['rat'] + '_' + new_boxplot_df['task']+ '_' + new_boxplot_df['date']\n",
    "print(new_boxplot_df.columns)\n",
    "# Group by unique_id and channel, then take the mean across rows for columns containing 'pre' or 'post'\n",
    "pre_post_cols = [col for col in new_boxplot_df.columns if ('pre' in col or 'post' in col)]\n",
    "mean_data_list = []\n",
    "\n",
    "mean_boxplot_df=new_boxplot_df.__deepcopy__()\n",
    "unique_id_list=list(np.unique(mean_boxplot_df['unique_id']))\n",
    "mean_data_list=[]\n",
    "\n",
    "for unique_id in unique_id_list:\n",
    "    unique_id_df=mean_boxplot_df[mean_boxplot_df['unique_id']==unique_id]\n",
    "    unique_id_df_grouped=unique_id_df.groupby(['channel'])\n",
    "    for channel, group in unique_id_df_grouped:\n",
    "        print(channel)\n",
    "        group=group.reset_index(drop=True)\n",
    "        columns = [col for col in group.columns if 'pre' in col or 'post' in col]\n",
    "        print(columns)\n",
    "        rat_id=group['rat'].iloc[0]\n",
    "        task_id=group['task'].iloc[0]\n",
    "        date_id=group['date'].iloc[0]\n",
    "        channel_id=group['channel'].iloc[0]\n",
    "        mean_data_dict={}\n",
    "        for col in columns:\n",
    "            data=np.array(group[col])\n",
    "            data_mean=np.mean(data,axis=0)\n",
    "            data_sem=scipy.stats.sem(data,axis=0)\n",
    "            mean_data_dict[col+'_mean']=data_mean\n",
    "            mean_data_dict[col+'_sem']=data_sem\n",
    "        mean_data_dict['rat']=rat_id\n",
    "        mean_data_dict['task']=task_id\n",
    "        mean_data_dict['date']=date_id\n",
    "        mean_data_dict['channel']=channel_id\n",
    "        mean_data_list.append(mean_data_dict)\n",
    "mean_df=pd.DataFrame(mean_data_list)\n",
    "mean_df.drop(columns=['date'], inplace=True)\n",
    "\n",
    "\n",
    "mean_df_melted=pd.melt(mean_df, id_vars=['rat','task','channel'], var_name='band', value_name='power')\n",
    "mean_df_melted['band name']=mean_df_melted['band'].apply(lambda x: x.split('_')[0])\n",
    "mean_df_melted['event']=mean_df_melted['band'].apply(lambda x: x.split('_')[1:3])\n",
    "mean_df_melted['event']=mean_df_melted['event'].apply(lambda x: '_'.join(x))\n",
    "mean_df_melted['type']=mean_df_melted['band'].apply(lambda x: x.split('_')[-1])\n",
    "cols = list(mean_df_melted.columns)\n",
    "cols.append(cols.pop(cols.index('power')))\n",
    "mean_df_melted = mean_df_melted[cols]\n",
    "mean_df_melted.drop(columns=['band'], inplace=True)\n",
    "mean_df_melted=mean_df_melted[mean_df_melted['band name']!= 'total'] #Remove total band if it exists\n",
    "mean_df_melted_grouped=mean_df_melted.groupby(['event'])\n",
    "writer=pd.ExcelWriter(savepath+'mean_across_trials_power_truncated.xlsx')\n",
    "for event, group in mean_df_melted_grouped:\n",
    "    print(event)\n",
    "    group=group.reset_index(drop=True)\n",
    "    group.to_excel(writer, sheet_name=event[0])\n",
    "writer.close()\n",
    "arealist=['AON','vHp']\n",
    "for area in arealist:\n",
    "    fig,axs=plt.subplots(1,4,figsize=(40,10), sharey=True)\n",
    "    axs=axs.flatten()\n",
    "    for ax in axs:\n",
    "        ax.tick_params(axis='y', which='both', labelleft=True)\n",
    "    events_dict={'pre_door':'Pre Door', 'post_door':'Post Door', 'pre_dig':'Pre Dig', 'post_dig':'Post Dig'}\n",
    "    for i,event in enumerate(events_dict.keys()):\n",
    "        ax=axs[i]\n",
    "        ## Plotting AON mean power\n",
    "        plotting_df=mean_df_melted[(mean_df_melted['channel'].str.contains(area)) & (mean_df_melted['type']=='mean') & (mean_df_melted['event']==event)] \n",
    "        # Remove outliers using the IQR method for each band name\n",
    "        def remove_outliers_iqr(df, value_col='power', group_col='band name'):\n",
    "            def iqr_filter(group):\n",
    "                q1 = group[value_col].quantile(0.25)\n",
    "                q3 = group[value_col].quantile(0.75)\n",
    "                iqr = q3 - q1\n",
    "                lower = q1 - 1.5 * iqr\n",
    "                upper = q3 + 1.5 * iqr\n",
    "                return group[(group[value_col] >= lower) & (group[value_col] <= upper)]\n",
    "            return df.groupby(group_col, group_keys=False).apply(iqr_filter)\n",
    "\n",
    "        plotting_df = remove_outliers_iqr(plotting_df)\n",
    "        sns.boxplot(x='band name',y='power',hue='task',data=plotting_df,ax=ax, showfliers=False)\n",
    "        sns.stripplot(x='band name',y='power',hue='task',data=plotting_df,dodge=True,edgecolor='black',linewidth=1,jitter=True, legend=False, ax=ax, color=\".3\", size=2)\n",
    "        #ax.set_yscale('log')\n",
    "        ax.set_xlabel('Band', fontsize=20)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        ax.legend(loc='upper left', fontsize=15)\n",
    "        if i == 0:\n",
    "            ax.set_ylabel('Power (V^2)', fontsize=25)\n",
    "        ax.set_title(f'{area} power {events_dict[event]}', fontsize=20)\n",
    "    fig.savefig(savepath+f'mean_power_across_trials_{area}_truncated.png', format='png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "#mean_df=pd.DataFrame(mean_data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multitaper Mean across trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "\n",
    "time_window = 0.4\n",
    "fs = 2000\n",
    "\n",
    "##############\n",
    "\n",
    "from mne.time_frequency import psd_array_multitaper\n",
    "\n",
    "importlib.reload(power_functions)\n",
    "\n",
    "compiled_data_all_epochs = pd.read_pickle(savepath+f'compiled_data_all_epochs_truncated_{int(time_window*fs)}.pkl')\n",
    "power_df=compiled_data_all_epochs.__deepcopy__()\n",
    "# number_per_segment = 700\n",
    "# tukey_window = scipy.signal.get_window(('tukey', 0.1), number_per_segment)\n",
    "columns= ['pre_door', 'post_door', 'pre_dig', 'post_dig', 'around_door', 'around_dig']\n",
    "# Apply multitaper PSD to each event column\n",
    "def multitaper_transform(x):\n",
    "    # x is a 1D array or list of values\n",
    "    psd, _ = psd_array_multitaper(x, sfreq=2000, fmin=0, fmax=1000, adaptive=True, bandwidth=6, normalization='full', verbose=0, max_iter=1000)\n",
    "    return psd\n",
    "\n",
    "event_list=['pre_door','post_door','pre_dig','post_dig']\n",
    "power_df.loc[:, columns] = power_df.loc[:, columns].applymap(multitaper_transform)\n",
    "new_boxplot_df = power_functions.get_all_band_power_from_mt(power_df, event_list)\n",
    "#new_boxplot_df['channel'] = new_boxplot_df['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp') #TRIAL\n",
    "new_boxplot_df['unique_id'] = new_boxplot_df['rat'] + '_' + new_boxplot_df['task'] + '_' + new_boxplot_df['date']\n",
    "print(new_boxplot_df.columns)\n",
    "\n",
    "pre_post_cols = [col for col in new_boxplot_df.columns if ('pre' in col or 'post' in col)]\n",
    "mean_data_list = []\n",
    "\n",
    "mean_boxplot_df = new_boxplot_df.__deepcopy__()\n",
    "unique_id_list = list(np.unique(mean_boxplot_df['unique_id']))\n",
    "mean_data_list = []\n",
    "\n",
    "for unique_id in unique_id_list:\n",
    "    unique_id_df = mean_boxplot_df[mean_boxplot_df['unique_id'] == unique_id]\n",
    "    unique_id_df_grouped = unique_id_df.groupby(['channel'])\n",
    "    for channel, group in unique_id_df_grouped:\n",
    "        print(channel)\n",
    "        group = group.reset_index(drop=True)\n",
    "        columns = [col for col in group.columns if 'pre' in col or 'post' in col]\n",
    "        print(columns)\n",
    "        rat_id = group['rat'].iloc[0]\n",
    "        task_id = group['task'].iloc[0]\n",
    "        date_id = group['date'].iloc[0]\n",
    "        channel_id = group['channel'].iloc[0]\n",
    "        mean_data_dict = {}\n",
    "        for col in columns:\n",
    "            data = np.array(group[col])\n",
    "            data_mean = np.mean(data, axis=0)\n",
    "            data_sem = scipy.stats.sem(data, axis=0)\n",
    "            mean_data_dict[col + '_mean'] = data_mean\n",
    "            mean_data_dict[col + '_sem'] = data_sem\n",
    "        mean_data_dict['rat'] = rat_id\n",
    "        mean_data_dict['task'] = task_id\n",
    "        mean_data_dict['date'] = date_id\n",
    "        mean_data_dict['channel'] = channel_id\n",
    "        mean_data_list.append(mean_data_dict)\n",
    "mean_df = pd.DataFrame(mean_data_list)\n",
    "mean_df.drop(columns=['date'], inplace=True)\n",
    "\n",
    "\n",
    "def remove_outliers_iqr(df, value_col='power', group_col='band name'):\n",
    "    def iqr_filter(group):\n",
    "        q1 = group[value_col].quantile(0.25)\n",
    "        q3 = group[value_col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower = q1 - 1.5 * iqr\n",
    "        upper = q3 + 1.5 * iqr\n",
    "        return group[(group[value_col] >= lower) & (group[value_col] <= upper)]\n",
    "    return df.groupby(group_col, group_keys=False).apply(iqr_filter)\n",
    "\n",
    "mean_df_melted = pd.melt(mean_df, id_vars=['rat', 'task', 'channel'], var_name='band', value_name='power')\n",
    "mean_df_melted['band name'] = mean_df_melted['band'].apply(lambda x: x.split('_')[0])\n",
    "mean_df_melted['event'] = mean_df_melted['band'].apply(lambda x: x.split('_')[1:3])\n",
    "mean_df_melted['event'] = mean_df_melted['event'].apply(lambda x: '_'.join(x))\n",
    "mean_df_melted['type'] = mean_df_melted['band'].apply(lambda x: x.split('_')[-1])\n",
    "cols = list(mean_df_melted.columns)\n",
    "cols.append(cols.pop(cols.index('power')))\n",
    "mean_df_melted = mean_df_melted[cols]\n",
    "mean_df_melted.drop(columns=['band'], inplace=True)\n",
    "#mean_df_melted=mean_df_melted[mean_df_melted['band name']!= 'total'] #Remove total band if it exists\n",
    "mean_df_melted_grouped = mean_df_melted.groupby(['event'])\n",
    "\n",
    "writer_mt = pd.ExcelWriter(savepath + f'pow_events_perband_{int(time_window*fs/2)}ms.xlsx')\n",
    "for event, group in mean_df_melted_grouped:\n",
    "    print(event)\n",
    "    group = group.reset_index(drop=True)\n",
    "    group.to_excel(writer_mt, sheet_name=event[0])\n",
    "writer_mt.close()\n",
    "arealist = ['AON', 'vHp']\n",
    "fig = plt.figure(figsize=(40, 20), layout='constrained')\n",
    "fig.suptitle('Power per band (Multitaper)', fontsize=30)\n",
    "subfigs = fig.subfigures(nrows=2, ncols=1)\n",
    "\n",
    "for area_num,area in enumerate(arealist):\n",
    "    axs = subfigs[area_num].subplots(nrows=1, ncols=4, sharey=True)\n",
    "    axs = axs.flatten()\n",
    "    for ax in axs:\n",
    "        ax.tick_params(axis='y', which='both', labelleft=True)\n",
    "    events_dict = {'pre_door': 'Pre Door', 'post_door': 'Post Door', 'pre_dig': 'Pre Dig', 'post_dig': 'Post Dig'}\n",
    "    for i, event in enumerate(events_dict.keys()):\n",
    "        ax = axs[i]\n",
    "        plotting_df = mean_df_melted[\n",
    "            (mean_df_melted['channel'].str.contains(area)) &\n",
    "            (mean_df_melted['type'] == 'mean') &\n",
    "            (mean_df_melted['event'] == event)\n",
    "        ]\n",
    "\n",
    "        plotting_df = remove_outliers_iqr(plotting_df) ## REMOVE OUTLIERS\n",
    "        band_order = ['theta', 'beta', 'gamma','total']\n",
    "        sns.barplot(x='band name', y='power', hue='task', data=plotting_df, order=band_order, ax=ax)\n",
    "        sns.stripplot(x='band name', y='power', hue='task', data=plotting_df, order=band_order, dodge=True, palette=colors, jitter=True, legend=False, ax=ax, linewidth=1, alpha=0.8)\n",
    "        ax.set_xlabel('Band', fontsize=20)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        task_dict = {'BWcontext': 'Context', 'BWnocontext': 'No Context'}\n",
    "        ax.legend(handles, [task_dict[l] for l in labels], loc='upper right', fontsize=15)\n",
    "        #ax.legend(loc='upper left', fontsize=15)\n",
    "        if i == 0:\n",
    "            ax.set_ylabel('Power (uV^2)', fontsize=25)\n",
    "        else:\n",
    "            ax.set_ylabel('')\n",
    "        ax.set_title(f'{area} power {events_dict[event]}', fontsize=20)\n",
    "fig.savefig(savepath + f'pow_events_perband_{int(time_window*fs/2)}ms.png', format='png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a mean across channels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_boxplot_df=boxplot_df.__deepcopy__()\n",
    "unique_id_list=list(np.unique(mean_boxplot_df['unique_id']))\n",
    "mean_data_list=[]\n",
    "sem_data_list=[]\n",
    "mean_boxplot_df['channel']=mean_boxplot_df['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "mean_boxplot_df_grouped=mean_boxplot_df.groupby(['task', 'channel', 'trial'])\n",
    "for (task, channel, trial), group in mean_boxplot_df_grouped:\n",
    "    print(task, channel, trial)\n",
    "    group=group.reset_index(drop=True)\n",
    "    columns=group.columns[-21:-1]\n",
    "    data_array=np.array(group[columns])\n",
    "    data_mean=np.mean(data_array, axis=0)\n",
    "    data_sem=scipy.stats.sem(data_array, axis=0)\n",
    "    print(data_mean)\n",
    "    print(data_sem)\n",
    "    mean_data_dict = {col: data_mean[idx] for idx, col in enumerate(columns)}\n",
    "    sem_data_dict = {col: data_sem[idx] for idx, col in enumerate(columns)}\n",
    "    mean_data_dict['task'] = task\n",
    "    mean_data_dict['channel'] = channel\n",
    "    mean_data_dict['trial'] = trial\n",
    "    sem_data_dict['task'] = task\n",
    "    sem_data_dict['channel'] = channel\n",
    "    sem_data_dict['trial'] = trial\n",
    "    mean_data_list.append(mean_data_dict)\n",
    "    sem_data_list.append(sem_data_dict)\n",
    "\n",
    "mean_df = pd.DataFrame(mean_data_list)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for task in ['BWcontext', 'BWnocontext']:\n",
    "    task_data = mean_df[(mean_df['task'] == task) & (mean_df['channel'] == 'AON')]\n",
    "    ax.plot(task_data['trial'], task_data['total_complete_trial'], label=task, marker='o')\n",
    "ax.set_xlabel('Trial Number')\n",
    "ax.set_ylabel('AON Power in total complete trial')\n",
    "ax.set_title('AON Power in total complete trial across Trials')\n",
    "ax.set_xticks(np.arange(0, 20, 1))\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean in groups of 5 trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_df_grouped=boxplot_df.groupby(['unique_id'])\n",
    "mean_data_list=[]\n",
    "for unique_id, group in boxplot_df_grouped:\n",
    "    print(unique_id)\n",
    "    num_of_trials=len(group['trial'].unique())\n",
    "    print(num_of_trials)\n",
    "    group=group.reset_index(drop=True)\n",
    "    print()\n",
    "    for channel in group['channel'].unique():\n",
    "        i=0\n",
    "        group_channel=group[group['channel']==channel]\n",
    "        group_channel=group_channel.reset_index(drop=True)\n",
    "        \n",
    "        while i < 16:\n",
    "            print(i)\n",
    "            group_trial = group_channel[(group_channel['trial'] >= i) & (group_channel['trial'] < i + 4)]\n",
    "            group_trial_data_array = np.array(group_trial.loc[:, 'beta_pre_door':'total_around_dig'])\n",
    "            data_mean= group_trial_data_array.mean(axis=0)\n",
    "            row = {**group_channel.iloc[0][['rat', 'task', 'channel', 'unique_id']].to_dict(),\n",
    "                   **{'trial': f'{i}-{i + 4}'},\n",
    "                   **dict(zip(group_trial.loc[:, 'beta_pre_door':'total_around_dig'].columns, data_mean))}\n",
    "            mean_data_list.append(row)\n",
    "\n",
    "            i=i+4\n",
    "mean_df = pd.DataFrame(mean_data_list)\n",
    "mean_df_melted=pd.melt(mean_df, id_vars=['rat','task','channel','trial', 'unique_id'], var_name='band_event', value_name='power')\n",
    "mean_df_melted['band name']=mean_df_melted['band_event'].apply(lambda x: x.split('_')[0])\n",
    "mean_df_melted['event']=mean_df_melted['band_event'].apply(lambda x: x.split('_')[1:3])\n",
    "mean_df_melted['event']=mean_df_melted['event'].apply(lambda x: '_'.join(x))\n",
    "mean_df_melted_grouped=mean_df_melted.groupby(['event'])\n",
    "writer=pd.ExcelWriter(savepath+'power_boxplot_average_per_4_trials.xlsx')\n",
    "for event, group in mean_df_melted_grouped:\n",
    "    print(event)\n",
    "    group=group.reset_index(drop=True)\n",
    "    group.drop(columns=['band_event','event'], inplace=True)\n",
    "    group.to_excel(writer, sheet_name=event[0])\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Power Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import spectrogram\n",
    "\n",
    "power_spec_df = compiled_data_all_epochs.__deepcopy__()\n",
    "print(power_spec_df.iloc[0,-2].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_spec_df.iloc[:, -2:] = power_spec_df.iloc[:, -2:].applymap(lambda x: spectrogram(x, fs=2000, nperseg=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(power_spec_df.iloc[0,-2][1])\n",
    "for col in ['around_door','around_dig']:\n",
    "\n",
    "    power_spec_df[col+'_f'] = power_spec_df[col].apply(lambda x: x[0])\n",
    "    power_spec_df[col+'_t'] = power_spec_df[col].apply(lambda x: x[1])\n",
    "    power_spec_df[col+'_sxx'] = power_spec_df[col].apply(lambda x: x[2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_spec_df['channel'] = power_spec_df['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "power_spec_df_grouped = power_spec_df.groupby(['task', 'channel'])\n",
    "for (task, channel), group in power_spec_df_grouped:\n",
    "    group = group.reset_index(drop=True)\n",
    "    for col in ['around_door', 'around_dig']:\n",
    "        data = np.array(group[col + '_sxx'])\n",
    "        data_mean = np.mean(data, axis=0)\n",
    "        print(data_mean.shape)\n",
    "        freq = group[col + '_f'].iloc[0]\n",
    "        time = group[col + '_t'].iloc[0]\n",
    "        time_adjusted=np.linspace(-2,2,len(time))\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(20, 10), constrained_layout=True)\n",
    "        im = ax.pcolormesh(time_adjusted, freq, data_mean, shading='gouraud', vmin=0, vmax=0.5)\n",
    "        fig.colorbar(im, ax=ax)\n",
    "        ax.set_title(f'{task} {channel} {col}')\n",
    "        ax.set_ylim(0, 100)\n",
    "        # ax.set_xticks(np.arange(-2, 3, 1))  # Set x-ticks from -2 to 2 seconds\n",
    "        # ax.set_xticklabels(np.arange(-2, 3, 1))  # Set x-tick labels from -2 to 2 seconds\n",
    "        ax.vlines(0, 0, 100, color='red', linestyle='--')\n",
    "        ax.set_xlabel('Time (s)', fontsize=20)\n",
    "\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        ax.set_ylabel('Frequency (Hz)', fontsize=20)\n",
    "        i = i + 1\n",
    "        fig.savefig(savepath + f'power_mean_spectrogram_{task}_{channel}_{col}.png', dpi=300)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Coherence Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Coherence functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating static data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "importlib.reload(coherence_functions)\n",
    "# --- Basic Parameters ---\n",
    "sfreq = 1000  # Sampling frequency in Hz\n",
    "n_epochs = 20  # Number of trials\n",
    "n_times = 2000  # Number of time points per trial (2 seconds of data)\n",
    "times = np.arange(n_times) / sfreq  # Time vector for one epoch\n",
    "n_signals = 3  # We'll create 3 channels\n",
    "\n",
    "# We will test connectivity in the beta band\n",
    "freq_of_interest = 20.0  # 20 Hz\n",
    "# --- Generate Data for Static Connectivity ---\n",
    "\n",
    "# Initialize data array: (n_epochs, n_signals, n_times)\n",
    "static_data = np.random.randn(n_epochs, n_signals, n_times) * 0.1  # Add background noise\n",
    "\n",
    "# Create the shared 20 Hz sine wave component\n",
    "shared_signal = np.sin(2 * np.pi * freq_of_interest * times)\n",
    "\n",
    "# Add the shared signal to the first two channels for all epochs\n",
    "static_data[:, 0, :] += shared_signal\n",
    "static_data[:, 1, :] += shared_signal\n",
    "\n",
    "print(\"Shape of static_data:\", static_data.shape)\n",
    "ch_names=['AON', 'vHp', 'PFC']  # Example channel names\n",
    "info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types='eeg')\n",
    "static_data_mne=mne.EpochsArray(static_data, info)\n",
    "print(static_data_mne)\n",
    "# Plot the static_data for each channel in the first epoch\n",
    "fig, axs = plt.subplots(n_signals, 1, figsize=(12, 6), sharex=True)\n",
    "for i, ch in enumerate(ch_names):\n",
    "    axs[i].plot(times, static_data[0, i, :], label=f'Channel: {ch}')\n",
    "    axs[i].set_ylabel('Amplitude')\n",
    "    axs[i].legend(loc='upper right')\n",
    "axs[-1].set_xlabel('Time (s)')\n",
    "plt.suptitle('Static Data Example (Epoch 0)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "coherence_band_sce = coherence_functions.convert_epoch_to_coherence(static_data_mne)\n",
    "print(coherence_band_sce)\n",
    "coherence_band_time=coherence_functions.convert_epoch_to_coherence_time(static_data_mne)\n",
    "print(coherence_band_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truncating LFP data and loading it into MNE arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import mne_connectivity\n",
    "import sys\n",
    "importlib.reload(lfp_pre_processing_functions)\n",
    "#files=[f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230616_dk6_BW_context_day2.mat']\n",
    "event_data_df=[]\n",
    "con_data_df=[]\n",
    "\n",
    "con_data_df_shuffled=[]\n",
    "shuffled_event_data_df=[]\n",
    "events_codes_all = {}\n",
    "random_baseline_data=[]\n",
    "baseline_lfp_all=[]\n",
    "\n",
    "time_window = 0.7\n",
    "fs= 2000\n",
    "\n",
    "files_short=[files[10]] ### TEST CHANGE THIS \n",
    "\n",
    "\n",
    "for file_num,file in enumerate(files):\n",
    "    #if 'dk1' in file:\n",
    "        \n",
    "        #print(file)\n",
    "        base_name = os.path.basename(file)\n",
    "        base_name, _ = os.path.splitext(base_name)\n",
    "\n",
    "        date, rat_id, task = lfp_pre_processing_functions.exp_params(base_name)\n",
    "        print(date, rat_id, task)\n",
    "        if task == 'nocontextday2' or task == 'nocontextos2':\n",
    "            task = 'nocontext'\n",
    "        if task =='nocontext':\n",
    "            continue\n",
    "        # if rat_id=='dk1': #REMOVING DK1 TEMPORARLILY . PLEASE CHANGE LATER\n",
    "        #     continue\n",
    "        f = h5py.File(file, 'r')\n",
    "        channels = list(f.keys())\n",
    "        #print(channels)\n",
    "         \n",
    "        if not any(\"AON\" in channel or \"vHp\" in channel for channel in channels):\n",
    "            print(\"No AON or vHp channels in this file\")\n",
    "            continue\n",
    "\n",
    "        events,reference_electrode=lfp_pre_processing_functions.get_keyboard_and_ref_channels(f,channels)\n",
    "\n",
    "    #finding global start and end time of all channels, since they start and end recordings at different times\n",
    "        global_start_time, global_end_time=lfp_pre_processing_functions.find_global_start_end_times(f,channels)\n",
    "        \n",
    "        ## Reference electrode finding and padding\n",
    "        reference_time = np.array(reference_electrode['times']).flatten()\n",
    "        reference_value = np.array(reference_electrode['values']).flatten()\n",
    "        padd_ref_data,padded_ref_time=lfp_pre_processing_functions.pad_raw_data_raw_time(reference_value,reference_time,global_start_time,global_end_time,sampling_rate=2000)\n",
    "\n",
    "        events_codes = np.array(events['codes'][0])\n",
    "        events_times = np.array(events['times'][0])\n",
    "        events_codes_all[base_name] = events_codes\n",
    "        epochs = lfp_pre_processing_functions.generate_epochs_with_first_event(events_codes, events_times)\n",
    "        #epochs = functions.generate_specific_num_of_epochs_with_first_event(events_codes, events_times,5)\n",
    "        aon_lfp_channels=[x for x in channels if 'AON' in x ]\n",
    "        vHp_lfp_channels=[x for x in channels if 'vHp' in x ]\n",
    "        all_channels=np.concatenate((aon_lfp_channels,vHp_lfp_channels))\n",
    "        #print(all_channels)\n",
    "        \n",
    "        mne_baseline_data=np.zeros((1,len(all_channels),4000))\n",
    "        mne_epoch_door_before=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_door_after=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_dig_before=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_dig_after=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_around_door=np.zeros((len(epochs),len(all_channels),int(time_window*fs)*2))\n",
    "        mne_epoch_around_dig=np.zeros((len(epochs),len(all_channels),int(time_window*fs)*2))\n",
    "        \n",
    "        mne_baseline_data_shuffled=np.zeros((1,len(all_channels),4000))\n",
    "        mne_epoch_door_before_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_door_after_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_dig_before_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_dig_after_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs)))\n",
    "        mne_epoch_around_door_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs*2)))\n",
    "        mne_epoch_around_dig_shuffled=np.zeros((len(epochs),len(all_channels),int(time_window*fs*2)))\n",
    "\n",
    "        print(f'File {rat_id} {task} {date} has {len(epochs)} epochs and {len(all_channels)} channels')\n",
    "\n",
    "\n",
    "        first_event = events_times[0]\n",
    "        \n",
    "        for channel_num,channeli in enumerate(all_channels):\n",
    "            if \"AON\" in channeli or \"vHp\" in channeli:\n",
    "                channel_id = channeli\n",
    "                data_all = f[channeli]\n",
    "                raw_data = np.array(data_all['values']).flatten()\n",
    "                raw_time = np.array(data_all['times']).flatten()\n",
    "                sampling_rate = int(1 / data_all['interval'][0][0])\n",
    "                #print(raw_data.shape, raw_time.shape, sampling_rate)\n",
    "                padded_data,padded_time=lfp_pre_processing_functions.pad_raw_data_raw_time(raw_data,raw_time,global_start_time,global_end_time,sampling_rate)\n",
    "                subtracted_data = padded_data - padd_ref_data\n",
    "                raw_data=subtracted_data\n",
    "                notch_data = lfp_pre_processing_functions.iir_notch(raw_data, sampling_rate, 60) ###CHANGE notch_data to notch_filtered_data\n",
    "\n",
    "                print(notch_data.nbytes)\n",
    "                notch_data_detrended = scipy.signal.detrend(notch_data)\n",
    "                notch_filtered_data=lfp_pre_processing_functions.sosbandpass(notch_data_detrended, fs=2000, start_freq=1,end_freq=100, order=8) ###CHANGE THIS FOR NOT BANDBASS FILTERTING\n",
    "                print(notch_filtered_data.nbytes)\n",
    "                \n",
    "                data_before, time, baseline_mean, baseline_std=lfp_pre_processing_functions.baseline_data_normalization(notch_filtered_data, raw_time, first_event, sampling_rate)\n",
    "                first_event_index=np.where(raw_time>first_event)[0][0]\n",
    "\n",
    "                mne_baseline_data[0,channel_num,:]=list(data_before)\n",
    "                mne_baseline_data_shuffled[0,channel_num,:]=list(np.random.permutation(data_before))\n",
    "                total = notch_filtered_data\n",
    "\n",
    "                \n",
    "                for i, epochi in enumerate(epochs):\n",
    "                    door_timestamp = epochi[0][0]\n",
    "                    trial_type = epochi[0][1]\n",
    "                    dig_type = epochi[1, 1]\n",
    "                    #print(dig_type)\n",
    "                    dig_timestamp = epochi[1, 0]\n",
    "                    #print(door_timestamp, trial_type, dig_timestamp, dig_type)\n",
    "                    data_trial_before, data_trial_after=lfp_pre_processing_functions.extract_event_data(notch_filtered_data,time,door_timestamp,sampling_rate,truncation_time=time_window)\n",
    "                    data_dig_before, data_dig_after=lfp_pre_processing_functions.extract_event_data(notch_filtered_data,time,dig_timestamp,sampling_rate,truncation_time=time_window)\n",
    "                    data_around_door=np.concatenate((data_trial_before, data_trial_after))\n",
    "                    data_around_dig=np.concatenate((data_dig_before, data_dig_after))\n",
    "\n",
    "                    epoch_data = [data_trial_before, data_trial_after, data_dig_before, data_dig_after, data_around_door, data_around_dig]\n",
    "                    event_data_list = [lfp_pre_processing_functions.zscore_event_data(x, baseline_std) for x in epoch_data]\n",
    "\n",
    "                    mne_epoch_door_before[i,channel_num,:]=list(event_data_list[0][-int(time_window*fs):])\n",
    "                    mne_epoch_door_after[i,channel_num,:]=list(event_data_list[1][:int(time_window*fs)])\n",
    "                    mne_epoch_dig_before[i,channel_num,:]=list(event_data_list[2][-int(time_window*fs):])\n",
    "                    mne_epoch_dig_after[i,channel_num,:]=list(event_data_list[3][:int(time_window*fs)])\n",
    "                    mid_point = int(len(event_data_list[4]) / 2)\n",
    "                    mne_epoch_around_door[i,channel_num,:]=list(event_data_list[4][mid_point-int(time_window*fs):mid_point+int(time_window*fs)])\n",
    "                    mne_epoch_around_dig[i,channel_num,:]=list(event_data_list[5][mid_point-int(time_window*fs):mid_point+int(time_window*fs)])\n",
    "\n",
    "                    mne_epoch_door_before_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[0][-int(time_window*fs):]))\n",
    "                    mne_epoch_door_after_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[1][:int(time_window*fs)]))\n",
    "                    mne_epoch_dig_before_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[2][-int(time_window*fs):]))\n",
    "                    mne_epoch_dig_after_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[3][:int(time_window*fs)]))\n",
    "                    mne_epoch_around_door_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[4][mid_point-int(time_window*fs):mid_point+int(time_window*fs)]))\n",
    "                    mne_epoch_around_dig_shuffled[i,channel_num,:]=list(np.random.permutation(event_data_list[5][mid_point-int(time_window*fs):mid_point+int(time_window*fs)]))\n",
    "\n",
    "        if len(all_channels)>0:\n",
    "            fs=2000\n",
    "            freqs = np.arange(1,100)\n",
    "            n_cycles = freqs/3\n",
    "            info = mne.create_info(ch_names=list(all_channels), sfreq=fs, ch_types='eeg')\n",
    "            mne_baseline = mne.EpochsArray(mne_baseline_data, info)\n",
    "            mne_epoch_door_before = mne.EpochsArray(mne_epoch_door_before, info)\n",
    "            mne_epoch_door_after= mne.EpochsArray(mne_epoch_door_after, info)\n",
    "            mne_epoch_dig_before = mne.EpochsArray(mne_epoch_dig_before, info)\n",
    "            mne_epoch_dig_after = mne.EpochsArray(mne_epoch_dig_after, info)\n",
    "            mne_epoch_around_door = mne.EpochsArray(mne_epoch_around_door, info)\n",
    "            mne_epoch_around_dig = mne.EpochsArray(mne_epoch_around_dig, info)\n",
    "            \n",
    "            row_list=[file_num,date,rat_id,task,mne_baseline,mne_epoch_door_before,mne_epoch_door_after,mne_epoch_dig_before,mne_epoch_dig_after,mne_epoch_around_door,mne_epoch_around_dig]\n",
    "            \n",
    "            mne_baseline_shuffled = mne.EpochsArray(mne_baseline_data_shuffled, info)\n",
    "            mne_epoch_door_before_shuffled = mne.EpochsArray(mne_epoch_door_before_shuffled, info)\n",
    "            mne_epoch_door_after_shuffled = mne.EpochsArray(mne_epoch_door_after_shuffled, info)\n",
    "            mne_epoch_dig_before_shuffled = mne.EpochsArray(mne_epoch_dig_before_shuffled, info)\n",
    "            mne_epoch_dig_after_shuffled = mne.EpochsArray(mne_epoch_dig_after_shuffled, info)\n",
    "            mne_epoch_around_door_shuffled = mne.EpochsArray(mne_epoch_around_door_shuffled, info)\n",
    "            mne_epoch_around_dig_shuffled = mne.EpochsArray(mne_epoch_around_dig_shuffled, info)\n",
    "            row_list_shuffled=[file_num,date,rat_id,task,mne_baseline_shuffled,mne_epoch_door_before_shuffled,mne_epoch_door_after_shuffled,mne_epoch_dig_before_shuffled,mne_epoch_dig_after_shuffled,mne_epoch_around_door_shuffled,mne_epoch_around_dig_shuffled]\n",
    "            shuffled_event_data_df.append(row_list_shuffled)\n",
    "\n",
    "            con_data_df.append(row_list)\n",
    "            con_data_df_shuffled.append(row_list_shuffled)\n",
    "\n",
    "\n",
    "con_data_df=pd.DataFrame(con_data_df, columns=['experiment','date','rat_id','task','mne_baseline','mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after','mne_epoch_around_door','mne_epoch_around_dig'])\n",
    "con_data_df.to_pickle(savepath+f'mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "con_data_df_shuffled=pd.DataFrame(con_data_df_shuffled, columns=['experiment','date','rat_id','task','mne_baseline','mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after','mne_epoch_around_door','mne_epoch_around_dig'])\n",
    "con_data_df_shuffled.to_pickle(savepath+f'mne_epochs_array_df_shuffled_truncated_{int(time_window*fs)}.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from scipy import signal\n",
    "\n",
    "# Before filter\n",
    "f_before, psd_before = signal.welch(notch_data, fs=2000, nperseg=1024)\n",
    "\n",
    "# After filter\n",
    "f_after, psd_after = signal.welch(notch_data_detrended, fs=2000, nperseg=1024)\n",
    "fig,axs=plt.subplots(2,1, figsize = (10,5), sharey=True)\n",
    "axs=axs.flatten()\n",
    "axs[0].semilogy(f_before, psd_before, label='Before filter')\n",
    "axs[1].semilogy(f_after, psd_after, label='After filter')\n",
    "# .xlabel('Frequency (Hz)')\n",
    "# plt.ylabel('Power Spectral Density')\n",
    "axs[0].set_xlim([0, 10])  # Focus on your filter range\n",
    "axs[1].set_xlim([0, 10])  # Focus on your filter range\n",
    "\n",
    "# plt.axvline(1, color='r', linestyle='--', label='Start freq (1 Hz)')\n",
    "# plt.axvline(100, color='g', linestyle='--', label='End freq (100 Hz)')\n",
    "# plt.legend()\n",
    "# plt.title('Frequency Domain')\n",
    "# plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Baseline Coherence Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(coherence_functions)\n",
    "time_window = 0.7\n",
    "fs = 2000\n",
    "con_data_df=pd.read_pickle(savepath+f'mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n",
    "baseline_df=con_data_df.__deepcopy__()\n",
    "baseline_df['mne_baseline']=baseline_df['mne_baseline'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_density(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax= plt.subplots(1, 1, figsize=(10, 5), sharex=True, sharey=True)\n",
    "writer=pd.ExcelWriter(savepath+'baseline_coherence_density.xlsx')\n",
    "baseline_df_grouped=baseline_df.groupby(['task'])\n",
    "task_dict={'BWcontext':'Context','BWnocontext':'No Context'}\n",
    "baseline_dict={}\n",
    "for (task, group) in baseline_df_grouped:\n",
    "    print(task[0])\n",
    "    group=group.reset_index(drop=True)\n",
    "    data = np.array(group['mne_baseline'].tolist())\n",
    "    data_mean = np.mean(data, axis=0)\n",
    "    data_sem = scipy.stats.sem(data, axis=0)\n",
    "    freq = np.linspace(0, 100, len(data_mean))\n",
    "    ax.plot(freq, data_mean, label=task_dict[task[0]])\n",
    "    ax.fill_between(freq, data_mean - data_sem, data_mean + data_sem, alpha=0.2)\n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.set_title(f'Baseline AON-vHp Coherence Density')\n",
    "    ax.set_xlabel('Frequency (Hz)')\n",
    "    ax.set_ylabel('Coherence (Z-transformed)')\n",
    "    ax.legend()\n",
    "    baseline_dict[f'{task[0]}_mean'] = data_mean\n",
    "    baseline_dict[f'{task[0]}_sem'] = data_sem\n",
    "baseline_dict['frequency'] = freq\n",
    "mean_df = pd.DataFrame(baseline_dict)\n",
    "mean_df.to_excel(writer, sheet_name='mean_coherence_density')\n",
    "writer.close()\n",
    "\n",
    "fig.savefig(savepath+'baseline_coherence_density.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Baseline Coherence Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(coherence_functions)\n",
    "time_window = 0.7\n",
    "fs = 2000\n",
    "con_data_df=pd.read_pickle(savepath+f'mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n",
    "baseline_df=con_data_df.__deepcopy__()\n",
    "task_dict={'BWcontext':'Context','BWnocontext':'No Context'}\n",
    "bands_dict = {'beta': [12, 30], 'gamma': [30, 80],'theta':[4,12], 'total': [1, 100]}\n",
    "for col in ['mne_baseline']:\n",
    "    print(col)\n",
    "    for band, (band_start, band_end) in bands_dict.items():\n",
    "        baseline_df[band + '_' + col] = baseline_df[col].apply(lambda x: coherence_functions.convert_epoch_to_coherence_baseline(x, band_start=band_start, band_end=band_end))\n",
    "baseline_df.drop(columns=['mne_baseline', 'mne_epoch_door_before', 'mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after','mne_epoch_around_door','mne_epoch_around_dig'], inplace=True)\n",
    "baseline_df_melted=pd.melt(baseline_df, id_vars=['experiment','rat_id','task','date'], var_name='band', value_name='coherence')\n",
    "baseline_df_melted['band']=baseline_df_melted['band'].apply(lambda x: x.split('_')[0])\n",
    "\n",
    "\n",
    "####Plotting coherence per band\n",
    "fig, axs= plt.subplots(1, 1, figsize=(20, 10), sharex=True, sharey=True)\n",
    "sns.barplot(x='band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=baseline_df_melted, legend=True, ax=axs)\n",
    "sns.stripplot(x='band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=baseline_df_melted, dodge=True, edgecolor='black', linewidth=1, jitter=True, legend=False, ax=axs)\n",
    "axs.set_title('Baseline Coherence per band', fontsize=20)\n",
    "axs.set_ylabel('Coherence (Z-transformed)', fontsize=20)\n",
    "axs.set_xlabel('')\n",
    "axs.tick_params(axis='both', which='major', labelsize=20)\n",
    "handles, labels = axs.get_legend_handles_labels()\n",
    "axs.legend(handles, [task_dict[l] for l in labels], loc='upper left', fontsize=15)\n",
    "fig.savefig(savepath+'baseline_coherence_per_band_truncated.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "###Writing the baseline coherence per band to excel\n",
    "writer=pd.ExcelWriter(savepath+'baseline_coherence_per_band_truncated.xlsx')\n",
    "baseline_df_melted.to_excel(writer)\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "\n",
    "time_window = 0.7\n",
    "fs = 2000\n",
    "tanh_norm = False\n",
    "###########\n",
    "\n",
    "if tanh_norm:\n",
    "    suffix ='normalized'\n",
    "else:\n",
    "    suffix = 'nonnormalized'\n",
    "importlib.reload(coherence_functions)\n",
    "\n",
    "con_data_df=pd.read_pickle(savepath+f'mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n",
    "columns_to_process = ['mne_epoch_door_before', 'mne_epoch_door_after', 'mne_baseline']\n",
    "baseline_df = coherence_functions.convert_baseline_to_coherence_mt_expanded(con_data_df[columns_to_process + ['rat_id', 'task']],rat_ids=con_data_df['rat_id'], tasks=con_data_df['task'],columns_to_process=['mne_baseline'], tanh_norm=tanh_norm)\n",
    "baseline_df.drop(columns=['event_type'], inplace=True)\n",
    "baseline_df.to_excel(savepath+f'coherence_baseline_per_band_channelpair_{suffix}.xlsx')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Coherence around events [door before, door after, dig before, dig after]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating AON-vHp connectivity Spectrogram from Epochs Array and Saving if as a pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "\n",
    "time_window = 0.7\n",
    "fs = 2000  # Sampling frequency\n",
    "tanh_norm = True\n",
    "###############\n",
    "def coherogram_pkl(time_window, fs, tanh_norm):\n",
    "    if tanh_norm:\n",
    "        suffix ='_normalized'\n",
    "    else:\n",
    "        suffix ='_non-normalized'\n",
    "\n",
    "\n",
    "    con_data_df_clean=pd.read_pickle(savepath+'mne_epochs_array_df_truncated_{}.pkl'.format(int(time_window*fs)))\n",
    "\n",
    "    event_list=['mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after']\n",
    "\n",
    "    print(event_list)\n",
    "    BWcontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWcontext')]\n",
    "    BWnocontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWnocontext')]\n",
    "    task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "\n",
    "    all_con_data=[]\n",
    "    all_con_data_mean=[]\n",
    "    for task_num,task_name in enumerate(task_data_dict.keys()):\n",
    "            task_data=task_data_dict[task_name]\n",
    "            row=[task_name]\n",
    "            #print(row)\n",
    "            row_2=[task_name]\n",
    "            for event in event_list:\n",
    "                #print(event)\n",
    "                event_epoch_list=task_data[event]\n",
    "                aon_vHp_con=[]\n",
    "                for event_epoch in event_epoch_list:\n",
    "                        #print(row,event, event_epoch) \n",
    "                        if event_epoch.events.shape[0] <5:\n",
    "                            print(f\"Skipping {event} for {task_name} due to insufficient events\")\n",
    "                            continue\n",
    "                        fmin=1\n",
    "                        fmax=100\n",
    "                        fs=2000\n",
    "                        freqs = np.arange(fmin,fmax)\n",
    "                        n_cycles = freqs/3\n",
    "\n",
    "                        con = mne_connectivity.spectral_connectivity_epochs(event_epoch, method='coh', sfreq=int(fs),\n",
    "                                                            mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                                            cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False)\n",
    "                        \n",
    "                        ########TRYING HIGHPASS FILTERING FOR ARTIFACT REMOVAL################\n",
    "                        # epoch_highpass = event_epoch.copy().filter(l_freq = 1, h_freq=None, filter_length = \"0.7s\" )\n",
    "                        # con = mne_connectivity.spectral_connectivity_epochs(epoch_highpass, method='coh', sfreq=int(fs),\n",
    "                        #                                     mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                        #                                     cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "                        coh = con.get_data(output='dense')\n",
    "                        indices = con.names\n",
    "                        \n",
    "\n",
    "                        for i in range(coh.shape[0]):\n",
    "                            for j in range(coh.shape[1]):\n",
    "                                if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                                    coherence= coh[i,j,:,:]\n",
    "                                    if tanh_norm:\n",
    "                                        coherence=np.arctanh(coherence)\n",
    "                                    aon_vHp_con.append(coherence)\n",
    "                row.append(np.mean(aon_vHp_con, axis=0))\n",
    "                row_2.append(np.mean(aon_vHp_con))\n",
    "            all_con_data.append(row)                    \n",
    "            all_con_data_mean.append(row_2)\n",
    "    # Convert all_con_data to a DataFrame for easier manipulation\n",
    "    all_con_data_df = pd.DataFrame(all_con_data, columns=['task'] + event_list)\n",
    "    all_con_data_df.to_pickle(savepath+'coherence_spectrogram_before_after_door_dig_truncated_{}{}.pkl'.format(int(time_window*fs), suffix))\n",
    "\n",
    "coherogram_pkl(time_window=time_window, fs=fs, tanh_norm=tanh_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "\n",
    "time_window = 0.7\n",
    "fs = 2000  # Sampling frequency\n",
    "tanh_norm = True\n",
    "###############\n",
    "\n",
    "if tanh_norm:\n",
    "    suffix ='_normalized'\n",
    "else:\n",
    "    suffix ='_non-normalized'\n",
    "\n",
    "all_con_data_df=pd.read_pickle(savepath+'coherence_spectrogram_before_after_door_dig_truncated_{}{}.pkl'.format(int(time_window*fs), suffix))\n",
    "event_list=['mne_epoch_door_before','mne_epoch_dig_before','mne_epoch_dig_after']\n",
    "\n",
    "times=np.arange(0, time_window, 1/fs)\n",
    "fig, axs=plt.subplots(2,3, figsize=(15,10), sharey=True)\n",
    "vmin = all_con_data_df[event_list].applymap(np.min).min().min()\n",
    "vmax = all_con_data_df[event_list].applymap(np.max).max().max()\n",
    "event_names=['Before Door','Before Dig','After Dig']\n",
    "for i, event in enumerate(event_list):\n",
    "    axs[0,i].imshow(all_con_data_df[event][0], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[0,i].set_xlabel('Time (s)')\n",
    "    axs[0,i].set_ylabel('Frequency (Hz)')\n",
    "    axs[0,i].set_title(event_names[i])\n",
    "\n",
    "    axs[1,i].imshow(all_con_data_df[event][1], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[1,i].set_xlabel('Time (s)')\n",
    "    axs[1,i].set_ylabel('Frequency (Hz)')\n",
    "    axs[1,i].set_title(event_names[i])\n",
    "    axs[0,0].text(-0.3, 0.5, 'Context', transform=axs[0,0].transAxes, fontsize=14, verticalalignment='center', rotation=90)\n",
    "    axs[1,0].text(-0.3, 0.5, 'No Context', transform=axs[1,0].transAxes, fontsize=14, verticalalignment='center', rotation=90)\n",
    "    # Add a colorbar\n",
    "cbar = fig.colorbar(axs[0,0].images[0], ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "cbar.set_label('Coherence (A.U.)', fontsize=12)\n",
    "fig.savefig(savepath+f'coherence_spectrogram_before_after_door_dig_{int(time_window*fs/2)}ms{suffix}.png', dpi=1200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Coherence around events [around door and around dig]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating AON-vHp connectivity around door and dig and saving it in a pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "\n",
    "time_window = 0.4\n",
    "fs = 2000  # Sampling frequency\n",
    "tanh_norm = True\n",
    "###############\n",
    "def coherogram_pkl(time_window, fs, tanh_norm):\n",
    "    if tanh_norm:\n",
    "        suffix ='_normalized'\n",
    "    else:\n",
    "        suffix ='_non-normalized'\n",
    "\n",
    "\n",
    "    con_data_df_clean=pd.read_pickle(savepath+'mne_epochs_array_df_truncated_{}.pkl'.format(int(time_window*fs)))\n",
    "\n",
    "    event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "\n",
    "    print(event_list)\n",
    "    BWcontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWcontext')]\n",
    "    BWnocontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWnocontext')]\n",
    "    task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "\n",
    "    all_con_data=[]\n",
    "    all_con_data_mean=[]\n",
    "    for task_num,task_name in enumerate(task_data_dict.keys()):\n",
    "            task_data=task_data_dict[task_name]\n",
    "            row=[task_name]\n",
    "            #print(row)\n",
    "            row_2=[task_name]\n",
    "            for event in event_list:\n",
    "                #print(event)\n",
    "                event_epoch_list=task_data[event]\n",
    "                aon_vHp_con=[]\n",
    "                for event_epoch in event_epoch_list:\n",
    "                        #print(row,event, event_epoch) \n",
    "                        if event_epoch.events.shape[0] <5:\n",
    "                            print(f\"Skipping {event} for {task_name} due to insufficient events\")\n",
    "                            continue\n",
    "                        fmin=1\n",
    "                        fmax=100\n",
    "                        fs=2000\n",
    "                        freqs = np.arange(fmin,fmax)\n",
    "                        n_cycles = freqs/3\n",
    "\n",
    "\n",
    "                        con = mne_connectivity.spectral_connectivity_epochs(event_epoch, method='coh', sfreq=int(fs),\n",
    "                                                            mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                                            cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False)\n",
    "                        coh = con.get_data(output='dense')\n",
    "                        indices = con.names\n",
    "                        \n",
    "\n",
    "                        for i in range(coh.shape[0]):\n",
    "                            for j in range(coh.shape[1]):\n",
    "                                if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                                    coherence= coh[i,j,:,:]\n",
    "                                    if tanh_norm:\n",
    "                                        coherence=np.arctanh(coherence)\n",
    "                                    aon_vHp_con.append(coherence)\n",
    "                row.append(np.mean(aon_vHp_con, axis=0))\n",
    "                row_2.append(np.mean(aon_vHp_con))\n",
    "            all_con_data.append(row)                    \n",
    "            all_con_data_mean.append(row_2)\n",
    "    # Convert all_con_data to a DataFrame for easier manipulation\n",
    "    all_con_data_df = pd.DataFrame(all_con_data, columns=['task'] + event_list)\n",
    "    all_con_data_df.to_pickle(savepath+'coherence_spectrogram_around_door_dig_truncated_{}{}.pkl'.format(int(time_window*fs), suffix))\n",
    "\n",
    "coherogram_pkl(time_window=time_window, fs=fs, tanh_norm=tanh_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected range check\n",
    "print(\"Expected coherence ranges:\")\n",
    "print(\"Raw coherence: 0 to 1\")\n",
    "print(\"Fisher Z-transform: 0 to infinity (practically -3 to 3)\")\n",
    "\n",
    "# Check if your values are reasonable\n",
    "all_data = np.concatenate([all_con_data_df[event][i].flatten() \n",
    "                          for event in event_list for i in range(2)])\n",
    "\n",
    "if np.any(all_data < -5) or np.any(all_data > 5):\n",
    "    print(\"WARNING: Unusually extreme Fisher Z values detected!\")\n",
    "    print(\"Consider checking your coherence calculation.\")\n",
    "\n",
    "# Check the actual data values\n",
    "print(\"Data statistics:\")\n",
    "for event in event_list:\n",
    "    for i, task in enumerate(['Context', 'No Context']):\n",
    "        data = all_con_data_df[event][i]\n",
    "        print(f\"{task} - {event}:\")\n",
    "        print(f\"  Min: {np.min(data):.3f}\")\n",
    "        print(f\"  Max: {np.max(data):.3f}\")\n",
    "        print(f\"  Mean: {np.mean(data):.3f}\")\n",
    "        print(f\"  Std: {np.std(data):.3f}\")\n",
    "        print(f\"  Median: {np.median(data):.3f}\")\n",
    "        print(f\"  25th percentile: {np.percentile(data, 25):.3f}\")\n",
    "        print(f\"  75th percentile: {np.percentile(data, 75):.3f}\")\n",
    "\n",
    "print(f\"\\nGlobal vmin: {vmin:.3f}\")\n",
    "print(f\"Global vmax: {vmax:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "time_window = 0.4\n",
    "fs = 2000  # Sampling frequency\n",
    "tanh_norm = True\n",
    "###############\n",
    "\n",
    "def plot_coherogram(time_window, fs, tanh_norm):\n",
    "    if tanh_norm:\n",
    "        suffix ='_normalized'\n",
    "    else:\n",
    "        suffix ='_non-normalized'\n",
    "\n",
    "\n",
    "    all_con_data_df=pd.read_pickle(savepath+'coherence_spectrogram_around_door_dig_truncated_{}{}.pkl'.format(int(time_window*fs),suffix))\n",
    "    event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "    fs=2000\n",
    "    times=np.arange(-1*time_window, time_window, 1/fs)\n",
    "    fig, axs=plt.subplots(2,2, figsize=(20,10), sharey=True)\n",
    "    vmin = all_con_data_df[event_list].applymap(np.min).min().min()\n",
    "    vmax = all_con_data_df[event_list].applymap(np.max).max().max()\n",
    "    event_names=['Around Door','Around Dig']\n",
    "    writer = pd.ExcelWriter(savepath + 'coherogram_values_{}{}.xlsx'.format(int(time_window*fs),suffix))\n",
    "\n",
    "    for i, event in enumerate(event_list):\n",
    "        axs[0,i].imshow(all_con_data_df[event][0], extent=[times[0], times[-1], 1, 100],\n",
    "                    aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        axs[0,i].set_xlabel('')\n",
    "\n",
    "        axs[0,i].set_ylabel('Frequency (Hz)', fontsize=20)\n",
    "        axs[0,i].set_title(event_names[i], fontsize=20)\n",
    "        axs[0,i].vlines(0, 0, 100, color='k', linestyle='--')\n",
    "        axs[0,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "        axs[1,i].imshow(all_con_data_df[event][1], extent=[times[0], times[-1], 1, 100],\n",
    "                    aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        axs[1,i].set_xlabel('Time (s)', fontsize=20)\n",
    "        axs[1,i].set_ylabel('Frequency (Hz)', fontsize=20)\n",
    "        axs[1,i].set_title(event_names[i], fontsize=20)\n",
    "        axs[1,i].vlines(0, 0, 100, color='k', linestyle='--')\n",
    "\n",
    "        axs[0,0].text(-0.2, 0.5, 'Context', transform=axs[0,0].transAxes, fontsize=18, verticalalignment='center', rotation=90)\n",
    "        axs[1,0].text(-0.2, 0.5, 'No Context', transform=axs[1,0].transAxes, fontsize=18, verticalalignment='center', rotation=90)\n",
    "        axs[0,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "        axs[1,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "        axs[0,i].set_xticks(np.arange(-1*time_window, time_window+0.1, time_window))  # Set x-ticks from -1 to 1 seconds\n",
    "        axs[0,i].set_xticklabels(np.arange(-1*time_window, time_window+0.1, time_window))  # Set x-tick labels from -1 to 1 seconds\n",
    "        axs[1,i].set_xticks(np.arange(-1*time_window, time_window+0.1, time_window))  # Set x-ticks from -1 to 1 seconds\n",
    "        axs[1,i].set_xticklabels(np.arange(-1*time_window, time_window+0.1, time_window))  # Set x-tick labels from -1 to 1 seconds\n",
    "\n",
    "        print(all_con_data_df[event][0].shape)\n",
    "        \n",
    "        freqs = [f'{int(freq)}Hz' for freq in np.linspace(1, 100, all_con_data_df[event][0].shape[0])]\n",
    "        freqs.insert(0, 'Frequency (Hz) / Time (s)')\n",
    "        print(len(freqs))\n",
    "        time_points = [f'{np.round(t, 3)}s' for t in np.linspace(-1*time_window, time_window, all_con_data_df[event][0].shape[1])]\n",
    "\n",
    "        df_context = pd.DataFrame(all_con_data_df[event][0])\n",
    "        df_context.loc[-1] = time_points  # Add time points as the first row\n",
    "        df_context.index = df_context.index + 1  # Shift index\n",
    "        df_context = df_context.sort_index()\n",
    "        df_context.insert(0, 'Frequency (Hz)/ Time (s)', freqs)\n",
    "        df_context.to_excel(writer, sheet_name=f'{event_names[i]}_Context', index=False)\n",
    "    \n",
    "        df_nocontext = pd.DataFrame(all_con_data_df[event][1])\n",
    "        df_nocontext.loc[-1] = time_points  # Add time points as the first row\n",
    "        df_nocontext.index = df_nocontext.index + 1  # Shift index\n",
    "        df_nocontext = df_nocontext.sort_index()\n",
    "        df_nocontext.insert(0, 'Frequency (Hz)/ Time (s)', freqs)\n",
    "        df_nocontext.to_excel(writer, sheet_name=f'{event_names[i]}_NoContext', index=False)\n",
    "\n",
    "    writer.close()\n",
    "    # Add a colorbar\n",
    "    cbar = fig.colorbar(axs[0,0].images[0], ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "    cbar.set_label('Coherence', loc='center', fontsize=20, labelpad=10)\n",
    "    cbar.ax.tick_params(labelsize=20)  # Set colorbar tick label size\n",
    "\n",
    "    fig.savefig(savepath + f'\\\\aon_vhp_coherence_event_spectrogram_{int(time_window*fs)}{suffix}.png',format='png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_coherogram(time_window=time_window, fs=fs, tanh_norm=tanh_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Coherogram for each experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "time_window = 0.4\n",
    "fs = 2000  # Sampling frequency\n",
    "tanh_norm = True\n",
    "###############\n",
    "def coherogram_perexperiment_pkl(time_window, fs, tanh_norm):\n",
    "\n",
    "    if tanh_norm:\n",
    "        suffix ='normalized'\n",
    "    else:\n",
    "        suffix ='nonnormalized'\n",
    "\n",
    "\n",
    "\n",
    "    con_data_df_clean=pd.read_pickle(savepath+'mne_epochs_array_df_truncated_{}.pkl'.format(int(time_window*fs)))\n",
    "\n",
    "    event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "\n",
    "    print(event_list)\n",
    "\n",
    "    test_list = [con_data_df_clean.iloc[0]]\n",
    "    mean_con_data=pd.DataFrame()\n",
    "    def epoch_coherogram(epoch, fmin=1, fmax=100, fs=2000):\n",
    "            print(epoch.events.shape)\n",
    "        # if epoch.events.shape[0] < 5:\n",
    "        #     print(\"Not enough events in the epoch\")\n",
    "        #     return None\n",
    "        # else:\n",
    "            freqs = np.arange(fmin, fmax)\n",
    "            n_cycles = freqs / 3\n",
    "            con = mne_connectivity.spectral_connectivity_epochs(epoch, method='coh', sfreq=int(fs),\n",
    "                                                                mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                                                cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False)\n",
    "            coh = con.get_data(output='dense')\n",
    "            indices = con.names\n",
    "            aon_vHp_con = []\n",
    "            for i in range(coh.shape[0]):\n",
    "                for j in range(coh.shape[1]):\n",
    "                    if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                        coherence= coh[i,j,:,:]\n",
    "                        if tanh_norm:\n",
    "                            coherence=np.arctanh(coherence)\n",
    "                        aon_vHp_con.append(coherence)\n",
    "            \n",
    "            mean_con = np.mean(aon_vHp_con, axis=0)\n",
    "            return mean_con\n",
    "    mean_con_data['around_dig_mean_con'] = con_data_df_clean['mne_epoch_around_dig'].apply(epoch_coherogram)\n",
    "    mean_con_data['around_door_mean_con'] = con_data_df_clean['mne_epoch_around_door'].apply(epoch_coherogram)\n",
    "\n",
    "    mean_con_data['experiment'] = con_data_df_clean['experiment']\n",
    "    mean_con_data['date'] = con_data_df_clean['date']\n",
    "    mean_con_data['task'] = con_data_df_clean['task']\n",
    "    mean_con_data['rat_id'] = con_data_df_clean['rat_id']\n",
    "    mean_con_data.dropna(inplace=True)\n",
    "    mean_con_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    mean_con_data.to_pickle(savepath + f'coherence_around_events_mean_{int(time_window*fs)}_{suffix}.pkl')\n",
    "\n",
    "#coherogram_perexperiment_pkl(time_window=time_window, fs=fs, tanh_norm=tanh_norm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "time_window = 0.4\n",
    "fs = 2000  # Sampling frequency\n",
    "tanh_norm = True\n",
    "#################\n",
    "\n",
    "\n",
    "\n",
    "def plot_coherogram_perexperiment(time_window, fs, tanh_norm):\n",
    "    if tanh_norm:\n",
    "        suffix ='normalized'\n",
    "    else:\n",
    "        suffix ='nonnormalized'\n",
    "\n",
    "    mean_con_data=pd.read_pickle(savepath + f'coherence_around_events_mean_{int(time_window*fs)}_{suffix}.pkl')\n",
    "    vmin = mean_con_data['around_dig_mean_con'].apply(np.min).min()\n",
    "    vmax = mean_con_data['around_dig_mean_con'].apply(np.max).max()\n",
    "\n",
    "    BWcontext_data=mean_con_data[(mean_con_data['task']=='BWcontext')]\n",
    "    BWnocontext_data=mean_con_data[(mean_con_data['task']=='BWnocontext')]\n",
    "    task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "    rat_ids, rat_nums = np.unique(BWcontext_data['rat_id'], return_counts=True)\n",
    "    print(rat_ids, rat_nums)\n",
    "    rat_nums_max = rat_nums.max()\n",
    "    print(rat_nums_max)\n",
    "    import matplotlib.pyplot as plt\n",
    "    writer = pd.ExcelWriter(savepath + 'coherogram_perexperiment_{}_{}.xlsx'.format(int(time_window*fs),suffix))\n",
    "\n",
    "    for group_name, group_df in task_data_dict.items():\n",
    "        print(f\"Plotting group: {group_name}\")\n",
    "        group_dict = {'BWcontext': 'Context', 'BWnocontext': 'No Context'}\n",
    "        rat_ids, rat_nums = np.unique(group_df['rat_id'], return_counts=True)\n",
    "        rat_nums_max = rat_nums.max()\n",
    "\n",
    "        num_of_rows = 4 # Each row should be a rats\n",
    "        num_of_cols = rat_nums_max # Each column should be the max number of experiments for a rat\n",
    "\n",
    "        fig, axs = plt.subplots(num_of_rows, num_of_cols, figsize=(25, 10), sharex=True, sharey=True)\n",
    "        dk1_count = 0\n",
    "        dk3_count = 0\n",
    "        dk5_count = 0\n",
    "        dk6_count = 0\n",
    "        for i, (idx, row) in enumerate(group_df.iterrows()):\n",
    "            rat_id = row['rat_id']\n",
    "            data = np.array(row['around_dig_mean_con'])\n",
    "            if rat_id == 'dk1':\n",
    "                ax=axs[0, dk1_count]\n",
    "                dk1_count += 1\n",
    "            elif rat_id == 'dk3':\n",
    "                ax=axs[1, dk3_count]\n",
    "                dk3_count += 1\n",
    "            elif rat_id == 'dk5':\n",
    "                ax=axs[2, dk5_count]\n",
    "                dk5_count += 1\n",
    "            elif rat_id == 'dk6':\n",
    "                ax=axs[3, dk6_count]\n",
    "                dk6_count += 1\n",
    "            im = ax.imshow(data, extent=[-1*time_window, time_window, 1, 100], aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "            ax.set_title(f\"{row['rat_id']} {row['date']}\")\n",
    "            ax.axvline(0, color='k', linestyle='--', linewidth=2)\n",
    "            ax.axhline(12, color='green', linestyle='--')\n",
    "            ax.axhline(30, color='green', linestyle='--')\n",
    "\n",
    "            ##### Writing to excel\n",
    "\n",
    "            freqs = [f'{int(freq)}Hz' for freq in np.linspace(1, 100, data.shape[0])]\n",
    "            freqs.insert(0, 'Frequency (Hz) / Time (s)')\n",
    "            print(len(freqs))\n",
    "            time_points = [f'{np.round(t, 3)}s' for t in np.linspace(-1*time_window, time_window, data.shape[1])]\n",
    "\n",
    "            df_towrite = pd.DataFrame(data)\n",
    "            df_towrite.loc[-1] = time_points  # Add time points as the first row\n",
    "            df_towrite.index = df_towrite.index + 1  # Shift index\n",
    "            df_towrite = df_towrite.sort_index()\n",
    "            df_towrite.insert(0, 'Frequency (Hz)/ Time (s)', freqs)\n",
    "            df_towrite.to_excel(writer, sheet_name=f'{group_dict[group_name]}_{rat_id}_{row[\"date\"]}', index=False)\n",
    "\n",
    "        for j in range(i + 1, len(axs)):\n",
    "            fig.delaxes(axs[j])\n",
    "        fig.suptitle(f\"{group_dict[group_name]} Coherence {suffix} Around Dig\", fontsize=16)\n",
    "        fig.colorbar(im, ax=axs, orientation='vertical', fraction=0.02, label=f'Coherence {suffix}')\n",
    "        fig.savefig(savepath + f'{group_name}_coherogram_per_experiment_around_dig_{int(time_window*fs)}_{suffix}.png', dpi=300, bbox_inches='tight')\n",
    "        #plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.show()\n",
    "    writer.close()\n",
    "#plot_coherogram_perexperiment(time_window=time_window, fs=fs, tanh_norm=tanh_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Coherograms of single trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 0.7  # seconds\n",
    "fs = 2000  # Sampling frequency\n",
    "single_epochs_df=pd.read_pickle(savepath+f'behavior_coherence_single_epochs_mne_truncated_{int(time_window*fs)}.pkl')\n",
    "unique_id_list = single_epochs_df['unique_id'].unique()\n",
    "\n",
    "print(unique_id_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_id_list = unique_id_list[0:1]\n",
    "for unique_id in unique_id_list:\n",
    "    unique_id_df = single_epochs_df[single_epochs_df['unique_id'] == unique_id]\n",
    "    trial_nums = len(unique_id_df['trial'].unique())\n",
    "    fig, axs = plt.subplots(8, trial_nums, figsize=(20, 10), sharex=True)\n",
    "    fig.suptitle(f'Unique ID: {unique_id} - AON-vHp Coherence Around Dig', fontsize=16)\n",
    "    for trial_idi in unique_id_df['trial'].unique():\n",
    "        trial_df = unique_id_df[unique_id_df['trial'] == trial_idi]\n",
    "        mne_epoch_around_dig = trial_df['around_dig'].iloc[0]\n",
    "        fmin=1\n",
    "        fmax=100\n",
    "        freqs = np.arange(fmin, fmax)\n",
    "        n_cycles = freqs / 3\n",
    "        con = mne_connectivity.spectral_connectivity_epochs(mne_epoch_around_dig, method='coh', sfreq=int(fs),\n",
    "                                                mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                                cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False, n_jobs=-1)\n",
    "        coh = con.get_data(output='dense')\n",
    "        indices = con.names\n",
    "        aon_vHp_con = []\n",
    "        channel_pair =0\n",
    "        for i in range(coh.shape[0]):\n",
    "            for j in range(coh.shape[1]):\n",
    "                if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                    coherence= coh[i,j,:,:]\n",
    "                    coherence=np.arctanh(coherence)\n",
    "                    aon_vHp_con.append(coherence)\n",
    "                    axs[channel_pair, trial_idi].imshow(coherence, extent=[-time_window, time_window, 1, 100], aspect='auto', origin='lower')\n",
    "                    if channel_pair == 0:\n",
    "                        axs[channel_pair, trial_idi].set_title(f'Trial {trial_idi}')\n",
    "                    if trial_idi == 0:\n",
    "                        axs[channel_pair, trial_idi].set_ylabel(f'{indices[i]}-{indices[j]}')\n",
    "                    channel_pair += 1\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Coherence Boxplots "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Aon-vHp connectivity per band and storing it in pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "time_window = 0.7\n",
    "fs = 2000  # Sampling frequency\n",
    "############\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "importlib.reload(coherence_functions)\n",
    "\n",
    "\n",
    "con_data_df_clean=pd.read_pickle(savepath+f'mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "single_baseline_epoch=con_data_df_clean['mne_epoch_door_before'].iloc[0]\n",
    "theta_band=[4,8]\n",
    "\n",
    "theta_coherence=coherence_functions.convert_epoch_to_coherence_time(single_baseline_epoch)\n",
    "print(theta_coherence)\n",
    "\n",
    "print(coherence_functions.convert_epoch_to_coherence_mt(single_baseline_epoch, tanh_norm=True))\n",
    "\n",
    "\n",
    "def convert_epoch_to_coherence_mt_per_channel(epoch, tanh_norm=True, fmin=1, fmax=100, fs=2000):\n",
    "    band_dict={'beta':[12,30],'gamma':[30,80],'total':[1,100], 'theta':[4,12]}\n",
    "    coherence_dict={}\n",
    "    coherence_channel_dict={}\n",
    "    for band in band_dict.keys():\n",
    "        \n",
    "        fmin=band_dict[band][0]\n",
    "        fmax=band_dict[band][1]\n",
    "        freqs = np.arange(fmin,fmax)\n",
    "        #print(n_cycles)\n",
    "        con=mne_connectivity.spectral_connectivity_epochs(epoch, method='coh', sfreq=int(2000), fmin=fmin, fmax=fmax,faverage=True, mode='multitaper',mt_bandwidth = 2.8,mt_adaptive=True, mt_low_bias=True, verbose=False, n_jobs=-1)\n",
    "        coh = con.get_data(output='dense')\n",
    "        #print(coh)\n",
    "        indices = con.names\n",
    "        #print(indices)\n",
    "        aon_vhp_con=[]\n",
    "        print(coh.shape)\n",
    "        channel_dict={}\n",
    "        for i in range(coh.shape[0]):\n",
    "            for j in range(coh.shape[1]):\n",
    "                #print(i,j)\n",
    "                if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                    print('AON and vHp found')\n",
    "                    coherence = coh[i,j,:]\n",
    "                    if tanh_norm:\n",
    "                        coherence=np.arctanh(coherence)  # Convert to Fisher Z-score\n",
    "                    channel_dict[f'{indices[i]}-{indices[j]}']=coherence\n",
    "                    \n",
    "                    aon_vhp_con.append(np.mean(coherence))\n",
    "                    #print('freqs averaged',coh[i,j,0,:].shape)\n",
    "                    #print(coh[0,i,j,:])\n",
    "                else:\n",
    "                    continue\n",
    "        if aon_vhp_con==[]:\n",
    "            print('no coherence found')\n",
    "        else:\n",
    "            #print(aon_vhp_con)\n",
    "            aon_vhp_con_mean=np.mean(aon_vhp_con, axis=0)\n",
    "            #print(aon_vhp_con_mean, 'coherenece')\n",
    "            coherence_dict[band]=aon_vhp_con_mean\n",
    "            coherence_channel_dict[band]=channel_dict\n",
    "    return coherence_dict, coherence_channel_dict\n",
    "\n",
    "single_baseline_epoch=con_data_df_clean['mne_epoch_door_before'].iloc[0]\n",
    "band_coherence, channel_coherence=convert_epoch_to_coherence_mt_per_channel(single_baseline_epoch, tanh_norm=True)\n",
    "print(band_coherence)\n",
    "print(channel_coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def epoch_coherence_channelpair_multiple(time_window, fs, tanh_norm=True):\n",
    "    \"\"\"\n",
    "    Process multiple epoch columns and return coherence DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    time_window : float\n",
    "        Time window in seconds\n",
    "    fs : int\n",
    "        Sampling frequency\n",
    "    tanh_norm : bool\n",
    "        Whether to apply Fisher Z-transformation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Coherence DataFrame with event types\n",
    "    \n",
    "    \"\"\"\n",
    "    if tanh_norm:\n",
    "        suffix ='normalized'\n",
    "    else:\n",
    "        suffix ='nonnormalized'\n",
    "\n",
    "    con_data_df_clean = pd.read_pickle(savepath + f'mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n",
    "    con_data_df_shuffled = pd.read_pickle(savepath + f'mne_epochs_array_df_shuffled_truncated_{int(time_window*fs)}.pkl')\n",
    "    columns_to_process = ['mne_epoch_door_before', 'mne_epoch_door_after', 'mne_epoch_dig_before', 'mne_epoch_dig_after']\n",
    "\n",
    "    coherence_df = coherence_functions.convert_epochs_to_coherence_mt_expanded(\n",
    "        \n",
    "        con_data_df_clean[columns_to_process + ['rat_id', 'task']],  # Include necessary columns\n",
    "        con_data_df_clean['rat_id'], \n",
    "        con_data_df_clean['task'],\n",
    "        columns_to_process,\n",
    "        tanh_norm=tanh_norm\n",
    "    )\n",
    "    shuffled_coherence_df = coherence_functions.convert_epochs_to_coherence_mt_expanded(\n",
    "        con_data_df_shuffled[columns_to_process + ['rat_id', 'task']],  # Include necessary columns\n",
    "        con_data_df_shuffled['rat_id'], \n",
    "        con_data_df_shuffled['task'],\n",
    "        columns_to_process,\n",
    "        tanh_norm=tanh_norm\n",
    "    )\n",
    "\n",
    "    coherence_df.to_pickle(savepath + f'coherence_channelpair_{int(time_window*fs)}_{suffix}.pkl')\n",
    "    shuffled_coherence_df.to_pickle(savepath + f'coherence_channelpair_shuffled_{int(time_window*fs)}_{suffix}.pkl')\n",
    "\n",
    "def plot_coherence_channelpair(time_window, fs, tanh_norm=True):\n",
    "    if tanh_norm:\n",
    "        suffix ='normalized'\n",
    "    else:\n",
    "        suffix ='nonnormalized'\n",
    "    coherence_df = pd.read_pickle(savepath + f'coherence_channelpair_{int(time_window*fs)}_{suffix}.pkl')\n",
    "    shuffled_coherence_df = pd.read_pickle(savepath + f'coherence_channelpair_shuffled_{int(time_window*fs)}_{suffix}.pkl')\n",
    "    generate_events_boxplots(time_window, fs, suffix, coherence_df)\n",
    "    generate_events_boxplots(time_window, fs, suffix+'_shuffled', shuffled_coherence_df)\n",
    "\n",
    "\n",
    "def generate_events_boxplots(time_window, fs, suffix, coherence_df):\n",
    "    event_dict = {\n",
    "        'mne_epoch_door_before': 'Door Before',\n",
    "        'mne_epoch_door_after': 'Door After',\n",
    "        'mne_epoch_dig_before': 'Dig Before',\n",
    "        'mne_epoch_dig_after': 'Dig After'\n",
    "    }\n",
    "    coherence_df['event_type'] = coherence_df['event_type'].map(event_dict)\n",
    "    \n",
    "    vmin = coherence_df['coherence'].min()\n",
    "    vmax = coherence_df['coherence'].max()\n",
    "    print(f\"Global vmin: {vmin}, vmax: {vmax}\")\n",
    "\n",
    "    event_types = coherence_df['event_type'].unique()\n",
    "    num_event_types = len(event_types)\n",
    "    writer=pd.ExcelWriter(savepath + f'\\\\events_coherence_per_band_channelpair_{int(time_window*fs)}_{suffix}.xlsx')\n",
    "\n",
    "    fig, axs = plt.subplots(1, num_event_types, figsize=(40,10), sharey=True)\n",
    "    task_dict = {'BWcontext': 'Context', 'BWnocontext': 'No Context'}\n",
    "    band_order = ['theta', 'beta', 'gamma', 'total']\n",
    "    \n",
    "    for i, event_type in enumerate(event_types):\n",
    "        ax=axs[i] if num_event_types > 1 else axs\n",
    "        event_data_df_melted = coherence_df[coherence_df['event_type'] == event_type]\n",
    "        event_data_df_melted['band'] = pd.Categorical(event_data_df_melted['frequency_band'], categories=band_order, ordered=True)\n",
    "        sns.barplot(x='band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=event_data_df_melted, order=band_order, legend=True, ax=axs[i])\n",
    "        sns.stripplot(x='band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=event_data_df_melted, order=band_order, dodge=True, edgecolor='black', linewidth=1, jitter=True, legend=False, ax=axs[i])\n",
    "        #axs[i].set_xticklabels(['Total', 'Theta', 'Beta', 'Gamma'])\n",
    "        handles, labels = axs[i].get_legend_handles_labels()\n",
    "        axs[i].legend(handles, [task_dict[l] for l in labels], loc='upper right', fontsize=15)\n",
    "        axs[i].set_title(f'{event_type}', fontsize=20)\n",
    "        if i == 0:\n",
    "            axs[i].set_ylabel(f'Coherence ({suffix})', fontsize=20)\n",
    "        else:\n",
    "            axs[i].set_ylabel('')\n",
    "        axs[i].set_xlabel('')\n",
    "        axs[i].tick_params(axis='both', which='major', labelsize=20)\n",
    "        event_data_df_melted.drop(columns=['event_type'], inplace=True)\n",
    "        event_data_df_melted.rename(columns={'frequency_band': 'band', 'epoch_idx': 'experiment'}, inplace=True)\n",
    "        event_data_df_melted.to_excel(writer, sheet_name=event_type)\n",
    "\n",
    "    writer.close()\n",
    "    plt.suptitle(f'AON-vHp Coherence per Channel Pair ({suffix})', fontsize=18)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    fig.savefig(savepath + f'coherence_channelpair_{int(time_window*fs)}_{suffix}.png', dpi=300)\n",
    "    plt.show()\n",
    "# Usage example:\n",
    "time_window = 0.4\n",
    "fs = 2000\n",
    "tanh_norm = True\n",
    "\n",
    "# Specify which columns to process\n",
    "\n",
    "# Process multiple columns\n",
    "epoch_coherence_channelpair_multiple(\n",
    "    time_window=time_window, \n",
    "    fs=fs, \n",
    "    tanh_norm=tanh_norm\n",
    ")\n",
    "plot_coherence_channelpair(\n",
    "    time_window=time_window, \n",
    "    fs=fs, \n",
    "    tanh_norm=tanh_norm\n",
    ")  \n",
    "\n",
    "# print(\"Coherence DataFrame shape:\", coherence_df.shape)\n",
    "# print(\"Coherence DataFrame columns:\", coherence_df.columns.tolist())\n",
    "# print(\"Sample output:\")\n",
    "# print(coherence_df.head())\n",
    "# print(\"\\nEvent types:\")\n",
    "# print(coherence_df['event_type'].unique())\n",
    "# print(\"\\nFrequency bands:\")\n",
    "# print(coherence_df['frequency_band'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "\n",
    "time_window = 0.4\n",
    "fs=2000\n",
    "tanh_norm = False\n",
    "##############\n",
    "\n",
    "\n",
    "\n",
    "def coherence_boxplot_pkl(time_window, fs, tanh_norm):\n",
    "\n",
    "    importlib.reload(coherence_functions)\n",
    "\n",
    "    if tanh_norm:\n",
    "        suffix ='normalized'\n",
    "    else:\n",
    "        suffix ='nonnormalized'\n",
    "\n",
    "    con_data_df_clean=pd.read_pickle(savepath+f'mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "    con_data_df_clean['coherence_door_before']=con_data_df_clean['mne_epoch_door_before'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_mt(x, tanh_norm=tanh_norm))\n",
    "    con_data_df_clean['coherence_door_after']=con_data_df_clean['mne_epoch_door_after'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_mt(x, tanh_norm=tanh_norm))\n",
    "    con_data_df_clean['coherence_dig_before']=con_data_df_clean['mne_epoch_dig_before'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_mt(x, tanh_norm=tanh_norm))\n",
    "    con_data_df_clean['coherence_dig_after']=con_data_df_clean['mne_epoch_dig_after'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_mt(x, tanh_norm=tanh_norm))\n",
    "    con_data_df_clean.drop(columns=['mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after'], inplace=True)\n",
    "    con_data_df_clean.to_pickle(savepath+f'coherence_boxplot_mt_{int(fs*time_window)}_{suffix}.pkl')\n",
    "\n",
    "    con_data_df_clean=pd.read_pickle(savepath+f'mne_epochs_array_df_shuffled_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "    con_data_df_clean['coherence_door_before']=con_data_df_clean['mne_epoch_door_before'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_mt(x,tanh_norm=tanh_norm))\n",
    "    con_data_df_clean['coherence_door_after']=con_data_df_clean['mne_epoch_door_after'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_mt(x,tanh_norm=tanh_norm))\n",
    "    con_data_df_clean['coherence_dig_before']=con_data_df_clean['mne_epoch_dig_before'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_mt(x,tanh_norm=tanh_norm))\n",
    "    con_data_df_clean['coherence_dig_after']=con_data_df_clean['mne_epoch_dig_after'].apply(lambda x: coherence_functions.convert_epoch_to_coherence_mt(x,tanh_norm=tanh_norm))\n",
    "    con_data_df_clean.drop(columns=['mne_epoch_door_before','mne_epoch_door_after','mne_epoch_dig_before','mne_epoch_dig_after'], inplace=True)\n",
    "    con_data_df_clean.to_pickle(savepath+f'coherence_boxplot_mt_shuffled_{int(fs*time_window)}_{suffix}.pkl')\n",
    "\n",
    "coherence_boxplot_pkl(time_window=time_window, fs=fs, tanh_norm=tanh_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "\n",
    "time_window = 0.4\n",
    "fs=2000\n",
    "tanh_norm = False\n",
    "###################\n",
    "def plot_coherence_boxplot(time_window, fs, tanh_norm):\n",
    "    importlib.reload(coherence_functions)\n",
    "    if tanh_norm:\n",
    "        suffix ='normalized'\n",
    "    else:\n",
    "        suffix ='nonnormalized'\n",
    "    print(suffix)\n",
    "\n",
    "    con_data_df_clean=pd.read_pickle(savepath+f'coherence_boxplot_mt_{int(fs*time_window)}_{suffix}.pkl')\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(40, 10), sharey=True)\n",
    "    fig.suptitle(f'Coherence {time_window}s', fontsize=24)\n",
    "    axs = axs.flatten()\n",
    "    for ax in axs:\n",
    "        ax.tick_params(axis='y', which='both', labelleft=True)\n",
    "    writer=pd.ExcelWriter(savepath + f'\\\\events_coherence_per_band_{int(time_window*fs)}_{suffix}.xlsx')\n",
    "    events_dict={'coherence_door_before':'Pre Door', 'coherence_door_after': 'Post Door', 'coherence_dig_before':'Pre Dig', 'coherence_dig_after':'Post Dig'}\n",
    "    task_dict={'BWcontext':'Context','BWnocontext':'No Context'}\n",
    "    band_order = ['theta', 'beta', 'gamma', 'total']\n",
    "    for i, event in enumerate(events_dict.keys()):\n",
    "        event_data = con_data_df_clean[event]\n",
    "        event_data_df = pd.DataFrame(event_data.tolist())\n",
    "        event_data_df.reset_index(drop=True, inplace=True)\n",
    "        event_data_df['rat_id'] = con_data_df_clean['rat_id'].reset_index(drop=True)\n",
    "        event_data_df['task'] = con_data_df_clean['task'].reset_index(drop=True)\n",
    "        event_data_df_melted = pd.melt(event_data_df, id_vars=['rat_id', 'task'], value_vars=band_order, var_name='band', value_name='coherence')\n",
    "        sns.barplot(x='band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=event_data_df_melted, legend=True, ax=axs[i])\n",
    "        sns.stripplot(x='band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=event_data_df_melted, dodge=True, edgecolor='black', linewidth=1, jitter=True, legend=False, ax=axs[i])\n",
    "        #axs[i].set_xticklabels(['Total', 'Theta', 'Beta', 'Gamma'])\n",
    "        handles, labels = axs[i].get_legend_handles_labels()\n",
    "        axs[i].legend(handles, [task_dict[l] for l in labels], loc='upper right', fontsize=15)\n",
    "        axs[i].set_title(f'{events_dict[event]}', fontsize=20)\n",
    "        if i == 0:\n",
    "            axs[i].set_ylabel(f'Coherence ({suffix})', fontsize=20)\n",
    "        else:\n",
    "            axs[i].set_ylabel('')\n",
    "        axs[i].set_xlabel('')\n",
    "        axs[i].tick_params(axis='both', which='major', labelsize=20)\n",
    "        event_data_df_melted.to_excel(writer, sheet_name=event)\n",
    "    writer.close()\n",
    "    fig.savefig(savepath+f'events_coherence_per_band_{int(time_window*fs)}_{suffix}.png', format='png',dpi=300, bbox_inches='tight')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \"\"\"Shuffled coherence boxplot per band\"\"\"\n",
    "\n",
    "\n",
    "    con_data_df_clean=pd.read_pickle(savepath+f'coherence_boxplot_mt_shuffled_{int(fs*time_window)}_{suffix}.pkl')\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(40, 10), sharey=True)\n",
    "    fig.suptitle(f'Shuffled Coherence {time_window}s', fontsize=24)\n",
    "    axs = axs.flatten()\n",
    "    for ax in axs:\n",
    "        ax.tick_params(axis='y', which='both', labelleft=True)\n",
    "    writer=pd.ExcelWriter(savepath + f'\\\\events_coherence_per_band_shuffled_{int(time_window*fs)}_{suffix}.xlsx')\n",
    "    events_dict={'coherence_door_before':'Pre Door', 'coherence_door_after': 'Post Door', 'coherence_dig_before':'Pre Dig', 'coherence_dig_after':'Post Dig'}\n",
    "    for i, event in enumerate(events_dict.keys()):\n",
    "        event_data = con_data_df_clean[event]\n",
    "        event_data_df = pd.DataFrame(event_data.tolist())\n",
    "        event_data_df.reset_index(drop=True, inplace=True)\n",
    "        event_data_df['rat_id'] = con_data_df_clean['rat_id'].reset_index(drop=True)\n",
    "        event_data_df['task'] = con_data_df_clean['task'].reset_index(drop=True)\n",
    "        event_data_df_melted = pd.melt(event_data_df, id_vars=['rat_id', 'task'], value_vars=band_order, var_name='band', value_name='coherence')\n",
    "        sns.barplot(x='band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=event_data_df_melted, legend=True, ax=axs[i])\n",
    "        sns.stripplot(x='band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=event_data_df_melted, dodge=True, edgecolor='black', linewidth=1, jitter=True, legend=False, ax=axs[i])\n",
    "        #axs[i].set_xticklabels(['Total', 'Theta', 'Beta', 'Gamma'])\n",
    "        handles, labels = axs[i].get_legend_handles_labels()\n",
    "        axs[i].legend(handles, [task_dict[l] for l in labels], loc='upper right', fontsize=15)\n",
    "\n",
    "        axs[i].set_title(f'{events_dict[event]}', fontsize=20)\n",
    "        if i == 0:\n",
    "            axs[i].set_ylabel(f'Coherence ({suffix})', fontsize=20)\n",
    "        else:\n",
    "            axs[i].set_ylabel('')\n",
    "        axs[i].set_xlabel('')\n",
    "        axs[i].tick_params(axis='both', which='major', labelsize=20)\n",
    "        event_data_df_melted.to_excel(writer, sheet_name=event)\n",
    "    writer.close()\n",
    "    fig.savefig(savepath+f'events_coherence_per_band_shuffled_{int(time_window*fs)}_{suffix}.png', format='png',dpi=300, bbox_inches='tight')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_coherence_boxplot(time_window=time_window, fs=fs, tanh_norm=tanh_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.anova import AnovaRM\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "\n",
    "con_data_df_clean=pd.read_pickle(savepath+f'coherence_boxplot_mt_shuffled_{int(fs*time_window)}.pkl')\n",
    "fig, axs = plt.subplots(1, 4, figsize=(40, 10), sharey=True)\n",
    "axs = axs.flatten()\n",
    "for ax in axs:\n",
    "    ax.tick_params(axis='y', which='both', labelleft=True)\n",
    "writer=pd.ExcelWriter(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\coherence_boxplot_mt_shuffled_{int(fs*time_window)}.xlsx')\n",
    "events_dict={'coherence_door_before':'Pre Door', 'coherence_door_after': 'Post Door', 'coherence_dig_before':'Pre Dig', 'coherence_dig_after':'Post Dig'}\n",
    "for i, event in enumerate(events_dict.keys()):\n",
    "    event_data = con_data_df_clean[event]\n",
    "    event_data_df = pd.DataFrame(event_data.tolist())\n",
    "    event_data_df.reset_index(drop=True, inplace=True)\n",
    "    event_data_df['rat_id'] = con_data_df_clean['rat_id'].reset_index(drop=True)\n",
    "    event_data_df['task'] = con_data_df_clean['task'].reset_index(drop=True)\n",
    "    event_data_df_melted = pd.melt(event_data_df, id_vars=['rat_id', 'task'], value_vars=['total', 'theta', 'beta', 'gamma'], var_name='band', value_name='coherence')\n",
    "    sns.barplot(x='band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=event_data_df_melted, legend=True, ax=axs[i])\n",
    "    sns.stripplot(x='band', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=event_data_df_melted, dodge=True, edgecolor='black', linewidth=1, jitter=True, legend=False, ax=axs[i])\n",
    "    #axs[i].set_xticklabels(['Total', 'Theta', 'Beta', 'Gamma'])\n",
    "    axs[i].legend(title='Task', fontsize=20, title_fontsize=20, loc='upper right')\n",
    "    \n",
    "    axs[i].set_title(f'{events_dict[event]}', fontsize=20)\n",
    "    if i == 0:\n",
    "        axs[i].set_ylabel('Coherence (A.U.)', fontsize=20)\n",
    "    else:\n",
    "        axs[i].set_ylabel('')\n",
    "    axs[i].set_xlabel('')\n",
    "    axs[i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    event_data_df_melted.to_excel(writer, sheet_name=event)\n",
    "    import statsmodels.api as sm\n",
    "\n",
    "    # Prepare data for repeated measures ANOVA\n",
    "    # Each rat_id is a subject, band is within-subject, task is between-subject\n",
    "    anova_results = {}\n",
    "    posthoc_results = {}\n",
    "\n",
    "    # Only keep rats that have both tasks for proper repeated measures\n",
    "    rats_with_both = event_data_df_melted.groupby('rat_id')['task'].nunique()\n",
    "    rats_with_both = rats_with_both[rats_with_both == 2].index.tolist()\n",
    "    filtered_df = event_data_df_melted[event_data_df_melted['rat_id'].isin(rats_with_both)]\n",
    "\n",
    "    # Pivot to wide format for repeated measures ANOVA\n",
    "    for band in ['total', 'theta', 'beta', 'gamma']:\n",
    "        band_df = filtered_df[filtered_df['band'] == band]\n",
    "        # ANOVA: repeated measures on band, between on task\n",
    "        # For each rat, we need both tasks\n",
    "        # We'll use a mixed-effects model for repeated measures\n",
    "        model = ols('coherence ~ C(task)', data=band_df).fit()\n",
    "        aov_table = sm.stats.anova_lm(model, typ=2)\n",
    "        anova_results[band] = aov_table\n",
    "\n",
    "        # Posthoc: LSD (least significant difference) test\n",
    "        mc = MultiComparison(band_df['coherence'], band_df['task'])\n",
    "        posthoc = mc.tukeyhsd()  # Tukey is more conservative, but LSD is not directly available in statsmodels\n",
    "        posthoc_results[band] = posthoc.summary()\n",
    "        print(f\"ANOVA results for {band} band in {events_dict[event]}\")\n",
    "        print(aov_table)\n",
    "        print(f\"Posthoc (Tukey HSD) results for {band} band:\")\n",
    "        print(posthoc.summary())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherogram and Boxplots together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs=2000\n",
    "for time_window in [0.4, 0.7]:\n",
    "    for tanh_norm in [True, False]:\n",
    "\n",
    "        # coherogram_pkl(time_window=time_window, fs=fs, tanh_norm=tanh_norm)\n",
    "        # plot_coherogram(time_window=time_window, fs=fs, tanh_norm=tanh_norm)\n",
    "        \n",
    "        # coherence_boxplot_pkl(time_window=time_window, fs=fs, tanh_norm=tanh_norm)\n",
    "        # plot_coherence_boxplot(time_window=time_window, fs=fs, tanh_norm=tanh_norm)\n",
    "\n",
    "        # coherogram_perexperiment_pkl(time_window=time_window, fs=fs, tanh_norm=tanh_norm)\n",
    "        # plot_coherogram_perexperiment(time_window=time_window, fs=fs, tanh_norm=tanh_norm)\n",
    "\n",
    "        epoch_coherence_channelpair_multiple(time_window=time_window, fs=fs, tanh_norm=tanh_norm)\n",
    "        plot_coherence_channelpair(time_window=time_window, fs=fs, tanh_norm=tanh_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting AON-vHp connectivity separated by Bands ## [NOT USED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df_clean=pd.read_pickle(savepath+'coherence_boxplot_per_event_per_band_single_value.pkl')\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20, 10))\n",
    "axs = axs.flatten()\n",
    "writer = pd.ExcelWriter(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\coherence_band_per_event.xlsx')\n",
    "\n",
    "bands = ['total', 'theta', 'beta', 'gamma']\n",
    "events = ['coherence_door_before', 'coherence_door_after', 'coherence_dig_before', 'coherence_dig_after']\n",
    "\n",
    "for i, band in enumerate(bands):\n",
    "    band_data = []\n",
    "    for event in events:\n",
    "        event_data = con_data_df_clean[event]\n",
    "        event_data_df = pd.DataFrame(event_data.tolist())\n",
    "        event_data_df.reset_index(drop=True, inplace=True)\n",
    "        event_data_df['rat_id'] = con_data_df_clean['rat_id'].reset_index(drop=True)\n",
    "        event_data_df['task'] = con_data_df_clean['task'].reset_index(drop=True)\n",
    "        event_data_df['event'] = event\n",
    "        event_data_df['band'] = band\n",
    "        event_data_df['coherence'] = event_data_df[band]\n",
    "        band_data.append(event_data_df[['rat_id', 'task', 'event', 'band', 'coherence']])\n",
    "    \n",
    "    band_data_df = pd.concat(band_data, ignore_index=True)\n",
    "    sns.boxplot(x='event', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=band_data_df, showfliers=False, legend=False, ax=axs[i])\n",
    "    sns.stripplot(x='event', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=band_data_df, dodge=True, edgecolor='black', linewidth=1, jitter=True, legend=False, ax=axs[i])\n",
    "    axs[i].set_xticklabels(['Door Before', 'Door After', 'Dig Before', 'Dig After'], rotation=0)\n",
    "    axs[i].set_title(band.capitalize())\n",
    "    axs[i].set_ylabel('Coherence')\n",
    "    axs[i].set_xlabel('')\n",
    "    band_data_df.to_excel(writer, sheet_name=band)\n",
    "\n",
    "writer.close()\n",
    "\n",
    "# Create custom legend handles and labels\n",
    "from matplotlib.lines import Line2D\n",
    "colors = {'BWnocontext': '#ff7f0e', 'BWcontext': '#1f77b4'}\n",
    "\n",
    "handles = [\n",
    "    Line2D([0], [0], color=colors['BWcontext'], marker='o', linestyle='', markersize=10, label='BWcontext'),\n",
    "    Line2D([0], [0], color=colors['BWnocontext'], marker='o', linestyle='', markersize=10, label='BWnocontext')\n",
    "]\n",
    "\n",
    "# Add the custom legend to the figure\n",
    "fig.legend(handles=handles, loc='upper right', bbox_to_anchor=(1.1, 1), title='Task')\n",
    "fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\coherence_band_per_event.png', dpi=600)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same boxplot as above but for a single band ## [NOT USED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df_clean=pd.read_pickle(savepath+'coherence_boxplot_per_event_per_band_single_value.pkl')\n",
    "\n",
    "fig, axs = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "bands = ['beta']\n",
    "events = ['coherence_door_before', 'coherence_door_after', 'coherence_dig_before', 'coherence_dig_after']\n",
    "\n",
    "for i, band in enumerate(bands):\n",
    "    band_data = []\n",
    "    for event in events:\n",
    "        event_data = con_data_df_clean[event]\n",
    "        event_data_df = pd.DataFrame(event_data.tolist())\n",
    "        event_data_df.reset_index(drop=True, inplace=True)\n",
    "        event_data_df['rat_id'] = con_data_df_clean['rat_id'].reset_index(drop=True)\n",
    "        event_data_df['task'] = con_data_df_clean['task'].reset_index(drop=True)\n",
    "        event_data_df['event'] = event\n",
    "        event_data_df['band'] = band\n",
    "        event_data_df['coherence'] = event_data_df[band]\n",
    "        band_data.append(event_data_df[['rat_id', 'task', 'event', 'band', 'coherence']])\n",
    "    \n",
    "    band_data_df = pd.concat(band_data, ignore_index=True)\n",
    "    sns.boxplot(x='event', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=band_data_df, showfliers=False, legend=False, ax=axs)\n",
    "    sns.stripplot(x='event', y='coherence', hue='task', hue_order=['BWcontext', 'BWnocontext'], data=band_data_df, dodge=True, edgecolor='black', linewidth=1, jitter=True, legend=False, ax=axs)\n",
    "    axs.set_xticklabels(['Pre Door', 'Post Door', 'Pre Dig', 'Post Dig'], rotation=0)\n",
    "    axs.set_title(band.capitalize()+' Band Coherence between AON and vHp', fontsize=20)\n",
    "    \n",
    "    axs.set_ylabel('Coherence', fontsize=20)\n",
    "    axs.set_xlabel('Behavior Events', fontsize=20)\n",
    "    axs.tick_params(axis='both', which='major', labelsize=20)\n",
    "    axs.tick_params(axis='both', which='minor', labelsize=20)\n",
    "    #axs.legend(title='', fontsize=20, loc='upper right' )\n",
    "# # Create custom legend handles and labels\n",
    "from matplotlib.lines import Line2D\n",
    "colors = {'BWnocontext': '#ff7f0e', 'BWcontext': '#1f77b4'}\n",
    "\n",
    "handles = [\n",
    "    Line2D([0], [0], color=colors['BWcontext'], marker='o', linestyle='', markersize=10, label='Context'),\n",
    "    Line2D([0], [0], color=colors['BWnocontext'], marker='o', linestyle='', markersize=10, label='No Context')\n",
    "]\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add the custom legend to the figure\n",
    "fig.legend(handles=handles, loc='upper right', bbox_to_anchor=(0.4, 0.95), title='', fontsize=20, ncol=1)\n",
    "fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\coherence_beta_band_per_event.png', dpi=600, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase Based Connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating phase coherograms for each experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "time_window = 0.7\n",
    "fs=2000\n",
    "#############\n",
    "\n",
    "con_data_df_clean=pd.read_pickle(savepath+'mne_epochs_array_df_truncated_{}.pkl'.format(int(time_window*fs)))\n",
    "\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "\n",
    "print(event_list)\n",
    "\n",
    "test_list = [con_data_df_clean.iloc[0]]\n",
    "mean_con_data=pd.DataFrame()\n",
    "def epoch_coherogram(epoch, fmin=1, fmax=100, fs=2000):\n",
    "    print(epoch.events.shape)\n",
    "    if epoch.events.shape[0] < 5:\n",
    "        print(\"Not enough events in the epoch\")\n",
    "        return None\n",
    "    else:\n",
    "        freqs = np.arange(fmin, fmax)\n",
    "        n_cycles = freqs / 3\n",
    "        con = mne_connectivity.spectral_connectivity_epochs(epoch, method='plv', sfreq=int(fs),\n",
    "                                                            mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                                            cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False)\n",
    "        coh = con.get_data(output='dense')\n",
    "        indices = con.names\n",
    "        aon_vHp_con = []\n",
    "        for i in range(coh.shape[0]):\n",
    "            for j in range(coh.shape[1]):\n",
    "                if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                    coherence= coh[i,j,:,:]\n",
    "                    #coherence=np.arctanh(coherence)\n",
    "                    aon_vHp_con.append(coherence)\n",
    "        \n",
    "        mean_con = np.mean(aon_vHp_con, axis=0)\n",
    "        return mean_con\n",
    "test_pli = epoch_coherogram(test_list[0]['mne_epoch_around_door'])\n",
    "plt.imshow(test_pli, extent=[-0.7, 0.7, 1, 100], aspect='auto', origin='lower', cmap='jet')\n",
    "plt.colorbar()\n",
    "\n",
    "mean_con_data['around_dig_mean_con'] = con_data_df_clean['mne_epoch_around_dig'].apply(epoch_coherogram)\n",
    "mean_con_data['around_door_mean_con'] = con_data_df_clean['mne_epoch_around_door'].apply(epoch_coherogram)\n",
    "\n",
    "mean_con_data['experiment'] = con_data_df_clean['experiment']\n",
    "mean_con_data['task'] = con_data_df_clean['task']\n",
    "mean_con_data['rat_id'] = con_data_df_clean['rat_id']\n",
    "mean_con_data['date'] = con_data_df_clean['date']\n",
    "\n",
    "mean_con_data.dropna(inplace=True)\n",
    "mean_con_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin = mean_con_data['around_dig_mean_con'].apply(np.min).min()\n",
    "vmax = mean_con_data['around_dig_mean_con'].apply(np.max).max()\n",
    "\n",
    "BWcontext_data=mean_con_data[(mean_con_data['task']=='BWcontext')]\n",
    "BWnocontext_data=mean_con_data[(mean_con_data['task']=='BWnocontext')]\n",
    "task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for group_name, group_df in task_data_dict.items():\n",
    "    \n",
    "    rat_ids, rat_nums = np.unique(group_df['rat_id'], return_counts=True)\n",
    "    rat_nums_max = rat_nums.max()\n",
    "\n",
    "    num_of_rows = 4 # Each row should be a rats\n",
    "    num_of_cols = rat_nums_max # Each column should be the max number of experiments for a rat\n",
    "\n",
    "    fig, axs = plt.subplots(num_of_rows, num_of_cols, figsize=(25, 10), sharex=True, sharey=True)\n",
    "    dk1_count = 0\n",
    "    dk3_count = 0\n",
    "    dk5_count = 0\n",
    "    dk6_count = 0\n",
    "    for i, (idx, row) in enumerate(group_df.iterrows()):\n",
    "        rat_id = row['rat_id']\n",
    "        data = np.array(row['around_dig_mean_con'])\n",
    "        if rat_id == 'dk1':\n",
    "            ax=axs[0, dk1_count]\n",
    "            dk1_count += 1\n",
    "        elif rat_id == 'dk3':\n",
    "            ax=axs[1, dk3_count]\n",
    "            dk3_count += 1\n",
    "        elif rat_id == 'dk5':\n",
    "            ax=axs[2, dk5_count]\n",
    "            dk5_count += 1\n",
    "        elif rat_id == 'dk6':\n",
    "            ax=axs[3, dk6_count]\n",
    "            dk6_count += 1\n",
    "        im = ax.imshow(data, extent=[-1*time_window, time_window, 1, 100], aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        ax.set_title(f\"{row['rat_id']} {row['date']}\")\n",
    "        ax.axvline(0, color='k', linestyle='--', linewidth=2)\n",
    "        ax.axhline(12, color='green', linestyle='--')\n",
    "        ax.axhline(30, color='green', linestyle='--')\n",
    "    \n",
    "    \n",
    "    # fig, axs = plt.subplots(group_df.shape[0] // 5 + int(group_df.shape[0] % 5 != 0), 5, figsize=(25, 10), sharex=True, sharey=True)\n",
    "    # axs = axs.flatten()\n",
    "    # for i, (idx, row) in enumerate(group_df.iterrows()):\n",
    "    #     data = np.array(row['around_dig_mean_con'])\n",
    "    #     ax = axs[i]\n",
    "    #     im = ax.imshow(data, extent=[-0.7, 0.7, 1, 100], aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    #     ax.set_title(f\"{row['rat_id']} {row['experiment']}\")\n",
    "    #     ax.axvline(0, color='k', linestyle='--', linewidth=2)\n",
    "    #     ax.axhline(12, color='green', linestyle='--')\n",
    "    #     ax.axhline(30, color='green', linestyle='--')\n",
    "    # for j in range(i + 1, len(axs)):\n",
    "    #     fig.delaxes(axs[j])\n",
    "    fig.suptitle(f\"{group_name} AON-vHp PLV Around Dig\", fontsize=16)\n",
    "    fig.colorbar(im, ax=axs, orientation='vertical', fraction=0.02)\n",
    "    fig.savefig(savepath + f'plv_around_dig_{group_name}.png', dpi=300, bbox_inches='tight')\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average PLI around door and dig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df_clean=pd.read_pickle(savepath+'mne_epochs_array_df_truncated_1400.pkl')\n",
    "\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "\n",
    "print(event_list)\n",
    "BWcontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWcontext')]\n",
    "BWnocontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWnocontext')]\n",
    "task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "\n",
    "\n",
    "all_con_data=[]\n",
    "all_con_data_mean=[]\n",
    "for task_num,task_name in enumerate(task_data_dict.keys()):\n",
    "        task_data=task_data_dict[task_name]\n",
    "        row=[task_name]\n",
    "         #print(row)\n",
    "        row_2=[task_name]\n",
    "        for event in event_list:\n",
    "            #print(event)\n",
    "            event_epoch_list=task_data[event]\n",
    "            aon_vHp_con=[]\n",
    "            for event_epoch in event_epoch_list:\n",
    "                    #print(row,event, event_epoch) \n",
    "                    if event_epoch.events.shape[0] <5:\n",
    "                        print(f\"Skipping {event} for {task_name} due to insufficient events\")\n",
    "                        continue\n",
    "                    fmin=1\n",
    "                    fmax=100\n",
    "                    fs=2000\n",
    "                    freqs = np.arange(fmin,fmax)\n",
    "                    n_cycles = freqs/3\n",
    "                    # con= mne_connectivity.spectral_connectivity_time(event_epoch, method='coh', sfreq=int(fs), average=False,\n",
    "                    #                                      mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                    #                                      n_cycles=n_cycles, verbose=False, fmin=1, fmax=100, faverage=False)\n",
    "                    # coh = con.get_data(output='dense')\n",
    "                    # indices = con.names\n",
    "                    # print(coh.shape, indices)a\n",
    "                    # for i in range(coh.shape[0]):\n",
    "                    #     for j in range(coh.shape[1]):\n",
    "                    #         if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                    #             coherence= coh[i,j,:]\n",
    "                    #             coherence=np.arctanh(coherence)\n",
    "                    #             aon_vHp_con.append(coherence)\n",
    "\n",
    "                    con = mne_connectivity.spectral_connectivity_epochs(event_epoch, method='pli', sfreq=int(fs),\n",
    "                                                         mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                                         cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False)\n",
    "                    coh = con.get_data(output='dense')\n",
    "                    indices = con.names\n",
    "                    \n",
    "\n",
    "                    for i in range(coh.shape[0]):\n",
    "                        for j in range(coh.shape[1]):\n",
    "                            if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                                coherence= coh[i,j,:,:]\n",
    "                                #coherence=np.arctanh(coherence)\n",
    "                                aon_vHp_con.append(coherence)\n",
    "            row.append(np.mean(aon_vHp_con, axis=0))\n",
    "            row_2.append(np.mean(aon_vHp_con))\n",
    "        all_con_data.append(row)                    \n",
    "        all_con_data_mean.append(row_2)\n",
    "# Convert all_con_data to a DataFrame for easier manipulation\n",
    "all_con_data_df = pd.DataFrame(all_con_data, columns=['task'] + event_list)\n",
    "all_con_data_df.to_pickle(savepath+'pli_coherogram_around_door_dig_truncated.pkl')\n",
    "fs=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_con_data_df=pd.read_pickle(savepath+'pli_coherogram_around_door_dig_truncated.pkl')\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "fs=2000\n",
    "times=np.arange(-0.7, 0.7, 1/fs)\n",
    "fig, axs=plt.subplots(2,2, figsize=(20,10), sharey=True)\n",
    "fig.suptitle('AON-vHp Phase Lag Index Around Door and Dig', fontsize=20)\n",
    "vmin = all_con_data_df[event_list].applymap(np.min).min().min()\n",
    "vmax = all_con_data_df[event_list].applymap(np.max).max().max()\n",
    "event_names=['Around Door','Around Dig']\n",
    "for i, event in enumerate(event_list):\n",
    "    axs[0,i].imshow(all_con_data_df[event][0], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=0, vmax=1)\n",
    "    axs[0,i].set_xlabel('')\n",
    "    axs[0,i].set_ylabel('Frequency (Hz)', fontsize=20)\n",
    "    axs[0,i].set_title(event_names[i], fontsize=20)\n",
    "    axs[0,i].vlines(0, 0, 100, color='k', linestyle='--')\n",
    "    axs[0,i].hlines(12, times[0], times[-1], color='green', linestyle='--')\n",
    "    axs[0,i].hlines(30, times[0], times[-1], color='green', linestyle='--')\n",
    "    \n",
    "    axs[1,i].imshow(all_con_data_df[event][1], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=0, vmax=1)\n",
    "    axs[1,i].set_xlabel('Time (s)', fontsize=20)\n",
    "    axs[1,i].set_ylabel('Frequency (Hz)', fontsize=20)\n",
    "    axs[1,i].set_title(event_names[i], fontsize=20)\n",
    "    axs[1,i].vlines(0, 0, 100, color='k', linestyle='--')\n",
    "    axs[1,i].hlines(12, times[0], times[-1], color='green', linestyle='--')\n",
    "    axs[1,i].hlines(30, times[0], times[-1], color='green', linestyle='--')\n",
    "    \n",
    "    axs[0,0].text(-0.2, 0.5, 'Context', transform=axs[0,0].transAxes, fontsize=18, verticalalignment='center', rotation=90)\n",
    "    axs[1,0].text(-0.2, 0.5, 'No Context', transform=axs[1,0].transAxes, fontsize=18, verticalalignment='center', rotation=90)\n",
    "    axs[0,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    axs[1,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    # axs[0,i].set_xticks(np.arange(-2, 3, 1))  # Set x-ticks from -2 to 2 seconds\n",
    "    # axs[0,i].set_xticklabels(np.arange(-2, 3, 1))  # Set x-tick labels from -2 to 2 seconds\n",
    "    # axs[1,i].set_xticks(np.arange(-2, 3, 1))  # Set x-ticks from -2 to 2 seconds\n",
    "    # axs[1,i].set_xticklabels(np.arange(-2, 3, 1))  # Set x-tick labels from -2 to 2 seconds\n",
    "\n",
    "    # Add a colorbar\n",
    "cbar = fig.colorbar(axs[0,0].images[0], ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "cbar.set_label('PLI', loc='center', fontsize=20, labelpad=10)\n",
    "cbar.ax.tick_params(labelsize=20)  # Set colorbar tick label size\n",
    "\n",
    "fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\aon_vhp_pli_coherogram.png',format='png', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase Slope Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 0.7\n",
    "fs=2000\n",
    "con_data_df_clean=pd.read_pickle(savepath+f'mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "\n",
    "print(event_list)\n",
    "\n",
    "test_list = [con_data_df_clean.iloc[0]]\n",
    "mean_con_data=pd.DataFrame()\n",
    "#epoch_psi(epoch, fmin=1, fmax=100, fs=2000):\n",
    "def epoch_psi(epoch, fmin, fmax, fs=2000):\n",
    "    print(epoch.events.shape)\n",
    "    aon_indices = [i for i, ch in enumerate(epoch.ch_names) if 'AON' in ch]\n",
    "    vHp_indices = [i for i, ch in enumerate(epoch.ch_names) if 'vHp' in ch]\n",
    "    indices = mne_connectivity.seed_target_indices(aon_indices, vHp_indices)\n",
    "\n",
    "    if epoch.events.shape[0] < 5:\n",
    "        print(\"Not enough events in the epoch\")\n",
    "        # Return empty arrays or np.nan to avoid TypeError\n",
    "        return [], []\n",
    "    else:\n",
    "        freqs = np.arange(fmin, fmax)\n",
    "        n_cycles = freqs / 3\n",
    "        con = mne_connectivity.phase_slope_index(\n",
    "            epoch, indices=indices, sfreq=int(fs),\n",
    "            mode='cwt_morlet', cwt_freqs=freqs,\n",
    "            cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax\n",
    "        )\n",
    "        coh = con.get_data()\n",
    "        print(coh.shape)\n",
    "        indices = con.names\n",
    "\n",
    "        mean_con = np.mean(coh, axis=0)\n",
    "        mean_con = list(mean_con[0, :])\n",
    "        all_cons = np.array([coh[i, 0, :] for i in range(coh.shape[0])])\n",
    "        return mean_con, all_cons\n",
    "\n",
    "epoch = test_list[0]['mne_epoch_around_door']\n",
    "mean_con, all_cons = epoch_psi(epoch, fmin=12, fmax=30, fs=2000)\n",
    "\n",
    "def generate_simulated_epoch(n_channels=4, n_times=2000, n_events=10, sfreq=2000):\n",
    "    ch_names = ['AON_1', 'AON_2', 'vHp_1', 'vHp_2']\n",
    "    ch_types = ['eeg'] * n_channels\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "    data = np.random.randn(n_events, n_channels, n_times)  # Random data for simulation\n",
    "    events = np.array([[i, 0, 1] for i in range(n_events)])  # Dummy events\n",
    "    epoch = mne.EpochsArray(data, info, events)\n",
    "    return epoch\n",
    "\n",
    "simulated_epoch = generate_simulated_epoch()\n",
    "mean_con, all_cons = epoch_psi(simulated_epoch)\n",
    "plt.plot(mean_con)\n",
    "\n",
    "psi_data_df = pd.DataFrame()\n",
    "psi_data_df['around_dig_mean_con'], psi_data_df['around_dig_all_cons'] = zip(*con_data_df_clean['mne_epoch_around_dig'].apply(epoch_psi))\n",
    "psi_data_df['around_door_mean_con'], psi_data_df['around_door_all_cons'] = zip(*con_data_df_clean['mne_epoch_around_door'].apply(epoch_psi))\n",
    "psi_data_df['experiment'] = con_data_df_clean['experiment']\n",
    "psi_data_df['task'] = con_data_df_clean['task']\n",
    "psi_data_df['rat_id'] = con_data_df_clean['rat_id']\n",
    "psi_data_df.dropna(inplace=True)\n",
    "psi_data_df = psi_data_df[psi_data_df['around_dig_mean_con'].apply(lambda x: len(x) > 0) & psi_data_df['around_door_mean_con'].apply(lambda x: len(x) > 0)]\n",
    "psi_data_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "BWcontext_data = psi_data_df[(psi_data_df['task'] == 'BWcontext')]\n",
    "BWnocontext_data = psi_data_df[(psi_data_df['task'] == 'BWnocontext')]\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Stack the lists vertically and compute the mean across axis 0\n",
    "bwcontext_stacked = np.vstack(BWcontext_data['around_dig_mean_con'].values)\n",
    "bwcontext_mean = np.mean(bwcontext_stacked, axis=0)\n",
    "\n",
    "bwnocontext_stacked = np.vstack(BWnocontext_data['around_dig_mean_con'].values)\n",
    "bwnocontext_mean = np.mean(bwnocontext_stacked, axis=0)\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(20, 10))\n",
    "ax.plot(times, bwnocontext_mean, label=' No Context', color='orange')\n",
    "ax.plot(times, bwcontext_mean, label='Context', color='blue')\n",
    "ax.set_title('AON-vHp PSI Beta Band Around Dig', fontsize=20)\n",
    "plt.axhline(0, color='k', linestyle='--')\n",
    "plt.axvline(0, color='k', linestyle='-', linewidth=2)\n",
    "ax.set_xlabel('Time (s)', fontsize=20)\n",
    "ax.set_ylabel('Phase Slope Index', fontsize=20)\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, labels, loc='upper left', fontsize=15)\n",
    "fig.savefig(savepath + 'aon_vhp_psi_around_dig.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Stack the lists vertically and compute the mean across axis 0\n",
    "bwcontext_stacked = np.vstack(BWcontext_data['around_dig_all_cons'].values)\n",
    "bwcontext_mean = np.mean(bwcontext_stacked, axis=0)\n",
    "bw_context_sem = np.std(bwcontext_stacked, axis=0) / np.sqrt(bwcontext_stacked.shape[0])\n",
    "\n",
    "bwnocontext_stacked = np.vstack(BWnocontext_data['around_dig_all_cons'].values)\n",
    "bwnocontext_mean = np.mean(bwnocontext_stacked, axis=0)\n",
    "bwnocontext_sem = np.std(bwnocontext_stacked, axis=0) / np.sqrt(bwnocontext_stacked.shape[0])\n",
    "fig,ax=plt.subplots(figsize=(20, 10))\n",
    "ax.plot(times, bwnocontext_mean, label=' No Context', color='orange')\n",
    "ax.fill_between(times, bwnocontext_mean - bwnocontext_sem, bwnocontext_mean + bwnocontext_sem, color='orange', alpha=0.3)\n",
    "ax.plot(times, bwcontext_mean, label='Context', color='blue')\n",
    "ax.fill_between(times, bwcontext_mean - bw_context_sem, bwcontext_mean + bw_context_sem, color='blue', alpha=0.3)\n",
    "ax.set_title('AON-vHp PSI Beta Band Around Dig', fontsize=20)\n",
    "plt.axhline(0, color='k', linestyle='--')\n",
    "plt.axvline(0, color='k', linestyle='-', linewidth=2)\n",
    "ax.set_xlabel('Time (s)', fontsize=20)\n",
    "ax.set_ylabel('Phase Slope Index', fontsize=20)\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, labels, loc='upper left', fontsize=15)\n",
    "fig.savefig(savepath + 'aon_vhp_psi_around_dig.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making a cumulative figure with all bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "time_window = 0.7\n",
    "fs=2000\n",
    "times=np.arange(-0.7, 0.7, 1/fs)\n",
    "##############\n",
    "\n",
    "\n",
    "bands_list =[(4,8), (12,30), (30,80)]  # Theta, Beta, Gamma\n",
    "band_names = ['theta', 'beta', 'gamma']\n",
    "\n",
    "real_con_data = pd.read_pickle(savepath+f'mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n",
    "shuffled_con_data = pd.read_pickle(savepath+f'mne_epochs_array_df_shuffled_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "def clean_and_merge_data(psi_data_df, con_data_df_clean):\n",
    "    psi_data_df['experiment'] = con_data_df_clean['experiment']\n",
    "    psi_data_df['task'] = con_data_df_clean['task']\n",
    "    psi_data_df['rat_id'] = con_data_df_clean['rat_id']\n",
    "    psi_data_df.dropna(inplace=True)\n",
    "    #psi_data_df = psi_data_df[psi_data_df['around_dig_mean_con'].apply(lambda x: len(x) > 0)]\n",
    "    psi_data_df.reset_index(drop=True, inplace=True)\n",
    "    return psi_data_df\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(60, 10), sharey='row', sharex=True)\n",
    "fig.suptitle('AON-vHp Phase Slope Index Around Dig', fontsize=24)\n",
    "for band_idx, (fmin, fmax) in enumerate(bands_list):\n",
    "    print(f\"Processing band: {band_names[band_idx]} ({fmin}-{fmax} Hz)\")\n",
    "    real_data_psi = pd.DataFrame()\n",
    "    real_data_psi['around_dig_mean_con'], real_data_psi['around_dig_all_cons'] = zip(*real_con_data['mne_epoch_around_dig'].apply(lambda x: epoch_psi(x, fmin=fmin, fmax=fmax)))\n",
    "\n",
    "    shuffled_data_psi = pd.DataFrame()\n",
    "    shuffled_data_psi['around_dig_mean_con'], shuffled_data_psi['around_dig_all_cons'] = zip(*shuffled_con_data['mne_epoch_around_dig'].apply(lambda x: epoch_psi(x, fmin=fmin, fmax=fmax)))\n",
    "\n",
    "    real_data_psi = clean_and_merge_data(real_data_psi, real_con_data)\n",
    "    shuffled_data_psi = clean_and_merge_data(shuffled_data_psi, shuffled_con_data)\n",
    "\n",
    "    for i, data in enumerate([real_data_psi, shuffled_data_psi]):\n",
    "        print(f\"Processing {'real' if i == 0 else 'shuffled'} data\")\n",
    "        BWcontext_data = data[(data['task'] == 'BWcontext')]\n",
    "        BWnocontext_data = data[(data['task'] == 'BWnocontext')]\n",
    "        \n",
    "        bwcontext_stacked = np.vstack(BWcontext_data['around_dig_all_cons'].values)\n",
    "        bwcontext_mean = np.mean(bwcontext_stacked, axis=0)\n",
    "        bw_context_sem = np.std(bwcontext_stacked, axis=0) / np.sqrt(bwcontext_stacked.shape[0])\n",
    "\n",
    "        bwnocontext_stacked = np.vstack(BWnocontext_data['around_dig_all_cons'].values)\n",
    "        bwnocontext_mean = np.mean(bwnocontext_stacked, axis=0)\n",
    "        bwnocontext_sem = np.std(bwnocontext_stacked, axis=0) / np.sqrt(bwnocontext_stacked.shape[0])\n",
    "        \n",
    "        ax = axs[band_idx, i]\n",
    "        ax.plot(times, bwnocontext_mean, label=' No Context', color='grey')\n",
    "        ax.fill_between(times, bwnocontext_mean - bwnocontext_sem, bwnocontext_mean + bwnocontext_sem, color='grey', alpha=0.3)\n",
    "        ax.plot(times, bwcontext_mean, label='Context', color='black')\n",
    "        ax.fill_between(times, bwcontext_mean - bw_context_sem, bwcontext_mean + bw_context_sem, color='black', alpha=0.3)\n",
    "        \n",
    "        ax.set_title(f'{band_names[band_idx].capitalize()} {\"Real\" if i == 0 else \"Shuffled\"}', fontsize=16)\n",
    "        ax.axhline(0, color='blue', linestyle='--')\n",
    "        ax.axvline(0, color='red', linestyle='-', linewidth=2)\n",
    "        if band_idx == 2:\n",
    "            ax.set_xlabel('Time (s)', fontsize=14)\n",
    "        else:\n",
    "            ax.set_xlabel('')\n",
    "        ax.set_ylabel('Phase Slope Index', fontsize=14)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        ax.legend(handles, labels, loc='upper left', fontsize=12)\n",
    "        \n",
    "fig.savefig(savepath + 'aon_vhp_psi_around_dig_bands_real_vs_shuffled.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PSI for each frequency point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(type(low_fs), low_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 0.7\n",
    "fs=2000\n",
    "con_data_df_clean=pd.read_pickle(savepath+f'mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "\n",
    "print(event_list)\n",
    "\n",
    "test_list = [con_data_df_clean.iloc[0]]\n",
    "mean_con_data=pd.DataFrame()\n",
    "#epoch_psi(epoch, fmin=1, fmax=100, fs=2000):\n",
    "def epoch_psi(epoch, fs=2000):\n",
    "    print(epoch.events.shape)\n",
    "    aon_indices = [i for i, ch in enumerate(epoch.ch_names) if 'AON' in ch]\n",
    "    vHp_indices = [i for i, ch in enumerate(epoch.ch_names) if 'vHp' in ch]\n",
    "    indices = mne_connectivity.seed_target_indices(aon_indices, vHp_indices)\n",
    "    print(indices)\n",
    "    if epoch.events.shape[0] < 5:\n",
    "        print(\"Not enough events in the epoch\")\n",
    "        # Return empty arrays or np.nan to avoid TypeError\n",
    "        return [], []\n",
    "    else:\n",
    "        low_fs = np.arange(1, 100,1)\n",
    "        high_fs = np.arange(2, 101,1)\n",
    "        \n",
    "        for bandi,(fmin, fmax) in enumerate(zip(low_fs, high_fs)):\n",
    "            \n",
    "            print(f\"{bandi}:Processing frequency band: {fmin}-{fmax} Hz\")\n",
    "            freqs = np.arange(fmin, fmax)\n",
    "            n_cycles = freqs / 3\n",
    "            con = mne_connectivity.phase_slope_index(\n",
    "                epoch, indices=indices, sfreq=int(fs),\n",
    "                mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax\n",
    "            )\n",
    "            coh = con.get_data()\n",
    "            print(coh.shape)\n",
    "            indices = con.names\n",
    "\n",
    "            # mean_con = np.mean(coh, axis=0)\n",
    "            # mean_con = list(mean_con[0, :])\n",
    "            # all_cons = np.array([coh[i, 0, :] for i in range(coh.shape[0])])\n",
    "#    return mean_con, all_cons\n",
    "\n",
    "epoch = test_list[0]['mne_epoch_around_door']\n",
    "epoch_psi(epoch, fs=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_simulated_epoch(n_channels=4, n_times=2000, n_events=10, sfreq=2000):\n",
    "    ch_names = ['AON_1', 'AON_2', 'vHp_1', 'vHp_2']\n",
    "    ch_types = ['eeg'] * n_channels\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "    data = np.random.randn(n_events, n_channels, n_times)  # Random data for simulation\n",
    "    events = np.array([[i, 0, 1] for i in range(n_events)])  # Dummy events\n",
    "    epoch = mne.EpochsArray(data, info, events)\n",
    "    return epoch\n",
    "\n",
    "simulated_epoch = generate_simulated_epoch()\n",
    "mean_con, all_cons = epoch_psi(simulated_epoch)\n",
    "plt.plot(mean_con)\n",
    "\n",
    "psi_data_df = pd.DataFrame()\n",
    "psi_data_df['around_dig_mean_con'], psi_data_df['around_dig_all_cons'] = zip(*con_data_df_clean['mne_epoch_around_dig'].apply(epoch_psi))\n",
    "psi_data_df['around_door_mean_con'], psi_data_df['around_door_all_cons'] = zip(*con_data_df_clean['mne_epoch_around_door'].apply(epoch_psi))\n",
    "psi_data_df['experiment'] = con_data_df_clean['experiment']\n",
    "psi_data_df['task'] = con_data_df_clean['task']\n",
    "psi_data_df['rat_id'] = con_data_df_clean['rat_id']\n",
    "psi_data_df.dropna(inplace=True)\n",
    "psi_data_df = psi_data_df[psi_data_df['around_dig_mean_con'].apply(lambda x: len(x) > 0) & psi_data_df['around_door_mean_con'].apply(lambda x: len(x) > 0)]\n",
    "psi_data_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "BWcontext_data = psi_data_df[(psi_data_df['task'] == 'BWcontext')]\n",
    "BWnocontext_data = psi_data_df[(psi_data_df['task'] == 'BWnocontext')]\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Stack the lists vertically and compute the mean across axis 0\n",
    "bwcontext_stacked = np.vstack(BWcontext_data['around_dig_mean_con'].values)\n",
    "bwcontext_mean = np.mean(bwcontext_stacked, axis=0)\n",
    "\n",
    "bwnocontext_stacked = np.vstack(BWnocontext_data['around_dig_mean_con'].values)\n",
    "bwnocontext_mean = np.mean(bwnocontext_stacked, axis=0)\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(20, 10))\n",
    "ax.plot(times, bwnocontext_mean, label=' No Context', color='orange')\n",
    "ax.plot(times, bwcontext_mean, label='Context', color='blue')\n",
    "ax.set_title('AON-vHp PSI Beta Band Around Dig', fontsize=20)\n",
    "plt.axhline(0, color='k', linestyle='--')\n",
    "plt.axvline(0, color='k', linestyle='-', linewidth=2)\n",
    "ax.set_xlabel('Time (s)', fontsize=20)\n",
    "ax.set_ylabel('Phase Slope Index', fontsize=20)\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, labels, loc='upper left', fontsize=15)\n",
    "fig.savefig(savepath + 'aon_vhp_psi_around_dig.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Stack the lists vertically and compute the mean across axis 0\n",
    "bwcontext_stacked = np.vstack(BWcontext_data['around_dig_all_cons'].values)\n",
    "bwcontext_mean = np.mean(bwcontext_stacked, axis=0)\n",
    "bw_context_sem = np.std(bwcontext_stacked, axis=0) / np.sqrt(bwcontext_stacked.shape[0])\n",
    "\n",
    "bwnocontext_stacked = np.vstack(BWnocontext_data['around_dig_all_cons'].values)\n",
    "bwnocontext_mean = np.mean(bwnocontext_stacked, axis=0)\n",
    "bwnocontext_sem = np.std(bwnocontext_stacked, axis=0) / np.sqrt(bwnocontext_stacked.shape[0])\n",
    "fig,ax=plt.subplots(figsize=(20, 10))\n",
    "ax.plot(times, bwnocontext_mean, label=' No Context', color='orange')\n",
    "ax.fill_between(times, bwnocontext_mean - bwnocontext_sem, bwnocontext_mean + bwnocontext_sem, color='orange', alpha=0.3)\n",
    "ax.plot(times, bwcontext_mean, label='Context', color='blue')\n",
    "ax.fill_between(times, bwcontext_mean - bw_context_sem, bwcontext_mean + bw_context_sem, color='blue', alpha=0.3)\n",
    "ax.set_title('AON-vHp PSI Beta Band Around Dig', fontsize=20)\n",
    "plt.axhline(0, color='k', linestyle='--')\n",
    "plt.axvline(0, color='k', linestyle='-', linewidth=2)\n",
    "ax.set_xlabel('Time (s)', fontsize=20)\n",
    "ax.set_ylabel('Phase Slope Index', fontsize=20)\n",
    "ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles, labels, loc='upper left', fontsize=15)\n",
    "fig.savefig(savepath + 'aon_vhp_psi_around_dig.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=test_list[0]['mne_epoch_around_door']\n",
    "epoch.ch_names\n",
    "\n",
    "print(aon_indices, vHp_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_con_data['around_dig_mean_con'] = con_data_df_clean['mne_epoch_around_dig'].apply(epoch_coherogram)\n",
    "mean_con_data['around_door_mean_con'] = con_data_df_clean['mne_epoch_around_door'].apply(epoch_coherogram)\n",
    "\n",
    "mean_con_data['experiment'] = con_data_df_clean['experiment']\n",
    "mean_con_data['task'] = con_data_df_clean['task']\n",
    "mean_con_data['rat_id'] = con_data_df_clean['rat_id']\n",
    "mean_con_data.dropna(inplace=True)\n",
    "mean_con_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference in coherence between BWContext and BWnOContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_con_data_df_net=all_con_data_df.__deepcopy__()\n",
    "all_con_data_df_net.set_index('task', inplace=True)\n",
    "all_con_data_df_net.loc['difference'] = all_con_data_df_net.loc['BWcontext'] - all_con_data_df_net.loc['BWnocontext']\n",
    "all_con_data_df_net.reset_index(inplace=True)\n",
    "\n",
    "fs=2000\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "times=np.arange(-2, 2, 1/fs)\n",
    "fig, axs=plt.subplots(1,2, figsize=(20,10), sharey=True)\n",
    "fig.suptitle('Difference in Coherence between BW Context and BW No Context')\n",
    "axs=axs.flatten()\n",
    "vmin = all_con_data_df_net[event_list].applymap(np.min).min().min()\n",
    "vmax = all_con_data_df_net[event_list].applymap(np.max).max().max()\n",
    "event_names=['Around Door','Around Dig']\n",
    "for i, event in enumerate(event_list):\n",
    "    axs[i].imshow(all_con_data_df_net[event][2], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[i].set_xlabel('Time (s)')\n",
    "    axs[i].set_ylabel('Frequency (Hz)')\n",
    "    axs[i].set_title(event_names[i])\n",
    "    axs[i].vlines(0, 0, 100, color='k', linestyle='--')\n",
    "\n",
    "cbar = fig.colorbar(axs[0].images[0], ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "cbar.set_label('Coherence')\n",
    "#fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\aon_vhp_coherence_event_spectrogram.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase Difference manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df_clean=pd.read_pickle(savepath+'mne_epochs_array_df_truncated_1400.pkl')\n",
    "\n",
    "\n",
    "for row in con_data_df_clean.itertuples(index=False):\n",
    "    experiment = row.experiment\n",
    "    rat_id = row.rat_id\n",
    "    task = row.task\n",
    "    mne_epoch = row.mne_epoch_door_before\n",
    "    data_around_dig = row.mne_epoch_around_dig\n",
    "    data_before_dig = row.mne_epoch_dig_before\n",
    "    data_after_dig = row.mne_epoch_dig_after\n",
    "    data_before_door = row.mne_epoch_door_before\n",
    "    data_after_door = row.mne_epoch_door_after\n",
    "\n",
    "    event_of_interest = data_before_dig ### CHANGE THIS TO THE DESIRED EVENT\n",
    "\n",
    "    print(f'Processing Rat: {rat_id}, Experiment: {experiment}, Task: {task}')\n",
    "    print(event_of_interest.get_data().shape)  # Should be (n_epochs, n_channels,n_times)\n",
    "    single_data = event_of_interest.get_data()[0, 0, :]  # Get data for the first channel\n",
    "    print(single_data.shape)  # Should be (n_times,)\n",
    "    fs=2000\n",
    "    l_freq =12\n",
    "    h_freq = 30\n",
    "    iir_filter = mne.filter.create_filter(single_data, sfreq=fs,l_freq=l_freq, h_freq=h_freq, method='iir', verbose=False)\n",
    "    event_of_interest.filter(l_freq=l_freq, h_freq=h_freq, method='iir', iir_params=iir_filter, verbose=False)\n",
    "    event_of_interest.apply_hilbert(envelope=False, n_jobs=1, verbose=False)\n",
    "\n",
    "    aon_indices = [i for i, ch in enumerate(event_of_interest.ch_names) if 'AON' in ch]\n",
    "    vhp_indices = [i for i, ch in enumerate(event_of_interest.ch_names) if 'vHp' in ch]\n",
    "    aon_channels = [event_of_interest.ch_names[i] for i in aon_indices]\n",
    "    vhp_channels = [event_of_interest.ch_names[i] for i in vhp_indices]\n",
    "    print(aon_indices, vhp_indices, aon_channels, vhp_channels)\n",
    "    aon_vhp_pairs = [(aon_ch, vhp_ch) for aon_ch in aon_channels for vhp_ch in vhp_channels]\n",
    "    print(aon_vhp_pairs)\n",
    "    num_of_cols = event_of_interest.get_data().shape[0]\n",
    "    num_of_rows = len(aon_vhp_pairs)\n",
    "\n",
    "    fig, axs = plt.subplots(num_of_rows, num_of_cols, subplot_kw={'projection': 'polar'},figsize=(40, 10))\n",
    "    fig.suptitle(f'AON-vHp Phase Difference Around Dig for Rat: {rat_id}, Experiment: {experiment}, Task: {task}', fontsize=20)\n",
    "    for i, (aon_ch, vhp_ch) in enumerate(aon_vhp_pairs):\n",
    "        for j in range(num_of_cols):\n",
    "            ax = axs[i, j]\n",
    "            ax.set_xticklabels([])          # remove theta labels\n",
    "            ax.set_yticklabels([])          # remove radial labels\n",
    "            # or hide ticks entirely:\n",
    "            #ax.xaxis.set_ticks([])\n",
    "            ax.yaxis.set_ticks([])\n",
    "\n",
    "            if j == 0:\n",
    "                ax.set_ylabel(f'{aon_ch} - {vhp_ch}', fontsize=10)\n",
    "            aon_index = aon_indices[aon_channels.index(aon_ch)]\n",
    "            vhp_index = vhp_indices[vhp_channels.index(vhp_ch)]\n",
    "            aon_epoch_data = np.angle(event_of_interest.get_data()[j,aon_index, :])\n",
    "            vhp_epoch_data = np.angle(event_of_interest.get_data()[j,vhp_index, :])\n",
    "            #print(aon_index, vhp_index, aon_epoch_data.shape, vhp_epoch_data.shape)\n",
    "            phase_diff = aon_epoch_data - vhp_epoch_data\n",
    "            ispc = np.abs(np.mean(np.exp(1j * phase_diff)))\n",
    "            pli = abs(np.mean(np.sign(np.imag(np.exp(1j * phase_diff)))))\n",
    "            phase_diff_wrapped = np.mod(phase_diff, 2 * np.pi)\n",
    "            ax.hist(phase_diff_wrapped, bins=50, color='red', alpha=0.7, density=True)\n",
    "            if i== 0:\n",
    "                ax.set_title(f'trial {j}\\nispc:{ispc:.2f} pli:{pli:.2f}', fontsize=10)\n",
    "            else:\n",
    "                ax.set_title(f'ispc:{ispc:.2f} pli:{pli:.2f}', fontsize=10)\n",
    "    fig.savefig(savepath + f'{task}_{rat_id}_{experiment}_phase_difference_aon_vhp_before_dig.png', dpi=100, bbox_inches='tight')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GC measures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "time_window = 0.7\n",
    "fs=2000\n",
    "##############\n",
    "\n",
    "\n",
    "\n",
    "con_data_df_clean=pd.read_pickle(savepath+f'mne_epochs_array_df_truncated_{int(time_window*fs)}.pkl')\n",
    "#con_data_df=pd.DataFrame(con_data_df, columns=['rat_id','task','mne_baseline','mne_epoch_door_before','mne_epoch_door_after',\n",
    "                                               #'mne_epoch_dig_before','mne_epoch_dig_after', 'mne_epoch_around_door', 'mne_epoch_around_dig'])\n",
    "\n",
    "def calculate_net_gc(mne_data):\n",
    "        \n",
    "        mne_data=mne_data.resample(500)\n",
    "        \n",
    "        aon_signals=[\n",
    "        idx\n",
    "        for idx, ch_info in enumerate(mne_data.info[\"chs\"])\n",
    "        if \"AON\" in ch_info[\"ch_name\"]\n",
    "        ]\n",
    "        print(aon_signals)\n",
    "        vhp_signals=[\n",
    "            idx\n",
    "            for idx, ch_info in enumerate(mne_data.info[\"chs\"])\n",
    "            if \"vHp\" in ch_info[\"ch_name\"]\n",
    "        ]\n",
    "        print(vhp_signals)\n",
    "\n",
    "        indices_aon_vhp = (np.array([aon_signals]), np.array([vhp_signals]))\n",
    "        indices_vhp_aon = (np.array([vhp_signals]), np.array([aon_signals]))\n",
    "        print(\"indices_aon_vhp:\", indices_aon_vhp, \"indices_vhp_aon:\", indices_vhp_aon)\n",
    "        gc_ab = mne_connectivity.spectral_connectivity_epochs(\n",
    "        mne_data,\n",
    "        method=[\"gc\"],\n",
    "        indices=indices_aon_vhp,\n",
    "        fmin=2.5,\n",
    "        fmax=100,\n",
    "        rank=None,\n",
    "        gc_n_lags=50,\n",
    "        )\n",
    "        freqs = gc_ab.freqs\n",
    "\n",
    "        gc_ba = mne_connectivity.spectral_connectivity_epochs(\n",
    "            mne_data,\n",
    "            method=[\"gc\"],\n",
    "            indices=indices_vhp_aon,\n",
    "            fmin=2.5,\n",
    "            fmax=100,\n",
    "            rank=None,\n",
    "            gc_n_lags=50,\n",
    "        )\n",
    "        freqs = gc_ba.freqs\n",
    "\n",
    "        net_gc = gc_ab.get_data() - gc_ba.get_data()\n",
    "        return gc_ab.get_data()[0], gc_ba.get_data()[0],net_gc[0], freqs\n",
    "\n",
    "def calculate_gc_indch(mne_data):\n",
    "    mne_data = mne_data.resample(500)\n",
    "\n",
    "    aon_signals = [\n",
    "        idx\n",
    "        for idx, ch_info in enumerate(mne_data.info[\"chs\"])\n",
    "        if \"AON\" in ch_info[\"ch_name\"]\n",
    "    ]\n",
    "    vhp_signals = [\n",
    "        idx\n",
    "        for idx, ch_info in enumerate(mne_data.info[\"chs\"])\n",
    "        if \"vHp\" in ch_info[\"ch_name\"]\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "    for aon_idx in aon_signals:\n",
    "        for vhp_idx in vhp_signals:\n",
    "            indices_aon_vhp = (np.array([[aon_idx]]), np.array([[vhp_idx]]))\n",
    "            indices_vhp_aon = (np.array([[vhp_idx]]), np.array([[aon_idx]]))\n",
    "            print(\"indices_aon_vhp:\", indices_aon_vhp, \"indices_vhp_aon:\", indices_vhp_aon)\n",
    "            gc_ab = mne_connectivity.spectral_connectivity_epochs(\n",
    "                mne_data,\n",
    "                method=[\"gc\"],\n",
    "                indices=indices_aon_vhp,\n",
    "                fmin=2.5,\n",
    "                fmax=100,\n",
    "                rank=None,\n",
    "                gc_n_lags=50,\n",
    "            )\n",
    "            gc_ba = mne_connectivity.spectral_connectivity_epochs(\n",
    "                mne_data,\n",
    "                method=[\"gc\"],\n",
    "                indices=indices_vhp_aon,\n",
    "                fmin=2.5,\n",
    "                fmax=100,\n",
    "                rank=None,\n",
    "                gc_n_lags=50,\n",
    "            )\n",
    "            freqs = gc_ab.freqs\n",
    "            net_gc = gc_ab.get_data()[0] - gc_ba.get_data()[0]\n",
    "\n",
    "            aon_ch = mne_data.info[\"chs\"][aon_idx][\"ch_name\"]\n",
    "            vhp_ch = mne_data.info[\"chs\"][vhp_idx][\"ch_name\"]\n",
    "\n",
    "            results.append({\n",
    "                \"channelpair\": (aon_ch, vhp_ch),\n",
    "                \"gc_aon_vhp\": gc_ab.get_data()[0],\n",
    "                \"gc_vhp_aon\": gc_ba.get_data()[0],\n",
    "                \"net_gc\": net_gc,\n",
    "                \"freqs\": freqs\n",
    "            })\n",
    "    return results\n",
    "\n",
    "test_mne = con_data_df_clean.iloc[0]['mne_epoch_dig_after']\n",
    "test_netgc = calculate_net_gc(test_mne)\n",
    "test_indch = calculate_gc_indch(test_mne)\n",
    "print(test_indch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gc_columns = ['mne_epoch_door_before', 'mne_epoch_door_after', 'mne_epoch_dig_before', 'mne_epoch_dig_after', 'mne_epoch_around_door', 'mne_epoch_around_dig']\n",
    "\n",
    "aon_vhp_gc_results = []\n",
    "vhp_aon_gc_results = []\n",
    "net_gc_results = []\n",
    "\n",
    "for idx, row in con_data_df_clean.iterrows():\n",
    "\n",
    "    for col in gc_columns:\n",
    "        gc_list = calculate_gc_indch(row[col])\n",
    "\n",
    "        for pair_idx, gc_dict in enumerate(gc_list):\n",
    "            aon_vhp_gc_results.append({\n",
    "                'rat_id': row['rat_id'],\n",
    "                'task': row['task'],\n",
    "                'experiment': row['experiment'],\n",
    "                'date': row['date'],\n",
    "                'epoch_type': col,\n",
    "                'channelpair_idx': pair_idx,\n",
    "                'channelpair': gc_dict['channelpair'],\n",
    "                'gc_values': gc_dict['gc_aon_vhp'],\n",
    "                'freqs': gc_dict['freqs']\n",
    "            })\n",
    "            vhp_aon_gc_results.append({\n",
    "                'rat_id': row['rat_id'],\n",
    "                'task': row['task'],\n",
    "                'experiment': row['experiment'],\n",
    "                'date': row['date'],\n",
    "                'epoch_type': col,\n",
    "                'channelpair_idx': pair_idx,\n",
    "                'channelpair': gc_dict['channelpair'],\n",
    "                'gc_values': gc_dict['gc_vhp_aon'],\n",
    "                'freqs': gc_dict['freqs']\n",
    "            })\n",
    "            net_gc_results.append({\n",
    "                'rat_id': row['rat_id'],\n",
    "                'task': row['task'],\n",
    "                'experiment': row['experiment'],\n",
    "                'date': row['date'],\n",
    "                'epoch_type': col,\n",
    "                'channelpair_idx': pair_idx,\n",
    "                'channelpair': gc_dict['channelpair'],\n",
    "                'gc_values': gc_dict['net_gc'],\n",
    "                'freqs': gc_dict['freqs']\n",
    "            })\n",
    "        # aon_vhp_gc_results.append(row_result_aon_vhp)\n",
    "        # vhp_aon_gc_results.append(row_result_vhp_aon)\n",
    "        # net_gc_results.append(row_result_net_gc)\n",
    "\n",
    "# Convert to DataFrames\n",
    "aon_vhp_gc_df = pd.DataFrame(aon_vhp_gc_results)\n",
    "vhp_aon_gc_df = pd.DataFrame(vhp_aon_gc_results)\n",
    "net_gc_df = pd.DataFrame(net_gc_results)\n",
    "\n",
    "# Expand the DataFrame so each AON-vHp channel pair has its own column for each epoch type\n",
    "\n",
    "# Example: save to disk\n",
    "aon_vhp_gc_df.to_pickle(savepath + f'vhp_aon_gc_shuffled.pkl')\n",
    "vhp_aon_gc_df.to_pickle(savepath + f'aon_vhp_gc_shuffled.pkl')\n",
    "net_gc_df.to_pickle(savepath + f'net_gc_shuffled.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 0.7\n",
    "fs=2000\n",
    "aon_vhp_gc_df=pd.read_pickle(savepath + f'aon_vhp_gc.pkl')\n",
    "vhp_aon_gc_df=pd.read_pickle(savepath + f'vhp_aon_gc.pkl')\n",
    "net_gc_df=pd.read_pickle(savepath + f'net_gc.pkl')\n",
    "\n",
    "def calculate_gc_per_band(gc_array,freqs_array, bands_dict):\n",
    "    freqs_array = np.array(freqs_array)  # Convert freqs_array to numpy array\n",
    "    print(len(gc_array))\n",
    "    gc_bands_dict={}\n",
    "    for band in bands_dict.keys():\n",
    "        band_indices=np.where((freqs_array>=bands_dict[band][0]) & (freqs_array<=bands_dict[band][1]))\n",
    "        gc_band=gc_array[band_indices]\n",
    "        gc_bands_dict[band]=np.sum(gc_band)*(freqs_array[1]-freqs_array[0])\n",
    "        #gc_bands_dict[band]=(np.sum(gc_band)*0.5)/len(gc_band)\n",
    "    return gc_bands_dict\n",
    "\n",
    "test_row = aon_vhp_gc_df.iloc[0]\n",
    "row_list= [test_row]\n",
    "bands_dict = {\n",
    "    'theta': (4, 8),\n",
    "    'beta': (12, 30),\n",
    "    'gamma': (30, 100),\n",
    "    'theta+early_beta': (4, 20),\n",
    "    'total': (2.5, 100)\n",
    "}\n",
    "\n",
    "for row in row_list:\n",
    "    gc_values = row['gc_values']\n",
    "    freqs = row['freqs']\n",
    "    gc_bands = calculate_gc_per_band(gc_values, freqs, bands_dict)\n",
    "    print(row['channelpair'], row['epoch_type'], gc_bands)\n",
    "    for band in gc_bands.keys():\n",
    "        row[f'{band}'] = gc_bands[band]\n",
    "    print(row)\n",
    "events_dict = {\n",
    "    'mne_epoch_door_before': 'door_before',\n",
    "    'mne_epoch_door_after': 'door_after',\n",
    "    'mne_epoch_dig_before': 'dig_before',\n",
    "    'mne_epoch_dig_after': 'dig_after',\n",
    "    'mne_epoch_around_door': 'around_door',\n",
    "    'mne_epoch_around_dig': 'around_dig'\n",
    "}\n",
    "gc_df_dict = {'AON to vHp': aon_vhp_gc_df, 'vHp to AON': vhp_aon_gc_df, 'Net GC': net_gc_df}\n",
    "writer = pd.ExcelWriter(savepath + 'gc_band_results.xlsx')\n",
    "for i, gc_df in enumerate(gc_df_dict.values()):\n",
    "    band_results = []\n",
    "\n",
    "    for idx, row in gc_df.iterrows():\n",
    "        gc_values = row['gc_values']\n",
    "        freqs = row['freqs']\n",
    "        gc_bands = calculate_gc_per_band(gc_values, freqs, bands_dict)\n",
    "        for band in gc_bands.keys():\n",
    "            row[f'{band}'] = gc_bands[band]\n",
    "        row['event'] = events_dict[row['epoch_type']]\n",
    "        row.drop(['gc_values', 'freqs', 'epoch_type'], inplace=True)\n",
    "        band_results.append(row)\n",
    "    band_df = pd.DataFrame(band_results)\n",
    "    band_df_melted = band_df.melt(id_vars=['rat_id', 'task', 'experiment', 'date', 'event', 'channelpair'],\n",
    "                                  value_vars=list(bands_dict.keys()), var_name='band', value_name='gc_value')\n",
    "    band_df_melted.to_excel(writer, sheet_name=list(gc_df_dict.keys())[i], index=False)\n",
    "    \n",
    "    ########### Plotting ###########\n",
    "    \n",
    "\n",
    "    fig, axs=plt.subplots(3,2, sharex=False, sharey=True, figsize=(15,10))\n",
    "    axs=axs.flatten()\n",
    "    fig.suptitle(f'{list(gc_df_dict.keys())[i]} Granger causality per band')\n",
    "    for axi, event in enumerate(events_dict.values()):\n",
    "        print(axi, event)\n",
    "        ax=axs[axi]\n",
    "        ax.axhline(0, color='black', lw=1)\n",
    "        sns.barplot(x='band',y='gc_value',hue='task',hue_order=['BWcontext','BWnocontext'],data=band_df_melted[band_df_melted['event']==event], ax=ax)\n",
    "        #sns.stripplot(x='band',y='gc_value',hue='task',hue_order=['BWcontext','BWnocontext'],data=band_df_melted,dodge=True,edgecolor='black',linewidth=1,jitter=True, legend=False, ax=ax)\n",
    "        ax.set_title(f\"{event}\", fontsize=10)\n",
    "        ax.set_xlabel('')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(savepath+f'gc_events_perband_{list(gc_df_dict.keys())[i]}_{int(time_window*fs/2)}ms.png')\n",
    "    plt.show()\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net_gc_df=pd.DataFrame()\n",
    "aon_vhp_gc_df = pd.DataFrame()\n",
    "vhp_aon_gc_df = pd.DataFrame()\n",
    "\n",
    "for col in ['rat_id', 'task', 'experiment', 'date']:\n",
    "    net_gc_df[col] = con_data_df_clean[col]\n",
    "    aon_vhp_gc_df[col] = con_data_df_clean[col]\n",
    "    vhp_aon_gc_df[col] = con_data_df_clean[col]\n",
    "for event in ['door_before', 'door_after', 'dig_before', 'dig_after', 'around_door', 'around_dig']:\n",
    "    mne_col = f'mne_epoch_{event}' if event not in ['around_door', 'around_dig'] else f'mne_epoch_{event}'\n",
    "    aon_vhp_gc_df[event] = con_data_df_clean[mne_col].apply(lambda x: calculate_gc_indch(x)[0])\n",
    "    vhp_aon_gc_df[event] = con_data_df_clean[mne_col].apply(lambda x: calculate_gc_indch(x)[1])\n",
    "    net_gc_df[event] = con_data_df_clean[mne_col].apply(lambda x: calculate_gc_indch(x)[2])\n",
    "\n",
    "\n",
    "net_gc_df['freqs'] = con_data_df_clean['mne_epoch_dig_after'].apply(lambda x: calculate_gc_indch(x)[3])\n",
    "net_gc_df['freqs_door'] = con_data_df_clean['mne_epoch_door_after'].apply(lambda x: calculate_gc_indch(x)[3])\n",
    "net_gc_df=pd.DataFrame(net_gc_df, columns=['rat_id','task','experiment','date','door_before','door_after','dig_before','dig_after','around_door','around_dig', 'freqs', 'freqs_door'])\n",
    "\n",
    "aon_vhp_gc_df['freqs'] = con_data_df_clean['mne_epoch_dig_after'].apply(lambda x: calculate_gc_indch(x)[3])\n",
    "aon_vhp_gc_df['freqs_door'] = con_data_df_clean['mne_epoch_door_after'].apply(lambda x: calculate_gc_indch(x)[3])\n",
    "aon_vhp_gc_df=pd.DataFrame(aon_vhp_gc_df, columns=['rat_id','task','experiment','date','door_before','door_after','dig_before','dig_after','around_door',\n",
    "                                                'around_dig', 'freqs', 'freqs_door'])\n",
    "\n",
    "vhp_aon_gc_df['freqs'] = con_data_df_clean['mne_epoch_dig_after'].apply(lambda x: calculate_net_gc(x)[3])\n",
    "vhp_aon_gc_df['freqs_door'] = con_data_df_clean['mne_epoch_door_after'].apply(lambda x: calculate_net_gc(x)[3])\n",
    "vhp_aon_gc_df=pd.DataFrame(vhp_aon_gc_df, columns=['rat_id','task','experiment','date','door_before','door_after','dig_before','dig_after','around_door',\n",
    "                                                    'around_dig', 'freqs', 'freqs_door']) \n",
    "\n",
    "\n",
    "net_gc_df.to_pickle(savepath+f'net_gc_events_density_{int(time_window*fs/2)}ms.pkl')\n",
    "aon_vhp_gc_df.to_pickle(savepath+f'aon_vhp_gc_events_density_{int(time_window*fs/2)}ms.pkl')\n",
    "vhp_aon_gc_df.to_pickle(savepath+f'vhp_aon_gc_events_density_{int(time_window*fs/2)}ms.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "time_window = 0.7\n",
    "fs=2000\n",
    "##############\n",
    "\n",
    "\n",
    "import scipy.stats\n",
    "gc_dict = {'net': 'Net', 'aon_vhp': 'AON -> vHP', 'vhp_aon': 'vHP -> AON'}\n",
    "\n",
    "for gc_type in gc_dict.keys():\n",
    "    gc_data_df=pd.read_pickle(savepath+f'{gc_type}_gc_events_density_{int(time_window*fs/2)}ms.pkl')\n",
    "\n",
    "    gc_data_df_bwcontext=gc_data_df[gc_data_df['task']=='BWcontext']\n",
    "    gc_data_df_bwnocontext=gc_data_df[gc_data_df['task']=='BWnocontext']\n",
    "    writer=pd.ExcelWriter(savepath+f'{gc_type}_gc_events_density_{int(time_window*fs/2)}ms.xlsx')\n",
    "\n",
    "    fig,axs=plt.subplots(3,2, sharex=True, sharey=True, figsize=(15,10))\n",
    "    axs=axs.flatten()\n",
    "    fig.suptitle(f'Context vs No Context {gc_dict[gc_type]} Granger Causality')\n",
    "    events_dict={'door_before': 'Door Before','door_after': 'Door After','dig_before': 'Dig Before','dig_after': 'Dig After','around_door': 'Around Door','around_dig': 'Around Dig'}\n",
    "    for i,event in enumerate(events_dict.keys()):\n",
    "        ax=axs[i]\n",
    "        bwcontext_mean=np.mean(gc_data_df_bwcontext[event], axis=0)\n",
    "        bwnocontext_mean=np.mean(gc_data_df_bwnocontext[event], axis=0)\n",
    "        bwcontext_sem=scipy.stats.sem(gc_data_df_bwcontext[event], axis=0)\n",
    "        bwnocontext_sem=scipy.stats.sem(gc_data_df_bwnocontext[event], axis=0)\n",
    "        \n",
    "        freqs=np.linspace(2.5,100,len(bwcontext_mean))\n",
    "        \n",
    "        mean_dict={'frequency':freqs,'bwcontext':bwcontext_mean,'bwnocontext':bwnocontext_mean,'bwcontext_sem':bwcontext_sem,'bwnocontext_sem':bwnocontext_sem}\n",
    "        mean_df=pd.DataFrame(mean_dict)\n",
    "        mean_df.to_excel(writer, sheet_name=event)\n",
    "\n",
    "\n",
    "        ax.plot(freqs, bwcontext_mean, linewidth=2, label='Context')\n",
    "        ax.fill_between(freqs, bwcontext_mean - bwcontext_sem, bwcontext_mean + bwcontext_sem, alpha=0.2)\n",
    "        ax.plot(freqs, gc_data_df_bwnocontext[event].mean(), linewidth=2, label='No Context')\n",
    "        ax.fill_between(freqs, bwnocontext_mean - bwnocontext_sem, bwnocontext_mean + bwnocontext_sem, alpha=0.2)    \n",
    "        ax.plot((freqs[0], freqs[-1]), (0, 0), linewidth=2, linestyle=\"--\", color=\"k\")\n",
    "        ax.axvspan(4,12, alpha=0.2, color='red', label='Theta Range')\n",
    "        ax.axvspan(12,30, alpha=0.2, color='green', label='Beta Range')\n",
    "        ax.axvspan(30,80, alpha=0.2, color='grey', label='Gamma Range')\n",
    "        ax.set_title(f\"{events_dict[event]}\", fontsize=8)\n",
    "        ax.legend(loc='upper right', fontsize=8)\n",
    "    writer.close()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(savepath+f'{gc_type}_gc_events_density_{int(time_window*fs/2)}ms.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "#%matplotlib qt\n",
    "bands_dict={'total':[2.5,100],'theta': [4,12],'beta':[12,30],'gamma':[30,80], 'beta+theta':[4,30], 'early_beta':[12,20], 'late_beta':[20,30]}\n",
    "\n",
    "gc_dict = {'net': 'Net', 'aon_vhp': 'AON -> vHP', 'vhp_aon': 'vHP -> AON'}\n",
    "\n",
    "for gc_type in gc_dict.keys():\n",
    "    gc_data_df=pd.read_pickle(savepath+f'{gc_type}_gc_events_density_{int(time_window*fs/2)}ms.pkl')\n",
    "    gc_data_df_bwcontext=gc_data_df[gc_data_df['task']=='BWcontext']\n",
    "    gc_data_df_bwnocontext=gc_data_df[gc_data_df['task']=='BWnocontext']\n",
    "    def calculate_gc_per_band(gc_array,freqs_array, bands_dict):\n",
    "        freqs_array = np.array(freqs_array)  # Convert freqs_array to numpy array\n",
    "        print(len(gc_array))\n",
    "        gc_bands_dict={}\n",
    "        for band in bands_dict.keys():\n",
    "            band_indices=np.where((freqs_array>=bands_dict[band][0]) & (freqs_array<=bands_dict[band][1]))\n",
    "            gc_band=gc_array[band_indices]\n",
    "            gc_bands_dict[band]=np.sum(gc_band)*(freqs_array[1]-freqs_array[0])\n",
    "            #gc_bands_dict[band]=(np.sum(gc_band)*0.5)/len(gc_band)\n",
    "        return gc_bands_dict\n",
    "\n",
    "    test_gc=gc_data_df_bwcontext['door_before'].iloc[0]\n",
    "    test_freqs=gc_data_df_bwcontext['freqs'].iloc[0]\n",
    "    test_gc_band=calculate_gc_per_band(test_gc,test_freqs, bands_dict)\n",
    "    print(test_gc_band)\n",
    "\n",
    "    gc_cols = ['door_before', 'door_after', 'dig_before', 'dig_after','around_door','around_dig']\n",
    "    gc_data_df_bands = []\n",
    "\n",
    "    for index, row in gc_data_df.iterrows():\n",
    "        rat_id = row['rat_id']\n",
    "        task = row['task']\n",
    "        for gc_col in gc_cols:\n",
    "            if gc_col=='around_door_truncated' or gc_col=='around_dig_truncated':\n",
    "                freqs = row['freqs_trunc']\n",
    "            elif gc_col=='around_door' or gc_col=='around_dig':\n",
    "                freqs = row['freqs_door']\n",
    "            else:\n",
    "                freqs = row['freqs']\n",
    "            gc_values = calculate_gc_per_band(row[gc_col], freqs, bands_dict)\n",
    "            for band, gc_value in gc_values.items():\n",
    "                gc_data_df_bands.append({\n",
    "                    'rat_id': rat_id,\n",
    "                    'task': task,\n",
    "                    'event': gc_col,\n",
    "                    'band': band,\n",
    "                    'gcvalue': gc_value\n",
    "                })\n",
    "\n",
    "    gc_data_df_bands = pd.DataFrame(gc_data_df_bands)\n",
    "    gc_data_df_bands=gc_data_df_bands[gc_data_df_bands['task']!='nocontext']\n",
    "    print(gc_data_df_bands)\n",
    "    writer=pd.ExcelWriter(savepath+f'gc_events_perband_{int(time_window*fs/2)}ms.xlsx')\n",
    "    fig, axs=plt.subplots(3,2, sharex=False, sharey=True, figsize=(15,10))\n",
    "    axs=axs.flatten()\n",
    "    fig.suptitle('Average Net AON -> vHp granger causality per band')\n",
    "    for i, event in enumerate(gc_cols):\n",
    "        print(i, event)\n",
    "        ax=axs[i]\n",
    "        gc_event=gc_data_df_bands[gc_data_df_bands['event']==event]\n",
    "        gc_event.to_excel(writer, sheet_name=event)\n",
    "        ax.axhline(0, color='black', lw=1)\n",
    "        sns.boxplot(x='band',y='gcvalue',hue='task',hue_order=['BWcontext','BWnocontext'],data=gc_event,showfliers=False, ax=ax)\n",
    "        sns.stripplot(x='band',y='gcvalue',hue='task',hue_order=['BWcontext','BWnocontext'],data=gc_event,dodge=True,edgecolor='black',linewidth=1,jitter=True, legend=False, ax=ax)\n",
    "        ax.set_title(f\"{event}\", fontsize=10)\n",
    "        ax.set_xlabel('')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(savepath+f'gc_events_perband_{int(time_window*fs/2)}ms.png')\n",
    "    plt.show()\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making GC Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test Case\n",
    "con_data_df_clean=pd.read_pickle(savepath+'mne_epochs_array_df_truncated_1400.pkl')\n",
    "\n",
    "test_epoch = con_data_df_clean['mne_epoch_around_dig'].iloc[0]\n",
    "fmin=2.5\n",
    "fmax=100\n",
    "fs=2000\n",
    "freqs = np.arange(fmin,fmax)\n",
    "n_cycles = freqs/3\n",
    "\n",
    "###Specifying the Indices for AON and vHp channels\n",
    "aon_signals=[\n",
    "idx\n",
    "for idx, ch_info in enumerate(test_epoch.info[\"chs\"])\n",
    "if \"AON\" in ch_info[\"ch_name\"]\n",
    "]\n",
    "print(aon_signals)\n",
    "vhp_signals=[\n",
    "    idx\n",
    "    for idx, ch_info in enumerate(test_epoch.info[\"chs\"])\n",
    "    if \"vHp\" in ch_info[\"ch_name\"]\n",
    "]\n",
    "print(vhp_signals)\n",
    "\n",
    "indices_aon_vhp = (np.array([aon_signals]), np.array([vhp_signals]))\n",
    "print(indices_aon_vhp)\n",
    "import itertools\n",
    "\n",
    "indices_pairs = list(itertools.product(aon_signals, vhp_signals))\n",
    "indices = (\n",
    "    np.array([pair[0] for pair in indices_pairs]),\n",
    "    np.array([pair[1] for pair in indices_pairs])\n",
    ")\n",
    "print(indices)\n",
    "# indices = [([aon], [vhp]) for aon in aon_signals for vhp in vhp_signals]\n",
    "# print(indices)\n",
    "\n",
    "\n",
    "con = mne_connectivity.spectral_connectivity_epochs(test_epoch, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n",
    "                                        mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                        cwt_n_cycles=n_cycles, verbose=True, fmin=fmin, fmax=fmax, faverage=False, gc_n_lags=40, n_jobs=-1)\n",
    "coh = con.get_data()\n",
    "indices = con.names\n",
    "aon_vHp_con = []\n",
    "print(coh.shape, indices)\n",
    "\n",
    "plt.imshow(coh[0, :, :], extent=[-0.7, 0.7, 1, 100], aspect='auto', origin='lower', cmap='jet')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df_clean=pd.read_pickle(savepath+'mne_epochs_array_df_truncated_1400.pkl')\n",
    "all_gc_data=[]\n",
    "for row in con_data_df_clean.itertuples(index=False):\n",
    "    experiment = row.experiment\n",
    "    rat_id = row.rat_id\n",
    "    task = row.task\n",
    "    mne_epoch = row.mne_epoch_door_before\n",
    "    data_around_dig = row.mne_epoch_around_dig\n",
    "    data_before_dig = row.mne_epoch_dig_before\n",
    "    data_after_dig = row.mne_epoch_dig_after\n",
    "    data_before_door = row.mne_epoch_door_before\n",
    "    data_after_door = row.mne_epoch_door_after\n",
    "\n",
    "    \n",
    "    event_of_interest = data_around_dig  ### CHANGE THIS TO THE DESIRED EVENT\n",
    "\n",
    "    print(f'Processing Rat: {rat_id}, Experiment: {experiment}, Task: {task}')\n",
    "    #print(event_of_interest.get_data().shape)  # Should be (n_epochs, n_channels,n_times)\n",
    "\n",
    "    aon_signals=[\n",
    "        idx\n",
    "        for idx, ch_info in enumerate(event_of_interest.info[\"chs\"])\n",
    "        if \"AON\" in ch_info[\"ch_name\"]\n",
    "        ]\n",
    "    #print(aon_signals)\n",
    "    vhp_signals=[\n",
    "        idx\n",
    "        for idx, ch_info in enumerate(event_of_interest.info[\"chs\"])\n",
    "        if \"vHp\" in ch_info[\"ch_name\"]\n",
    "    ]\n",
    "    #print(vhp_signals)\n",
    "\n",
    "    indices_aon_vhp = (np.array([aon_signals]), np.array([vhp_signals]))\n",
    "    print(indices_aon_vhp)\n",
    "    indices_vhp_aon = (np.array([vhp_signals]), np.array([aon_signals]))\n",
    "\n",
    "    con_aon_vhp = mne_connectivity.spectral_connectivity_epochs(event_of_interest, method='gc', sfreq=int(fs), indices=indices_aon_vhp,\n",
    "                                        mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                        cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False, gc_n_lags=20, n_jobs=-1)\n",
    "    con_vhp_aon = mne_connectivity.spectral_connectivity_epochs(event_of_interest, method='gc', sfreq=int(fs), indices=indices_vhp_aon,\n",
    "                                        mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                        cwt_n_cycles=n_cycles, verbose=False, fmin=fmin, fmax=fmax, faverage=False, gc_n_lags=20, n_jobs=-1)\n",
    "    coh_aon_vhp = con_aon_vhp.get_data()[0,:,:]\n",
    "    coh_vhp_aon = con_vhp_aon.get_data()[0,:,:]\n",
    "    \n",
    "    new_row = [experiment, rat_id, task, coh_aon_vhp, coh_vhp_aon]\n",
    "    all_gc_data.append(new_row)\n",
    "all_gc_data_df = pd.DataFrame(all_gc_data, columns=['experiment', 'rat_id', 'task', 'gc_aon_vhp', 'gc_vhp_aon'])\n",
    "all_gc_data_df.to_pickle(savepath+'gc_around_dig.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making 2 plots of BWcontext and BWNocontext with Net GC in each subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gc_data_df.to_pickle(savepath+'gc_around_dig.pkl')\n",
    "\n",
    "all_gc_data_df['net_gc'] = all_gc_data_df['gc_aon_vhp'] - all_gc_data_df['gc_vhp_aon']\n",
    "\n",
    "vmin = all_gc_data_df['net_gc'].apply(np.min).min()\n",
    "vmax = all_gc_data_df['net_gc'].apply(np.max).max()\n",
    "\n",
    "BWcontext_data=all_gc_data_df[(all_gc_data_df['task']=='BWcontext')]\n",
    "BWnocontext_data=all_gc_data_df[(all_gc_data_df['task']=='BWnocontext')]\n",
    "task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "\n",
    "for group_name, group_df in task_data_dict.items():\n",
    "    fig, axs = plt.subplots(group_df.shape[0] // 5 + int(group_df.shape[0] % 5 != 0), 5, figsize=(25, 10), sharex=True, sharey=True)\n",
    "    axs = axs.flatten()\n",
    "    for i, (idx, row) in enumerate(group_df.iterrows()):\n",
    "        data = np.array(row['net_gc'])\n",
    "        ax = axs[i]\n",
    "        im = ax.imshow(data, extent=[-0.7, 0.7, 1, 100], aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "        ax.set_title(f\"{row['rat_id']} {row['experiment']}\")\n",
    "        ax.axvline(0, color='k', linestyle='--', linewidth=2)\n",
    "        ax.axhline(12, color='green', linestyle='--')\n",
    "        ax.axhline(30, color='green', linestyle='--')\n",
    "    for j in range(i + 1, len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "    fig.suptitle(f\"{group_name} AON -> vHp net GC\", fontsize=16)\n",
    "    fig.colorbar(im, ax=axs, orientation='vertical', fraction=0.02)\n",
    "    #fig.savefig(savepath + f'coherence_around_dig_{group_name}.png', dpi=300, bbox_inches='tight')\n",
    "    #plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "gc_list = ['gc_aon_vhp', 'gc_vhp_aon', 'net_gc']\n",
    "for gc in gc_list:\n",
    "    bw_context_mean=BWcontext_data[gc].mean()\n",
    "    bw_nocontext_mean=BWnocontext_data[gc].mean()\n",
    "    #print(bw_context_mean.shape, bw_nocontext_mean.shape)\n",
    "    vmin = min(bw_context_mean.min(), bw_nocontext_mean.min())\n",
    "    vmax = max(bw_context_mean.max(), bw_nocontext_mean.max())\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    fig.suptitle(f'Average {gc} Around Dig')\n",
    "    axs[0].imshow(bw_context_mean, extent=[-0.7, 0.7, 1, 100], aspect='auto', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[0].set_title('Context')\n",
    "    axs[1].imshow(bw_nocontext_mean, extent=[-0.7, 0.7, 1, 100], aspect='auto', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[1].set_title('No Context')\n",
    "    for ax in axs:\n",
    "        ax.axvline(0, color='k', linestyle='--', linewidth=2)\n",
    "        ax.axhline(12, color='green', linestyle='--')\n",
    "        ax.axhline(30, color='green', linestyle='--')\n",
    "    fig.colorbar(axs[0].images[0], ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gc_data_df = pd.read_pickle(savepath+'gc_around_dig.pkl')\n",
    "\n",
    "for row in all_gc_data_df.itertuples(index=False):\n",
    "    experiment = row.experiment\n",
    "    rat_id = row.rat_id\n",
    "    task = row.task\n",
    "    gc_aon_vhp = row.gc_aon_vhp\n",
    "    gc_vhp_aon = row.gc_vhp_aon\n",
    "    net_gc = gc_aon_vhp - gc_vhp_aon\n",
    "    vmin = min(gc_aon_vhp.min(), gc_vhp_aon.min(), net_gc.min())\n",
    "    vmax = max(gc_aon_vhp.max(), gc_vhp_aon.max(), net_gc.max())\n",
    "    print(f'Processing Rat: {rat_id}, Experiment: {experiment}, Task: {task}')\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "    fig.suptitle(f'Granger Causality Rat: {rat_id}, Experiment: {experiment}, Task: {task}', fontsize=20)\n",
    "\n",
    "    im = axs[0].imshow(gc_aon_vhp, extent=[-0.7, 0.7, 1, 100], aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[0].set_title('AON -> vHp')\n",
    "    axs[0].set_xlabel('Time (s)')\n",
    "    axs[0].set_ylabel('Frequency (Hz)')\n",
    "    \n",
    "    axs[1].imshow(gc_vhp_aon, extent=[-0.7, 0.7, 1, 100], aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[1].set_title('vHp -> AON')\n",
    "    axs[1].set_xlabel('Time (s)')\n",
    "\n",
    "    axs[2].imshow(net_gc, extent=[-0.7, 0.7, 1, 100], aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[2].set_title('Difference (AON -> vHp) - (vHp -> AON)')\n",
    "    axs[2].set_xlabel('Time (s)')\n",
    "    axs[2].set_ylabel('Frequency (Hz)')\n",
    "\n",
    "    for ax in axs:\n",
    "        ax.axvline(0, color='k', linestyle='--', linewidth=2)\n",
    "        ax.axhline(12, color='green', linestyle='--')\n",
    "        ax.axhline(30, color='green', linestyle='--')\n",
    "    # Create a common colorbar for all three subplots\n",
    "    # Remove the previous imshow call for axs[2] above, use this one for colorbar\n",
    "    cbar = fig.colorbar(im, ax=axs, orientation='vertical', fraction=0.02)\n",
    "    #plt.tight_layout()\n",
    "    #plt.savefig(savepath + f'{task}_{rat_id}_{experiment}_gc_around_dig.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_data_df_clean=pd.read_pickle(savepath+'mne_epochs_array_df.pkl')\n",
    "\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "\n",
    "print(event_list)\n",
    "BWcontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWcontext')]\n",
    "BWnocontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWnocontext')]\n",
    "task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "\n",
    "rat_list=np.unique(con_data_df_clean['rat_id'])\n",
    "print(rat_list)\n",
    "all_con_data=[]\n",
    "all_con_data_mean=[]\n",
    "for task_num,task_name in enumerate(task_data_dict.keys()):\n",
    "        task_data=task_data_dict[task_name]\n",
    "    #print(task_name)\n",
    "    # for rat_num, rat_name in enumerate(rat_list):\n",
    "    #     rat_task_data=task_data[task_data['rat_id']==rat_name]\n",
    "        row=[task_name]\n",
    "    #     #print(row)\n",
    "        row_2=[task_name]\n",
    "        for event in event_list:\n",
    "            #print(event)\n",
    "            event_epoch_list=task_data[event]\n",
    "            aon_vHp_con=[]\n",
    "            for event_epoch in event_epoch_list:\n",
    "                    #print(row,event, event_epoch) \n",
    "                    fmin=1\n",
    "                    fmax=100\n",
    "                    freqs = np.arange(fmin,fmax)\n",
    "                    n_cycles = freqs/2\n",
    "                    \n",
    "                    ###Specifying the Indices for AON and vHp channels\n",
    "                    aon_signals=[\n",
    "                    idx\n",
    "                    for idx, ch_info in enumerate(event_epoch.info[\"chs\"])\n",
    "                    if \"AON\" in ch_info[\"ch_name\"]\n",
    "                    ]\n",
    "                    print(aon_signals)\n",
    "                    vhp_signals=[\n",
    "                        idx\n",
    "                        for idx, ch_info in enumerate(event_epoch.info[\"chs\"])\n",
    "                        if \"vHp\" in ch_info[\"ch_name\"]\n",
    "                    ]\n",
    "                    print(vhp_signals)\n",
    "\n",
    "                    indices_aon_vhp = (np.array([aon_signals]), np.array([vhp_signals]))\n",
    "                    indices_vhp_aon = (np.array([vhp_signals]), np.array([aon_signals]))      \n",
    "                    gc_ab = mne_connectivity.spectral_connectivity_epochs(event_epoch, method=[\"gc\"], indices=indices_aon_vhp, fmin=2.5, fmax=100, rank=None,gc_n_lags=20)\n",
    "                    gc_ba= mne_connectivity.spectral_connectivity_epochs(event_epoch, method=[\"gc\"], indices=indices_vhp_aon, fmin=2.5, fmax=100, rank=None,gc_n_lags=20)\n",
    "                    net_gc= gc_ab.get_data() - gc_ba.get_data()\n",
    "                    print(net_gc.shape)\n",
    "\n",
    "                    coh = net_gc[0]\n",
    "                    #coh=np.abs(coh)\n",
    "                    print(coh.shape)\n",
    "                    indices = coh.names\n",
    "                    print(indices)\n",
    "\n",
    "                    for i in range(coh.shape[0]):\n",
    "                        for j in range(coh.shape[1]):\n",
    "                            if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                                aon_vHp_con.append(coh[i,j,:,:])\n",
    "            row.append(np.mean(aon_vHp_con, axis=0))\n",
    "            row_2.append(np.mean(aon_vHp_con))\n",
    "        all_con_data.append(row)                    \n",
    "        all_con_data_mean.append(row_2)\n",
    "# Convert all_con_data to a DataFrame for easier manipulation\n",
    "all_con_data_df = pd.DataFrame(all_con_data, columns=['task'] + event_list)\n",
    "#all_con_data_df.to_pickle(savepath+'coherence_spectrogram_around_door_dig.pkl')\n",
    "fs=2000\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "fs=2000\n",
    "times=np.arange(-2, 2, 1/fs)\n",
    "fig, axs=plt.subplots(2,2, figsize=(20,10), sharey=True)\n",
    "vmin = all_con_data_df[event_list].applymap(np.min).min().min()\n",
    "vmax = all_con_data_df[event_list].applymap(np.max).max().max()\n",
    "event_names=['Around Door','Around Dig']\n",
    "for i, event in enumerate(event_list):\n",
    "    axs[0,i].imshow(all_con_data_df[event][0], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[0,i].set_xlabel('')\n",
    "\n",
    "    axs[0,i].set_ylabel('Frequency (Hz)', fontsize=20)\n",
    "    axs[0,i].set_title(event_names[i], fontsize=20)\n",
    "    axs[0,i].vlines(0, 0, 100, color='k', linestyle='--')\n",
    "    axs[0,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    axs[1,i].imshow(all_con_data_df[event][1], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    axs[1,i].set_xlabel('Time (s)', fontsize=20)\n",
    "    axs[1,i].set_ylabel('Frequency (Hz)', fontsize=20)\n",
    "    axs[1,i].set_title(event_names[i], fontsize=20)\n",
    "    axs[1,i].vlines(0, 0, 100, color='k', linestyle='--')\n",
    "\n",
    "    axs[0,0].text(-0.2, 0.5, 'Context', transform=axs[0,0].transAxes, fontsize=18, verticalalignment='center', rotation=90)\n",
    "    axs[1,0].text(-0.2, 0.5, 'No Context', transform=axs[1,0].transAxes, fontsize=18, verticalalignment='center', rotation=90)\n",
    "    axs[0,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    axs[1,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    axs[0,i].set_xticks(np.arange(-2, 3, 1))  # Set x-ticks from -2 to 2 seconds\n",
    "    axs[0,i].set_xticklabels(np.arange(-2, 3, 1))  # Set x-tick labels from -2 to 2 seconds\n",
    "    axs[1,i].set_xticks(np.arange(-2, 3, 1))  # Set x-ticks from -2 to 2 seconds\n",
    "    axs[1,i].set_xticklabels(np.arange(-2, 3, 1))  # Set x-tick labels from -2 to 2 seconds\n",
    "\n",
    "    # Add a colorbar\n",
    "cbar = fig.colorbar(axs[0,0].images[0], ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "cbar.set_label('Coherence', loc='center', fontsize=20, labelpad=10)\n",
    "cbar.ax.tick_params(labelsize=20)  # Set colorbar tick label size\n",
    "\n",
    "#fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\aon_vhp_coherence_event_spectrogram.png',format='png', dpi=600, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating complex coherence values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated Coherence with Quiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "time_window = 0.7\n",
    "fs=2000\n",
    "#################\n",
    "\n",
    "con_data_df_clean=pd.read_pickle(savepath+f'mne_epochs_array_df_shuffled_truncated_{int(time_window*fs)}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "event_list=['mne_epoch_door_before', 'mne_epoch_dig_before','mne_epoch_dig_after','mne_epoch_around_door','mne_epoch_around_dig']\n",
    "fs=2000\n",
    "print(event_list)\n",
    "BWcontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWcontext')]\n",
    "BWnocontext_data=con_data_df_clean[(con_data_df_clean['task']=='BWnocontext')]\n",
    "task_data_dict={'BWcontext':BWcontext_data,'BWnocontext':BWnocontext_data}\n",
    "\n",
    "rat_list=np.unique(con_data_df_clean['rat_id'])\n",
    "print(rat_list)\n",
    "all_coh_abs_data=[]\n",
    "all_coh_abs_data_mean=[]\n",
    "all_coh_phase_data=[]\n",
    "all_coh_phase_data_mean=[]\n",
    "ind_rows_df=[]\n",
    "for task_num,task_name in enumerate(task_data_dict.keys()):\n",
    "        task_data=task_data_dict[task_name]\n",
    "    #print(task_name)\n",
    "    # for rat_num, rat_name in enumerate(rat_list):\n",
    "    #     rat_task_data=task_data[task_data['rat_id']==rat_name]\n",
    "        row_coh_abs=[task_name]\n",
    "        row_coh_abs_mean=[task_name]\n",
    "        row_coh_phase=[task_name]\n",
    "        row_coh_phase_mean=[task_name]\n",
    "\n",
    "        for event in event_list:\n",
    "            #print(event)\n",
    "            event_epoch_list=task_data[event]\n",
    "            rat_id_list=task_data['rat_id']\n",
    "            exp_list=task_data['experiment']\n",
    "            \n",
    "            aon_vhp_coh_abs=[]\n",
    "            aon_vhp_coh_phase=[]\n",
    "            for rowi,event_epoch in enumerate(event_epoch_list):\n",
    "                    #print(row,event, event_epoch) \n",
    "                    rat_id=rat_id_list.iloc[rowi]\n",
    "                    experiment=exp_list.iloc[rowi]\n",
    "                    print(f'Processing Rat: {rat_id}, Experiment: {experiment}, Task: {task_name}, Event: {event}')\n",
    "                    fmin=1\n",
    "                    fmax=100\n",
    "                    freqs = np.arange(fmin,fmax)\n",
    "                    n_cycles = freqs/3\n",
    "                           \n",
    "                    con = mne_connectivity.spectral_connectivity_epochs(event_epoch, method='cohy', sfreq=int(fs),\n",
    "                                                         mode='cwt_morlet', cwt_freqs=freqs,\n",
    "                                                         cwt_n_cycles=n_cycles, verbose=False, fmin=1, fmax=100, faverage=False)\n",
    "                    coh = con.get_data(output='dense')\n",
    "                    coh_abs = np.abs(coh)\n",
    "                    coh_phase = np.angle(coh)\n",
    "\n",
    "                    indices = con.names\n",
    "                    print(indices)\n",
    "                    print(coh.shape)\n",
    "                    print(coh_abs.shape)\n",
    "                    print(coh_phase.shape)\n",
    "\n",
    "                    for i in range(coh.shape[0]):\n",
    "                        for j in range(coh.shape[1]):\n",
    "                            if 'AON' in indices[j] and 'vHp' in indices[i]:\n",
    "                                coherence_abs=coh_abs[i,j,:,:]\n",
    "                                coherence_abs=np.arctanh(coherence_abs)  # Apply Fisher transformation\n",
    "                                aon_vhp_coh_abs.append(coherence_abs)\n",
    "                                aon_vhp_coh_phase.append(coh_phase[i,j,:,:])\n",
    "                                ind_row =[rat_id, experiment, task_name, event, f'{indices[j]}-{indices[i]}', coh_phase[i,j,:,:]]\n",
    "\n",
    "                                ind_rows_df.append(ind_row)\n",
    "\n",
    "            row_coh_abs.append(np.mean(aon_vhp_coh_abs, axis=0))\n",
    "            row_coh_abs_mean.append(np.mean(aon_vhp_coh_abs))\n",
    "            row_coh_phase.append(np.mean(aon_vhp_coh_phase, axis=0))\n",
    "            row_coh_phase_mean.append(np.mean(aon_vhp_coh_phase))\n",
    "        all_coh_abs_data.append(row_coh_abs)\n",
    "        all_coh_abs_data_mean.append(row_coh_abs_mean)\n",
    "        all_coh_phase_data.append(row_coh_phase)\n",
    "        all_coh_phase_data_mean.append(row_coh_phase_mean)\n",
    "        \n",
    "ind_rows_df = pd.DataFrame(ind_rows_df, columns=['rat_id', 'experiment', 'task', 'event', 'channel_pair', 'coherence_phase'])\n",
    "ind_rows_df.to_pickle(savepath+f'coherence_phase_individual_epochs_around_door_dig_shuffled_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "all_coh_abs_data_df = pd.DataFrame(all_coh_abs_data, columns=['task'] + event_list)\n",
    "all_coh_abs_data_df.to_pickle(savepath+f'coherence_abs_spectrogram_around_door_dig_shuffled_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "all_coh_phase_data_df = pd.DataFrame(all_coh_phase_data, columns=['task'] + event_list)\n",
    "all_coh_phase_data_df.to_pickle(savepath+f'coherence_phase_spectrogram_around_door_dig_shuffled_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "\n",
    "fs=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = aon_vhp_coh_phase[0]\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 0.7\n",
    "fs=2000\n",
    "\n",
    "\n",
    "fmin = 12\n",
    "fmax = 30\n",
    "tmin = 0.5\n",
    "tmax = 0.9\n",
    "\n",
    "\n",
    "all_coh_phase_data_df_shuffled=pd.read_pickle(savepath+f'coherence_phase_spectrogram_around_door_dig_shuffled_truncated_{int(time_window*fs)}.pkl')\n",
    "all_coh_phase_data_df_real=pd.read_pickle(savepath+f'coherence_phase_spectrogram_around_door_dig_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract phase data for each condition\n",
    "\n",
    "truncated_phase_data_real = all_coh_phase_data_df_real['mne_epoch_around_dig'].apply(lambda x: x[fmin:fmax, int(tmin*fs):int(tmax*fs)])  # Theta band (8-12 Hz) and time window (0 to 0.4s)\n",
    "\n",
    "# phase_context = np.array(all_coh_phase_data_df.loc[all_coh_phase_data_df['task'] == 'BWcontext', 'mne_epoch_around_dig'].iloc[0]).flatten()\n",
    "# phase_nocontext = np.array(all_coh_phase_data_df.loc[all_coh_phase_data_df['task'] == 'BWnocontext', 'mne_epoch_around_dig'].iloc[0]).flatten()\n",
    "phase_context = np.array(truncated_phase_data_real[all_coh_phase_data_df_real['task'] == 'BWcontext'].iloc[0]).flatten()\n",
    "phase_nocontext = np.array(truncated_phase_data_real[all_coh_phase_data_df_real['task'] == 'BWnocontext'].iloc[0]).flatten()\n",
    "\n",
    "truncated_phase_data_shuffled = all_coh_phase_data_df_shuffled['mne_epoch_around_dig'].apply(lambda x: x[fmin:fmax, int(tmin*fs):int(tmax*fs)])  # Theta band (8-12 Hz) and time window (0 to 0.4s)\n",
    "phase_context_shuffled = np.array(truncated_phase_data_shuffled[all_coh_phase_data_df_shuffled['task'] == 'BWcontext'].iloc[0]).flatten()\n",
    "phase_nocontext_shuffled = np.array(truncated_phase_data_shuffled[all_coh_phase_data_df_shuffled['task'] == 'BWnocontext'].iloc[0]).flatten()\n",
    "\n",
    "# Convert phase to [0, 2pi]\n",
    "phase_context = np.mod(phase_context, 2 * np.pi)\n",
    "phase_nocontext = np.mod(phase_nocontext, 2 * np.pi)\n",
    "\n",
    "phase_context_shuffled = np.mod(phase_context_shuffled, 2 * np.pi)\n",
    "phase_nocontext_shuffled = np.mod(phase_nocontext_shuffled, 2 * np.pi)\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, subplot_kw={'projection': 'polar'}, figsize=(12, 6))\n",
    "axs=axs.flatten()\n",
    "colors = {'BWcontext': 'blue', 'BWnocontext': 'orange'}\n",
    "\n",
    "# Plot BW Context\n",
    "axs[0].hist(phase_context, bins=100, density=True, color=colors['BWcontext'], alpha=0.7)\n",
    "axs[0].set_title('Phase Difference Histogram\\nBW Context', fontsize=16)\n",
    "axs[0].set_xticks(np.linspace(0, 2 * np.pi, 8, endpoint=False))\n",
    "axs[0].set_xlim(0, 2 * np.pi)\n",
    "\n",
    "# Plot BW No Context\n",
    "axs[1].hist(phase_nocontext, bins=100, density=True, color=colors['BWnocontext'], alpha=0.7)\n",
    "axs[1].set_title('Phase Difference Histogram\\nBW No Context', fontsize=16)\n",
    "axs[1].set_xticks(np.linspace(0, 2 * np.pi, 8, endpoint=False))\n",
    "axs[1].set_xlim(0, 2 * np.pi)\n",
    "\n",
    "axs[2].hist(phase_context_shuffled, bins=100, density=True, color=colors['BWcontext'], alpha=0.7)\n",
    "axs[2].set_title('Phase Difference Histogram (Shuffled)\\nBW Context', fontsize=16)\n",
    "axs[2].set_xticks(np.linspace(0, 2 * np.pi, 8, endpoint=False))\n",
    "axs[2].set_xlim(0, 2 * np.pi)\n",
    "\n",
    "axs[3].hist(phase_nocontext_shuffled, bins=100, density=True, color=colors['BWnocontext'], alpha=0.7)\n",
    "axs[3].set_title('Phase Difference Histogram (Shuffled)\\nBW No Context', fontsize=16)\n",
    "axs[3].set_xticks(np.linspace(0, 2 * np.pi, 8, endpoint=False))\n",
    "axs[3].set_xlim(0, 2 * np.pi)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(savepath+f'coherence_phase_histogram_around_dig_truncated_{int(time_window*fs)}.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "# Statistical test for difference in phase distributions between tasks\n",
    "\n",
    "# Use the Kolmogorov-Smirnov test to compare the two phase distributions\n",
    "ks_stat, p_value = ks_2samp(phase_context, phase_nocontext)\n",
    "print(f\"KS statistic: {ks_stat:.4f}, p-value: {p_value:.4e}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"There is a significant difference between the phase distributions of the two tasks.\")\n",
    "else:\n",
    "    print(\"No significant difference between the phase distributions of the two tasks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the polar plot but without averaging across experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 0.7\n",
    "fs=2000\n",
    "\n",
    "\n",
    "fmin = 30 \n",
    "fmax = 80\n",
    "tmin = 0.4\n",
    "tmax = 0.7\n",
    "\n",
    "\n",
    "# all_coh_phase_data_df_shuffled=pd.read_pickle(savepath+f'coherence_phase_spectrogram_around_door_dig_shuffled_truncated_{int(time_window*fs)}.pkl')\n",
    "# all_coh_phase_data_df_real=pd.read_pickle(savepath+f'coherence_phase_spectrogram_around_door_dig_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "all_coh_phase_data_df_shuffled=pd.read_pickle(savepath+f'coherence_phase_individual_epochs_around_door_dig_shuffled_truncated_{int(time_window*fs)}.pkl')\n",
    "all_coh_phase_data_df_real=pd.read_pickle(savepath+f'coherence_phase_individual_epochs_around_door_dig_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract phase data for each condition\n",
    "writer = pd.ExcelWriter(savepath+f'coherence_phase_analysis_around_door_dig_truncated_{int(time_window*fs)}.xlsx')\n",
    "bands_dict = {\n",
    "    'Theta': (4, 8),\n",
    "    'Beta': (12, 30),\n",
    "    'Gamma': (30, 80)\n",
    "}\n",
    "\n",
    "for band_name, (fmin, fmax) in bands_dict.items():\n",
    "    all_coh_phase_data_df_shuffled=pd.read_pickle(savepath+f'coherence_phase_individual_epochs_around_door_dig_shuffled_truncated_{int(time_window*fs)}.pkl')\n",
    "    all_coh_phase_data_df_real=pd.read_pickle(savepath+f'coherence_phase_individual_epochs_around_door_dig_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "    all_coh_phase_data_df_real['coherence_phase'] = all_coh_phase_data_df_real['coherence_phase'].apply(lambda x: x[fmin:fmax, int(tmin*fs):int(tmax*fs)])  # Theta band (8-12 Hz) and time window (0 to 0.4s)\n",
    "\n",
    "    all_coh_phase_data_df_real['coherence_phase'] = all_coh_phase_data_df_real['coherence_phase'].apply(lambda x: x.flatten())  # Theta band (8-12 Hz) and time window (0 to 0.4s)\n",
    "\n",
    "    bw_context_data = all_coh_phase_data_df_real[(all_coh_phase_data_df_real['task'] == 'BWcontext') & (all_coh_phase_data_df_real['event']=='mne_epoch_around_dig')]\n",
    "    phase_context = np.array([value for lst in bw_context_data['coherence_phase'] for value in lst]).flatten()\n",
    "\n",
    "    bw_nocontext_data = all_coh_phase_data_df_real[(all_coh_phase_data_df_real['task'] == 'BWnocontext') & (all_coh_phase_data_df_real['event']=='mne_epoch_around_dig')]\n",
    "    phase_nocontext = np.array([value for lst in bw_nocontext_data['coherence_phase'] for value in lst]).flatten()\n",
    "\n",
    "\n",
    "    all_coh_phase_data_df_shuffled['coherence_phase'] = all_coh_phase_data_df_shuffled['coherence_phase'].apply(lambda x: x[fmin:fmax, int(tmin*fs):int(tmax*fs)])  # Theta band (8-12 Hz) and time window (0 to 0.4s)\n",
    "    all_coh_phase_data_df_shuffled['coherence_phase'] = all_coh_phase_data_df_shuffled['coherence_phase'].apply(lambda x: x.flatten())  # Theta band (8-12 Hz) and time window (0 to 0.4s)\n",
    "    bw_context_data_shuffled = all_coh_phase_data_df_shuffled[(all_coh_phase_data_df_shuffled['task'] == 'BWcontext') & (all_coh_phase_data_df_shuffled['event']=='mne_epoch_around_dig')]\n",
    "    phase_context_shuffled = np.array([value for lst in bw_context_data_shuffled['coherence_phase'] for value in lst]).flatten()\n",
    "\n",
    "\n",
    "    bw_nocontext_data_shuffled = all_coh_phase_data_df_shuffled[(all_coh_phase_data_df_shuffled['task'] == 'BWnocontext') & (all_coh_phase_data_df_shuffled['event']=='mne_epoch_around_dig')]\n",
    "    phase_nocontext_shuffled = np.array([value for lst in bw_nocontext_data_shuffled['coherence_phase'] for value in lst]).flatten()\n",
    "\n",
    "\n",
    "    # Convert phase to [0, 2pi]\n",
    "    phase_context = np.mod(phase_context, 2 * np.pi)\n",
    "    phase_nocontext = np.mod(phase_nocontext, 2 * np.pi)\n",
    "\n",
    "    phase_context_shuffled = np.mod(phase_context_shuffled, 2 * np.pi)\n",
    "    phase_nocontext_shuffled = np.mod(phase_nocontext_shuffled, 2 * np.pi)\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, subplot_kw={'projection': 'polar'}, figsize=(12, 6))\n",
    "    fig.suptitle('Coherence Phase Freq - {}-{} Hz, Time - {}-{} s'.format(fmin, fmax, tmin, tmax), fontsize=16)\n",
    "    axs=axs.flatten()\n",
    "    colors = {'BWcontext': 'black', 'BWnocontext': 'grey'}\n",
    "\n",
    "    # Plot BW Context\n",
    "    axs[0].hist(phase_context, bins=100, density=True, color=colors['BWcontext'], alpha=0.7)\n",
    "    axs[0].set_title('Context', fontsize=16)\n",
    "    axs[0].set_xticks(np.linspace(0, 2 * np.pi, 8, endpoint=False))\n",
    "    axs[0].set_xlim(0, 2 * np.pi)\n",
    "    counts, edges = np.histogram(phase_context, bins=100, range=(0, 2*np.pi))\n",
    "\n",
    "    # 5. Get bin centers\n",
    "    # This is the Python/NumPy way to find the midpoint of each bin\n",
    "    locs = (edges[:-1] + edges[1:]) / 2\n",
    "    closed_locs = np.append(locs, locs[0])\n",
    "    closed_counts = np.append(counts, counts[0])\n",
    "    # 6. Plot the line (MATLAB: plot(locs, counts, 'LineWidth', 3);)\n",
    "    axs[0].plot(closed_locs, closed_counts, 'black', linewidth=1, label='Bin Centers Line') # 'r-' adds color\n",
    "\n",
    "    # Plot BW No Context\n",
    "    axs[1].hist(phase_nocontext, bins=100, density=True, color=colors['BWnocontext'], alpha=0.7)\n",
    "    axs[1].set_title('No Context', fontsize=16)\n",
    "    axs[1].set_xticks(np.linspace(0, 2 * np.pi, 8, endpoint=False))\n",
    "    axs[1].set_xlim(0, 2 * np.pi)\n",
    "\n",
    "    axs[2].hist(phase_context_shuffled, bins=100, density=True, color=colors['BWcontext'], alpha=0.7)\n",
    "    axs[2].set_title('Context(Shuffled)', fontsize=16)\n",
    "    axs[2].set_xticks(np.linspace(0, 2 * np.pi, 8, endpoint=False))\n",
    "    axs[2].set_xlim(0, 2 * np.pi)\n",
    "\n",
    "    axs[3].hist(phase_nocontext_shuffled, bins=100, density=True, color=colors['BWnocontext'], alpha=0.7)\n",
    "    axs[3].set_title('No Context(Shuffled)', fontsize=16)\n",
    "    axs[3].set_xticks(np.linspace(0, 2 * np.pi, 8, endpoint=False))\n",
    "    axs[3].set_xlim(0, 2 * np.pi)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(savepath+f'coherence_phase_polarhist_around_dig_{fmin}_{fmax}_{int(time_window*fs)}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    # Statistical test for difference in phase distributions between tasks\n",
    "\n",
    "\n",
    "    #########Plotting in a simple histogram##########\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(8, 6))\n",
    "    axs=axs.flatten()\n",
    "    fig.suptitle('Coherence Phase Freq - {}-{} Hz, Time - {}-{} s'.format(fmin, fmax, tmin, tmax), fontsize=16)\n",
    "    colors = {'BWcontext': 'black', 'BWnocontext': 'grey'}\n",
    "\n",
    "    # Convert phase values from [0, 2] to [-, ]\n",
    "    phase_context_centered = np.where(phase_context > np.pi, phase_context - 2*np.pi, phase_context)\n",
    "    phase_nocontext_centered = np.where(phase_nocontext > np.pi, phase_nocontext - 2*np.pi, phase_nocontext)\n",
    "    phase_context_shuffled_centered = np.where(phase_context_shuffled > np.pi, phase_context_shuffled - 2*np.pi, phase_context_shuffled)\n",
    "    phase_nocontext_shuffled_centered = np.where(phase_nocontext_shuffled > np.pi, phase_nocontext_shuffled - 2*np.pi, phase_nocontext_shuffled)\n",
    "\n",
    "    # Plotting the histograms with centered x-axis\n",
    "    axs[0].hist(phase_context_centered, bins=100, density=True, color=colors['BWcontext'], alpha=0.7)\n",
    "    axs[0].set_title('Context', fontsize=16)\n",
    "    axs[0].set_xticks(np.linspace(-np.pi, np.pi, 7))\n",
    "    axs[0].set_xticklabels(['-', '-2/3', '-/3', '0', '/3', '2/3', ''])\n",
    "    axs[0].set_xlim(-np.pi, np.pi)\n",
    "\n",
    "    axs[1].hist(phase_nocontext_centered, bins=100, density=True, color=colors['BWnocontext'], alpha=0.7)\n",
    "    axs[1].set_title('No Context', fontsize=16)\n",
    "    axs[1].set_xticks(np.linspace(-np.pi, np.pi, 7))\n",
    "    axs[1].set_xticklabels(['-', '-2/3', '-/3', '0', '/3', '2/3', ''])\n",
    "    axs[1].set_xlim(-np.pi, np.pi)\n",
    "\n",
    "    axs[2].hist(phase_context_shuffled_centered, bins=100, density=True, color=colors['BWcontext'], alpha=0.7)\n",
    "    axs[2].set_title('Context(Shuffled)', fontsize=16)\n",
    "    axs[2].set_xticks(np.linspace(-np.pi, np.pi, 7))\n",
    "    axs[2].set_xticklabels(['-', '-2/3', '-/3', '0', '/3', '2/3', ''])\n",
    "    axs[2].set_xlim(-np.pi, np.pi)\n",
    "\n",
    "    axs[3].hist(phase_nocontext_shuffled_centered, bins=100, density=True, color=colors['BWnocontext'], alpha=0.7)\n",
    "    axs[3].set_title('No Context(Shuffled)', fontsize=16)\n",
    "    axs[3].set_xticks(np.linspace(-np.pi, np.pi, 7))\n",
    "    axs[3].set_xticklabels(['-', '-2/3', '-/3', '0', '/3', '2/3', ''])\n",
    "    axs[3].set_xlim(-np.pi, np.pi)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(savepath+f'coherence_phase_histogram_around_dig_{fmin}_{fmax}_{int(time_window*fs)}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    # Statistical test for difference in phase distributions between tasks\n",
    "\n",
    "    data_dict = {'context_real': phase_context,\n",
    "                 'nocontext_real': phase_nocontext,\n",
    "                 'context_shuffled': phase_context_shuffled,\n",
    "                 'nocontext_shuffled': phase_nocontext_shuffled}\n",
    "    df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in data_dict.items()]))\n",
    "    df.to_excel(writer, sheet_name=f'{band_name}_phase_data', index=False)\n",
    "writer.close()\n",
    "\n",
    "ks_stat, p_value = ks_2samp(phase_context, phase_nocontext)\n",
    "print(f\"KS statistic: {ks_stat:.4f}, p-value: {p_value:.4e}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"There is a significant difference between the phase distributions of the two tasks.\")\n",
    "else:\n",
    "    print(\"No significant difference between the phase distributions of the two tasks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making polar plots with Context and No Context in single plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 0.7\n",
    "fs=2000\n",
    "\n",
    "\n",
    "fmin = 30 \n",
    "fmax = 80\n",
    "tmin = 0.4\n",
    "tmax = 0.7\n",
    "\n",
    "\n",
    "# all_coh_phase_data_df_shuffled=pd.read_pickle(savepath+f'coherence_phase_spectrogram_around_door_dig_shuffled_truncated_{int(time_window*fs)}.pkl')\n",
    "# all_coh_phase_data_df_real=pd.read_pickle(savepath+f'coherence_phase_spectrogram_around_door_dig_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "all_coh_phase_data_df_shuffled=pd.read_pickle(savepath+f'coherence_phase_individual_epochs_around_door_dig_shuffled_truncated_{int(time_window*fs)}.pkl')\n",
    "all_coh_phase_data_df_real=pd.read_pickle(savepath+f'coherence_phase_individual_epochs_around_door_dig_truncated_{int(time_window*fs)}.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract phase data for each condition\n",
    "writer = pd.ExcelWriter(savepath+f'coherence_phase_analysis_around_door_dig_truncated_{int(time_window*fs)}.xlsx')\n",
    "bands_dict = {\n",
    "    'Theta': (4, 8),\n",
    "    'Beta': (12, 30),\n",
    "    'Gamma': (30, 80)\n",
    "}\n",
    "\n",
    "tmin = 0\n",
    "tmax = 0.7\n",
    "\n",
    "tmin_idx = int(tmin * fs)\n",
    "tmax_idx = int(tmax * fs)\n",
    "\n",
    "events_dict = {\n",
    "'mne_epoch_door_before': 'Door Before',\n",
    "'mne_epoch_dig_before': 'Dig Before',\n",
    "'mne_epoch_dig_after': 'Dig After',\n",
    "}\n",
    "\n",
    "fig, axs = plt.subplots(3,3, subplot_kw={'projection': 'polar'}, figsize=(18, 12))\n",
    "\n",
    "for axi_row,(band_name, (fmin, fmax)) in enumerate(bands_dict.items()):\n",
    "\n",
    "    for axi_col,(event_key, event_name) in enumerate(events_dict.items()):\n",
    "\n",
    "        for data_type in ['real', 'shuffled']:\n",
    "            if data_type == 'real':\n",
    "                all_coh_phase_data_df = pd.read_pickle(savepath+f'coherence_phase_individual_epochs_around_door_dig_truncated_{int(time_window*fs)}.pkl')\n",
    "                linestyle = 'solid'  \n",
    "            else:\n",
    "                all_coh_phase_data_df = pd.read_pickle(savepath+f'coherence_phase_individual_epochs_around_door_dig_shuffled_truncated_{int(time_window*fs)}.pkl')\n",
    "                linestyle = 'dashed'\n",
    "            print(f'{axi_row} {axi_col} Processing Band: {band_name}, Event: {event_name}, Data Type: {data_type}')\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "            all_coh_phase_data_df['coherence_phase'] = all_coh_phase_data_df['coherence_phase'].apply(lambda x: x[fmin:fmax, tmin_idx:tmax_idx])\n",
    "            all_coh_phase_data_df['coherence_phase'] = all_coh_phase_data_df['coherence_phase'].apply(lambda x: x.flatten())  \n",
    "\n",
    "            bw_context_data = all_coh_phase_data_df[(all_coh_phase_data_df['task'] == 'BWcontext') & (all_coh_phase_data_df['event']==event_key)]\n",
    "            phase_context = np.array([value for lst in bw_context_data['coherence_phase'] for value in lst]).flatten()\n",
    "\n",
    "            bw_nocontext_data = all_coh_phase_data_df[(all_coh_phase_data_df['task'] == 'BWnocontext') & (all_coh_phase_data_df['event']==event_key)]\n",
    "            phase_nocontext = np.array([value for lst in bw_nocontext_data['coherence_phase'] for value in lst]).flatten()\n",
    "\n",
    "            # Convert phase to [0, 2pi]\n",
    "            phase_context = np.mod(phase_context, 2 * np.pi)\n",
    "            phase_nocontext = np.mod(phase_nocontext, 2 * np.pi)\n",
    "\n",
    "            ax = axs[axi_row, axi_col]    \n",
    "            \n",
    "            counts, edges = np.histogram(phase_context, bins=100, range=(0, 2*np.pi), density=True)\n",
    "            locs = (edges[:-1] + edges[1:]) / 2\n",
    "            closed_locs = np.append(locs, locs[0])\n",
    "            closed_counts = np.append(counts, counts[0])\n",
    "            ax.fill(closed_locs, closed_counts, color='blue', alpha=0.3)  # Fill the area with black color\n",
    "            ax.plot(closed_locs, closed_counts, 'blue', linewidth=1, label=f'Context ({data_type})', linestyle=linestyle) # 'r-' adds color\n",
    "\n",
    "            counts, edges = np.histogram(phase_nocontext, bins=100, range=(0, 2*np.pi), density=True)\n",
    "            locs = (edges[:-1] + edges[1:]) / 2\n",
    "            closed_locs = np.append(locs, locs[0])\n",
    "            closed_counts = np.append(counts, counts[0])\n",
    "            ax.fill(closed_locs, closed_counts, color='orange', alpha=0.3)  # Fill the area with grey color\n",
    "            ax.plot(closed_locs, closed_counts, 'orange', linewidth=1, label=f'No Context ({data_type})', linestyle=linestyle)  #ax.set_xticks(np.linspace(0, 2 * np.pi, 8, endpoint=False))\n",
    "            ax.set_xlim(0, 2 * np.pi)\n",
    "            ax.set_rticks([])  # Remove radial ticks for clarity\n",
    "            # if axi_row == 0 and axi_col == 0:\n",
    "            #     ax.legend(loc='upper right', fontsize=8)\n",
    "\n",
    "            ax.set_title(f'{band_name} Band - {event_name}', fontsize=12)\n",
    "            #ax.set_rmax(0.5)  # Set maximum radius for better visualization\n",
    "# Create custom labels that combine color, style and condition\n",
    "legend_elements = [\n",
    "    plt.Line2D([0], [0], color='blue', linestyle='solid', label='Context (real)'),\n",
    "    plt.Line2D([0], [0], color='blue', linestyle='dashed', label='Context (shuffled)'),\n",
    "    plt.Line2D([0], [0], color='orange', linestyle='solid', label='No Context (real)'),\n",
    "    plt.Line2D([0], [0], color='orange', linestyle='dashed', label='No Context (shuffled)')\n",
    "]\n",
    "\n",
    "# Add legend at the bottom\n",
    "fig.legend(handles=legend_elements, loc='lower center', bbox_to_anchor=(0.5, 0), \n",
    "          ncol=4, fontsize=12)\n",
    "\n",
    "# Adjust subplot spacing to make room for legend\n",
    "plt.subplots_adjust(bottom=0.12)\n",
    "fig.suptitle(f'Coherence Phase Analysis {tmin}s-{tmax}s', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(savepath+f'coherence_phase_polarhist_all_events_{int(tmin*fs)}-{int(tmax*fs)}.png', dpi=300, bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "event_list=['mne_epoch_around_door','mne_epoch_around_dig']\n",
    "fs=2000\n",
    "times=np.arange(-0.7, 0.7, 1/fs)\n",
    "fig, axs=plt.subplots(2,2, figsize=(20,10), sharey=True)\n",
    "all_con_data_df=all_coh_abs_data_df\n",
    "aon_vhp_phase=all_coh_phase_data_df\n",
    "vmin = all_con_data_df[event_list].applymap(np.min).min().min()\n",
    "vmax = all_con_data_df[event_list].applymap(np.max).max().max()\n",
    "event_names=['Around Door','Around Dig']\n",
    "freqs=np.arange(2.5, 100, 0.5)\n",
    "for i, event in enumerate(event_list):\n",
    "    axs[0,i].imshow(all_con_data_df[event][0], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    coh_phase = aon_vhp_phase[event][0]\n",
    "    X, Y = np.meshgrid(np.linspace(times[0], times[-1], coh_phase.shape[1]), np.linspace(freqs[0], freqs[-1], coh_phase.shape[0]))\n",
    "\n",
    "    U = np.cos(coh_phase)\n",
    "    V = np.sin(coh_phase)\n",
    "    f_x = 100\n",
    "    f_y = 5\n",
    "    axs[0, i].quiver(X[2::f_y, ::f_x], Y[2::f_y, ::f_x], U[2::f_y, ::f_x], V[2::f_y, ::f_x], angles='uv', scale=40, alpha=0.7)\n",
    "\n",
    "    axs[0,i].set_xlabel('')\n",
    "\n",
    "    axs[0,i].set_ylabel('Frequency (Hz)', fontsize=20)\n",
    "    axs[0,i].set_title(event_names[i], fontsize=20)\n",
    "    axs[0,i].vlines(0, 0, 100, color='k', linestyle='--')\n",
    "    axs[0,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    \n",
    "    axs[1,i].imshow(all_con_data_df[event][1], extent=[times[0], times[-1], 1, 100],\n",
    "                   aspect='auto', origin='lower', cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    coh_phase = aon_vhp_phase[event][1]\n",
    "    X, Y = np.meshgrid(np.linspace(times[0], times[-1], coh_phase.shape[1]), np.linspace(freqs[0], freqs[-1], coh_phase.shape[0]))\n",
    "\n",
    "    U = np.cos(coh_phase)\n",
    "    V = np.sin(coh_phase)\n",
    "    f_x = 100\n",
    "    f_y = 5\n",
    "    axs[1, i].quiver(X[2::f_y, ::f_x], Y[2::f_y, ::f_x], U[2::f_y, ::f_x], V[2::f_y, ::f_x], angles='uv', scale=40, alpha=0.7)\n",
    "\n",
    "    \n",
    "    axs[1,i].set_xlabel('Time (s)', fontsize=20)\n",
    "    axs[1,i].set_ylabel('Frequency (Hz)', fontsize=20)\n",
    "    axs[1,i].set_title(event_names[i], fontsize=20)\n",
    "    axs[1,i].vlines(0, 0, 100, color='k', linestyle='--')\n",
    "\n",
    "    axs[0,0].text(-0.2, 0.5, 'Context', transform=axs[0,0].transAxes, fontsize=18, verticalalignment='center', rotation=90)\n",
    "    axs[1,0].text(-0.2, 0.5, 'No Context', transform=axs[1,0].transAxes, fontsize=18, verticalalignment='center', rotation=90)\n",
    "    axs[0,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    axs[1,i].tick_params(axis='both', which='major', labelsize=20)\n",
    "    # axs[0,i].set_xticks(np.arange(-2, 3, 1))  # Set x-ticks from -2 to 2 seconds\n",
    "    # axs[0,i].set_xticklabels(np.arange(-2, 3, 1))  # Set x-tick labels from -2 to 2 seconds\n",
    "    # axs[1,i].set_xticks(np.arange(-2, 3, 1))  # Set x-ticks from -2 to 2 seconds\n",
    "    # axs[1,i].set_xticklabels(np.arange(-2, 3, 1))  # Set x-tick labels from -2 to 2 seconds\n",
    "\n",
    "    # Add a colorbar\n",
    "cbar = fig.colorbar(axs[0,0].images[0], ax=axs, orientation='vertical', fraction=0.02, pad=0.04)\n",
    "cbar.set_label('Coherence (Z-transformed)', loc='center', fontsize=20, labelpad=10)\n",
    "cbar.ax.tick_params(labelsize=20)  # Set colorbar tick label size\n",
    "\n",
    "fig.savefig(savepath+'aon_vhp_coherogram.png',format='png', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Behavior Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Behavior Correlation with Power\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_df=pd.read_pickle(savepath+'compiled_data_all_epochs.pkl')\n",
    "behavior_df.iloc[:,-5:]=behavior_df.iloc[:,-5:].applymap(lambda x: scipy.signal.welch(x, fs=2000, nperseg=2000)[1])\n",
    "\n",
    "bands_dict = {'beta': [12, 30], 'gamma': [30, 80], 'theta': [4, 12], 'total': [1, 100]}\n",
    "for col in behavior_df.columns[-7:]:\n",
    "    for band, (band_start, band_end) in bands_dict.items():\n",
    "        behavior_df[band + '_' + col] = behavior_df[col].apply(lambda x: functions.get_band_power(x, band_start, band_end))\n",
    "\n",
    "behavior_df['channel'] = behavior_df['channel'].apply(lambda x: 'AON' if 'AON' in x else 'vHp')\n",
    "\n",
    "behavior_df_grouped=behavior_df.groupby(['task', 'channel'])\n",
    "writer=pd.ExcelWriter(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\behavior_power_correlation.xlsx')\n",
    "for (task, channel), group in behavior_df_grouped:\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(20, 10), sharex=True, constrained_layout=True)\n",
    "    axs = axs.flatten()\n",
    "    group=behavior_df[(behavior_df['channel']==channel) & (behavior_df['task']==task)]\n",
    "\n",
    "    power_columns=group.columns[17:]\n",
    "    print(power_columns)\n",
    "    group_melted=pd.melt(group, id_vars=['rat', 'task', 'channel', 'correct?'], value_vars=power_columns, var_name='band_event', value_name='power')\n",
    "    group_melted['band']=group_melted['band_event'].apply(lambda x: x.split('_')[0])\n",
    "    group_melted['event']=group_melted['band_event'].apply(lambda x: x.split('_')[1:])\n",
    "    group_melted['event']=group_melted['event'].apply(lambda x: x[0]+'_'+x[1])\n",
    "    group_melted['correct?']=group_melted['correct?'].apply(lambda x: 'Incorrect' if x=='0' else 'Correct')\n",
    "    \n",
    "    group_melted.to_excel(writer, sheet_name=f'{channel}_{task}')\n",
    "\n",
    "\n",
    "    correct_counts = group_melted[group_melted['correct?'] == 'Correct'].shape[0]\n",
    "    incorrect_counts = group_melted[group_melted['correct?'] == 'Incorrect'].shape[0]\n",
    "    print(f\"Number of Corrects: {correct_counts}\")\n",
    "    print(f\"Number of Incorrects: {incorrect_counts}\")\n",
    "    events_list=['pre_door','post_door','pre_dig','post_dig']\n",
    "    for i, event in enumerate(events_list):\n",
    "        ax=axs[i]\n",
    "        sns.boxplot(x='band', y='power', hue='correct?', data=group_melted[group_melted['event']==event], showfliers=False, ax=ax)\n",
    "        #sns.stripplot(x='band', y='power', hue='correct?', data=aon_behavior_df_melted[aon_behavior_df_melted['event']==event], dodge=True, edgecolor='black', linewidth=1, jitter=True, ax=ax)\n",
    "        ax.set_title(event)\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('Power')\n",
    "        ax.legend(title='Correct?')\n",
    "    fig.suptitle(f'{channel} {task}')\n",
    "    fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\behavior_power_{channel}_{task}.png')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Behavior Correlation with Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "time_window=  0.4\n",
    "fs=2000\n",
    "behavior_coherence_df=pd.read_pickle(savepath+f'compiled_data_all_epochs_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "behavior_coherence_df['unique_id']=behavior_coherence_df['rat']+'_'+behavior_coherence_df['task']+behavior_coherence_df['date']\n",
    "behavior_coherence_df_grouped=behavior_coherence_df.groupby(['unique_id', 'trial'])\n",
    "behavior_coherence_compiled_data_df=[]\n",
    "\n",
    "for (unique_id, trial), group in behavior_coherence_df_grouped:\n",
    "    print(unique_id, trial)\n",
    "    channels_list=list(group['channel'].unique())\n",
    "    print(channels_list)\n",
    "    info=mne.create_info(ch_names=channels_list, sfreq=fs, ch_types='eeg')\n",
    "\n",
    "    mne_epoch_door_before=np.zeros((1,len(channels_list),int(time_window*fs)))\n",
    "    mne_epoch_door_after=np.zeros((1,len(channels_list),int(time_window*fs)))\n",
    "    mne_epoch_dig_before=np.zeros((1,len(channels_list),int(time_window*fs)))\n",
    "    mne_epoch_dig_after=np.zeros((1,len(channels_list),int(time_window*fs)))\n",
    "    mne_epoch_around_door=np.zeros((1,len(channels_list),int(time_window*fs)*2))\n",
    "    mne_epoch_around_dig=np.zeros((1,len(channels_list),int(time_window*fs)*2))\n",
    "\n",
    "    for channel_num, channel_id in enumerate(channels_list):\n",
    "        data=group[group['channel']==channel_id]\n",
    "        mne_epoch_door_before[0,channel_num,:]=data['pre_door'].values[0][:int(time_window*fs)]\n",
    "        mne_epoch_door_after[0,channel_num,:]=data['post_door'].values[0][:int(time_window*fs)]\n",
    "        mne_epoch_dig_before[0,channel_num,:]=data['pre_dig'].values[0][:int(time_window*fs)]\n",
    "        mne_epoch_dig_after[0,channel_num,:]=data['post_dig'].values[0][:int(time_window*fs)]\n",
    "        mid_point = int(len(data['around_door'].values[0])/2)\n",
    "        mne_epoch_around_door[0,channel_num,:]=data['around_door'].values[0][mid_point-int(time_window*fs):mid_point+int(time_window*fs)]\n",
    "        mne_epoch_around_dig[0,channel_num,:]=data['around_dig'].values[0][mid_point-int(time_window*fs):mid_point+int(time_window*fs)]\n",
    "\n",
    "    # mne_epoch_around_door_truncated = mne_epoch_around_door[:, :, 3000:5000]\n",
    "    # mne_epoch_around_dig_truncated = mne_epoch_around_dig[:, :, 3000:5000]\n",
    "    mne_epoch_door_before = mne.EpochsArray(mne_epoch_door_before, info)\n",
    "    mne_epoch_door_after= mne.EpochsArray(mne_epoch_door_after, info)\n",
    "    mne_epoch_dig_before = mne.EpochsArray(mne_epoch_dig_before, info)\n",
    "    mne_epoch_dig_after = mne.EpochsArray(mne_epoch_dig_after, info)\n",
    "    mne_epoch_around_door = mne.EpochsArray(mne_epoch_around_door, info)\n",
    "    mne_epoch_around_dig = mne.EpochsArray(mne_epoch_around_dig, info)\n",
    "    \n",
    "    behavior_coherence_compiled_data={\n",
    "        'rat': group['rat'].values[0],\n",
    "        'task': group['task'].values[0],\n",
    "        'date': group['date'].values[0],\n",
    "        'unique_id': unique_id,\n",
    "        'trial': trial,\n",
    "        'side': group['side'].values[0],\n",
    "        'correct?': group['correct'].values[0],\n",
    "        'time_to_dig': group['timestamps'].iloc[0][1] - group['timestamps'].iloc[0][0],\n",
    "        'pre_door': mne_epoch_door_before,\n",
    "        'post_door': mne_epoch_door_after,\n",
    "        'pre_dig': mne_epoch_dig_before,\n",
    "        'post_dig': mne_epoch_dig_after,\n",
    "        'around_door': mne_epoch_around_door,\n",
    "        'around_dig': mne_epoch_around_dig\n",
    "        ,'around_door_truncated': mne_epoch_around_door,\n",
    "        'around_dig_truncated': mne_epoch_around_dig}\n",
    "    \n",
    "    behavior_coherence_compiled_data_df.append(behavior_coherence_compiled_data)\n",
    "behavior_coherence_compiled_data_df=pd.DataFrame(behavior_coherence_compiled_data_df)\n",
    "behavior_coherence_compiled_data_df.to_pickle(savepath+f'behavior_coherence_single_epochs_mne_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "# def lfp_to_mne_epoch(lfp_data):\n",
    "#     fs=2000\n",
    "#     freqs = np.arange(1,100)\n",
    "#     n_cycles = freqs/2\n",
    "#     empty_array=np.zeros((1,1,len(lfp_data)))\n",
    "#     empty_array[0,0,:]=lfp_data\n",
    "#     info = mne.create_info(ch_names=['1'], sfreq=fs, ch_types='eeg')\n",
    "#     mne_epoch = mne.EpochsArray(empty_array, info)\n",
    "#     return mne_epoch\n",
    "\n",
    "# behavior_coherence_df[['pre_door','post_door', 'pre_dig', 'post_dig']]=behavior_coherence_df[['pre_door','post_door', 'pre_dig', 'post_dig']].applymap(lambda x: lfp_to_mne_epoch(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the short MNE Epochs to coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window = 0.7  # seconds\n",
    "fs = 2000  # Sampling frequency\n",
    "for time_window in [0.4, 0.7]:\n",
    "    behavior_coherence_compiled_data_df_truncated=pd.read_pickle(savepath+f'behavior_coherence_single_epochs_mne_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "    print(behavior_coherence_compiled_data_df_truncated.head())\n",
    "    importlib.reload(coherence_functions)\n",
    "    bands_dict = {'beta': [12, 30], 'gamma': [30, 80],'theta':[4,12], 'total': [1, 100]}\n",
    "    for col in ['pre_door', 'post_door', 'pre_dig', 'post_dig', 'around_door','around_dig']:\n",
    "        print(col)\n",
    "        for band, (band_start, band_end) in bands_dict.items():\n",
    "            behavior_coherence_compiled_data_df_truncated[band + '_' + col] = behavior_coherence_compiled_data_df_truncated[col].apply(lambda x: coherence_functions.convert_epoch_to_coherence_behavior(x, band_start=band_start, band_end=band_end))\n",
    "    behavior_coherence_compiled_data_df_truncated.drop(columns=['pre_door', 'post_door', 'pre_dig', 'post_dig', 'around_door','around_dig'], inplace=True)\n",
    "    behavior_coherence_compiled_data_df_truncated.to_pickle(savepath+f'\\\\behavior_coherence_compiled_data_df_truncated_{int(time_window*fs)}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Coherence vs Time to Dig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window=  0.7\n",
    "fs=2000\n",
    "from scipy.stats import linregress\n",
    "behavior_coherence_compiled_data_df_truncated=pd.read_pickle(savepath+f'behavior_coherence_compiled_data_df_truncated_{int(time_window*fs)}.pkl')\n",
    "#behavior_coherence_compiled_data_df_truncated['beta_pre_dig'] = behavior_coherence_compiled_data_df_truncated['beta_pre_dig'] - behavior_coherence_compiled_data_df_truncated['beta_pre_door']\n",
    "#behavior_coherence_compiled_data_df_truncated['theta_pre_dig'] = behavior_coherence_compiled_data_df_truncated['theta_pre_dig'] - behavior_coherence_compiled_data_df_truncated['theta_pre_door']\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(3,2,figsize=(20, 20), sharey=True, constrained_layout=True)\n",
    "axs = axs.flatten()\n",
    "\n",
    "fig_hist, axs_hist = plt.subplots(3, 2, figsize=(20, 20), sharey=True, constrained_layout=True)\n",
    "axs_hist = axs_hist.flatten()\n",
    "\n",
    "grouped_df=behavior_coherence_compiled_data_df_truncated.groupby(['task'])\n",
    "band='theta'\n",
    "event='around_dig'\n",
    "task_dict = {'BWcontext': 'Context', 'BWnocontext': 'No Context'}\n",
    "\n",
    "events_dict ={'pre_door':'Pre Door', 'post_door':'Post Door', 'pre_dig':'Pre Dig', 'post_dig':'Post Dig', 'around_door':'Around Door', 'around_dig':'Around Dig'}\n",
    "for i, event in enumerate(events_dict.keys()):\n",
    "    ax = axs[i]\n",
    "    ax_hist = axs_hist[i]\n",
    "    print(np.where(behavior_coherence_compiled_data_df_truncated['{}_{}'.format(band,event)]==0))\n",
    "    for task, group in grouped_df:\n",
    "        print(task)\n",
    "        print('{}_{}'.format(band,event))\n",
    "        group = group[(np.abs(group['{}_{}'.format(band,event)]))>0]\n",
    "        print(group['{}_{}'.format(band,event)].mean())\n",
    "        group = group[(np.abs(group['time_to_dig'] - group['time_to_dig'].mean()) <= (3 * group['time_to_dig'].std()))] # Removing Outliers from Time\n",
    "        group = group[(np.abs(group['{}_{}'.format(band,event)] - group['{}_{}'.format(band,event)].mean()) <= (3 * group['{}_{}'.format(band,event)].std()))]  #Removing Outliers from Coherence\n",
    "        \n",
    "        #Plotting Regression\n",
    "        sns.regplot(y='time_to_dig', x='{}_{}'.format(band,event), data=group, ax=ax, label=task[0])\n",
    "        y= group['time_to_dig'].values\n",
    "        x= group['{}_{}'.format(band,event)].values\n",
    "        slope, intercept, r, p, se = linregress(x, y)\n",
    "        print(f'{task[0]}: Slope: {slope}, Intercept: {intercept}, R-squared: {r**2}, P-value: {p}')\n",
    "        \n",
    "        ## Plotting Histogram\n",
    "        sns.histplot(group['{}_{}'.format(band,event)], bins=30, kde=True, ax=ax_hist, label=task[0], color=colors[task[0]], stat='density', alpha=0.5)\n",
    "        ax_hist.axvline(group['{}_{}'.format(band,event)].mean(), color=colors[task[0]], linestyle='--', label=f'{task[0]} Mean')\n",
    "        ax_hist.axvline(group['{}_{}'.format(band,event)].median(), color=colors[task[0]], linestyle=':', label=f'{task[0]} Median')\n",
    "    \n",
    "    #Setting Histogram axis labels and title and legend\n",
    "    ax_hist.set_title(f'{events_dict[event]} - {band} Coherence Histogram', fontsize=16)\n",
    "    ax_hist.set_xlabel('Beta Coherence (Z-transformed)', fontsize=14)            \n",
    "    handles, labels = ax_hist.get_legend_handles_labels()\n",
    "    ax_hist.legend()\n",
    "    #ax_hist.legend(handles, [task_dict[l] for l in labels], title='Task', loc='upper right', fontsize=12)    \n",
    "    \n",
    "    \n",
    "    ax.set_title(f'{events_dict[event]}', fontsize=16)\n",
    "    ax.set_xlabel('Beta Coherence (Z-transformed)', fontsize=14)\n",
    "    ax.set_ylabel('Time to Dig (s)', fontsize=14)\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    ax.legend(handles, [task_dict[l] for l in labels], title='Task', loc='upper right', fontsize=12)\n",
    "    #ax.legend(title='Task')\n",
    "    \n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.tight_layout()\n",
    "fig.suptitle(f'{band} Coherence vs Time to Dig', fontsize=20, y=1.02)\n",
    "#fig.savefig(savepath+f'{band}_coherence_vs_time_to_dig_{int(time_window*fs)}.png', format='png', dpi=600, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherence vs Time to Dig per rat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window=  0.4\n",
    "fs=2000\n",
    "event_of_interest = 'post_dig'\n",
    "band_of_interest = 'gamma'\n",
    "\n",
    "event_band = f'{band_of_interest}_{event_of_interest}'\n",
    "\n",
    "from scipy.stats import linregress\n",
    "behavior_coherence_compiled_data_df_truncated=pd.read_pickle(savepath+f'behavior_coherence_compiled_data_df_truncated_{int(time_window*fs)}.pkl')\n",
    "task_experiments = behavior_coherence_compiled_data_df_truncated.groupby(['task','unique_id'])\n",
    "task_list= behavior_coherence_compiled_data_df_truncated['task'].unique()\n",
    "print(task_list)\n",
    "for task in task_list:\n",
    "    task_data = behavior_coherence_compiled_data_df_truncated[behavior_coherence_compiled_data_df_truncated['task'] == task]\n",
    "    unique_experiments = task_data['unique_id'].unique()\n",
    "\n",
    "    fig, axs = plt.subplots(4, 5, figsize=(20, 20))\n",
    "    fig.suptitle(f'{task} - Coherence vs Time to Dig', fontsize=20, y=1.02)\n",
    "    dk1_i=0\n",
    "    dk3_i=0\n",
    "    dk5_i=0\n",
    "    dk6_i=0\n",
    "    for experiment in unique_experiments:\n",
    "        print(task, experiment)    \n",
    "        task_exp_data =  task_data[task_data['unique_id'] == experiment]\n",
    "        time_to_dig = task_exp_data['time_to_dig']\n",
    "        coherence_value = task_exp_data[event_band]\n",
    "        \n",
    "        rat_id = task_exp_data['rat'].values[0]\n",
    "        if rat_id == 'dk1':\n",
    "            ax = axs[0, dk1_i]\n",
    "            dk1_i += 1\n",
    "        elif rat_id == 'dk3':\n",
    "            ax = axs[1, dk3_i]\n",
    "            dk3_i += 1\n",
    "        elif rat_id == 'dk5':\n",
    "            ax = axs[2, dk5_i]\n",
    "            dk5_i += 1\n",
    "        elif rat_id == 'dk6':\n",
    "            ax = axs[3, dk6_i]\n",
    "            dk6_i += 1\n",
    "        else:\n",
    "            continue  # Skip if rat_id is not one of the specified rats\n",
    "        ax.scatter(time_to_dig, coherence_value, label=experiment)\n",
    "        \n",
    "        ## Plotting Regression\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(time_to_dig, coherence_value)\n",
    "        ax.plot(time_to_dig, intercept + slope * time_to_dig, color='red', label=f'Fit: y={slope:.2f}x+{intercept:.2f}\\nR={r_value**2:.2f}, p={p_value:.4f}')\n",
    "        ax.set_title(f'Rat: {rat_id}', fontsize=16)\n",
    "                \n",
    "        \n",
    "        ax.set_title(f'Rat: {rat_id}', fontsize=16)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Coherence through trials as experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_of_interest = 'pre_dig' \n",
    "band_of_interest = 'beta'\n",
    "time_window = 0.4  # seconds\n",
    "fs = 2000  # Sampling frequency\n",
    "\n",
    "behavior_coherence_compiled_data_df_truncated=pd.read_pickle(savepath+f'behavior_coherence_compiled_data_df_truncated_{int(time_window*fs)}.pkl')\n",
    "power_per_trial_df = pd.read_excel(savepath+'power_per_trial_mt.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "band_event = band_of_interest+'_'+event_of_interest\n",
    "\n",
    "vmin = behavior_coherence_compiled_data_df_truncated[band_event].min()\n",
    "vmax = behavior_coherence_compiled_data_df_truncated[band_event].max()\n",
    "\n",
    "coherence_bwcontext_data = behavior_coherence_compiled_data_df_truncated[behavior_coherence_compiled_data_df_truncated['task'] == 'BWcontext']\n",
    "coherence_bwnocontext_data = behavior_coherence_compiled_data_df_truncated[behavior_coherence_compiled_data_df_truncated['task'] == 'BWnocontext']\n",
    "\n",
    "power_bwcontext_data = power_per_trial_df[power_per_trial_df['task'] == 'BWcontext']\n",
    "power_bwnocontext_data = power_per_trial_df[power_per_trial_df['task'] == 'BWnocontext']\n",
    "\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "# axs = axs.flatten()\n",
    "#task_data_dict = {'BWcontext': coherence_bwcontext_data, 'BWnocontext': coherence_bwnocontext_data}\n",
    "# task_list =[ 'BWcontext', 'BWnocontext']\n",
    "# for axi,(task_name, task_data) in enumerate(task_data_dict.items()):\n",
    "#     print(f\"Task: {task_name}\")\n",
    "#     experiment_ids = task_data['unique_id'].unique()\n",
    "#     print(f\"Number of unique IDs for {task_name}: {len(experiment_ids)}\")\n",
    "#     task_data_dict = {}\n",
    "#     for experiment_idi in experiment_ids:\n",
    "#         experiment_data=task_data[task_data['unique_id'] == experiment_idi]\n",
    "#         rat_date = experiment_idi.split('_')[0] +'_'+experiment_idi.split('_')[1][-8:]\n",
    "#         task_data_dict[rat_date] = experiment_data[band_event].values\n",
    "#     task_data_dict['trials'] = np.arange(start=1,stop=21,step=1, dtype=int)  # Assuming 20 trials per unique ID\n",
    "#     task_data_df = pd.DataFrame.from_dict(task_data_dict, orient='index').T\n",
    "#     task_data_df = task_data_df.fillna(0)  # Fill NaN values with 0\n",
    "#     task_data_df = task_data_df.loc[:, (task_data_df != 0).any(axis=0)]\n",
    "#     ax = axs[axi]\n",
    "#     ax.set_title(f'{task_name} - {band_of_interest} {event_of_interest}', fontsize=16)\n",
    "#     ax.set_xlabel('Trials', fontsize=14)\n",
    "#     ax.set_ylabel(f'{band_of_interest} Coherence (Z-transformed)', fontsize=14)\n",
    "#     sns.heatmap(task_data_df.set_index('trials').T, cmap='Purples', ax=ax, cbar_kws={'label': f'{band_of_interest} Coherence (Z-transformed)'}, vmin=vmin, vmax=vmax)\n",
    "    # If you want to see the unique IDs themselves, uncomment the next line\n",
    "    # print(f\"Unique IDs: {unique_ids}\")\n",
    "aon_power_per_trial_df = power_per_trial_df[power_per_trial_df['channel'] == 'AON']\n",
    "vHp_power_per_trial_df = power_per_trial_df[power_per_trial_df['channel'] == 'vHp']\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(20, 10), sharex=True, constrained_layout=True)    \n",
    "task_list = ['BWcontext', 'BWnocontext']\n",
    "for axi, task_name in enumerate(task_list):\n",
    "    print(f\"Task: {task_name}\")\n",
    "    coherence_task_data = behavior_coherence_compiled_data_df_truncated[behavior_coherence_compiled_data_df_truncated['task'] == task_name]\n",
    "    aon_power_task_data = aon_power_per_trial_df[aon_power_per_trial_df['task'] == task_name]\n",
    "    vhp_power_task_data = vHp_power_per_trial_df[vHp_power_per_trial_df['task'] == task_name]    \n",
    "    coherence_task_dict = {}\n",
    "    aon_power_task_dict = {}\n",
    "    vhp_power_task_dict = {}\n",
    "    experiment_ids = coherence_task_data['unique_id'].unique()\n",
    "    for experiment_idi in experiment_ids:\n",
    "        \n",
    "        rat_idi = experiment_idi.split('_')[0]\n",
    "        date_idi = experiment_idi.split('_')[1][-8:]\n",
    "        rat_date = rat_idi + '_' + date_idi\n",
    "\n",
    "        coherence_experiment_data=coherence_task_data[coherence_task_data['unique_id'] == experiment_idi]\n",
    "        \n",
    "        ## Power Data\n",
    "        aon_power_experiment_data = aon_power_task_data[aon_power_task_data['unique_id'] == rat_idi+\"_\"+task_name+'_'+date_idi]\n",
    "        vhp_power_experiment_data = vhp_power_task_data[vhp_power_task_data['unique_id'] == rat_idi+\"_\"+task_name+'_'+date_idi]\n",
    "        \n",
    "        aon_power_per_trial_list=[]\n",
    "        vhp_power_per_trial_list=[]\n",
    "        for triali in range(0, 20):\n",
    "            if triali not in aon_power_experiment_data['trial'].values:\n",
    "                print(f\"Trial {triali} not found in AON power data for {experiment_idi}. Skipping...\")\n",
    "                aon_power_per_trial_list.append(0)\n",
    "            else:\n",
    "                aon_power_trial = aon_power_experiment_data[aon_power_experiment_data['trial'] == triali][f'{band_event}_mt'].values\n",
    "                aon_power_per_trial_list.append(aon_power_trial.mean())\n",
    "            \n",
    "            if triali not in vhp_power_experiment_data['trial'].values:\n",
    "                print(f\"Trial {triali} not found in vHp power data for {experiment_idi}. Skipping...\")\n",
    "                vhp_power_per_trial_list.append(0)\n",
    "            else:\n",
    "                vhp_power_trial = vhp_power_experiment_data[vhp_power_experiment_data['trial'] == triali][f'{band_event}_mt'].values\n",
    "                vhp_power_per_trial_list.append(vhp_power_trial.mean())\n",
    "        aon_power_per_trial_list = np.array(aon_power_per_trial_list)\n",
    "        vhp_power_per_trial_list = np.array(vhp_power_per_trial_list)\n",
    "        \n",
    "        coherence_task_dict[rat_date] = coherence_experiment_data[band_event].values\n",
    "        aon_power_task_dict[rat_date] = aon_power_per_trial_list\n",
    "        vhp_power_task_dict[rat_date] = vhp_power_per_trial_list\n",
    "        \n",
    "    def dict_to_df(task_data_dict):\n",
    "        # Exclude 'trials' key from min/max calculation\n",
    "        arrays = [v for k, v in task_data_dict.items() if k != 'trials']\n",
    "        vmin = np.min(np.concatenate(arrays))\n",
    "        vmax = np.max(np.concatenate(arrays))\n",
    "        task_data_dict['trials'] = np.arange(start=1,stop=21,step=1, dtype=int)\n",
    "        task_data_df = pd.DataFrame.from_dict(task_data_dict, orient='index').T\n",
    "        task_data_df = task_data_df.fillna(0)  # Fill NaN values with 0\n",
    "        #task_data_df = task_data_df.loc[:, (task_data_df != 0).any(axis=0)]\n",
    "        return task_data_df, vmin, vmax\n",
    "    task_data_dicts ={ 'Coherence' : coherence_task_dict,\n",
    "                        'AON Power': aon_power_task_dict,\n",
    "                        'vHp Power': vhp_power_task_dict}\n",
    "    for j, (task_data_name, task_data_dict) in enumerate(task_data_dicts.items()):\n",
    "        task_data_df,vmin,vmax = dict_to_df(task_data_dict)\n",
    "        print(f\"Task Data for {task_data_name}:\", vmin, vmax)\n",
    "        ax = axs[j, axi]\n",
    "        ax.set_title(f'{task_name} - {band_of_interest} {event_of_interest} - {task_data_name}', fontsize=16)\n",
    "        ax.set_xlabel('Trials', fontsize=14)\n",
    "        ax.set_ylabel(f'{task_data_name} (Z-transformed)', fontsize=14)\n",
    "        sns.heatmap(task_data_df.set_index('trials').T, cmap='Purples', ax=ax, cbar_kws={'label': f'{task_data_name} (Z-transformed)'}, vmin=vmin, vmax=vmax)\n",
    "        ax.set_xticklabels(task_data_df['trials'], rotation=45)\n",
    "fig.savefig(savepath+f'{band_of_interest}_coherence_power_vs_trials_{event_of_interest}.png', format='png', dpi=600, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing Mann Whitney U Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window=0.4\n",
    "fs=2000\n",
    "behavior_coherence_compiled_data_df_truncated=pd.read_pickle(savepath+f'behavior_coherence_compiled_data_df_truncated_{int(time_window*fs)}.pkl')\n",
    "\n",
    "event_of_interest = 'around_dig' \n",
    "band_of_interest = 'beta'\n",
    "\n",
    "band_event = band_of_interest+'_'+event_of_interest\n",
    "\n",
    "vmin = behavior_coherence_compiled_data_df_truncated[band_event].min()\n",
    "vmax = behavior_coherence_compiled_data_df_truncated[band_event].max()\n",
    "\n",
    "bwcontext_data = behavior_coherence_compiled_data_df_truncated[behavior_coherence_compiled_data_df_truncated['task'] == 'BWcontext']\n",
    "bwnocontext_data = behavior_coherence_compiled_data_df_truncated[behavior_coherence_compiled_data_df_truncated['task'] == 'BWnocontext']\n",
    "\n",
    "t,p = scipy.stats.mannwhitneyu(bwcontext_data[band_event], bwnocontext_data[band_event])\n",
    "print(f\"Mann - Whitney U test between BWcontext and BWNocontext for {band_event}: t={t}, p={p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherence Phase Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the short MNE Epochs to Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window=0.7\n",
    "fs=2000\n",
    "behavior_coherence_compiled_data_df_truncated=pd.read_pickle(savepath+f'behavior_coherence_single_epochs_mne_truncated_{int(time_window*fs)}.pkl')\n",
    "print(behavior_coherence_compiled_data_df_truncated.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(coherence_functions)\n",
    "bands_dict = {'beta': [12, 30]}#, 'gamma': [30, 80],'theta':[4,12], 'total': [1, 100]}\n",
    "for col in ['pre_door', 'post_door', 'pre_dig', 'post_dig', 'around_door','around_dig']:\n",
    "    print(col)\n",
    "    for band, (band_start, band_end) in bands_dict.items():\n",
    "        behavior_coherence_compiled_data_df_truncated[band + '_' + col] = behavior_coherence_compiled_data_df_truncated[col].apply(lambda x: coherence_functions.convert_epoch_to_phase_behavior(x, band_start=band_start, band_end=band_end))\n",
    "behavior_coherence_compiled_data_df_truncated.drop(columns=['pre_door', 'post_door', 'pre_dig', 'post_dig', 'around_door','around_dig'], inplace=True)\n",
    "behavior_coherence_compiled_data_df_truncated.to_pickle(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\behavior_phase_compiled_data_df_truncated_{int(time_window*fs)}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_coherence_compiled_data_df_truncated=pd.read_pickle(savepath+f'behavior_phase_compiled_data_df_truncated_{int(time_window*fs)}.pkl')\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(3,2,figsize=(20, 20), sharey=True, constrained_layout=True)\n",
    "axs = axs.flatten()\n",
    "\n",
    "grouped_df=behavior_coherence_compiled_data_df_truncated.groupby(['task'])\n",
    "band='beta'\n",
    "event='around_dig'\n",
    "events_dict ={'pre_door':'Pre Door', 'post_door':'Post Door', 'pre_dig':'Pre Dig', 'post_dig':'Post Dig', 'around_door':'Around Door', 'around_dig':'Around Dig'}\n",
    "for i, event in enumerate(events_dict.keys()):\n",
    "    ax = axs[i]\n",
    "    ax.set_title(f'{events_dict[event]}', fontsize=16)\n",
    "    print(np.where(behavior_coherence_compiled_data_df_truncated['{}_{}'.format(band,event)]==0))\n",
    "    for task, group in grouped_df:\n",
    "        print(task)\n",
    "        print('{}_{}'.format(band,event))\n",
    "        group = group[(np.abs(group['{}_{}'.format(band,event)]))>0]\n",
    "        print(group['{}_{}'.format(band,event)].mean())\n",
    "        group['{}_{}'.format(band,event)] = group['{}_{}'.format(band,event)].apply(lambda x: np.arctanh(x))\n",
    "        print(group['{}_{}'.format(band,event)].mean())\n",
    "        group = group[(np.abs(group['time_to_dig'] - group['time_to_dig'].mean()) <= (3 * group['time_to_dig'].std()))]\n",
    "        group = group[(np.abs(group['{}_{}'.format(band,event)] - group['{}_{}'.format(band,event)].mean()) <= (3 * group['{}_{}'.format(band,event)].std()))]\n",
    "        sns.regplot(x='time_to_dig', y='{}_{}'.format(band,event), data=group, ax=ax, label=task)\n",
    "    ax.set_ylabel('Beta PLI', fontsize=14)\n",
    "    ax.set_xlabel('Time to Dig (s)', fontsize=14)\n",
    "    ax.legend(title='Task')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.tight_layout()\n",
    "fig.suptitle(f'Beta pli vs Time to Dig', fontsize=20, y=1.02)\n",
    "#fig.savefig(savepath+'beta_coherence_vs_time_to_dig.png', format='png', dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Plot histogram of beta values for each event, comparing BW Context and BW No Context\n",
    "fig, axs = plt.subplots(3, 2, figsize=(20, 20), sharey=True, constrained_layout=True)\n",
    "axs = axs.flatten()\n",
    "events = list(events_dict.keys())\n",
    "for i, event in enumerate(events):\n",
    "    ax = axs[i]\n",
    "    for task in ['BWcontext', 'BWnocontext']:\n",
    "        data = behavior_coherence_compiled_data_df_truncated[\n",
    "            behavior_coherence_compiled_data_df_truncated['task'] == task\n",
    "        ]['{}_{}'.format(band, event)].dropna()\n",
    "        ax.hist(data, bins=30, alpha=0.6, label=task, density=True)\n",
    "    ax.set_title(f'{events_dict[event]}', fontsize=16)\n",
    "    ax.set_xlabel('Beta Value', fontsize=14)\n",
    "    ax.set_ylabel('Density', fontsize=14)\n",
    "    ax.legend(title='Task')\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "plt.tight_layout()\n",
    "fig.suptitle('Histogram of Beta Values per Event', fontsize=20, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Behavior Coherence Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_window=  0.4\n",
    "fs=2000\n",
    "\n",
    "loaded_df=pd.read_pickle(savepath+f'\\\\behavior_coherence_compiled_data_df_truncated_{int(time_window*fs)}.pkl')\n",
    "print(loaded_df.head())\n",
    "coherence_band_event_df=loaded_df.loc[:,'beta_pre_door':]\n",
    "print(coherence_band_event_df.columns)\n",
    "group_melted=pd.melt(loaded_df, id_vars=['rat', 'task', 'date', 'trial','correct?', 'time_to_dig'], value_vars=coherence_band_event_df.columns, var_name='band_event', value_name='coherence')\n",
    "group_melted['band']=group_melted['band_event'].apply(lambda x: x.split('_')[0])\n",
    "group_melted['event']=group_melted['band_event'].apply(lambda x: x.split('_')[1:])\n",
    "group_melted['event']=group_melted['event'].apply(lambda x: x[0]+'_'+x[1])\n",
    "group_melted.drop(columns=['band_event'], inplace=True)\n",
    "group_melted['correct?']=group_melted['correct?'].apply(lambda x: 'Incorrect' if x=='0' else 'Correct')\n",
    "events_list=['pre_door','post_door','pre_dig','post_dig', 'around_door', 'around_dig']\n",
    "writer=pd.ExcelWriter(savepath+f'behavior_coherence_compiled_data_df_truncated_{int(time_window*fs)}.xlsx')\n",
    "for event in events_list:\n",
    "    event_df=group_melted[group_melted['event']==event]\n",
    "    event_df.to_excel(writer, sheet_name=event)\n",
    "writer.close()\n",
    "\n",
    "\n",
    "\n",
    "loaded_df=loaded_df.drop(columns=['around_door_truncated','around_dig_truncated'])\n",
    "writer=pd.ExcelWriter(savepath+f'beh_dig_coh_compiled_{int(time_window*fs/2)}ms.xlsx')\n",
    "loaded_df.to_excel(writer)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This plots the number of correct vs incorrect trials and the coherence. The idea is to check if the correct trials in general had a higher coherence than incorrect trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bwcontext_df=group_melted[group_melted['task']=='BWcontext']\n",
    "correct_counts = bwcontext_df[bwcontext_df['correct?'] == 'Correct'].shape[0]\n",
    "incorrect_counts = bwcontext_df[bwcontext_df['correct?'] == 'Incorrect'].shape[0]\n",
    "print(f\"Number of Corrects: {correct_counts}\", f\"Number of Incorrects: {incorrect_counts}\", 'bwcontext')\n",
    "bwnocontext_df=group_melted[group_melted['task']=='BWnocontext']\n",
    "correct_counts = bwnocontext_df[bwnocontext_df['correct?'] == 'Correct'].shape[0]\n",
    "incorrect_counts = bwnocontext_df[bwnocontext_df['correct?'] == 'Incorrect'].shape[0]\n",
    "print(f\"Number of Corrects: {correct_counts}\", f\"Number of Incorrects: {incorrect_counts}\", 'bwnocontext')\n",
    "%matplotlib inline\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20, 10), sharex=True, constrained_layout=True)\n",
    "axs = axs.flatten()\n",
    "for i, event in enumerate(events_list):\n",
    "    ax=axs[i]\n",
    "    sns.boxplot(x='band', y='coherence', hue='correct?',hue_order=['Correct', 'Incorrect'], data=bwcontext_df[group_melted['event']==event], showfliers=False, ax=ax)\n",
    "    sns.stripplot(x='band', y='coherence', hue='correct?',hue_order=['Correct', 'Incorrect'], data=bwcontext_df[group_melted['event']==event], dodge=True, edgecolor='black', linewidth=1, jitter=True, ax=ax, size=1, legend=False)\n",
    "    ax.set_title(event)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Coherence')\n",
    "    ax.legend(title='Correct?')\n",
    "fig.suptitle(f'BW Context Coherence and Correctness')\n",
    "#fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\behavior_coherence_BWcontext.png')\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20, 10), sharex=True, constrained_layout=True)\n",
    "axs = axs.flatten()\n",
    "for i, event in enumerate(events_list):\n",
    "    ax=axs[i]\n",
    "    sns.boxplot(x='band', y='coherence', hue='correct?',hue_order=['Correct', 'Incorrect'], data=bwnocontext_df[group_melted['event']==event], showfliers=False, ax=ax)\n",
    "    sns.stripplot(x='band', y='coherence', hue='correct?',hue_order=['Correct', 'Incorrect'], data=bwnocontext_df[group_melted['event']==event], dodge=True, edgecolor='black', linewidth=1, jitter=True, ax=ax, size=1, legend=False)\n",
    "    ax.set_title(event)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Coherence')\n",
    "    ax.legend(title='Correct?')\n",
    "fig.suptitle(f'BW No Context Coherence and Correctness')\n",
    "#fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\behavior_coherence_BWnocontext.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks=['BWcontext','BWnocontext']\n",
    "loaded_df['correct?']=loaded_df['correct?'].apply(lambda x: 'Correct' if x=='1' else 'Incorrect')\n",
    "correctness=['Correct','Incorrect']\n",
    "fig, axs=plt.subplots(2,2, figsize=(20,10))\n",
    "axs=axs.flatten()\n",
    "axi=0\n",
    "for task in tasks:\n",
    "    for dig_type in correctness:\n",
    "\n",
    "        ax=axs[axi]\n",
    "        task_df=loaded_df[(loaded_df['task']==task) & (loaded_df['correct?']==dig_type)]\n",
    "        events_list=['pre_door','post_door','pre_dig','post_dig']\n",
    "        bands=['total','beta','theta','gamma']\n",
    "        \n",
    "        correlation_matrix = np.zeros((len(events_list), len(bands)))\n",
    "\n",
    "        for i, event in enumerate(events_list):\n",
    "            for j, band in enumerate(bands):\n",
    "                column_name = f'{band}_{event}'\n",
    "                correlation_matrix[i, j] = task_df['time_to_dig'].corr(task_df[column_name])\n",
    "        \n",
    "        cax = ax.matshow(correlation_matrix, cmap='coolwarm')\n",
    "        fig.colorbar(cax, ax=ax)\n",
    "\n",
    "        ax.set_xticks(np.arange(len(bands)))\n",
    "        ax.set_yticks(np.arange(len(events_list)))\n",
    "        ax.set_xticklabels(bands)\n",
    "        ax.set_yticklabels(events_list)\n",
    "        ax.set_title(f'{task} {dig_type}')\n",
    "        axi=axi+1\n",
    "fig.tight_layout()\n",
    "# plt.show()\n",
    "# plt.xlabel('Bands')\n",
    "# plt.ylabel('Events')\n",
    "# plt.title('Correlation Matrix Heatmap')\n",
    "# plt.show()        \n",
    "# bwcontext_df=loaded_df[(loaded_df['task']=='BWcontext') & (loaded_df['correct?']=='0')]\n",
    "# events_list=['pre_door','post_door','pre_dig','post_dig']\n",
    "# bands=['total','beta','theta','gamma']\n",
    "\n",
    "# correlation_matrix = np.zeros((len(events_list), len(bands)))\n",
    "\n",
    "# for i, event in enumerate(events_list):\n",
    "#     for j, band in enumerate(bands):\n",
    "#         column_name = f'{band}_{event}'\n",
    "#         correlation_matrix[i, j] = bwcontext_df['time_to_dig'].corr(bwcontext_df[column_name])\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(10, 8))\n",
    "# cax = ax.matshow(correlation_matrix, cmap='coolwarm')\n",
    "# fig.colorbar(cax)\n",
    "\n",
    "# ax.set_xticks(np.arange(len(bands)))\n",
    "# ax.set_yticks(np.arange(len(events_list)))\n",
    "# ax.set_xticklabels(bands)\n",
    "# ax.set_yticklabels(events_list)\n",
    "\n",
    "# plt.xlabel('Bands')\n",
    "# plt.ylabel('Events')\n",
    "# plt.title('Correlation Matrix Heatmap')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import statsmodels\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "loaded_df=pd.read_pickle(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\behavior_coherence_compiled_data_df_truncated_{int(time_window*fs)}.pkl')\n",
    "print(loaded_df.head())\n",
    "loaded_df=loaded_df[loaded_df['rat']!='dk3']\n",
    "print(loaded_df.head())\n",
    "\n",
    "fig, ax=plt.subplots(1,1, figsize=(10,10))\n",
    "tasks=['BWcontext','BWnocontext']\n",
    "loaded_df['correct?']=loaded_df['correct?'].apply(lambda x: 'Correct' if x=='1' else 'Incorrect')\n",
    "correctness=['Correct','Incorrect']\n",
    "bwcontext_incorrect_df=loaded_df[(loaded_df['task']=='BWcontext')]\n",
    "bwnocontext_incorrect_df=loaded_df[(loaded_df['task']=='BWnocontext')]\n",
    "x=bwcontext_incorrect_df['time_to_dig']\n",
    "y=bwcontext_incorrect_df['beta_pre_dig']\n",
    "df = pd.DataFrame({'coherence': y, 'time': x})\n",
    "df.drop(df[df['coherence'] == 0].index, inplace=True)  # Remove rows with coherence = 0\n",
    "\n",
    "try:\n",
    "    correlation_coefficient, p_value = pearsonr(df['coherence'], df['time'])\n",
    "\n",
    "    print(f\"Pearson Correlation Coefficient (r): {correlation_coefficient:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "    correlation_coefficient_s, p_value_s = spearmanr(df['coherence'], df['time'])\n",
    "\n",
    "    print(f\"\\nSpearman Rank Correlation Coefficient (rs): {correlation_coefficient_s:.4f}\")\n",
    "    print(f\"P-value: {p_value_s:.4f}\")\n",
    "\n",
    "    # --- Interpretation ---\n",
    "    alpha = 0.05 # Set your significance level\n",
    "    print(f\"\\nSignificance Level (alpha): {alpha}\")\n",
    "\n",
    "    if p_value <= alpha:\n",
    "        print(\"Result: Reject the null hypothesis (H0).\")\n",
    "        print(\"Conclusion: There is a statistically significant linear relationship between the variables.\")\n",
    "    else:\n",
    "        print(\"Result: Fail to reject the null hypothesis (H0).\")\n",
    "        print(\"Conclusion: There is NOT enough evidence for a statistically significant linear relationship.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"Please ensure 'coherence_data' and 'time_data' are populated correctly with numerical lists or arrays of the same length.\")\n",
    "# --- Create a Pandas DataFrame ---\n",
    "\n",
    "print(f\"Original number of data points: {len(df)}\")\n",
    "\n",
    "# --- Calculate IQR Fences for BOTH variables ---\n",
    "Q1_coherence = df['coherence'].quantile(0.25)\n",
    "Q3_coherence = df['coherence'].quantile(0.75)\n",
    "IQR_coherence = Q3_coherence - Q1_coherence\n",
    "lower_fence_coherence = Q1_coherence - 1.5 * IQR_coherence\n",
    "upper_fence_coherence = Q3_coherence + 1.5 * IQR_coherence\n",
    "\n",
    "Q1_time = df['time'].quantile(0.25)\n",
    "Q3_time = df['time'].quantile(0.75)\n",
    "IQR_time = Q3_time - Q1_time\n",
    "lower_fence_time = Q1_time - 1.5 * IQR_time\n",
    "upper_fence_time = Q3_time + 1.5 * IQR_time\n",
    "\n",
    "\n",
    "print(\"\\n--- Outlier Fences ---\")\n",
    "print(f\"Coherence: Lower={lower_fence_coherence:.2f}, Upper={upper_fence_coherence:.2f}\")\n",
    "print(f\"Time:      Lower={lower_fence_time:.2f}, Upper={upper_fence_time:.2f}\")\n",
    "\n",
    "# --- Identify outliers (points outside fences for EITHER variable) ---\n",
    "outlier_condition = (\n",
    "    (df['coherence'] < lower_fence_coherence) | (df['coherence'] > upper_fence_coherence) |\n",
    "    (df['time'] < lower_fence_time) | (df['time'] > upper_fence_time)\n",
    ")\n",
    "\n",
    "outliers = df[outlier_condition]\n",
    "print(f\"\\nIdentified {len(outliers)} potential outliers:\")\n",
    "print(outliers)\n",
    "# --- Filter out the outliers ---\n",
    "df_filtered = df[~outlier_condition] # Use ~ to negate the condition, keeping non-outliers\n",
    "print(f\"\\nNumber of data points after removing outliers: {len(df_filtered)}\")\n",
    "\n",
    "\n",
    "# --- Recalculate Correlation on Filtered Data ---\n",
    "if len(df_filtered) > 1: # Need at least 2 points to calculate correlation\n",
    "    # Extract the filtered data columns\n",
    "    coherence_filtered = df_filtered['coherence']\n",
    "    time_filtered = df_filtered['time']\n",
    "\n",
    "    # Calculate original correlation (optional comparison)\n",
    "    try:\n",
    "      original_r, original_p = pearsonr(df['coherence'], df['time'])\n",
    "      print(f\"\\nOriginal Correlation (r): {original_r:.4f}, p-value: {original_p:.4f}\")\n",
    "    except Exception as e:\n",
    "      print(f\"\\nCould not calculate original correlation: {e}\")\n",
    "\n",
    "\n",
    "    # Calculate filtered correlation\n",
    "    try:\n",
    "      filtered_r, filtered_p = pearsonr(coherence_filtered, time_filtered)\n",
    "      print(f\"Filtered Correlation (r): {filtered_r:.4f}, p-value: {filtered_p:.4f}\")\n",
    "\n",
    "      # Interpretation (using alpha = 0.05)\n",
    "      alpha = 0.05\n",
    "      if filtered_p <= alpha:\n",
    "          print(\"Result (Filtered): Reject H0. Statistically significant linear relationship found.\")\n",
    "      else:\n",
    "          print(\"Result (Filtered): Fail to reject H0. No statistically significant linear relationship found.\")\n",
    "    except Exception as e:\n",
    "      print(f\"Could not calculate filtered correlation: {e}\")\n",
    "\n",
    "sns.regplot(y=df['coherence'], x=df['time'], label='Context', ax=ax)\n",
    "sns.regplot(x=bwnocontext_incorrect_df['time_to_dig'], y=bwnocontext_incorrect_df['beta_pre_dig'], label='No context', ax=ax)\n",
    "plt.xlabel('Time to Dig (s)', fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.ylabel('Coherence', fontsize=20)\n",
    "plt.title('AON-VHP Beta Pre Dig Coherence vs Time to Dig', fontsize=20)\n",
    "plt.legend(fontsize=20) \n",
    "plt.tight_layout()\n",
    "#fig.savefig(f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\results\\\\beta_pre_dig_vs_time_to_dig.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# bwcontext_correct_df=loaded_df[(loaded_df['task']=='BWcontext') & (loaded_df['correct?']=='Correct')]\n",
    "# bwnocontext_correct_df=loaded_df[(loaded_df['task']=='BWnocontext') & (loaded_df['correct?']=='Correct')]\n",
    "# sns.regplot(x=bwcontext_correct_df['time_to_dig'], y=bwcontext_correct_df['gamma_pre_dig'], label='BWcontext',robust=True, order=2)\n",
    "# sns.regplot(x=bwnocontext_correct_df['time_to_dig'], y=bwnocontext_correct_df['gamma_pre_dig'], label='BWnocontext',robust=True, order=2)\n",
    "# plt.xlabel('Time to Dig')\n",
    "# plt.ylabel('Beta Pre Dig')\n",
    "# plt.title('Beta Pre Dig vs Time to Dig for Correct Trials')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# --- Plot SVM regression for BWcontext and BWnocontext separately ---\n",
    "\n",
    "fig2, ax2 = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "# Prepare data for each task\n",
    "for task_name, color in colors.items():\n",
    "  task_df = loaded_df[(loaded_df['task'] == task_name) & (~outlier_condition)]\n",
    "  if task_df.empty:\n",
    "    print(f\"Skipping {task_name}: no data after outlier removal.\")\n",
    "    continue\n",
    "  X_task = task_df['time_to_dig'].values.reshape(-1, 1)\n",
    "  y_task = task_df['beta_pre_dig'].values\n",
    "\n",
    "  # Fit SVM regression\n",
    "  svm_poly_task = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVR(kernel='poly', degree=3, C=1.0, epsilon=0.1)\n",
    "  )\n",
    "  svm_poly_task.fit(X_task, y_task)\n",
    "  x_range_task = np.linspace(X_task.min(), X_task.max(), 200).reshape(-1, 1)\n",
    "  y_pred_task = svm_poly_task.predict(x_range_task)\n",
    "\n",
    "  # Scatter and SVM fit\n",
    "  ax2.scatter(X_task, y_task, color=color, alpha=0.5, label=f'{task_name} data')\n",
    "  ax2.plot(x_range_task, y_pred_task, color=color, linestyle='-', linewidth=2, label=f'{task_name} SVM fit')\n",
    "\n",
    "ax2.set_xlabel('Time to Dig (s)', fontsize=20)\n",
    "ax2.set_ylabel('Coherence', fontsize=20)\n",
    "ax2.set_title('SVM Poly Fit: BWcontext vs BWnocontext', fontsize=20)\n",
    "ax2.legend(fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Get residuals for each group\n",
    "residuals = {}\n",
    "for task_name in colors.keys():\n",
    "  task_df = loaded_df[(loaded_df['task'] == task_name) & (~outlier_condition)]\n",
    "  if task_df.empty:\n",
    "    print(f\"Skipping {task_name} residuals: no data after outlier removal.\")\n",
    "    continue\n",
    "  X_task = task_df['time_to_dig'].values.reshape(-1, 1)\n",
    "  y_task = task_df['beta_pre_dig'].values\n",
    "  svm_poly_task = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVR(kernel='poly', degree=3, C=1.0, epsilon=0.1)\n",
    "  )\n",
    "  svm_poly_task.fit(X_task, y_task)\n",
    "  y_pred_task = svm_poly_task.predict(X_task)\n",
    "  residuals[task_name] = y_task - y_pred_task\n",
    "\n",
    "# t-test on residuals (only if both groups have data)\n",
    "if all(k in residuals and len(residuals[k]) > 0 for k in ['BWcontext', 'BWnocontext']):\n",
    "  t_stat, p_val = ttest_ind(residuals['BWcontext'], residuals['BWnocontext'], equal_var=False)\n",
    "  print(f\"Residuals t-test: t={t_stat:.4f}, p={p_val:.4g}\")\n",
    "  if p_val < 0.05:\n",
    "    print(\"Statistically significant difference in SVM fit residuals between BWcontext and BWnocontext.\")\n",
    "  else:\n",
    "    print(\"No statistically significant difference in SVM fit residuals between BWcontext and BWnocontext.\")\n",
    "else:\n",
    "  print(\"Not enough data for both groups to perform residuals t-test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "print(f\"Number of data points: {len(df)}\")\n",
    "coherence_data = df['coherence']\n",
    "time_data = df['time']\n",
    "# --- Perform the Spearman Rank Correlation Test ---\n",
    "try:\n",
    "    correlation_coefficient_s, p_value_s = spearmanr(coherence_data, time_data)\n",
    "\n",
    "    print(f\"\\nSpearman Rank Correlation Coefficient (rs): {correlation_coefficient_s:.4f}\")\n",
    "    print(f\"P-value: {p_value_s:.4f}\")\n",
    "\n",
    "    # --- Interpretation ---\n",
    "    alpha = 0.05 # Set your significance level\n",
    "    print(f\"\\nSignificance Level (alpha): {alpha}\")\n",
    "\n",
    "    if p_value_s <= alpha:\n",
    "        print(\"Result: Reject the null hypothesis (H0).\")\n",
    "        print(\"Conclusion: There is a statistically significant monotonic relationship between the variables.\")\n",
    "    else:\n",
    "        print(\"Result: Fail to reject the null hypothesis (H0).\")\n",
    "        print(\"Conclusion: There is NOT enough evidence for a statistically significant monotonic relationship.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"Please ensure 'coherence_data' and 'time_data' are populated correctly with numerical lists or arrays of the same length.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
