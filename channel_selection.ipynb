{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c613306",
   "metadata": {},
   "source": [
    "# 1 Importing packages and the functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e426405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import getpass\n",
    "import glob\n",
    "import seaborn as sns\n",
    "import functions\n",
    "import lfp_pre_processing_functions\n",
    "import power_functions\n",
    "import coherence_functions\n",
    "import spectrogram_plotting_functions\n",
    "import plotting_styles\n",
    "import scipy.stats\n",
    "import mne_connectivity\n",
    "importlib.reload(functions) #loads our custom made functions.py file\n",
    "importlib.reload(spectrogram_plotting_functions)\n",
    "importlib.reload(plotting_styles)\n",
    "\n",
    "linestyle = plotting_styles.linestyles\n",
    "colors = plotting_styles.colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a447a0",
   "metadata": {},
   "source": [
    "# 2 - Loading the data files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db88638",
   "metadata": {},
   "source": [
    "This code fetches the current 'user' by using getpass. Then it sets the basepath, loads the files and specifies the savepath. Note that the basepath, files and savepath need to be changed depending on where you have kept the files and where you want the results to be stored. In this case, I have set it up to be in a particular folder in my Dropbox account, which is stored locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f840d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch the current user\n",
    "user= (getpass.getuser())\n",
    "print(\"Hello\", user)\n",
    "\n",
    "\n",
    "if user == 'CPLab':\n",
    "    base='D:\\\\Dropbox\\\\CPLab'\n",
    "else:\n",
    "    base='C:\\\\Users\\\\{}\\\\Dropbox\\\\CPLab'.format(user)\n",
    "#Set the basepath, savepath and load the data files\n",
    "files = glob.glob(base+'\\\\all_data_mat_unfiltered\\\\*.mat')\n",
    "savepath = base+'\\\\results\\\\'\n",
    "print(\"Base path:\", base)\n",
    "print(\"Save path:\", savepath)\n",
    "print(files)\n",
    "\n",
    "all_bands_dict = {'total':[1,100], 'beta':[12,30], 'gamma':[30,80], 'theta':[4,12]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfaf6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyboard_dict={'98':'b','119':'w','120':'nc','49':'1','48':'0'} #specifying the map of keyboard annotations to their meanings.\n",
    "all_bands={'total':[1,100],'beta':[12,30], 'gamma':[30,80], 'theta':[4,12]}\n",
    "importlib.reload(lfp_pre_processing_functions) #Reloading the lfp_pre_processing_functions module to ensure we have the latest version\n",
    "#files=[f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230615_dk6_BW_context_day1.mat', f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\all_data_mat\\\\20230626_dk6_BW_nocontext_day1.mat'] #This is just for testing purposes\n",
    "\n",
    "#Initializing a few empty things to store data\n",
    "events_codes_all = {}\n",
    "compiled_data_all_epochs = []\n",
    "compiled_data_list=[]\n",
    "compiled_shuffled_data_list = []\n",
    "baseline_lfp_all = []\n",
    "normalization_comparison_all = []\n",
    "baseline_dict = {}\n",
    "for file in files: #Looping through data files\n",
    "    \n",
    "    ## Get the date, mouse_id and task from the file name\n",
    "    base_name = os.path.basename(file)\n",
    "    base_name, _ = os.path.splitext(base_name)\n",
    "    date, mouse_id, task=lfp_pre_processing_functions.exp_params(base_name) #Using a custom made function [see functions.py]\n",
    "    print(date, mouse_id, task)\n",
    "    if task == 'nocontextday2' or task == 'nocontextos2':\n",
    "        task = 'nocontext'\n",
    "    if task =='nocontext':\n",
    "        continue\n",
    "    f=h5py.File(file, 'r')  ## Open the data file\n",
    "    channels = list(f.keys()) ## Extract channels list from the data file\n",
    "    print(base_name, channels)\n",
    "    if not any(\"AON\" in channel or \"vHp\" in channel for channel in channels):\n",
    "        continue\n",
    "    events,reference_electrode=lfp_pre_processing_functions.get_keyboard_and_ref_channels(f,channels)\n",
    "\n",
    "    events_codes=np.array(events['codes'][0]) #saving the keyboard annotations of the events (door open, door close etc.)\n",
    "    events_times=np.array(events['times'][0]) #saving when the events happened\n",
    "    events_codes_all[base_name] = events_codes #saving the codes in a dictionary to be analyzed later for events other than the ones in our keyboard_dict map\n",
    "    \n",
    "    #Generating epochs from events (epochs are basically start of a trial and end of a trial)\n",
    "    epochs=lfp_pre_processing_functions.generate_epochs_with_first_event(events_codes, events_times)\n",
    "\n",
    "    # task Start time\n",
    "    first_event=events_times[0]\n",
    "    #finding global start and end time of all channels, since they start and end recordings at different times\n",
    "    global_start_time, global_end_time=lfp_pre_processing_functions.find_global_start_end_times(f,channels)\n",
    "    \n",
    "    ## Reference electrode finding and padding\n",
    "    reference_time = np.array(reference_electrode['times']).flatten()\n",
    "    reference_value = np.array(reference_electrode['values']).flatten()\n",
    "    padd_ref_data,padded_ref_time=lfp_pre_processing_functions.pad_raw_data_raw_time(reference_value,reference_time,global_start_time,global_end_time,sampling_rate=2000)\n",
    "\n",
    "    for channeli in channels:\n",
    "        if \"AON\" in channeli or  \"vHp\" in channeli :\n",
    "            \n",
    "            channel_id=channeli\n",
    "            # Extracting raw data and time\n",
    "            data_all=f[channeli]\n",
    "            raw_data=np.array(data_all['values']).flatten()\n",
    "            raw_time = np.array(data_all['times']).flatten()\n",
    "            sampling_rate = 2000\n",
    "            print(channel_id)\n",
    "            print(raw_data.shape, raw_time.shape, sampling_rate)\n",
    "            \n",
    "            padded_data,padded_time=lfp_pre_processing_functions.pad_raw_data_raw_time(raw_data,raw_time,global_start_time,global_end_time,sampling_rate)\n",
    "            #ref_subtracted_data = padded_data - padd_ref_data # Subtracting the reference electrode data from the raw data\n",
    "            \n",
    "            #notch_filtered_data = lfp_pre_processing_functions.iir_notch(raw_data, sampling_rate, 60)\n",
    "            \n",
    "            def extract_baseline_data(data,time,first_event,sampling_rate):\n",
    "                if first_event>2.0:\n",
    "                    baseline_data=data[np.where(time>first_event)[0][0]-2*sampling_rate:np.where(time>first_event)[0][0]]\n",
    "                else:\n",
    "                    baseline_data=data[0:np.where(time>first_event)[0][0]]\n",
    "                baseline_mean=np.mean(baseline_data)\n",
    "                baseline_std=np.std(baseline_data)\n",
    "                \n",
    "                #baseline_data_norm=(baseline_data-baseline_mean)/baseline_std\n",
    "                print('normalizing data')\n",
    "                return baseline_data,time, baseline_mean, baseline_std\n",
    "            \n",
    "            # Extracting baseline data\n",
    "            #data_before, time, baseline_mean, baseline_std=extract_baseline_data(ref_subtracted_data, raw_time, first_event, sampling_rate)\n",
    "            data_before, time, baseline_mean, baseline_std=extract_baseline_data(padded_data, raw_time, first_event, sampling_rate)\n",
    "            print(len(data_before))\n",
    "            complete_baseline_data=padded_data[0:np.where(time>first_event)[0][0]]\n",
    "            baseline_row=[base_name, mouse_id,task,channel_id,first_event,np.array(data_before), np.array(complete_baseline_data)]\n",
    "            baseline_lfp_all.append(baseline_row)\n",
    "            baseline_dict[base_name] = baseline_row\n",
    "baseline_lfp_all_df=pd.DataFrame(baseline_lfp_all, columns=['base_name', 'mouse_id', 'task', 'channel_id','first_event', 'data_before','complete_baseline_data'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe88408",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9802b774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(4000,)\n",
      "(175456,)\n",
      "(175456,)\n",
      "(175457,)\n",
      "Data array length was odd, padded to even length: (175457,)\n",
      "(175457,)\n",
      "Data array length was odd, padded to even length: (175457,)\n",
      "(354227,)\n",
      "Data array length was odd, padded to even length: (354227,)\n",
      "(354227,)\n",
      "Data array length was odd, padded to even length: (354227,)\n",
      "(354227,)\n",
      "Data array length was odd, padded to even length: (354227,)\n",
      "(354227,)\n",
      "Data array length was odd, padded to even length: (354227,)\n",
      "(250518,)\n",
      "(250518,)\n",
      "(250518,)\n",
      "(250518,)\n",
      "(250518,)\n",
      "(250518,)\n",
      "(272528,)\n",
      "(272527,)\n",
      "Data array length was odd, padded to even length: (272527,)\n",
      "(272528,)\n",
      "(272527,)\n",
      "Data array length was odd, padded to even length: (272527,)\n",
      "(272528,)\n",
      "(272528,)\n",
      "(157296,)\n",
      "(157296,)\n",
      "(157296,)\n",
      "(157296,)\n",
      "(157296,)\n",
      "(157296,)\n",
      "(203428,)\n",
      "(203427,)\n",
      "Data array length was odd, padded to even length: (203427,)\n",
      "(203428,)\n",
      "(203428,)\n",
      "(273913,)\n",
      "Data array length was odd, padded to even length: (273913,)\n",
      "(273913,)\n",
      "Data array length was odd, padded to even length: (273913,)\n",
      "(273913,)\n",
      "Data array length was odd, padded to even length: (273913,)\n",
      "(273913,)\n",
      "Data array length was odd, padded to even length: (273913,)\n",
      "(151989,)\n",
      "Data array length was odd, padded to even length: (151989,)\n",
      "(151988,)\n",
      "(151989,)\n",
      "Data array length was odd, padded to even length: (151989,)\n",
      "(151989,)\n",
      "Data array length was odd, padded to even length: (151989,)\n",
      "(200796,)\n",
      "(200795,)\n",
      "Data array length was odd, padded to even length: (200795,)\n",
      "(200796,)\n",
      "(200795,)\n",
      "Data array length was odd, padded to even length: (200795,)\n",
      "(200796,)\n",
      "(200796,)\n",
      "(350135,)\n",
      "Data array length was odd, padded to even length: (350135,)\n",
      "(350135,)\n",
      "Data array length was odd, padded to even length: (350135,)\n",
      "(350135,)\n",
      "Data array length was odd, padded to even length: (350135,)\n",
      "(350134,)\n",
      "(350135,)\n",
      "Data array length was odd, padded to even length: (350135,)\n",
      "(350135,)\n",
      "Data array length was odd, padded to even length: (350135,)\n",
      "(202630,)\n",
      "(202630,)\n",
      "(202630,)\n",
      "(202630,)\n",
      "(202630,)\n",
      "(202630,)\n",
      "(259540,)\n",
      "(259540,)\n",
      "(259540,)\n",
      "(259540,)\n",
      "(259540,)\n",
      "(259540,)\n",
      "(278506,)\n",
      "(278506,)\n",
      "(278506,)\n",
      "(278506,)\n",
      "(278506,)\n",
      "(278506,)\n",
      "(244525,)\n",
      "Data array length was odd, padded to even length: (244525,)\n",
      "(244524,)\n",
      "(244525,)\n",
      "Data array length was odd, padded to even length: (244525,)\n",
      "(244524,)\n",
      "(244525,)\n",
      "Data array length was odd, padded to even length: (244525,)\n",
      "(244525,)\n",
      "Data array length was odd, padded to even length: (244525,)\n",
      "(201294,)\n",
      "(201294,)\n",
      "(201294,)\n",
      "(161946,)\n",
      "(161946,)\n",
      "(161946,)\n",
      "(161946,)\n",
      "(129482,)\n",
      "(129482,)\n",
      "(129483,)\n",
      "Data array length was odd, padded to even length: (129483,)\n",
      "(129483,)\n",
      "Data array length was odd, padded to even length: (129483,)\n",
      "(122922,)\n",
      "(122922,)\n",
      "(122922,)\n",
      "(122921,)\n",
      "Data array length was odd, padded to even length: (122921,)\n",
      "(122922,)\n",
      "(122922,)\n",
      "(345926,)\n",
      "(345926,)\n",
      "(345926,)\n",
      "(345926,)\n",
      "(345926,)\n",
      "(345926,)\n",
      "(316895,)\n",
      "Data array length was odd, padded to even length: (316895,)\n",
      "(316895,)\n",
      "Data array length was odd, padded to even length: (316895,)\n",
      "(316895,)\n",
      "Data array length was odd, padded to even length: (316895,)\n",
      "(316895,)\n",
      "Data array length was odd, padded to even length: (316895,)\n",
      "(269079,)\n",
      "Data array length was odd, padded to even length: (269079,)\n",
      "(269078,)\n",
      "(269079,)\n",
      "Data array length was odd, padded to even length: (269079,)\n",
      "(269079,)\n",
      "Data array length was odd, padded to even length: (269079,)\n",
      "(129798,)\n",
      "(129798,)\n",
      "(129798,)\n",
      "(129798,)\n",
      "(129798,)\n",
      "(129798,)\n",
      "(133653,)\n",
      "Data array length was odd, padded to even length: (133653,)\n",
      "(133653,)\n",
      "Data array length was odd, padded to even length: (133653,)\n",
      "(133653,)\n",
      "Data array length was odd, padded to even length: (133653,)\n",
      "(133653,)\n",
      "Data array length was odd, padded to even length: (133653,)\n",
      "(133653,)\n",
      "Data array length was odd, padded to even length: (133653,)\n",
      "(133653,)\n",
      "Data array length was odd, padded to even length: (133653,)\n",
      "(179014,)\n",
      "(179014,)\n",
      "(179014,)\n",
      "(189628,)\n",
      "(189628,)\n",
      "(189628,)\n",
      "(189627,)\n",
      "Data array length was odd, padded to even length: (189627,)\n",
      "(189628,)\n",
      "(189628,)\n",
      "(251139,)\n",
      "Data array length was odd, padded to even length: (251139,)\n",
      "(251139,)\n",
      "Data array length was odd, padded to even length: (251139,)\n",
      "(251139,)\n",
      "Data array length was odd, padded to even length: (251139,)\n",
      "(251138,)\n",
      "(251139,)\n",
      "Data array length was odd, padded to even length: (251139,)\n",
      "(251139,)\n",
      "Data array length was odd, padded to even length: (251139,)\n",
      "(300451,)\n",
      "Data array length was odd, padded to even length: (300451,)\n",
      "(300451,)\n",
      "Data array length was odd, padded to even length: (300451,)\n",
      "(300451,)\n",
      "Data array length was odd, padded to even length: (300451,)\n",
      "(84057,)\n",
      "Data array length was odd, padded to even length: (84057,)\n",
      "(84056,)\n",
      "(84057,)\n",
      "Data array length was odd, padded to even length: (84057,)\n",
      "(84056,)\n",
      "(84057,)\n",
      "Data array length was odd, padded to even length: (84057,)\n",
      "(84057,)\n",
      "Data array length was odd, padded to even length: (84057,)\n",
      "Computing CWT spectrograms with Morlet wavelets... (this may take a while)\n",
      "Generating spectrogram images...\n",
      "Creating Excel file with embedded images...\n",
      "Excel file with embedded images saved to: C:\\Users\\sinha\\Dropbox\\CPLab\\results\\baseline_power_analysis_with_spectrograms.xlsx\n",
      "Processing complete!\n",
      "Final dataframe shape: (141, 7)\n",
      "Excel file contains actual viewable spectrogram images!\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive backend\n",
    "import os\n",
    "import io\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.drawing.image import Image as OpenpyxlImage\n",
    "from PIL import Image\n",
    "import tempfile\n",
    "\n",
    "# Reload the power_functions module\n",
    "importlib.reload(power_functions)\n",
    "\n",
    "def compute_cwt_spectrogram(data, fs=2000, freqs=None, n_cycles_factor=3):\n",
    "    \"\"\"\n",
    "    Compute time-frequency representation using continuous wavelet transform with Morlet wavelets\n",
    "    \n",
    "    Parameters:\n",
    "    - data: 1D array of LFP data\n",
    "    - fs: sampling frequency (default: 1000 Hz)\n",
    "    - freqs: array of frequencies to analyze (default: 1-100 Hz in log scale)\n",
    "    - n_cycles_factor: factor for n_cycles = freqs/n_cycles_factor (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "    - dict containing frequencies, times, and time-frequency matrix\n",
    "    \"\"\"\n",
    "    if freqs is None:\n",
    "        # Create frequency array from 1 to 1000 Hz with more points at lower frequencies\n",
    "        freqs = np.arange(1, 100)\n",
    "    \n",
    "    # Calculate n_cycles for each frequency\n",
    "    n_cycles = freqs / n_cycles_factor\n",
    "    \n",
    "    # Create time vector\n",
    "    times = np.arange(len(data)) / fs\n",
    "    # Initialize output matrix\n",
    "    tfr = np.zeros((len(freqs), len(data)), dtype=complex)\n",
    "    \n",
    "    # Compute CWT for each frequency\n",
    "    for i, freq in enumerate(freqs):\n",
    "        # Create Morlet wavelet\n",
    "        # Morlet wavelet: complex exponential modulated by Gaussian\n",
    "        sigma_t = n_cycles[i] / (2 * np.pi * freq)  # Time domain standard deviation\n",
    "        \n",
    "        # Create wavelet in time domain\n",
    "        t_wavelet = np.arange(-4*sigma_t, 4*sigma_t, 1/fs)\n",
    "        if len(t_wavelet) % 2 == 0:\n",
    "            t_wavelet = t_wavelet[:-1]  # Make odd length\n",
    "            \n",
    "        # Morlet wavelet formula\n",
    "        morlet_wavelet = (1 / np.sqrt(np.pi * sigma_t)) * np.exp(1j * 2 * np.pi * freq * t_wavelet) * np.exp(-(t_wavelet**2) / (2 * sigma_t**2))\n",
    "        \n",
    "        # Convolve data with wavelet\n",
    "        convolution = np.convolve(data, morlet_wavelet, mode='same')\n",
    "        tfr[i, :] = convolution\n",
    "    \n",
    "    # Convert to power (magnitude squared)\n",
    "    power = np.abs(tfr) ** 2\n",
    "    \n",
    "    # Convert to dB scale\n",
    "    power_db = 10 * np.log10(power + 1e-12)  # Add small epsilon to avoid log(0)\n",
    "    \n",
    "    return {\n",
    "        'frequencies': freqs,\n",
    "        'times': times,\n",
    "        'spectrogram': power_db,\n",
    "        'power_spectral_density': power,\n",
    "        'complex_tfr': tfr,\n",
    "        'n_cycles': n_cycles\n",
    "    }\n",
    "\n",
    "def create_spectrogram_image(spectrogram_data, figsize=(6, 4), dpi=100):\n",
    "    \"\"\"\n",
    "    Create a matplotlib figure of the CWT spectrogram and return as image bytes\n",
    "    \n",
    "    Parameters:\n",
    "    - spectrogram_data: dict from compute_cwt_spectrogram()\n",
    "    - figsize: tuple for figure size\n",
    "    - dpi: resolution for the image\n",
    "    \n",
    "    Returns:\n",
    "    - bytes object containing PNG image data\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "    \n",
    "    # Create spectrogram plot\n",
    "    im = ax.pcolormesh(\n",
    "        spectrogram_data['times'], \n",
    "        spectrogram_data['frequencies'],\n",
    "        spectrogram_data['spectrogram'],\n",
    "        shading='gouraud',\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    \n",
    "    ax.set_ylabel('Frequency [Hz]')\n",
    "    ax.set_xlabel('Time [sec]')\n",
    "    ax.set_title('LFP CWT Spectrogram (Morlet Wavelets)')\n",
    "    # Log scale for better visualization of CWT\n",
    "    #ax.set_yscale('log')\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax)\n",
    "    cbar.set_label('Power [dB]')\n",
    "    \n",
    "    # Set frequency limits\n",
    "    ax.set_ylim([1, 100])  # Focus on 1-100 Hz (adjust as needed)\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save to bytes buffer\n",
    "    img_buffer = io.BytesIO()\n",
    "    plt.savefig(img_buffer, format='PNG', bbox_inches='tight', dpi=dpi)\n",
    "    img_buffer.seek(0)\n",
    "    img_bytes = img_buffer.getvalue()\n",
    "    \n",
    "    plt.close(fig)  # Important: close figure to free memory\n",
    "    \n",
    "    return img_bytes\n",
    "\n",
    "# Apply transformations to the dataframe\n",
    "baseline_lfp_all_df['welch'] = baseline_lfp_all_df['data_before'].apply(\n",
    "    lambda x: power_functions.apply_welch_transform(x)\n",
    ")\n",
    "\n",
    "baseline_lfp_all_df['welch_complete'] = baseline_lfp_all_df['complete_baseline_data'].apply(\n",
    "    lambda x: power_functions.apply_welch_transform(x)\n",
    ")\n",
    "\n",
    "# Add CWT spectrogram computation\n",
    "print(\"Computing CWT spectrograms with Morlet wavelets... (this may take a while)\")\n",
    "baseline_lfp_all_df['spectrogram'] = baseline_lfp_all_df['data_before'].apply(\n",
    "    lambda x: compute_cwt_spectrogram(x, fs=2000, n_cycles_factor=3)  # Adjust fs to your actual sampling rate\n",
    ")\n",
    "\n",
    "# Create spectrogram images\n",
    "print(\"Generating spectrogram images...\")\n",
    "baseline_lfp_all_df['spectrogram_image'] = baseline_lfp_all_df['spectrogram'].apply(\n",
    "    lambda x: create_spectrogram_image(x)\n",
    ")\n",
    "\n",
    "baseline_lfp_all_df['line_power'] = baseline_lfp_all_df['welch'].apply(\n",
    "    lambda x: power_functions.get_band_power(x, 58, 62)\n",
    ")\n",
    "\n",
    "baseline_lfp_all_df['total_power'] = baseline_lfp_all_df['welch'].apply(\n",
    "    lambda x: power_functions.get_band_power(x, 1, 100)\n",
    ")\n",
    "\n",
    "baseline_lfp_all_df['line_power_ratio'] = (\n",
    "    baseline_lfp_all_df['line_power'] / baseline_lfp_all_df['total_power']\n",
    ")\n",
    "\n",
    "baseline_lfp_all_df = baseline_lfp_all_df.sort_values(by=['mouse_id', 'task'])\n",
    "\n",
    "# Create final dataframe\n",
    "baseline_lfp_final = baseline_lfp_all_df[[\n",
    "    'base_name', 'mouse_id', 'task', 'channel_id', 'line_power_ratio', 'spectrogram', 'spectrogram_image'\n",
    "]]\n",
    "\n",
    "baseline_lfp_final = baseline_lfp_final.reset_index(drop=True)\n",
    "\n",
    "def save_excel_with_images(df, filepath):\n",
    "    \"\"\"\n",
    "    Save dataframe to Excel with embedded spectrogram images\n",
    "    \n",
    "    Parameters:\n",
    "    - df: dataframe containing spectrogram_image column with PNG bytes\n",
    "    - filepath: path to save Excel file\n",
    "    \"\"\"\n",
    "    print(\"Creating Excel file with embedded images...\")\n",
    "    \n",
    "    # Create workbook and worksheet\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = \"LFP_Analysis_with_Spectrograms\"\n",
    "    \n",
    "    # Define column headers (excluding image columns from data table)\n",
    "    data_columns = ['base_name', 'mouse_id', 'task', 'channel_id', 'line_power_ratio']\n",
    "    headers = data_columns + ['spectrogram_image']\n",
    "    \n",
    "    # Write headers\n",
    "    for col_idx, header in enumerate(headers, 1):\n",
    "        ws.cell(row=1, column=col_idx, value=header)\n",
    "    \n",
    "    # Set column widths\n",
    "    ws.column_dimensions['A'].width = 20  # base_name\n",
    "    ws.column_dimensions['B'].width = 12  # mouse_id\n",
    "    ws.column_dimensions['C'].width = 15  # task\n",
    "    ws.column_dimensions['D'].width = 12  # channel_id\n",
    "    ws.column_dimensions['E'].width = 18  # line_power_ratio\n",
    "    ws.column_dimensions['F'].width = 50  # spectrogram_image (wide for image)\n",
    "    \n",
    "    # Store temporary files to clean up later\n",
    "    temp_files = []\n",
    "    \n",
    "    try:\n",
    "        # Process each row\n",
    "        for idx, row in df.iterrows():\n",
    "            row_num = idx + 2  # Excel rows are 1-indexed, +1 for header\n",
    "            \n",
    "            # Write data columns\n",
    "            for col_idx, col_name in enumerate(data_columns, 1):\n",
    "                ws.cell(row=row_num, column=col_idx, value=row[col_name])\n",
    "            \n",
    "            # Add spectrogram image\n",
    "            try:\n",
    "                # Create temporary file for the image\n",
    "                tmp_file = tempfile.NamedTemporaryFile(suffix='.png', delete=False)\n",
    "                tmp_file.write(row['spectrogram_image'])\n",
    "                tmp_file.close()  # Close the file before using it\n",
    "                temp_files.append(tmp_file.name)\n",
    "                \n",
    "                # Add image to Excel\n",
    "                img = OpenpyxlImage(tmp_file.name)\n",
    "                img.width = 400  # Adjust size as needed\n",
    "                img.height = 300\n",
    "                \n",
    "                # Position image in the cell\n",
    "                cell_address = f'F{row_num}'\n",
    "                ws.add_image(img, cell_address)\n",
    "                \n",
    "                # Set row height to accommodate image\n",
    "                ws.row_dimensions[row_num].height = 225  # Adjust as needed\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not add image for row {idx}: {e}\")\n",
    "                ws.cell(row=row_num, column=6, value=\"Image generation failed\")\n",
    "        \n",
    "        # Save workbook\n",
    "        wb.save(filepath)\n",
    "        print(f\"Excel file with embedded images saved to: {filepath}\")\n",
    "        \n",
    "    finally:\n",
    "        # Clean up all temporary files\n",
    "        for temp_file in temp_files:\n",
    "            try:\n",
    "                if os.path.exists(temp_file):\n",
    "                    os.unlink(temp_file)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not delete temporary file {temp_file}: {e}\")\n",
    "\n",
    "# Alternative function to save images in separate sheet\n",
    "def save_excel_with_separate_image_sheet(df, filepath):\n",
    "    \"\"\"\n",
    "    Save dataframe to Excel with data in one sheet and images in another\n",
    "    \"\"\"\n",
    "    print(\"Creating Excel file with separate image sheet...\")\n",
    "    \n",
    "    # Create data sheet without images\n",
    "    df_for_excel = df[['base_name', 'mouse_id', 'task', 'channel_id', 'line_power_ratio']].copy()\n",
    "    \n",
    "    # Store temporary files to clean up later\n",
    "    temp_files = []\n",
    "    \n",
    "    try:\n",
    "        with pd.ExcelWriter(filepath, engine='openpyxl') as writer:\n",
    "            # Write main data\n",
    "            df_for_excel.to_excel(writer, sheet_name='LFP_Data', index=False)\n",
    "            \n",
    "            # Create images sheet\n",
    "            wb = writer.book\n",
    "            img_ws = wb.create_sheet('Spectrograms')\n",
    "            \n",
    "            # Headers for image sheet\n",
    "            img_ws.cell(row=1, column=1, value='Row_Index')\n",
    "            img_ws.cell(row=1, column=2, value='Identifier')\n",
    "            img_ws.cell(row=1, column=3, value='Spectrogram')\n",
    "            \n",
    "            # Add images\n",
    "            for idx, row in df.iterrows():\n",
    "                row_num = idx + 2\n",
    "                \n",
    "                # Add identifier\n",
    "                identifier = f\"{row['mouse_id']}_{row['task']}_ch{row['channel_id']}\"\n",
    "                img_ws.cell(row=row_num, column=1, value=idx)\n",
    "                img_ws.cell(row=row_num, column=2, value=identifier)\n",
    "                \n",
    "                # Add image\n",
    "                try:\n",
    "                    tmp_file = tempfile.NamedTemporaryFile(suffix='.png', delete=False)\n",
    "                    tmp_file.write(row['spectrogram_image'])\n",
    "                    tmp_file.close()\n",
    "                    temp_files.append(tmp_file.name)\n",
    "                    \n",
    "                    img = OpenpyxlImage(tmp_file.name)\n",
    "                    img.width = 400\n",
    "                    img.height = 300\n",
    "                    \n",
    "                    cell_address = f'C{row_num}'\n",
    "                    img_ws.add_image(img, cell_address)\n",
    "                    img_ws.row_dimensions[row_num].height = 225\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not add image for row {idx}: {e}\")\n",
    "                    img_ws.cell(row=row_num, column=3, value=\"Image generation failed\")\n",
    "            \n",
    "            # Adjust column widths\n",
    "            img_ws.column_dimensions['A'].width = 12\n",
    "            img_ws.column_dimensions['B'].width = 25\n",
    "            img_ws.column_dimensions['C'].width = 50\n",
    "\n",
    "        print(f\"Excel file with separate image sheet saved to: {filepath}\")\n",
    "        \n",
    "    finally:\n",
    "        # Clean up all temporary files\n",
    "        for temp_file in temp_files:\n",
    "            try:\n",
    "                if os.path.exists(temp_file):\n",
    "                    os.unlink(temp_file)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not delete temporary file {temp_file}: {e}\")\n",
    "\n",
    "# Save the Excel file with embedded images\n",
    "excel_filepath = os.path.join(savepath, 'baseline_power_analysis_with_spectrograms.xlsx')\n",
    "\n",
    "# Choose one of the following methods:\n",
    "\n",
    "# Method 1: Images embedded in the same sheet as data\n",
    "save_excel_with_images(baseline_lfp_final, excel_filepath)\n",
    "\n",
    "# Method 2: Images in a separate sheet (uncomment to use instead)\n",
    "# excel_filepath_separate = os.path.join(savepath, 'baseline_power_analysis_separate_images.xlsx')\n",
    "# save_excel_with_separate_image_sheet(baseline_lfp_final, excel_filepath_separate)\n",
    "\n",
    "print(\"Processing complete!\")\n",
    "print(f\"Final dataframe shape: {baseline_lfp_final.shape}\")\n",
    "print(\"Excel file contains actual viewable spectrogram images!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281c5dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(power_functions) #Reloading the power_functions module to ensure we have the latest version\n",
    "baseline_lfp_all_df['welch'] = baseline_lfp_all_df['data_before'].apply(lambda x: power_functions.apply_welch_transform(x))\n",
    "baseline_lfp_all_df['welch_complete']= baseline_lfp_all_df['complete_baseline_data'].apply(lambda x: power_functions.apply_welch_transform(x))\n",
    "baseline_lfp_all_df['line_power'] = baseline_lfp_all_df['welch'].apply(lambda x: power_functions.get_band_power(x, 58, 62))\n",
    "baseline_lfp_all_df['total_power'] = baseline_lfp_all_df['welch'].apply(lambda x: power_functions.get_band_power(x, 1, 100))\n",
    "baseline_lfp_all_df['line_power_ratio'] = baseline_lfp_all_df['line_power'] / baseline_lfp_all_df['total_power']\n",
    "baseline_lfp_all_df = baseline_lfp_all_df.sort_values(by=['mouse_id','task'])\n",
    "baseline_lfp_final = baseline_lfp_all_df[['base_name', 'mouse_id', 'task', 'channel_id', 'line_power_ratio']]\n",
    "#baseline_lfp_final=baseline_lfp_final.sort_values(by=['line_power_ratio'], ascending=False)\n",
    "baseline_lfp_final = baseline_lfp_final.reset_index(drop=True)\n",
    "#baseline_lfp_final.to_csv(os.path.join(savepath, 'baseline_power_ratio_noref.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6434456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_list= ['LFP1_AON', 'LFP2_AON', 'LFP3_AON', 'LFP4_AON', 'LFP1_vHp', 'LFP2_vHp']\n",
    "base_names = baseline_lfp_all_df['base_name'].unique()\n",
    "print(base_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2108a742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f15f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for base_namei in base_names:\n",
    "    fig, axs = plt.subplots(6, 1, figsize=(6, 10), sharex=True, sharey=True)\n",
    "    fig.suptitle(base_namei)\n",
    "    baseline_lfp_all_df_base = baseline_lfp_all_df[baseline_lfp_all_df['base_name'] == base_namei]\n",
    "    for axi,channel_id in enumerate(channel_list):\n",
    "        channel_data = baseline_lfp_all_df_base[baseline_lfp_all_df_base['channel_id'] == channel_id]\n",
    "        if channel_data.empty:\n",
    "            continue\n",
    "        mouse_id = channel_data['mouse_id'].values[0]\n",
    "        task = channel_data['task'].values[0]\n",
    "        welch_data = channel_data['welch'].values[0]\n",
    "        frequency = np.linspace(0, 1000, len(welch_data))\n",
    "        ax= axs[axi]\n",
    "        ax.plot(frequency, welch_data, label=channel_id)\n",
    "        ax.set_title(channel_id)\n",
    "        ax.set_xlim(0, 6)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    #fig.savefig(os.path.join(savepath, f'base_psd_{base_namei}.png'), dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3449a8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_lfp_all_df = baseline_lfp_all_df.sort_values(by=['mouse_id', 'task', 'base_name'])\n",
    "final_table = baseline_lfp_all_df.pivot(index=['base_name'], columns='channel_id', values='line_power_ratio')\n",
    "final_table = final_table.reindex(\n",
    "    baseline_lfp_all_df.drop_duplicates('base_name').sort_values(['mouse_id', 'task', 'base_name'])['base_name']\n",
    ")\n",
    "final_table = final_table[['LFP1_AON', 'LFP2_AON', 'LFP3_AON', 'LFP4_AON', 'LFP1_vHp', 'LFP2_vHp']]\n",
    "final_table_melted = final_table.reset_index().melt(id_vars='base_name', var_name='channel_id', value_name='line_power_ratio')\n",
    "final_table.to_csv(os.path.join(savepath, 'baseline_channel_selection.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdc1966",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cca11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_check=pd.read_excel(os.path.join(savepath, 'filecheck_8325.xlsx'))\n",
    "manual_check=manual_check[['base_name', 'LFP1AON', 'LFP2AON', 'LFP3AON', 'LFP4AON', 'LFP1vHC', 'LFP2vHC']]\n",
    "manual_check = manual_check.rename(columns={\n",
    "    'LFP1AON': 'LFP1_AON', \n",
    "    'LFP2AON': 'LFP2_AON', \n",
    "    'LFP3AON': 'LFP3_AON', \n",
    "    'LFP4AON': 'LFP4_AON', \n",
    "    'LFP1vHC': 'LFP1_vHp', \n",
    "    'LFP2vHC': 'LFP2_vHp'\n",
    "})\n",
    "manual_check = manual_check.set_index('base_name')\n",
    "manual_check.to_csv(os.path.join(savepath, 'manual_check_channel_selection.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813a1e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs=2000\n",
    "data_secs=11\n",
    "spike_start_sec=9\n",
    "spike_end_sec=10\n",
    "sine_wave = np.sin(2 * np.pi * 10 * np.arange(spike_start_sec, spike_end_sec, 1/fs)) + 1  # 10 Hz sine wave\n",
    "time= np.arange(0, data_secs, 1/fs)\n",
    "test_data = np.random.rand(fs*data_secs)\n",
    "test_data[spike_start_sec*fs:spike_end_sec*fs] = sine_wave  # Adding a spike in the first 2 seconds\n",
    "plt.plot(time, test_data)\n",
    "#plt.xlim(0,0)\n",
    "plt.ylim(-1,5)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "data_before, time, baseline_mean, baseline_std=lfp_pre_processing_functions.baseline_data_normalization(test_data, time, 10, fs)\n",
    "plt.plot(data_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4af1f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66cec672",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aca9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2624b3e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lfp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
