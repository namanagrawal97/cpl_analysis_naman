{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Importing all the required modules and the functions.py file that contains our custom made functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import getpass\n",
    "import glob\n",
    "\n",
    "import functions\n",
    "import spectrogram_plotting_functions\n",
    "import scipy.stats\n",
    "importlib.reload(functions) #loads our custom made functions.py file\n",
    "importlib.reload(spectrogram_plotting_functions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Loading the data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code fetches the current 'user' by using getpass. Then it sets the basepath, loads the files and specifies the savepath. Note that the basepath, files and savepath need to be changed depending on where you have kept the files and where you want the results to be stored. In this case, I have set it up to be in a particular folder in my Dropbox account, which is stored locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch the current user\n",
    "user= (getpass.getuser())\n",
    "print(\"Hello\", user)\n",
    "\n",
    "#Set the basepath, savepath and load the data files\n",
    "base='C:\\\\Users\\\\{}\\\\Dropbox\\\\CPLab'.format(user)\n",
    "files = glob.glob(base+'\\\\all_data_mat\\\\*.mat')\n",
    "savepath = base+'\\\\results\\\\'\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Extracting band data around events and saving it in a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  The keyboard dict maps the keyboard codes that were the annotation in .mat files to their meanings. '98':'b' is the beginning of a black trial,'119':'w' is the beginning of a white trial,'120':'nc' is the beginning of a no context trial,'49':'1' signifies a correct dig by the rat,'48':'0' signifies an incorrect dig by the rat.\n",
    "2. Then we initialize some empty dictionaries and dataframes to store data.\n",
    "3. Then we start looping through each of the data files.\n",
    "4. We first get the basename of the file using `os.path.basename(file)`\n",
    "5. We split the basename using `os.path.splitext(base_name)` , and use  `functions.exp_params` to get the date of the experiment, rat id and the task the rat was doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keyboard_dict={'98':'b','119':'w','120':'nc','49':'1','48':'0'} #specifying the map of keyboard annotations to their meanings.\n",
    "\n",
    "#files=[f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230615_dk6_BW_context_day1.mat'] #This is just for testing purposes\n",
    "\n",
    "#Initializing a few empty things to store data\n",
    "events_codes_all = {}\n",
    "compiled_data_all_epochs = pd.DataFrame()\n",
    "compiled_data_list=[]\n",
    "\n",
    "\n",
    "for file in files: #Looping through data files\n",
    "    \n",
    "    ## Get the date, mouse_id and task from the file name\n",
    "    base_name = os.path.basename(file)\n",
    "    base_name, _ = os.path.splitext(base_name)\n",
    "    date, mouse_id, task=functions.exp_params(base_name) #Using a custom made function [see functions.py]\n",
    "    print(date, mouse_id, task)\n",
    "    \n",
    "   \n",
    "    f=h5py.File(file, 'r')  ## Open the data file\n",
    "    channels = list(f.keys()) ## Extract channels list from the data file\n",
    "    print(channels)\n",
    "\n",
    "    events = functions.identify_events_channel(f, channels) #Extracting events from the data file\n",
    "\n",
    "    events_codes=np.array(events['codes'][0]) #saving the keyboard annotations of the events (door open, door close etc.)\n",
    "    events_times=np.array(events['times'][0]) #saving when the events happened\n",
    "    events_codes_all[base_name] = events_codes #saving the codes in a dictionary to be analyzed later for events other than the ones in our keyboard_dict map\n",
    "    \n",
    "    #Generating epochs from events (epochs are basically start of a trial and end of a trial)\n",
    "    epochs=functions.generate_epochs_with_first_event(events_codes, events_times)\n",
    "\n",
    "    # Experiment Start time\n",
    "    first_event=events_times[0]\n",
    "\n",
    "    for channeli in channels:\n",
    "        if \"AON\" in channeli or  \"vHp\" in channeli :\n",
    "            \n",
    "            channel_id=channeli\n",
    "            # Extracting raw data and time\n",
    "            data_all=f[channeli]\n",
    "            raw_data=np.array(data_all['values']).flatten()\n",
    "            raw_time = np.array(data_all['times']).flatten()\n",
    "            sampling_rate = int(1/data_all['interval'][0][0])\n",
    "            print(raw_data.shape, raw_time.shape, sampling_rate)\n",
    "            \n",
    "            # Normalizing the data by subtracting the mean and std of data 30sec before the first event\n",
    "            normalized_data,time,data_before=functions.data_normalization(raw_data,raw_time,first_event, sampling_rate)\n",
    "\n",
    "            # Applying a notch filter\n",
    "            notch_filtered_data=functions.iir_notch(normalized_data, sampling_rate,60)\n",
    "            total=notch_filtered_data\n",
    "            \n",
    "            # Extracting the bands\n",
    "            beta=functions.beta_band(notch_filtered_data, sampling_rate)\n",
    "            gamma=functions.gamma_band(notch_filtered_data, sampling_rate)\n",
    "            theta=functions.theta_band(notch_filtered_data, sampling_rate)\n",
    "\n",
    "            all_bands=[total,beta, gamma, theta]\n",
    "\n",
    "            for i,epochi in enumerate(epochs):\n",
    "                \n",
    "                compiled_data = pd.DataFrame() # Initializing a dataframe to store the data of a single epoch\n",
    "\n",
    "                door_timestamp = epochi[0][0]\n",
    "                trial_type = epochi[0][1]\n",
    "                dig_type = epochi[1, 1]\n",
    "                dig_timestamp = epochi[1, 0]\n",
    "                print(door_timestamp,trial_type,dig_timestamp,dig_type)\n",
    "                \n",
    "                for bandi in all_bands:\n",
    "                    \n",
    "                    data_complete_trial=functions.extract_complete_trial_data(bandi,time,door_timestamp,dig_timestamp,sampling_rate)\n",
    "                    data_trial_before, data_trial_after=functions.extract_door_data(bandi,time,door_timestamp,sampling_rate)\n",
    "                    data_dig_before, data_dig_after=functions.extract_dig_data(bandi,time,dig_timestamp,sampling_rate)\n",
    "\n",
    "                    # Create a DataFrame for the current bandi\n",
    "                    bandi_data = pd.DataFrame({\n",
    "                        'data_complete_trial': [data_complete_trial],\n",
    "                        'data_trial_before': [data_trial_before],\n",
    "                        'data_trial_after': [data_trial_after],\n",
    "                        'data_dig_before': [data_dig_before],\n",
    "                        'data_dig_after': [data_dig_after]\n",
    "                    })\n",
    "                    \n",
    "                    # Concatenate the current bandi DataFrame with the compiled_data DataFrame along axis=1\n",
    "                    compiled_data = pd.concat([compiled_data, bandi_data], axis=1)\n",
    "                \n",
    "                compiled_data.columns = ['total_complete_trial','total_pre_door', 'total_post_door', 'total_pre_odor', 'total_post_odor',\n",
    "                            'beta_complete_trial','beta_pre_door', 'beta_post_door', 'beta_pre_odor', 'beta_post_odor',\n",
    "                            'gamma_complete_trial','gamma_pre_door', 'gamma_post_door', 'gamma_pre_odor', 'gamma_post_odor',\n",
    "                            'theta_complete_trial','theta_pre_door', 'theta_post_door', 'theta_pre_odor', 'theta_post_odor']\n",
    "                compiled_data.insert(0, 'rat', mouse_id)\n",
    "                compiled_data.insert(1, 'date', date)\n",
    "                compiled_data.insert(2, 'experiment', task)\n",
    "                compiled_data.insert(3, 'channel', channel_id)\n",
    "                compiled_data.insert(4, 'trial', i)\n",
    "                compiled_data.insert(5, 'timestamps', [[door_timestamp, dig_timestamp]])\n",
    "\n",
    "                compiled_data.insert(6, 'side', keyboard_dict[str(int(trial_type))])\n",
    "                compiled_data.insert(7, 'correct?', keyboard_dict[str(int(dig_type))])\n",
    "                compiled_data.insert(8, 'first 30 seconds power', functions.calculate_power_1D(data_before))\n",
    "                compiled_data.insert(9, 'time', [time])\n",
    "                compiled_data_list.append(compiled_data)\n",
    "\n",
    "\n",
    "compiled_data_all_epochs=[]\n",
    "compiled_data_all_epochs.extend(compiled_data_list)\n",
    "# Flatten the list of lists\n",
    "compiled_data_all_epochs = pd.concat(compiled_data_all_epochs, axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the variable size is really big, we can get an idea of how much RAM space is occupied by this variable.\n",
    "import sys\n",
    "sys.getsizeof(compiled_data_all_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going through the tasks, and renaming the tasks if they contain day2 and os2 suffixes\n",
    "task_list=np.unique(compiled_data_all_epochs['experiment'])\n",
    "print(task_list)\n",
    "compiled_data_all_epochs['experiment'] = compiled_data_all_epochs['experiment'].apply(functions.clean_task)\n",
    "task_list=np.unique(compiled_data_all_epochs['experiment'])\n",
    "print(task_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_data_all_epochs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat_list=list(np.unique(compiled_data_all_epochs['rat']))\n",
    "window=[-2,2] #Set the window for the the waveform\n",
    "\n",
    "all_bands=['total','beta','gamma','theta']\n",
    "band='total' #insert the band of interest\n",
    "\n",
    "#rat_list=['dk1']\n",
    "for rati in rat_list:\n",
    "    rat_dict={}\n",
    "    rat_data=compiled_data_all_epochs[compiled_data_all_epochs['rat']==rati]\n",
    "    rat_data=rat_data.reset_index(drop=True)\n",
    "    fig, axs=plt.subplots(2,2,figsize=(20,10))\n",
    "    fig.suptitle(f'{rati} band {band} {window[0]}s to {window[1]}s')\n",
    "    axs=axs.flatten()\n",
    "    tasks=list(np.unique(rat_data['experiment']))\n",
    "    print(tasks)\n",
    "    for taski in tasks:\n",
    "        rat_dict['experiment']=taski\n",
    "        task_data=rat_data[rat_data['experiment']==taski]\n",
    "        data_channels=np.unique(task_data['channel'])\n",
    "        \n",
    "        aon_channels=[channel for channel in data_channels if 'AON' in channel]\n",
    "        vhp_channels=[channel for channel in data_channels if 'vHp' in channel]\n",
    "        aon_data_door=[]\n",
    "        vhp_data_door=[]\n",
    "        aon_data_dig=[]\n",
    "        vhp_data_dig=[]\n",
    "        print(aon_channels)\n",
    "        print(vhp_channels)\n",
    "        if aon_channels:\n",
    "            for aon_channeli in aon_channels:\n",
    "                channel_data=task_data[task_data['channel']==aon_channeli]\n",
    "                #print(taski, channeli,channel_data.shape)\n",
    "                door_data=np.array(np.append(channel_data[band+'_pre_door'],channel_data[band+'_post_door'],axis=0))\n",
    "                dig_data=np.array(np.append(channel_data[band+'_pre_odor'],channel_data[band+'_post_odor'],axis=0))\n",
    "                aon_data_door=np.append(aon_data_door,door_data)\n",
    "                aon_data_dig=np.append(aon_data_dig,dig_data)\n",
    "        if vhp_channels:\n",
    "            for vhp_channeli in vhp_channels:\n",
    "                channel_data=task_data[task_data['channel']==vhp_channeli]\n",
    "                #print(taski, channeli,channel_data.shape)\n",
    "                door_data=np.array(np.append(channel_data[band+'_pre_door'],channel_data[band+'_post_door'],axis=0))\n",
    "                dig_data=np.array(np.append(channel_data[band+'_pre_odor'],channel_data[band+'_post_odor'],axis=0))\n",
    "                vhp_data_door=np.append(vhp_data_door,door_data)\n",
    "                vhp_data_dig=np.append(vhp_data_dig,dig_data)\n",
    "        aon_data_dig=np.array(aon_data_dig)\n",
    "        aon_data_door=np.array(aon_data_door)\n",
    "        vhp_data_dig=np.array(vhp_data_dig)\n",
    "        vhp_data_door=np.array(vhp_data_door)\n",
    "        \n",
    "        data_dict={'AON LFP door':aon_data_door,'AON LFP dig':aon_data_dig,'vHp LFP door':vhp_data_door,'vHp LFP dig':vhp_data_dig}\n",
    "        for i,datai in enumerate(list(data_dict.keys())):\n",
    "            ax=axs[i]\n",
    "            ax.set_title(datai)\n",
    "            data=data_dict[datai]\n",
    "            if data.size==0:\n",
    "                continue\n",
    "            data_mean=np.mean(data, axis=0)\n",
    "            data_sem=scipy.stats.sem(data, axis=0)\n",
    "            \n",
    "            l=int(len(data_mean)/2+int(window[0]*2000))\n",
    "            r=int(len(data_mean)/2+int(window[1]*2000))\n",
    "            print(l,r)\n",
    "            #print([4000-2000*int(abs(window[0])),4000+2000*int(abs(window[1]))])\n",
    "            #print(len(data_mean))\n",
    "            data_mean_windowed=data_mean[l:r]\n",
    "            data_sem_windowed = data_sem[l:r]\n",
    "\n",
    "            print(len(data_mean_windowed))\n",
    "            \n",
    "            time_axis = np.linspace(window[0], window[1], len(data_mean_windowed))\n",
    "            ax.plot(time_axis, data_mean_windowed, label=taski)\n",
    "            ax.fill_between(time_axis, data_mean_windowed - data_sem_windowed, data_mean_windowed + data_sem_windowed, alpha=0.2)\n",
    "            \n",
    "            ax.legend()\n",
    "            ax.set_xlabel('Time(s)')            \n",
    "    #fig.savefig(os.path.join(savepath,f'waveform {rati} {band}'),bbox_inches='tight',dpi=300)\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels=np.unique(waveform_df['channel'])\n",
    "print(channels)\n",
    "bands=['total_pre_door', 'total_post_door', 'total_pre_odor', 'total_post_odor',\n",
    "                            'beta_pre_door', 'beta_post_door', 'beta_pre_odor', 'beta_post_odor',\n",
    "                            'gamma_pre_door', 'gamma_post_door', 'gamma_pre_odor', 'gamma_post_odor',\n",
    "                            'theta_pre_door', 'theta_post_door', 'theta_pre_odor', 'theta_post_odor']\n",
    "for channel in channels:\n",
    "    channel_df=waveform_df[waveform_df['channel']==channel]\n",
    "    for bandi in bands:\n",
    "\n",
    "        data=np.stack(channel_df[bandi].values)\n",
    "        data_mean = np.mean(data, axis=0)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        \n",
    "        # Plot individual rows as faint lines\n",
    "        for row in data:\n",
    "            plt.plot(row, color='gray', alpha=0.3)\n",
    "        \n",
    "        # Plot the mean as a prominent line\n",
    "        plt.plot(data_mean, color='blue', linewidth=2)\n",
    "        \n",
    "        plt.title(f'{bandi} - Channel {channel}')\n",
    "        plt.xlabel('Sample Index')\n",
    "        plt.ylabel('Mean Value')\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "channels = np.unique(waveform_df['channel'])\n",
    "print(channels)\n",
    "bands = ['total_pre_door', 'total_post_door', 'total_pre_odor', 'total_post_odor',\n",
    "         'beta_pre_door', 'beta_post_door', 'beta_pre_odor', 'beta_post_odor',\n",
    "         'gamma_pre_door', 'gamma_post_door', 'gamma_pre_odor', 'gamma_post_odor',\n",
    "         'theta_pre_door', 'theta_post_door', 'theta_pre_odor', 'theta_post_odor']\n",
    "\n",
    "for bandi in bands:\n",
    "    band_df = waveform_df[[bandi]].copy()  # Extract the band column and make a copy\n",
    "    band_df = pd.concat([band_df, waveform_df[['channel']]], axis=1)  # Concatenate along columns\n",
    "    fig, axs=plt.subplots(len(channels),1, sharex=True)\n",
    "    fig.suptitle(bandi)\n",
    "    axs=axs.flatten()\n",
    "    for i,channeli in enumerate(channels):\n",
    "        channel_df=band_df[band_df['channel']==channeli]\n",
    "        \n",
    "        data=np.stack(channel_df[bandi].values)\n",
    "        data_mean = np.mean(data, axis=0)\n",
    "        ax=axs[i]\n",
    "        # Plot individual rows as faint lines\n",
    "        # for row in data:\n",
    "        #     ax.plot(row, color='gray', alpha=0.3)\n",
    "        \n",
    "        # Plot the mean as a prominent line\n",
    "        ax.plot(data_mean, color='blue', linewidth=2)\n",
    "        ax.grid(True)\n",
    "        ax.set_xlabel('Time (ms)')\n",
    "        ax.set_ylabel('Signal Amp (uV)')\n",
    "        ax.set_title(f' Channel {channeli}')\n",
    "\n",
    "    plt.tight_layout()  # Adjust the vertical space between subplots\n",
    "    plt.show()\n",
    "        #continue  # Placeholder for further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will try to make the power spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import welch, spectrogram\n",
    "\n",
    "test=waveform_df['total_post_door'].iloc[0]\n",
    "print(test)\n",
    "f, Pxx = welch(test, fs=2000, nperseg=250, noverlap=100)\n",
    "f = f[np.where(f<=100)] #select only frequencies below 100Hz\n",
    "Pxx = Pxx[np.where(f<=100)]\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(f, Pxx)\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Power Spectral Density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import welch\n",
    "import numpy as np\n",
    "\n",
    "total_events = ['total_pre_door', 'total_post_door', 'total_pre_odor', 'total_post_odor']\n",
    "spectrum_dict={}\n",
    "for event in total_events:\n",
    "    aon_context=[]\n",
    "    aon_nocontext=[]\n",
    "    vhp_context=[]\n",
    "    vhp_nocontext=[]\n",
    "    for i in range(len(waveform_df)):\n",
    "        test = waveform_df[event].iloc[i]\n",
    "        f, Pxx = welch(test, fs=2000, nperseg=400, noverlap=None)\n",
    "        f = f[np.where(f <= 100)]  # select only frequencies below 100Hz\n",
    "        Pxx = Pxx[np.where(f <= 100)]\n",
    "        if (waveform_df['experiment'].iloc[i] == 'BWcontext') and ('AON' in waveform_df['channel'].iloc[i]):\n",
    "            aon_context.append(Pxx)\n",
    "        elif (waveform_df['experiment'].iloc[i] == 'BWnocontext') and ('AON' in waveform_df['channel'].iloc[i]):\n",
    "            aon_nocontext.append(Pxx)\n",
    "        elif (waveform_df['experiment'].iloc[i] == 'BWcontext') and ('vHp' in waveform_df['channel'].iloc[i]):\n",
    "            vhp_context.append(Pxx)\n",
    "        elif (waveform_df['experiment'].iloc[i] == 'BWnocontext') and ('vHp' in waveform_df['channel'].iloc[i]):\n",
    "            vhp_nocontext.append(Pxx)\n",
    "        # Convert lists to numpy arrays\n",
    "    aon_context = np.array(aon_context)\n",
    "    aon_nocontext = np.array(aon_nocontext)\n",
    "    vhp_context = np.array(vhp_context)\n",
    "    vhp_nocontext = np.array(vhp_nocontext)\n",
    "    \n",
    "    spectrum_dict[event] = {'aon_context': aon_context, 'aon_nocontext': aon_nocontext,\n",
    "                            'vhp_context': vhp_context, 'vhp_nocontext': vhp_nocontext}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming aon_context is already defined and is a 2D numpy array\n",
    "# where rows are different samples and columns are the frequency bins\n",
    "\n",
    "# Calculate the mean and standard error\n",
    "mean_aon_context = np.mean(aon_context, axis=0)\n",
    "stderr_aon_context = np.std(aon_context, axis=0) / np.sqrt(aon_context.shape[0])\n",
    "mean_vhp_context = np.mean(vhp_context, axis=0)\n",
    "stderr_vhp_context = np.std(vhp_context, axis=0) / np.sqrt(vhp_context.shape[0])\n",
    "# Plot the mean line\n",
    "plt.plot(f,mean_aon_context, label='Mean AON Context')\n",
    "plt.plot(f,mean_vhp_context, label='Mean vHp Context')\n",
    "# Plot the standard error shades\n",
    "plt.fill_between(f,\n",
    "                 mean_aon_context - stderr_aon_context, \n",
    "                 mean_aon_context + stderr_aon_context, \n",
    "                 color='b', alpha=0.2, label='Standard Error')\n",
    "plt.fill_between(f,\n",
    "                    mean_vhp_context - stderr_vhp_context,\n",
    "                    mean_vhp_context + stderr_vhp_context,\n",
    "                    color='r', alpha=0.2, label='Standard Error')\n",
    "# Add labels and title\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Power Spectral Density')\n",
    "plt.title('AON and vHp Context Power Spectral Density')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bandi in bands:\n",
    "    band_df = waveform_df[[bandi]].copy()  # Extract the band column and make a copy\n",
    "    band_df = pd.concat([band_df, waveform_df[['channel']]], axis=1)  # Concatenate along columns\n",
    "    fig, axs=plt.subplots(len(channels),1, sharex=True)\n",
    "    fig.suptitle(bandi)\n",
    "    axs=axs.flatten()\n",
    "    for i,channeli in enumerate(channels):\n",
    "        channel_df=band_df[band_df['channel']==channeli]\n",
    "        \n",
    "        data=np.stack(channel_df[bandi].values)\n",
    "        data_mean = np.mean(data, axis=0)\n",
    "        ax=axs[i]\n",
    "        # Plot individual rows as faint lines\n",
    "        # for row in data:\n",
    "        #     ax.plot(row, color='gray', alpha=0.3)\n",
    "        \n",
    "        # Plot the mean as a prominent line\n",
    "        ax.plot(data_mean, color='blue', linewidth=2)\n",
    "        ax.grid(True)\n",
    "        ax.set_xlabel('Time (ms)')\n",
    "        ax.set_ylabel('Signal Amp (uV)')\n",
    "        ax.set_title(f' Channel {channeli}')\n",
    "\n",
    "    plt.tight_layout()  # Adjust the vertical space between subplots\n",
    "    plt.show()\n",
    "        #continue  # Placeholder for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "channels = np.unique(waveform_df['channel'])\n",
    "print(channels)\n",
    "\n",
    "for channel in channels:\n",
    "    channel_df = waveform_df[waveform_df['channel'] == channel]\n",
    "    \n",
    "    # Convert string representations of arrays to actual numpy arrays\n",
    "    def convert_to_array(x):\n",
    "        if isinstance(x, str):\n",
    "            # Remove any non-numeric characters except for commas, dots, and minus signs\n",
    "            cleaned_str = re.sub(r'[^\\d.,-]', '', x)\n",
    "            # Convert the cleaned string to a numpy array\n",
    "            return np.fromstring(cleaned_str, sep=',')\n",
    "        else:\n",
    "            return np.array(x)\n",
    "    \n",
    "    beta_pre_door_data = channel_df['beta_pre_door'].apply(convert_to_array)\n",
    "    \n",
    "    # Stack the values into a numpy array\n",
    "    beta_pre_door_data = np.stack(beta_pre_door_data.values)\n",
    "    \n",
    "    # Calculate the mean along the specified axis\n",
    "    beta_pre_door_mean = np.mean(beta_pre_door_data, axis=0)\n",
    "    \n",
    "    print(f\"Mean of 'beta_pre_door' for channel {channel}: {beta_pre_door_mean}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lfp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
