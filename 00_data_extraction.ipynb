{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'spectrogram_plotting_functions' from 'c:\\\\Users\\\\Thomas\\\\Desktop\\\\cpl_analysis_naman\\\\spectrogram_plotting_functions.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import functions\n",
    "import spectrogram_plotting_functions\n",
    "import scipy.stats\n",
    "importlib.reload(functions)\n",
    "importlib.reload(spectrogram_plotting_functions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Thomas\n",
      "['C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230529_dk1_nocontext.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230529_dk3_nocontext.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230529_dk5_nocontext.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230529_dk6_nocontext.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230531_dk1_nocontext_day2.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230531_dk3_nocontext_day2.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230531_dk5_nocontext_day2.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230531_dk6_nocontext_day2.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230609_dk1_BW_nocontext_day1.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230609_dk3_BW_nocontext_day1.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230610_dk1_BW_nocontext_day2.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230610_dk3_BW_nocontext_day2.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230615_dk5_BW_context_day1.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230615_dk6_BW_context_day1.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230616_dk5_BW_context_day2.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230616_dk6_BW_context_day2.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230623_dk1_BW_context_day1.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230626_dk1_BW_context_day1.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230626_dk5_BW_nocontext_day1.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230626_dk6_BW_nocontext_day1.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230627_dk1_BW_context_day2.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230627_dk5_BW_nocontext_day2.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230628_dk6_BW_nocontext_day2.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230718_dk1_nocontext_os2.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230718_dk5_nocontext_os2.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230718_dk6_nocontext_os2.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230719_dk1_nocontext_os2_day2_part1.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230719_dk1_nocontext_os2_day2_part2.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230719_dk5_nocontext_os2_day2.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230719_dk6_nocontext_os2_day2.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230807_dk3_BW_context_day1.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230808_dk3_BW_context_day2.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230808_dk5_BW_nocontext_day1_os2.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230810_dk5_BW_nocontext_day2_os2.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230817_dk1_BW_context_os2_day1_pt1.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230817_dk1_BW_context_os2_day1_pt2.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230818_dk1_BW_context_os2_day2.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230818_dk3_BW_context_os2_day1.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230821_dk3_BW_context_os2_day2.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230821_dk5_BW_context_day1_os2.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230822_dk1_BW_nocontext_os2_day1.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230823_dk1_BW_nocontext_os2_day2.mat', 'C:\\\\Users\\\\Thomas\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230823_dk5_BW_context_day2_os2.mat']\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "user= (getpass.getuser())\n",
    "print(\"Hello\", user)\n",
    "\n",
    "import glob\n",
    "base='C:\\\\Users\\\\{}\\\\Dropbox\\\\CPLab'.format(user)\n",
    "files = glob.glob(base+'\\\\all_data_mat_filtered\\\\*.mat')\n",
    "savepath = base+'\\\\results\\\\'\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keyboard_dict={'98':'b','119':'w','120':'nc','49':'1','48':'0'}\n",
    "#files=[f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230615_dk6_BW_context_day1.mat'] #This is just for testing purposes\n",
    "events_codes_all = {}\n",
    "compiled_data_all_epochs = pd.DataFrame()\n",
    "\n",
    "for file in files:\n",
    "    # Initialize an empty DataFrame to store the results for all epochs\n",
    "    ## Get the date, mouse_id and task from the file name\n",
    "    base_name = os.path.basename(file)\n",
    "    base_name, _ = os.path.splitext(base_name)\n",
    "\n",
    "    date, mouse_id, task=functions.exp_params(base_name)\n",
    "    print(date, mouse_id, task)\n",
    "    ## Open the file\n",
    "    f=h5py.File(file, 'r')\n",
    "    channels = list(f.keys())\n",
    "    print(channels)\n",
    "\n",
    "    if 'Keyboard' in channels:\n",
    "        events = f['Keyboard']\n",
    "    elif 'keyboard' in channels:\n",
    "        events = f['keyboard']\n",
    "    elif 'memory' in channels:\n",
    "        events = f['memory']\n",
    "    elif 'Memory' in channels:\n",
    "        events = f['Memory']\n",
    "\n",
    "    events_codes=np.array(events['codes'][0])\n",
    "    events_times=np.array(events['times'][0])\n",
    "    events_codes_all[base_name] = events_codes\n",
    "    #Generating epochs from events (epochs are basically start of a trial and end of a trial)\n",
    "    epochs=functions.generate_epochs_with_first_event(events_codes, events_times)\n",
    "\n",
    "    # Experiment Start time\n",
    "    first_event=events_times[0]\n",
    "\n",
    "    for channeli in channels:\n",
    "        if \"AON\" in channeli or  \"vHp\" in channeli :\n",
    "            \n",
    "            channel_id=channeli\n",
    "            # Extracting raw data and time\n",
    "            data_all=f[channeli]\n",
    "            raw_data=np.array(data_all['values']).flatten()\n",
    "            raw_time = np.array(data_all['times']).flatten()\n",
    "            sampling_rate = int(1/data_all['interval'][0][0])\n",
    "            print(raw_data.shape, raw_time.shape, sampling_rate)\n",
    "            # Normalizing the data by subtracting the mean and std of data 30sec before the first event\n",
    "            normalized_data,time,data_before=functions.data_normalization(raw_data,raw_time,first_event, sampling_rate)\n",
    "\n",
    "            # Applying a notch filter\n",
    "            notch_filtered_data=functions.iir_notch(normalized_data, sampling_rate,60)\n",
    "            total=notch_filtered_data\n",
    "            # Extracting the bands\n",
    "            beta=functions.beta_band(notch_filtered_data, sampling_rate)\n",
    "            gamma=functions.gamma_band(notch_filtered_data, sampling_rate)\n",
    "            theta=functions.theta_band(notch_filtered_data, sampling_rate)\n",
    "\n",
    "            all_bands=[total,beta, gamma, theta]\n",
    "\n",
    "            for i,epochi in enumerate(epochs):\n",
    "                compiled_data = pd.DataFrame()\n",
    "\n",
    "                door_timestamp = epochi[0][0]\n",
    "                trial_type = epochi[0][1]\n",
    "                dig_type = epochi[1, 1]\n",
    "                print(dig_type)\n",
    "                dig_timestamp = epochi[1, 0]\n",
    "                print(door_timestamp,trial_type,dig_timestamp,dig_type)\n",
    "                for bandi in all_bands:\n",
    "                    data_complete_trial=functions.extract_complete_trial_data(bandi,time,door_timestamp,dig_timestamp,sampling_rate)\n",
    "                        # Create a DataFrame for the current bandi\n",
    "                    bandi_data = pd.DataFrame({\n",
    "                        'data_complete_trial': [data_complete_trial]\n",
    "\n",
    "                    })\n",
    "                    \n",
    "                    # Concatenate the current bandi DataFrame with the compiled_data DataFrame along axis=1\n",
    "                    compiled_data = pd.concat([compiled_data, bandi_data], axis=1)\n",
    "                compiled_data.columns = ['total_complete_trial',\n",
    "                            'beta_complete_trial',\n",
    "                            'gamma_complete_trial',\n",
    "                            'theta_complete_trial']\n",
    "                compiled_data.insert(0, 'rat', mouse_id)\n",
    "                compiled_data.insert(1, 'date', date)\n",
    "                compiled_data.insert(2, 'experiment', task)\n",
    "                compiled_data.insert(3, 'channel', channel_id)\n",
    "                compiled_data.insert(4, 'trial', i)\n",
    "                compiled_data.insert(5, 'timestamps', [[door_timestamp, dig_timestamp]])\n",
    "\n",
    "                compiled_data.insert(6, 'side', keyboard_dict[str(int(trial_type))])\n",
    "                compiled_data.insert(7, 'correct?', keyboard_dict[str(int(dig_type))])\n",
    "                compiled_data.insert(8, 'first 30 seconds power', functions.calculate_power_1D(data_before))\n",
    "                compiled_data.insert(9, 'time', [time])\n",
    "                \n",
    "                compiled_data_all_epochs = pd.concat([compiled_data_all_epochs, compiled_data], axis=0, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "keyboard_dict = {'98': 'b', '119': 'w', '120': 'nc', '49': '1', '48': '0'}\n",
    "# files = [f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230615_dk6_BW_context_day1.mat'] # This is just for testing purposes\n",
    "events_codes_all = {}\n",
    "compiled_data_all_epochs = []\n",
    "\n",
    "def process_file(file):\n",
    "    compiled_data_list = []\n",
    "    \n",
    "    # Get the date, mouse_id and task from the file name\n",
    "    base_name = os.path.basename(file)\n",
    "    base_name, _ = os.path.splitext(base_name)\n",
    "    date, mouse_id, task = functions.exp_params(base_name)\n",
    "    print(date, mouse_id, task)\n",
    "    \n",
    "    # Open the file\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        channels = list(f.keys())\n",
    "        print(channels)\n",
    "\n",
    "        if 'Keyboard' in channels:\n",
    "            events = f['Keyboard']\n",
    "        elif 'keyboard' in channels:\n",
    "            events = f['keyboard']\n",
    "        elif 'memory' in channels:\n",
    "            events = f['memory']\n",
    "        elif 'Memory' in channels:\n",
    "            events = f['Memory']\n",
    "\n",
    "        events_codes = np.array(events['codes'][0])\n",
    "        events_times = np.array(events['times'][0])\n",
    "        events_codes_all[base_name] = events_codes\n",
    "        \n",
    "        # Generating epochs from events (epochs are basically start of a trial and end of a trial)\n",
    "        epochs = functions.generate_epochs_with_first_event(events_codes, events_times)\n",
    "        \n",
    "        # Experiment Start time\n",
    "        first_event = events_times[0]\n",
    "\n",
    "        for channeli in channels:\n",
    "            if \"AON\" in channeli or \"vHp\" in channeli:\n",
    "                channel_id = channeli\n",
    "                # Extracting raw data and time\n",
    "                data_all = f[channeli]\n",
    "                raw_data = np.array(data_all['values']).flatten()\n",
    "                raw_time = np.array(data_all['times']).flatten()\n",
    "                sampling_rate = int(1 / data_all['interval'][0][0])\n",
    "                print(raw_data.shape, raw_time.shape, sampling_rate)\n",
    "                \n",
    "                # Normalizing the data by subtracting the mean and std of data 30sec before the first event\n",
    "                normalized_data, time, data_before = functions.data_normalization(raw_data, raw_time, first_event, sampling_rate)\n",
    "                \n",
    "                # Applying a notch filter\n",
    "                notch_filtered_data = functions.iir_notch(normalized_data, sampling_rate, 60)\n",
    "                total = notch_filtered_data\n",
    "                \n",
    "                # Extracting the bands\n",
    "                beta = functions.beta_band(notch_filtered_data, sampling_rate)\n",
    "                gamma = functions.gamma_band(notch_filtered_data, sampling_rate)\n",
    "                theta = functions.theta_band(notch_filtered_data, sampling_rate)\n",
    "                \n",
    "                all_bands = [total, beta, gamma, theta]\n",
    "\n",
    "                for i, epochi in enumerate(epochs):\n",
    "                    door_timestamp = epochi[0][0]\n",
    "                    trial_type = epochi[0][1]\n",
    "                    dig_type = epochi[1, 1]\n",
    "                    dig_timestamp = epochi[1, 0]\n",
    "                    print(dig_type)\n",
    "                    print(door_timestamp, trial_type, dig_timestamp, dig_type)\n",
    "                    \n",
    "                    band_data_dict = {}\n",
    "                    for bandi, band_name in zip(all_bands, ['total', 'beta', 'gamma', 'theta']):\n",
    "                        data_complete_trial = functions.extract_complete_trial_data(bandi, time, door_timestamp, dig_timestamp, sampling_rate)\n",
    "                        band_data_dict[f'{band_name}_complete_trial'] = [data_complete_trial]\n",
    "                    \n",
    "                    compiled_data = pd.DataFrame(band_data_dict)\n",
    "                    compiled_data.insert(0, 'rat', mouse_id)\n",
    "                    compiled_data.insert(1, 'date', date)\n",
    "                    compiled_data.insert(2, 'experiment', task)\n",
    "                    compiled_data.insert(3, 'channel', channel_id)\n",
    "                    compiled_data.insert(4, 'trial', i)\n",
    "\n",
    "                    # Assuming 'door_timestamp' and 'dig_timestamp' are lists or arrays of timestamps\n",
    "                    door_timestamp_array = np.array(door_timestamp, dtype=float)\n",
    "                    dig_timestamp_array = np.array(dig_timestamp, dtype=float)\n",
    "\n",
    "                    formatted_door_timestamp = np.round(door_timestamp_array, 2)\n",
    "                    formatted_dig_timestamp = np.round(dig_timestamp_array, 2)\n",
    "                    compiled_data.insert(5, 'timestamps', [[formatted_door_timestamp.tolist(), formatted_dig_timestamp.tolist()]])\n",
    "                    #compiled_data.insert(5, 'timestamps', [[door_timestamp, dig_timestamp]])\n",
    "                    compiled_data.insert(6, 'side', keyboard_dict[str(int(trial_type))])\n",
    "                    compiled_data.insert(7, 'correct?', keyboard_dict[str(int(dig_type))])\n",
    "                    compiled_data.insert(8, 'first 30 seconds power', functions.calculate_power_1D(data_before))\n",
    "                    # Assuming 'time' is a list or array of time values\n",
    "                    time_array = np.array(time, dtype=float)\n",
    "                    formatted_time = np.round(time_array, 8)\n",
    "\n",
    "                    # Insert the formatted 'time' values into the DataFrame\n",
    "                    compiled_data.insert(9, 'time', [formatted_time])\n",
    "                    #compiled_data.insert(9, 'time', [time])\n",
    "\n",
    "                    compiled_data_list.append(compiled_data)\n",
    "    \n",
    "    return compiled_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files=[f'C:\\\\Users\\\\{user}\\\\Dropbox\\\\CPLab\\\\all_data_mat_filtered\\\\20230615_dk6_BW_context_day1.mat'] #This is just for testing purposes\n",
    "\n",
    "# Process files sequentially\n",
    "for file in files:\n",
    "    compiled_data_list = process_file(file)\n",
    "    compiled_data_all_epochs.extend(compiled_data_list)\n",
    "# Flatten the list of lists\n",
    "\n",
    "compiled_data_all_epochs = pd.concat(compiled_data_all_epochs, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compiled_data_all_epochs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_data_all_epochs.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for large integers in a DataFrame with array elements\n",
    "def check_large_integers_in_arrays(df):\n",
    "    large_int_columns = []\n",
    "    max_int64 = np.iinfo(np.int64).max\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].apply(lambda x: isinstance(x, (list, np.ndarray))).all():\n",
    "            # Check each array element for large integers\n",
    "            if df[col].apply(lambda arr: any(isinstance(i, int) and i > max_int64 for i in arr)).any():\n",
    "                large_int_columns.append(col)\n",
    "    \n",
    "    return large_int_columns\n",
    "\n",
    "# Check for large integers\n",
    "large_int_columns = check_large_integers_in_arrays(compiled_data_all_epochs)\n",
    "print(\"Columns with large integers:\", large_int_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert large integers to float64\n",
    "compiled_data_all_epochs = compiled_data_all_epochs.applymap(\n",
    "    lambda x: np.float64(x) if isinstance(x, int) and x > np.iinfo(np.int64).max else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame with compression and chunking\n",
    "filename = 'compiled_data_all_rats_all_epoch.h5'\n",
    "compiled_data_all_epochs.to_hdf(os.path.join(savepath,filename), key='df', mode='w', complevel=9, complib='blosc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_data_all_epochs.to_parquet(os.path.join(savepath, 'compiled_data_all_rats_all_epoch.parquet'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gc\n",
    "\n",
    "# Free up memory\n",
    "del compiled_data_all_epochs, compiled_data_list, events_codes_all\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.maxsize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.06871715099940695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming compiled_data_all_epochs is a pandas DataFrame\n",
    "in_memory_size = sys.getsizeof(compiled_data_all_epochs)\n",
    "print(f\"In-memory size: {in_memory_size / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "# Rough estimate of compressed size (assuming a compression ratio of 2:1)\n",
    "compression_ratio = 9\n",
    "estimated_compressed_size = in_memory_size / compression_ratio\n",
    "print(f\"Estimated compressed size: {estimated_compressed_size / (1024 ** 3):.2f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lfp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
